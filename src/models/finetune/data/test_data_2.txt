Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present a class of neural process models that are able to produce correlated predictions. By using invertible transformations (gaussian copula), the model is able to capture non-Gaussian output distributions. Experiments with artificial and real data highlight the predictive ability of the proposed model.This paper addresses a core issue of the popular conditional neural process. The predictions at each test point are conditionally independent given the conditioning set. This is an inappropriate modeling assumption for many real-world datasets. The authors propose to go beyond a non-diagonal Gaussian to describe the joint distribution.The manuscript proposes variants of Neural process (NP), which can model correlation in the input (and in the output for multiouput regression) The main idea is to directly parameterize the mean and the covariance functions of a Gaussian predictive via neural networks. The authors also propose to use Copulae to handle non-Gaussian marginal distributions.	This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non-Gaussian prediction maps are obtained using copulas.   Technically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \epsilon-greedy) or at thelevel of experiments. This paper proposes the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode.This paper studies switching between exploit and explore modes in reinforcement learning. Studying seven Atari games, an empirical analysis of different switching mechanisms is performed. Overall, the paper is very strong, well-motivated, and empirically sound. Most of my concerns are with clarifying details of the experiments and improving the exposition.This paper investigates when to switch between exploitation and exploration and how long to stay in each exploration mode during RL learning. It proposes new ways to explore the subject, especially with intra-episodic exploration variants. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions.This paper proposes a mode-switching strategy for the exploration/exploitation dilemma instead of monolithic behaviour policies. Different granularities for the timing of the switches as well as different switching mechanisms are investigated (blind vs. informed switching). The focus for exploration is not on how, but when.	Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.  Strenghts: ------------ The study is well motivated and the manuscript is overall well written Studies a new problem area, and proposes an initial novel method for this problem extensive study on atari problems  Weaknesses -------------- some clarity issues as pointed out by the reviewers no illustrative task is given to give a more intuitive exposition of the "when to explore" problem comparison to some extra baselines like GoExplore would have been insightful  Rebuttal: ---------- Most clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.  Summary: ------------ All reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a self-distillation based graph augmentation mechanism to alleviate the drawbacks of existing MI based models w.r.t. their high dependency towards negative sampling. Quantitatively the proposed model achieves encouraging results. It would have been better if the system designs and significant difference of IGSD from existing work are discussed.This paper applies the ideas from semi-supervised classification task to improve the representation quality learned by graph neural network. It combines several kinds of existing techniques including diffusion graph augmentation, mean teacher consistency, debiased contrastive loss and pseudo class consistency.This paper proposed a distillation approach for unsupervised graph representation learning. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The authors performed evaluation in graph classification and regression tasks.This paper proposed a method for learning graph-level representation in an unsupervised contrastive way. The major concern about this paer is that the proposed method encourages the closeness of augmented views from the same graph instances. For example, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule.	This paper proposes an unsupervised graph learning method [Iterative Graph Self-Distillation (IGSD)] by iteratively performing self-distillation to contrast graph pairs under different augmented views. This idea is then extended to semi-supervised setting where via a supervised contrastive loss and self-training. The method is empirically evaluated on some semi-supervised graph classification and molecular property prediction tasks, and has achieved promising results.  Reviewers agree that the method is interesting and the paper is well-written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1's post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies how the vulnerability of a neural network model depends on its input dimension. While I find the results interesting, I do not see clear implications. Given that the experimental results are also not particularly novel, I recommend rejection.This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations.The authors provide a compelling theoretical explanation for a large class of adversarial examples. They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures. They provide thorough empirical evidence of their work.This paper analyzes the relationship between "adversarial vulnerability" with input dimensionality of neural network. It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. The paper does not cover the adversarially-augmented training based iterative attacks.	This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally.   The work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.)   However, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper is about the design and analysis of policy optimization algorithms that provably converge to a Nash equilibrium at a sublinear rate for a class of zero sum Markov games. Each players adopts an entropy-regularized policy optimization method (which the authors call as smooth Fictitious Self Play). The Lipschitz regularity assumption being made is important enough that it is good to add it in the abstract.This paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games. The main algorithm is a version of the classic fictitious play algorithm. It operates on the Q-values, but a key novelty is the use of a particular form of regularization.The paper is well written but I did not check everything in detail (see specific comments). My main concern is that the different assumptions made are a bit ad hoc (sometimes the assumption relates directly on the sequence of policies generated by the algorithm). And thus it is hard to assess if the provided bound is relevant or a trivial consequence of the assumptions. I’m also curious about the reduction of the presented algorithm to matrix game. Do we recover a known algorithm, and what can weThis paper studies the two-player zero-sum Markov game using fictitious self-play strategies. The authors proposed a novel entropy regularized policy optimization method for both agents. They proved the sequence of joint policies converges to a neighborhood of a Nash equilibrium at a sublinear rate.	The paper shows that a form of Fictitious Self-Play converges to the Nash equilibria in Markov games. Understanding the theoretical properties of Fictitious Self-Play is important, however the paper in its current form is not ready for publication. The paper needs a more thorough discussion on related works, the assumptions made, and as pointed out by Reviewer3, the convergence argument needs to be expanded and explained in more detail. Further, I encourage authors to add experiments and compare their algorithm with other methods.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors proposed a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment and feature consistency. The authors demonstrate extensive experiments on three datasets and verify the performance of the proposed method.This paper presents a source-free domain adaptation method. The proposed method divides target samples into source-like and target-specific ones. Source-like samples are used for global class clustering. Target-specific samples are use for learning local structures.This paper proposes a new source-free unsupervised domain adaptation method named DaC. DaC uses the source model to split the target data into source-like and target-specific samples. An adaptive contrastive learning strategy is used to achieve class-wise adaptation and local consistency.This paper is about unsupervised domain adaptation when the source data is unavailable at the time of adaptation. Authors propose a new SFUDA approach that they call Divide and Contrast (DaC) which divides the target data into two disjoint groups: source-like samples and target-specific samples. Authors present both theoretical results and numerical results that illustrate the superiority of DaC.	This paper proposes a relatively complicated method for source-free unsupervised domain adaptation, which integrates several techniques into a divide and contrast framework. The idea of dividing the target data into source-like subset and target-specific subset and employing global alignment and feature consistency for each subset is novel when the source data is inaccessible. The contrastive learning and memory-based MMD are novel in the context of source-free domain adaptation and introduce theoretical benefits in terms of the expansion theory and domain alignment theory, respectively. Reviewers were on the positive side while holding some concerns on the marginal improvement over the SoTA methods, which were addressed in the author rebuttal. AC generally agreed that the paper has introduced a novel and solid contribution to the field, with a nice connection between algorithmic methods and theoretical insights, and  recommended the paper for acceptance. Authors are suggested to incorporate all rebuttal material in the revision and if possible, to work out a recipe for easing the adoption of their relatively complicated framework that comes with many modules and loss terms.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper describes an SMC algorithm to sample the posterior distribution of latent states. It is difficult for an expert in SMC algorithms to understand the algorithm as it is described. Here again, everything is done to frighten the reader.The paper puts together several ideas from prior works (partial rejection control/SMC, variational inference, dice-enterprise) The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks.The submission suggests a new variational bound for sequential latent variable models. Unlike previous work that optimize this bound using ‘standard’ particle filters with unbiased resampling, the new bound is constructed based on a partial rejection control step. Empirical experiments suggest that the method outperforms previous work.The paper considers SMC to construct variational approximations. It is not obvious that one can obtain unbiased estimators of the normalizing constant. The experiments include a mix of toy and more realistic examples.	This paper explores the use of partial rejection control (PRC) for improved SMC-based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance.   This paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4's central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4's concerns were not fully addressed.  On the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.  On the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.  I realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes *Bootstrapped Meta-Learning,* a new meta-learning algorithm for hyperparameter optimization. Drawing inspiration from temporal difference learning techniques in reinforcement learning, the meta-learner is asked to predict the result of additional unrolled steps of the optimization process. The empirical results show considerable improvement w.t. well-performing baselines.The paper presents a new meta-learning algorithm to address two shortcomings of standard meta-optimization algorithms. The proposed algorithm addresses these two issues by minimizing the distance to a bootstrapped target under a chosen metric. Empirically, the new algorithm achieved a new state-of-the art for model-free agents on the Atari ALE benchmark.This paper broadly considers meta-learning, a.k.a. bilevel optimization. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters. The paper is well-written and features exemplary empirical execution. I think this paper should be an oral, best-reviewed paper. It is arguably the best in my batch.The paper presents Bootstrapped meta-gradients (BMG) BMG is an extension of typical meta- gradients for task tuning meta-parameters. BMG provides significant improvement over meta-learning baselines. The paper is generally clear and well written.	This paper addresses a meta-learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo-metric. The algorithm first bootstraps a target from the meta- learner, then optimizes the meta-learner by minimizing the distance to that target under a chosen pseudo-metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta-optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L=1$, the method is attractive for meta-learning community. All reviewers agree to champion this paper. Congratulations on a nice work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The presented paper introduces an extension to existing pattern generation algorithms for PDB heuristics. The way the paper is currently written, by interleaving background on existing methods (which makes more than half of the paper), makes it hard to follow. The write-up is not very good and could besignificantly improved.The paper presents two algorithms but many details are left out. This makes it impossible to understand how the algorithms work. The experiment section is missing many essential comparisons. The limits of 50s and 75s for bin packing seem very large.	Dear Authors, thank you very much for your submission. We are happy to inform you that we have decided to accept it and we look forward to your talk in the workshop. Please, go over the feedback in the reviews and correct or update your papers in time for the camera ready date (May 24). In particular, please address the comments raised by both reviewers regarding clarity, discussion of related work, and additional experimental results, by making use of the additional space (9 pages)  allowed by HSDIP. Best regards HSDIP organizers
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper discusses the problem of shift between train and test data distributions. The authors assume that training data originate from multiple 'environments', each with particular distribution. They propose a new framework (EDNIL) through which they can infer the environments from data and then train an environment invariant model.Paper proposes a new method to infer environment labels that can then be used for invariant learning (such as IRM methods) Essentially, it uses a neural network for clustering data into environments with appropriate loss functions. The method is more scalable and less sensitive to initializations than the prior methods.The authors propose a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features. In my opinion, the experimental results are sufficient but the presentation should be improved (see below for specific suggestions)The EDNIL method is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations and the label predictions. The novel method is shown to perform extremely well when compared to its competitors, achieving close to the performance of IRM.	This work presents a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. The method is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations, and the label predictions, produced by a multi-headed neural network. The proposed model is compared to different alternative models from the literature of the field, in different challenging benchmarks, and the results show that it closely achieves the best possible invariant learning performance.  After some initial discussions, all reviewers agreed that this work is ready for publication, as the work addresses an important problem, presents good empirical results, and will be of significant interest to the community.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This contribution consists of an OpenAI Gym style wrapper around the SUMO traffic simulation package. The main weakness of this paper is that its main contribution, the benchmark, is provided under a no-derivatives license. The packaging and documentation would also both benefit from additional work.This paper introduces a benchmark for RL-based control of traffic lights. The primary advantages touted over existing work include use of the SUMO simulator. Experiments show that independent PPO/DQN tend to do quite well at the end of training.The benchmarking proposed in this paper could be very helpful for future study on using RL for traffic signal control. The paper is well written, and the benchmarking tasks are carefully chosen to match the real-world scenarios. The experiments are well-conducted.Proposes benchmarks to study the problem of congestion control using realistic traffic situations. Presents baselines and thorough comparative evaluations. Current progress in congestion control algorithms are evaluated using different settings avoiding fair comparisons between them.	All the reviewers appreciate the value of the proposed benchmarks. The remaining concerns seem addressable. I recommend accepting the paper while asking the authors to incorporate the review feedback into the camera-ready paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.SOPGOL can autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. This works introduces a lifelong RL problem in which new tasks can be possibly be expressed as a logical composition of previous ones.This paper builds upon previous work on the Boolean Task Algebra For Reinforcement Learning extending it to the discounted and stochastic tasks and to the lifelong RL setup. Authors show that techniques introduce in the previous work can perform close to optimal in a zero-shot transfer scenario.The paper proposes a new framework for lifelong RL that focuses not only on transfer between tasks for faster RL, but also gives guarantees on the performance of the agent over an unknown task distribution. Experiments are performed to verify the theoretical results in both transfer learning and lifelong RL settings in the PickUpObj and Fourrooms domains.The paper introduces an approach of logical decomposition of tasks as a particular set of MDPs. The paper continues define a lifelong learning algorithm that relies on this logic to form a set of base skills. This algorithm is then evaluated on a multi-task and lifelong learning task, where it decidedly outperforms a standard deep q-learner.The authors study goal-based lifelong RL. They leverage logical composition to create an algorithm for this setting, with the goal of better generalization. They provide theoretical bounds for their approach, and provide empirical evidence that their approach generalizes well in practice.This paper considers lifelong reinforcement learning. It extends a line of recent work in which logical composition is combined with goal-based reinforcement learning to achieve generalization to new tasks. The main contribution is theoretical, extending prior work to handle stochastic transitions.	I thank the authors for their submission and active participation in the discussions. This papers is borderline. On the positive side, reviewers emphasized this is a well written [ovqB,1zPe] and sound paper [BUDa] with good theoretical [td5N,ovqB,1zPe] and empirical [BUDa,td5N,ovqB] results. On the negative side, reviewers remarked clarity [KyZj,AVki], incremental with respect to Tasse et al (2020) [KyZj], relatively restricted Boolean task algebra [td5N], toyish nature of the environments considered [ovqB], and some missing details [1zPe]. During discussion, the sentiment seems to be somewhat lukewarm with none of the reviewers strongly favoring acceptance or rejection. It seems the main remaining concern is around the toyish nature of the environments used in this paper. I acknowledge that and I believe the authors could include experiments on more complex environments. However, I also give the authors credit for addressing most of the reviewer's concerns during rebuttal and for presenting a solid empirical and theoretical result that the research community can build upon in the future. I am therefore recommending acceptance of this paper and highly encourage the authors to further improve their paper based on the reviewer feedback.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper is motivated by using reinforcement learning (RL) methods in inventory control. It customizes Q-learning for a special one-sided feedback/full-feedback setting. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on size of state and action space.This paper proposes two algorithms for the classic inventory control problem. It establishes cardinality-independent regret bonds for the two algorithms in terms of length of episode and horizon. Numerical results are presented to demonstrate the effectiveness of the algorithms.This paper proposes Q-learning based algorithms called Elimination-Based Half-Q-Learning (HQL) and Full- Q- learning (FQL) In the one-sided-feedback setting, the proposed algorithm improves the regret bounds over existing methods. Numerical experiments are provided to show the performance of the algorithm.The novelty of this paper lies in applying reinforcement learning algorithms to the inventory control problem. It is interesting to see how can Q-learning algorithm be customized to problems with more structure. Technically, it is also novel to use elimination based algorithms in tabular reinforcement learning. I'm not totally convinced that the assumptions in this paper is general and realistic. I still have some concerns about the technical soundness.	This paper explores the performance of Q-learning in the presence of either one-sided feedback or full feedback. Such feedbacks play an important role in improving the resulting regret bounds, which are (almost) not affected by the dimension of the state and action space. The motivation of such feedback settings stems from problems like inventory control. However, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. The dependency on the length per episode H can also be improved.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Backdoor attack is becoming increasingly relevant in ML applications. This paper introduces a benchmark for backdoor attacks and defenses in the context of NLP models. The OpenBackdoor framework can be used by future attackers or defenders coming up with new algorithms.This manuscript proposes a framework for evaluating backdoor learning in textual models. The authors provide a critique of evaluation in prior work and present "OpenBackdoor" which comprises implementation of several attacks and tasks. The manuscript makes a good observation on the lack of standard evaluation approaches for different backdooring attacks and defences.This paper argues the existing textual backdoor research (attack and defense) suffers from two deficiencies of ambiguous settings for different scenarios. The paper focuses on a potentially important research track textual backdoor learning, which may be relevant to broad deep learning researchers in the future.This paper categorizes existing NLP backdoor learning work into three practical scenarios. It also provides evaluation metrics not only considering ASR/CACC, but also stealthy and semantic-preserving of poisoned text. The benchmark experiments are clear and the parameters are provided.	This work provides evaluations on serveral backdoor attacks and defenses on NLP data. The evaluation can be further improved by discussing related defenses, maintaining high quality and clear documentation, and discussing the stealthiness of the attacks.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the effectiveness of using the restricted computational model class of low-degree polynomials for assessing statistical-computation gaps for high-dimensional statistical inference problems. Using the problem of reconstruction on trees, the authors identify problem settings for which average-case reconstruction is impossible.Low-degree polynomials as a computational model used to study (in)tractability of learning/inference problems seems to be a useful and important model. The main limitation I can see is that the tree structure that makes this analysis go through is somewhat limited.This paper studies the problem of reconstruction on trees through low degree polynomials. The topic of this paper is completely out of my area, and any technical comments I make will probably be unfair to the authors. I think the presentation can definitely be improved.This paper studies the problem of tree reconstruction on $d$-ary trees. The root of the tree is given a spin $X_\rho, which is then propagated down to the leaves according to a Markov channel $M$. The problem is then to recover the original root spin of the root. The paper is overall well-written and easy to read.	This paper studies using low-degree polynomials for analyzing statistical/computational gaps for high-dimensional inference problems and identify average-case settings that exhibit this gap.  This is a nice paper and above the bar, though it perhaps appeal to only a theoretical audience.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The IMPLANT algorithm is supposed to improve control policies in learning controllers. The reward function in equation (4) is just a restatement of the optimality principle. There were a few sentence structures in the paper that could use some revision.The authors propose a method to enhance imitation learning by using MPC on the reward function learned by an imitation learning approach. The chosen MPC approach uses the learned policy to generate candidates for a search process that maximizes the reward. The paper shows improved performance in a typical benchmark scenario.Imitation with Planning at Test-time (IMPLANT) is a new algorithm for imitation learning. It incorporates decision-time planning within an inverse reinforcement learning algorithm. IMPLANT outperforms BC and GAIL-based baselines in the ‘non-transfer’ setting of imitation with limited expert trajectories.The paper proposes to combine imitation learning (GAIL) with model predictive control to improve on the policy. MPC uses the reward function and value function learned by GAIL and assumes access to a model of the test-dynamics. The paper is well-written and very easy to follow.	The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like ICLR.  The idea of combining MPC (on a 'wrong' model)  with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed method works as follows. Given samples are partitioned into two parts; one is for classifier training and the other is for data synthesizer training. Both are trained in a differentially private manner. After training, the DP synthesizer generates samples and the DP classifier labels them so that the resulting samples can be used as training samples. By the post-processing theorems, the resulting are differentiallyPrivate, which are published as synthesized samples.The authors proposed QUAIL, an algorithm that uses a supervised model and a synthetic data model to generate synthetic data that is good for downstream tasks. It also shows some empirical evaluations of the algorithm. The technical part, especially the experiments, might need some improvement.My main concern with the paper is that there is no clear contribution. The paper proposes QUAIL, an ensemble of a generative model and a classifier. Both are trained with differential privacy, in order to generate a differentially private dataset.	The paper surveys existing differentially private data synthesis methods, and introduces an algorithm that learns both a generator and a classifier in a differentially private mode.  The problem is highly timely and important. Results are promising.  Main remaining concerns after discussion between the reviewers and the authors are:  - reason why the proposed scheme can give better classification accuracy, should be clarified more  - unclarity on conclusions that can be drawn from the experiments. The revised version has improved on this somewhat.  One explanation for the problems was suggested to be that the paper tries, at the same time, to both present a new method and be a survey. Is hard to do in a short paper, and as a result, the paper lacks focus. At the very least, more work is needed.  The authors are encouraged to continue their work on this important problem, and the review comments hopefully help in that.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies time series outlier detection. The current figures do not show clear performance gain from any method. The proposed approaches do not seem to offer any gain. It might be more interesting to develop on which methods perform better in which context.The paper compares non-RNN and RNN methods in the outlier detection context. The conclusion is drawn on 3 real-world data sets. For comprehensiveness, I would recommend to use existing relevant data sets from UCI Repository.This paper presents a comparative study of the performance of non-recurrent models and deep recurrent models for time series outlier detection. Seven models are evaluated on multiple synthetic and real-world time series datasets. The strengths of this paper are as follows: It can be applied to many real- world applications. It is unclear whether the used models are capable of detecting collective outliers.Anomaly detection in time series is an important problem. The evaluation seems incomplete. Motivation has to be reconsidered. Not sufficient datasets to make such claims. Many questions of this form: are CNNs better? Are subsequence methods better? are traditional vs modern approaches better?	This paper has been reviewed by four experts. Their independent evaluations were consistent, all recommended rejection. I agree with that assessment as this paper is not ready for publication at ICLR in its current form. The reviewers have provided the authors with ample constructive feedback and the authors have been encouraged to consider this feedback if they choose to continue the work on this topic.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Federated Averaging can generalize to new tasks (after fine-tuning) Theoretical guarantees are presented for a multi-task linear regression setting. Further empirical results demonstrate the effectiveness of learning representations with image classification tasks.This paper considers multi-task linear regression in a non-convex setting. The works shows that FedAvg can regress to the correct concept. Local client-updates in FedAvg encourage diverse high-rank representations, whereas Distributed SGD finds low- Rank representations.This paper provides theoretical analysis of FedAvg with Fine Tuning. When the local model weights for clients lie on a linear subspace, theorem 1 suggests FedAvg gets linear convergence, while proposition 1 suggests Distributed GD may diverge for representation learning. The main contribution of this paper is theoretical, while the assumptions are very strong.The paper analyzes both theoretically and empirically the performances of FedAvg with local fine-tuning as a post-processing step. The work by Collins et al. (ICML 2021) on shared representations should be compared more deeply with the proposed analysis since the two setting bear some similarity.	This work provides an analysis explaining why FedAvg can produce more generalizable representations than distributed SGD. Theoretical guarantees are presented for a multi-task linear regression setting and further empirical results demonstrate the effectiveness of learning representations with image classification tasks. The theoretical analysis presented can be an important building block for the study of more complex settings in federated optimization. All reviewers recommend acceptance.  Please take the (few) suggestions by the reviewer into account, and also incorporate the explanations and clarifications provided during the rebuttal in the camera ready version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The core idea of the paper is to enforce the Lipschitz constant of each linear layer of the network approximately close to 1. This results in a model which is robust to adversarial and random noise ad all directions in the model space are non-expansive. The paper add value to the research community through thorough experimental study as well as in industry.The paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks. The authors observe that fully quantized models are more exposed to attacks. They propose a Lipschitz constant filtering of the inner layers' input-output to fix the issue."This manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues" "Reading the other comments online, the authors seem to have addressed those concerns as well"	The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents an unsupervised learning model for disentangled latent representation learning. The model works on a combined latent space including both entangled variable and separable variable. The most impressive part is the one-at-a-time (OAT) factor learning approach.This study proposes a disentangled representation learning method called "one at a time" It is a VAE-GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner. The authors reported their experimental results for two datasets, i.e., dSprites and CelebA.The authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset. The latent variables consist of nuisance factors and disentangled factors that form the $k$ generative factors per dataset.The paper shows the performance of the proposed approach on dSprites and CelebA datasets. However, it has several issues, including inaccurate claims, and many typos. The paper mentions in many places that the idea of progressively learning one factor at a time is new. A similar idea has already been proposed in 'Robust Disentanglement of a Few Factors at a Time'	This paper presents a method for unsupervised learning of disentangled representations by first training a VAE with a tangled set of latents, and then sequentially learning disentangled latent variables one at a time from the entangled initial VAE latent space. On several toy disentanglement benchmarks, the method is shown to perform competitively with previous VAE and GAN approaches.   There were several concerns from reviewers around the clarity and description of the proposed one-factor-at a time (OAT) training procedure. While the updated draft addressed several typos and some clarity issues, multiple reviewers continued to find the method description problematic. There were additional concerns around the viability of the method on real-world datasets where the number of factors are not known, and as the authors stated the proposed method can also result in one factor of variation encoded into mulitple latent variables, which hurts on many of the disentanglement metrics.  The addition of CelebA downstream task evaluation begins to address this concern of real-world data, but more rigorous experiments (including more description of how models were selected) and discussion of the limtiations of the proposed method are needed. There is also no theoretical motivation as to why the proposed intervention-based factor learning algorithm should recover the ground truth factors.  Given the concerns over experimental results, clarity, and lack of theoretical motivation, I suggest rejecting this paper in the current form.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.2D animation data are expensive to collect since they have different characteristics with 3D animations. It would be better to show that their data have similar data distributions to the prior datasets. The number of images used for training strongly correlates to the performance.This paper presents a new 2D animation dataset for pixel-wise and region-wise (segment matching) correspondence. This dataset is made by converting three high-quality 3D movies into 2DAnimation. It is characterized by rich image composition and complex motion. Authors clearly explain he motivation behind this dataset and what makes it different from existing ones.This paper presents a new dataset for visual correspondence of 2D animation. It is generated from three open-source 3D movies. The dataset includes pixel- and region-wise correspondence labels to support some applications like frame interpolation and automatic colorization.This paper introduces AnimeRun, the first practical dataset for 2D animation correspondence. Unlike existing datasets, it is suitable for full-scene 2DAnimation. It contains pixel-wise correspondence labels for optical flow estimation. The results show the effectiveness of AnimeRun and the limitations of existing methods.Authors apply custom pre-processing on open-source 3D animation to transform it into 2D cartoons. There seems to be a lack of explicit Limitations section in this work, and parts of the writing seem incorrect.	This paper presents a rich dataset aimed at creating supervision for 2D cartoon tasks. The dataset is created by procedurally converting open source 3D computer graphics movies/shorts into "flattened" 2D frames. Care is taken in describing both the motivation and limitations of this approach. In an area with a dearth of good data, this paper's contributions are warmly welcomed. Reviews appreciated these aspects of the paper while highlighting potential weaknesses that appear addressed in the revisions. I recommend accepting this paper to the NeurIPS 2022 Datasets and Benchmarks program.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper considers the combinatorial semi-bandits problem under two different settings. It improves the dependence on batch size from linear to logarithmic in many practical applications. The paper is very well-written and provides clear intuition behind various ideas/assumptions.This paper proposes new smoothness conditions for the well-known combinatorial semi-bandit problem with probabilistically triggered arms. With the so-called triggering probability and variance modulated smoothness condition, the authors significantly improved the O(K) factor that appears in the previous regret bounds, where K represents the batch size.This paper studies Combinatorial Multi-Armed Bandit (CMAB) problems with probabilistically-Triggered arms. CMAB is a variant of the standard bandit setting where the learner chooses a subset of arms. The mean reward is a function of the chosen subset and of its component arms’ mean rewards. This formulation generalizes problems including cascading bandits for ranking search results.	Thank the authors for their submission.  The paper studies combinatorial multi-armed bandit with probabilistically triggered arms. That is an MAB setting in which, at each round, the learner chooses a subset of the arms and obtains a reward that is some function of expected rewards of the chosen arms. In addition, the learner only observes feedback on a random subset of her chosen arms (triggered arms).   The paper relaxes a smoothness assumption laid by a previous work, and further improves the dependence on K in the regret bound, where K is the batch size (maximum number of triggered arms) The authors provide computationally-efficient algorithms that are based on Bernstein concentration inequality, facilitating the improved bounds.  The paper is well-written and organized, and the theoretical results are sound.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal of their training scheme is to create an ensemble of models that has a high disagreement on the out-of-distribution (OOD) samples in the unlabeled set. The success of the method largely relies on the models in the ensemble having sufficiently different outputs on OOD samples.The paper is generally well-written, however, there is some confusion listed as follows. Some claims are vague in the submission, for example, the authors claimed that the Maximum discrepancy method (MCD) tends to result in ensembles that do not disagree enough on OOD data.The authors introduce a semi-supervised ensemble approach to novelty detection. The main idea consists in generating base classifiers that disagree on the out-of-distribution data (ODD) An early-stop criterion is used to achieve the wanted level of disagreement among the component of the ensemble.The authors develop an ensemble-based procedure for semi-supervised novelty detection (SSND) It utilizes a mixture of unlabeled ID and OOD samples to perform on near OOD data. A regularization technique is further used  to promote diversity on the Ood data while preserving agreement on ID data.The authors proposed a novel transductive novelty detection method using the disagreements of the ensemble models. The authors directly utilize the unlabeled test set samples, providing different labels for those unlabeling samples, and training multiple models with fine-tuning. This framework tries to identify the OOD samples using the disagreement of those samples.	The authors propose a semi-supervised novelty detection method which tries to identify out-of-distribution samples in the unlabeled data (consisting of in- and out-distribution samples) using a disagreement score of an ensemble. The ensemble is generated by fine-tuning the trained classiifer on the labeled training data plus the unlabeled data which all get a fixed label (which is repeated several times to generate the ensemble). The main idea is that one uses early stopping based on an in-distribution validation set in order to avoid overfitting on the unlabeled points which allows then identification of the out-distribution points via the disagreement score.  The reviewers appreciated the simplicity of the approach and the extensive experimental results. The authors did a good job in trying to answer all questions and concerns of the reviewers.   However, some concerns remained: - the setting assumes that the OOD data is fixed which was considered as partially unrealistic and thus evaluation of the OOD detection performance on unseen OOD distributions was requested in order to understand the limitations of the method (this was only partially done by the authors).  - the theoretical result is for a two-layer network and completely based on previous work. As the authors use much deeper networks later on in the experiments, this result cannot be used to theoretically justify the approach.  - there remained concerns about the necessary diversity of the ensemble and the early stopping procedure  While I think that the paper has its merits, it is not yet ready for publication. I encourage the authors to to take into account the above points and other remaining concerns of the reviewers in a revised version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper investigates the collapsing problem of contrastive learning. It attempts to attribute the collapsing phenomena to strong augmentation and implicit regularization, using simple linear network models. Based on the above analysis, the paper then propose a simple sub- vector based CL method called DirectCLR.This paper firstly studies the dimensional collapse problem in existing contrastive learning (CL) methods. Experiments on ImageNet demonstrate the effectiveness of the proposed method. The proposed DirectCLR dropping projector is a novel idea.This paper shows that contrastive methods also suffer from the “dimensionality collapse” phenomenon. Two underlying causes for this problem, i.e too strong data augmentation and implicit low-rank regularization, are proved in simplified settings of shallow, linear networks. theoretical analyses are used to motivate an alternative training technique without the need for the embedding projector.This paper theoretically proves that the contrastive learning results in the dimensional collapse in the feature representation space. The claims are theoretically well described and the process of proof is easy to follow. The gap between the model in the proof process and the ones in the experiment part are quite large.The paper focuses on the dimensional collapse problem faced by contrastive learning methods. The analysis in toy model (e.g. linear layer, additive noise) is convincing. Authors do not answer the question that what kinds of augmentations are "strong"	The theory and results presented in this paper provide a new method to avoid collapse in contrastive learning.  All but one reviewer recommend acceptance.  The lone negative reviewer is concerned with the limited experiments, but the other reviewers, and the AC, find the experimentation convincing enough to warrant acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies hyperparameter (HP) sensitivity for the outlier detection (OD) task. The paper doesn't actually answer **RQ1) how to design an unsupervised deep OD model that is robust to its HPs. Although rarely used in deep OD models, the ensemble ideas are widely used in OD tasks.The paper proposes an "ensemble" of autoencoders of varying depths and widths called RobOD. RobOD is competitive at outlier detection against other models, while being significantly more robust to the hyperparameter choices. The authors discuss that their method is costlier than a single training model.Hyperparameter selection is an important issue for almost all machine learning tasks. The authors propose to leverage skip connection and batch ensemble technique to speed up the hyper-ensemble calculation. The proposed ROBOD seems to be simple and easy-to-implement, but enjoys both good effectiveness and efficiency against other AE-based baselines.	This paper empirically demonstrates the sensitivity of unsupervised OD methods to hyperparameters and proposes ensembles of models with differing hyperparameters along with training techniques based on weight sharing to do so efficiently.   The authors provided additional experiments to answer some reviewer's major concerns regarding the (meta-)HP robustness of the ensemble methods. While there are natural fluctuations with respect to the (meta) hyperparameters, a larger number of hyperparameters included usually resulted in a close to optimal AUROC.  Another concern for two reviewers was the extendability of the efficient ensemble training techniques to other models such as GANs etc. As the authors replied, even though skip connections might not be adaptable to other techniques, e.g. for ensembling various widths, zero-masked layers with BatchEnsemble would also be broadly applicable to ensemble-learn other unsupervised representation learning models. Perhaps in the final version, the authors further discuss with a short experiment how the AE-specific scaling techniques add to the performance.  A further concern was the lack of theoretical underpinning about the (meta)HP-robustness. Given that the reviewers agree that this paper provides ample empirical evidence and praise the experimental value, we think theoretical work (providing HP sensitivity results is highly nontrivial in general, let alone for neural networks) can be part of future work but the lack of it in the current manuscript should not prevent the publication of this work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the last iterate rate of convergence of extra gradient and optimistic gradient algorithms in smooth monotone games with continuous convex action sets. The technique used for finding the potential function needed to prove the result is novel and interesting. This paper has no negative societal impact.This paper studies the last-iterate convergence rate to a Nash equilibrium in monotone games. It extends previous work to analyze the convergence of no-regret learning algorithm with constant step size in (possibly) constrained games. The authors propose a novel notion, namely the tangent residual.The paper focuses on proving last iterate convergence for both Extra Gradient and Optimistic Gradient Descent methods for monotone games. The authors show tight last-iterate convergence of $O(1/\sqrt{T})$ without any uniqueness assumptions that prior works had consider.This paper provides the first last-iterate convergence rates of extragradient algorithm (EG) or the optimistic gradient algorithm (OG) to Nash equilibrium of constrained smooth monotone games. Tangent residual is proposed as a new proximity measure to a Nash equilibrium.	This paper studies the last iterate rate of convergence of the well-known extragradient and optimistic gradient algorithms in smooth monotone games with continuous convex action sets. The main result of the paper is to show that both algorithms (with constant step size) enjoy tight last-iterate convergence rates for setting (previous papers either 1) only applied to unconstrained domains 2) were asymptotic, or 3) required dependence on arbitrarily large problem-dependent constants).  This paper resolves a well-known open problem within the min-max optimization community, and is likely to have significant impact. The reviewers agree that the paper is well-written, and the the techniques (using the "tangent residual" as a potential function) are novel. For the final version, the authors are encouraged to incorporate the reviewers' suggestions to improve the presentation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides a systematic study of “reward hacking” in the environments with the misspecified rewards. Authors conduct a set of experiments with 4 environments, several types of reward misspecification in each of them and several agents of different expressivity. They notice that often the agents that are more capable end up obtaining high proxy reward, but low real reward.The paper studies reward hacking, a common but understudied phenomenon, across a set of environments. A key finding is that reward hacking increases with agent capabilities so that increasing capability lowers the true reward. The authors also find ‘phase transitions’ where a small increase in capability results in qualitatively new reward hacking behavior.This paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking.This paper investigates the phenomenon of reward hacking as a function of agent capabilities. They introduce four diverse RL environments with nine misspecified rewards. More capable agents are better at exploiting the misspecification. They find instances of phase transitions where a small increase in agent capability produces a large change in behavior.	I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is presenting an interesting and systematic study of reward hacking [GVMn] that is useful to the research community [bfGN] and targets an important problem [uYeb] in a rigorous way [16uL]. I thus recommend accepting the paper, but I strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular in regards to improving positioning with respect to related work and a better formalization of their work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method for accelerating distributed high-dimensional regression under sparsity constraints. To do so the authors adapt Nesterov's proximal gradient method to the setting by combining it with a consensus step based on the connectivity of the graph.This manuscript studies the problem of sparse regression in a decentralized setting. An accelerated algorithm is proposed, and its convergence and statistical properties are proven. While there is sufficient contribution, many of the core technical ideas build upon classical approaches to decentralized optimization.This paper investigates distributed sparse regression in high-dimensions to speed up its computation. The method combines Nesterov’s proximal gradient with consensus and gradient-tracking mechanisms. This method can estimate locally the gradient of the empirical loss while enforcing agreement on the local estimates.The paper proposes an accelerated algorithm for distributed sparse regression in high dimensions. They combine accelerated Nesterov’s proximal gradient with consensus and gradient tracking mechanisms. They show it converges globally at a linear rate, achieving both optimal iteration complexity and communication complexity.	The paper provides novel guarantees for the well-studied distributed sparse regression problem. Their theoretical results improve upon the state of the art and extend to settings that many previous results could not handle. From a technical perspective, their result builds upon previous frameworks, but also requires a number new, novel ideas. The paper does have some downsides; as mentioned previously, some of their ideas do build quite strongly off of previous work, and the presentation of the paper is quite dense, as several reviewers noted. However, the consensus of the reviewers overall is that the technical contribution of the paper is above the bar for acceptance, and would be of interest to the distributed optimization community.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a generic method to construct conformal prediction sets in an adversarial setting. The idea of using randomized smoothing to construct a robust conformity score is quite novel and theoretically sound. Overall, I think the paper is well-written and reach the acceptance bar of ICLR.Post-hoc `` wrapper-style'' procedures for uncertainty quantification are of great value. The current work focuses on a setting where adversarial examples might be present at the test stage. For safety-critical applications, it is important to build robust ways of quantifying uncertainty.This paper generalized a data-splitting conformal prediction approach to the adversarial attack setting. The effectiveness of the proposed methods was demonstrated on the CifAR10, CIFAR100, and ImageNet datasets. The motivation of introducing randomized smoothing into the score function was very well illustrated.This paper tackles the conformal prediction problem under adversarial perturbations. The proposed approach combines the non-conformity score with randomized smoothing and the standard conformal Prediction. The paper is easy to follow.	This paper studies the problem of producing distribution-free prediction sets using conformal prediction that are robust to test-time adversarial perturbations of the input data. The authors point out that these perturbations could be label and covariate dependent, and hence different from covariate-shift handled in Tibshirani et al 19, the label-shift handled in Podkopaev and Ramdas 21, and the f-divergence shifts of Cauchois et al 2021.   The authors propose a relatively simple idea that has appeared in other literatures like optimization but appears to be new to the conformal literature: (i) use a smoothed (using Gaussian noise on X, and inverse Gaussian CDF) nonconformity score function, in order to control its Lipschitz constant, (ii) utilize a larger score cutoff than the standard 1-alpha quantile of calibration scores employed in conformal prediction. The observation that point (i) alone lends some robustness to adversarial perturbations of the data is interesting. As several experiments in the paper and responses to reviewers show, this comes at the (apparently necessary) price of larger prediction sets.   I read through all the comments and also the supplement. The authors have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement as a result. Three reviewers are convinced, but one remaining reviewer requested additional experiments to compare with Cauchois et al (in addition to all the others already produced by the authors originally and in response to reviewers). However, the authors point out that the code in the aforementioned paper was not public, but they were able to privately get the code from the authors during the rebuttal period. At this point, I recommend acceptance of the paper even without those additional experiments, since it is not the authors' fault that the original code was not public. Nevertheless, I suggest to the authors that, if possible, they could add some comparisons to the camera-ready since they now have the code.  I congratulate the authors on a nice work, a very solid rebuttal, and also the astute reviewers on pointing out various aspects that could be improved.   Minor point for the authors (for the camera-ready): I would like to comment on the Rebuttal point 4.4 in the supplement, which then got further discussed in the thread. The reviewer points out four references [R1-R4]. I will add one more to the list [R5] https://arxiv.org/pdf/1905.10634.pdf (Kivaranovic et al, appeared in 2019, published in 2020). I think the literature reviews in this area are starting to be messy, and all authors need to do a better job. Clearly, the original paper of Vovk et al already establishes various types of conditional validity (and calls it PAC-style guarantee), produces guarantees that others in this area produce, and it appears that much recreation of the wheel is occurring. For eg, [R2, R4] do not cite [R5], despite [R5] appearing earlier and being published earlier, and having PAC-style guarantees and experiments with neural nets, etc. However, in turn, [R5] do not cite Vovk [R1], but [R2, R4] do cite [R1]. (And [R3] does not seem to be relevant to this discussion of conditional validity?) In any case, I am not sure any of these papers need citing since the current paper does not deal with conditional validity. If at all, just one sentence like "Conditional validity guarantees, of the styles suggested by Vovk [2012], would be an interesting avenue for future work".
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Modeling room and scene acoustics is vital for AR and VR problems. This paper builds a representation learning model/schema for parametrizing such acoustic responses via a encoder-decoder style deep network interpolation machine.This paper introduces the concept of Neural Acoustic Fields. The idea is to learn using a neural network a function that maps a pair of 3D (emitter, receiver) locations. The function is learned using a balanced least square loss in the log-magnitude and phase domains and positions.This paper describes a NeRF-like representation for binaural room impulse responses in a given acoustic space. It is an implicit representation of listener position, orientation, and ear, as well as source position, time, and frequency. Experiments on the Soundspaces and MeshRIR datasets show that it is able to reproduce realistic effects in loudness maps.This paper proposed a model for learning to interpolate room impulse responses given room geometry and emitter/listening position. The paper is clear and well-written. The method appears to work well. The quantitative evaluations (eg Table 1) look good.	The authors present Neural Acoustic Fields (NAF), which render sounds for arbitrary emitter and listener positions in a scene. Overall, the reviewers are very positive (8-8-8-5). The authors addressed many of the reviewers' questions about previous related work and rendering spatial binaural audio.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper reinterprets the theory of PPO-clip based on the hinge policy optimization. It also generalizes the algorithm to a new family of policy-based algorithms. The paper does a good job in sharing new insights of policy optimization, and connecting the policy optimization with classification.The paper describes a new family of methods, called Hinge Policy Optimisation (HPO), generalising over proximal policy optimisation algorithm with a clipped surrogate objective (PPO-clip) Pros: novel idea and interesting analysis of reinforcement learning as classification. Cons: restrictive assumptions of the theoretical analysis; the experimental analysis could be given more space given that it actually exists in the appendix.The paper proposes hinge policy optimization, a new theoretical framework for interpreting policy gradient algorithms as classification problems to be solved with a hinge loss. The paper shows the equivalence between such a formulation and the popular PPO-clip objective and provides global converge guarantees.This paper suggests a generalization to the commonly used PPO(-clip) algorithm. The paper is very well-written both in terms of how the main parts of the paper are easy to follow. The only "weakness" is that PPO is a heavily used algorithm and the empirical results are not convincing enough.	All reviewers agreed that analysis of PPO is interesting.  During the discussion, however, there was an agreement that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. Meanwhile, for conventional policy gradient, recent works provided convergence rates. As one reviewer pointed out - this work does not further our theoretical understanding on why PPO is better than vanilla policy gradient, as all the established results hold for policy gradient, even with less assumptions. I encourage the authors to strengthen their paper by relaxing Assumption 4 (perhaps based on the robust classification idea raised in the discussion), and by further providing rate results.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper addresses the problem of sample-efficient inference for symbolic physical rules. Authors propose a generative model along with a symbolic regression framework. Forces are produced from a probabilistic context free grammar designed to mimic simple Newtonian physics.The paper proposes an EM-based method where in E-step object properties distribution are sampled using the current best estimated force laws. In M-step symbolic regression is used to update those laws. Ideally the symbolic physical laws are universal thus can be applied to all physical interactions.This paper proposes a fully Bayesian approach to learn an intuitive physics model by combining symbolic regression and statistical learning. Using symbolic regression has a long history, especially in material science, soft robotics and machine learning in general.The paper proposes an Bayesian-symbolic physics (BSP), an intuitive physics model that jointly infers symbolic force laws and object properties. The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility.	This paper proposes a method for learning physics combining symbolic computation and learning in an interesting way, targeting sample efficiency. At the initial evaluation, it was on the fence but leaning towards acceptance, with 3 slightly positive and one slightly negative review.   The strengths lie in the combination between symbolic reasoning and statistical ML with a formulation around the classical EM framework. On the other hand, an important issue of the paper is its quite simplistic evaluation on now very easy problems and benchmarks. While benchmarks tend to be simple in the field of learning physics, current work does address more difficult problems than the problems tackled in this paper.  Another issue discussed was the simple trade-off in injecting hand-crafted inductive bias into a system leading to increased sample efficiency, which was perceived as unsurprising by some reviewers. While this is common in ML, and even strongly more so in learning physics from data synthetically generated with known physical laws, it was perceived to be particularly unsurprising in this paper where the benchmarks are indeed very simple and the laws directly encoded.  The AC discussed this paper with the PCs, and it was judged that the weaknesses in evaluation, in particular the simplicity of the tasks, cannot compensate for the interesting hybrid symbolic/ML formulation, and decided to reject the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed method, AUTOMATA, consists of three components: hyperparameter search algorithm, data subset selection method, andhyperparameter scheduler. Each component of the proposed method can be selected from existing algorithms. It is quite natural that subset selection methods are also useful in hyper Parameter optimization because existing data subset Selection methods succeed in the cost reduction.This paper presents a gradient-based subset selection framework for hyper-parameter tuning. The experimental results show that the proposed framework can improve the speedup of parameter tuning significantly. The writing can be improved. There is no explanation and justification on some design decisions.This work proposed a framework to speed up HPO by gradient based dataset subset selection. The paper’s empirical work is very well done. It contains clear experiments setting, abundant real world examples, thoughtful baselines, detailed ablation studies and interesting analysis.Automata was developed with the goal of accelerating the search for the optimal configuration in order to minimize the cost and environmental impact of the hyper-parameter tuning process. The key mechanism described by the authors as the main contributor to the savings and speed-ups achieved by Automata is DSS.	This paper proposes AUTOMATA, an approach that uses GradMatch to select subsets of data in order to accelerate hyperparameter tuning. The reviewers all found the approach to be practical and empirically effective. There were concerns about the robustness to different subset sizes, particularly across different datasets, but the authors demonstrated that AUTOMATA works well across a number of settings during the rebuttal period. The remaining criticism largely revolves around the novelty of the approach, but the majority of the reviewers believe that this is a useful application of gradient-based subset selection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a method for improving tail-label performance in extreme multi-label learning setup. It is based on the finding that the distribution of the norms of the learnt weight vectors also follows a power-law. The main contribution of the paper is proposing methods for re-ranking which encourages precedence of tail-labels.This paper proposes two models for this problem. The first model is re-ranking-based, that is, it reranks the prediction scores of a standard XML model. The second model tries to augment the rarer labels to reduce the skew in data. Both the proposed methods outperform a host of highly competitive baselines on a variety of datasets by significant margins.Theorem 1 does not hold because an important condition is missing. The method is very ad-hoc, without any theoretical justification. It might also be computationally challenging to optimize the proposed loss for extreme multi-label datasets.	The paper presents some interesting insights, but all reviewers have agreed that it does not meet the bar of ICLR. The theoretical results require revision as several issues have been indicated in the reviews. The authors have tried to correct them during the rebuttal, but the reviewers remain unconvinced.  Also the novelty is limited as re-ranking is a well-known concept and decoupling of head and tail labels is an approach often used in practice across many applications.  The authors should also clarify the way the RankNet method is used and implemented to clarify the issue raised by Reviewer 1. Finally, let me notice that adjusting thresholds for labels has been considered in the XMLC literature, in the context of optimization of the macro F-measure (Extreme F-measure Maximization using Sparse Probability Estimates, ICML 2016).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper examines an interesting problem, so-called churn prediction. The primary limitation of this study is that, while the underlying problem framework is intriguing, it does not appear to make a significant impact. I encourage authors to make additional improvements, particularly for the experiments described in Section 5.In real-life applications of predictive models, predictions often form a step of the process. Changes in the predictive model often need to be validated end to end using methods such as A/B tests before they can be used in production. For certain class of problems, there is a need to control the churn. The authors present an approach to control this churn via a simple distillation method.The authors propose a novel churn reduction algorithm based on distillation. It involves the training of a classifier by minimizing a distilled loss and solving a convex program. The authors validate empirically their approach on 12 OpenML datasets.	The paper introduces a procedure to control the churn (i.e. differences in the predictive model due o retraining) using distillation.  This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb.   Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing.  Reviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately.  All in all, this paper is ready for and deserving of acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The draft is a vector extension of [1] on studying how to approximately solve the global optima of a two-layered Relu network. The key of the analysis is to enumerate all possible sign patterns of the ReLU unit generating from specific data.The paper proposes a convex formulation for shallow neural networks with one hidden layer and vectorial outputs. A Frank-Wolfe algorithm for finding the global optimum of the resulting convex program is proposed and evaluated on smaller datasets.This paper generalizes the results of Pilanci and Ergen (2020) showing that the non-convex optimization problem corresponding to the training of a one-hidden-layer muti-output ReLU neworks can be solved using convex programming. Unfortunately in the general case the complexity is exponential in the rank of the data matrix. The notation in some critical parts of the paper is not clear and makes reading difficult.This paper showed that a two-layer vector-output ReLU neural network training problem is equivalent to a finite-dimensional convex copositive program. Based on this connection, the authors gave the first algorithm that finds the global min of the network training Problem.	This paper extends an earlier work with scalar output to vector output. It establish a relationship of two-layer ReLu network and convex program. The result can be used to design training algorithms for ReLu networks with provably computational complexity. Overall, this is an interesting idea, leading to better theoretical insights to computational issues of two-layer ReLu networks.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a framework based on principle components analysis (PCA) to speed up the missing data imputation. It divides the feature sets into two partitions -- the fully observed one and the one that contains missing values. The proposed method applies PCA to the fully observation partition to do dimensionality reduction.PCAI and PIC allow for more efficient handling of missing data. There are two use cases, fashion MNIST (imaging data) and Parkinson (voice recording) dataset. Main assumption is that introducing random missing is not acceptable without sufficient evidence and justification.The authors present a PCA based framework for missing data imputation specially efficient for high dimensional data. The experiments indicate that the imputation presents low MSE and, when applied to classification tasks, results in similar or better accuracy values.The proposed methods are named PCAI and PCA-PIC. Experiments are provided on various datasets. Weaknesses: The authors only compared PCAI strategy with the traditional strategy. "Principle Component Analysis" should be "Principal Components Analysis"	This paper proposes a framework based on principle components analysis (PCA) to speed up the missing data imputation. It divides the feature sets into two partitions -- the fully observed one and the one that contains missing values. The proposed method applies PCA to the fully observed partition to do dimensionality reduction, followed by the existing imputation methods. The authors further propose to apply PCA to the imputed data to speed up the downstream classification task.  The major weakness is that the methodological contribution is quite limited. Projecting data into lower dimensional spaces to speed up downstream tasks is not new. In particular, the main assumption of random missingness has been considered before 10-20 years ago and the more challenging setting of non-random missingness was not considered. Overall the reviewers mostly agree that the contribution is limited.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a self-supervised learning with an information maximization criterion among alternative latent representations of the same input. It considers a second-order-statistics based mutual information measure, the log-determinant mutual information (LDMI), which is equivalent to Shannon mutual information under Gaussian distribution.This paper extensively addresses the collapse problem by proposing second-order statistics-based mutual information measure that reflects the level of correlation among the inputs. In general, the paper is well written and reasonably understandable. I found the underlying theory to be very strong, however the presented experiments aren't sufficient to show the strength of the work.CorInfoMax is a second order statistics based mutual information method to avoid collapse problem in self supervised learning (SSL) It used log-determinant mutual information (LDMI) as the measure between data pairs with different augmentations. Mathematically, the paper gave step-by-step formulation.This paper presents a new self-supervised learning method based on the log-determinant mutual information proposed earlier. Experiments on three small size datasets and one medium size dataset show the effectiveness of the proposed method.	The paper describes a self-supervised learning method based on an information maximization criterion that naturally prevents dimensional collapse. The authors consider the Shannon mutual information under the assumption that the data is Gaussian. A first-order approximation to the log-determinant of the sum of two matrices is used to simplify the final objective. Experiments on 4 image datasets show that the proposed approach gives better results than contrastive and non-contrastive methods.  Strengths:  1 - The paper is well written and easy to follow. 2 - The paper is theoretically grounded on correlative information measure of representation. 3 - Strong results on some downstream classification problems. 4 - Initially the experiments included only one downstream task regarding classification, but the paper has been updated to include also results for object segmentation and detection task. 5 - Novel and well motivated. 6 - state-of-the-art SSL performance.  Weaknesses:  - Some weaknesses are pointed out by reviewer GZwK, but these are not well justified.  Decision:  A majority of reviewers vote for acceptance. The only reviewer voting slightly towards rejection is GZwK, with a reasoning that is not well justified. For example, the main criticisms mentioned by reviewer GZwK  - The paper directly generalizes the earlier proposed log-determinant mutual information to the field of self-supervised learning.  - this paper does not give a deep-going analysis that why the second-order statistics can play a important role in self-supervised learning  are not mentioned by any of the other reviewers.  Because of this, I have decided to accept the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes modeling reaching distance between any start position and any goal (subject to obstacle avoidance) with a neural network. The network is trained in a supervised manner on data obtained from a traditional search method that assumes discrete states. The usefulness of the trained value network for navigation and its generalization properties are then experimentally validated in 2D and 3D environments.This work introduces a novel scene representation for agent navigation in 2D and 3D environments. At the core of the method is an implicit neural representation of environment. The method seems to be performing similarly to baselines or slightly better, while being more efficient.The paper proposes a method to learn an environment field that predicts the distance from any location in the map to a query location. The description of the method itself is simply devoid of all required detail, such as architecture, training regime, cost function regularization, biases induced due to the discretized nature of the training data.	This paper proposes environment fields, a representation that models reaching distances within a scene. Dense environment fields are learnt using a neural network, and the effectiveness of this representation is shown on 2D maze environments and 3D indoor environments. This paper received hugely contrasting reviews, with two reviewers being very supportive and one reviewer providing the lowest score of 1. In light of this, I'll start with providing my takeaways on the review and discussion with reviewer z3Y4 (rating of 1) and then proceed to the remaining discussion.  Reviewer z3Y4 has provided the score of 1 and has made strong remarks that include: "what is proposed in this paper is simply not comprehensible", "description of the method itself is simply devoid of all required detail", "The main claims of the paper are incorrect or not at all supported by theory or empirical results." and " what is being proposed in this paper is simply too unclear and vague to be assessed". **Such dismissive remarks, in my opinion, are completely unnecessary and create a toxic discussion and review environment.**  Reviewer z3Y4 has many criticisms of the submission, but the primary ones include: (a) the lack of details throughout the paper (b) the positioning of the paper in the abstract and introduction, and (c) the lack of experiments in continuous environments. Re (a): It is well understood in our research community that providing every last detail in the main submission is nearly impossible due to the restriction on the number of pages. Providing excess details in the main paper also often reduces the readability of the paper. Such details are better addressed in the appendix and crucially, the code. The authors have provided some details in the appendix and have indicated that they will release a code base.  I also agree with the authors that justifying every last detail in the network architecture such as choice of an activation function is not necessary for this submission. The same goes with describing methods in past works in detail vs referring the reader to the appropriate citation. As a result, I believe that the authors have addressed (a) well. Re (b): This has also been addressed by the authors, by pointing out relevant parts of the paper that had the necessary details. Re (c): In this regard, the paper clearly contains a well laid out experiment in 3D indoor scenes, so as far as I am concerned, this has been addressed in the main submission.  Reviewers AhgQ and fAEP have supported this submission but also laid out some concerns that include: (1) Are the gradients suboptimal ? (2) Positioning the paper with regards to past works (3) Motivation behind using the VAE (4) Qualitative analysis and failures The authors have addressed these 4 concerns well using the rebuttal as well as via a revision of the appendix. The reviewers, post discussion have indicated their satisfaction with the revised submission.  I think this paper is interesting and proposes a novel scene representation which can be useful for others in the Embodied AI community. I am in agreement with reviewers AhgQ and fAEP, and in spite of the strong reject score by z3Y4, I recommend accepting this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors introduce a neural network architecture that has three components. A VAE is used to encode images in to two latent states. An auxiliary network A attempts to classify the face attribute y. A GAN style discriminator attempts to distinguish the decoded image from the original input image.This paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. In order to disentangle the identity information from the attributes the paper proposes adversarial information factorization.The motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.	The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \hat{y} which contains information about y. Since the encoder also predicts \hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores.   Though none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant 'z' is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work.   With the borderline review scores, the paper can go in either of the half-spaces (accept/reject) but I am hesitant to recommend an "accept" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Surreal-GAN aims to create fake pathological data with a latent variable and a healthy input. It includes an inverse function that predicts the latent variable from a fake/real pathological data. The results show that the model has a higher c-index than NMF, LDA and FA predictions.This paper proposes a Surreal-GAN method for learning representations of underlying disease-related imaging patterns. This model has overcome limitations in previously published semi-supervised clustering methods and shown great performance on semi-synthetic data sets.Surreal-GAN is a tailored version of generative adversarial networks. It learns separate representations of neurological and neuropsychiatric diseases. The method frames the diseased brains as multiple disease-related features at various severity imposed on normal brains.In this paper, the authors present a method for learning a representation of disease-related image patterns from structural MRI images. The method is based on a GAN approach, and different regularization methods on the latent representation are presented to improve the learning. The paper is well clearly organized, and the general idea of the method is very interesting.	The authors present a GAN for learning a continuous representation of disease-related image patterns from regional volume information generated from structural MRI images. The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community.   The overall objective function includes several hyper-parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well-motivated in the paper and the author response.  Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimer’s disease. Overall, the reviews are positive in majority.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is very concise, easy-to-follow and well-illustrated. The authors do a great job motivating the four representative benchmark physical systems. The flexibility of their framework allows for the integration of other learning tasks or machine learning methods.The paper is motivated by the need for a) thorough evaluation of data-driven approaches in scientific computing pipelines and b) the lack of standardized benchmarks in the literature. The key conclusion of the paper is that, even in the simplest physical models, current Data-driven pipelines, while providing qualitatively acceptable solutions, are quantitatively far from numerically integrating physical models.This paper introduced a benchmark for the methods of data-driven-based physical system simulation. The work is interesting, and important for learning physical systems. However, the value of this benchmark needs to be more explained, please see Weaknesses.	This paper introduces a physical evaluation dataset framework for scientific computing pipelines that map one high-dim state space into another high- or low-dim one, providing a suite of simple representative physics problems.  Reviewers appreciated for motivation, clarity, comprehensiveness, and overall contribution to the space.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors show that pretraining on a domain specific corpus result in performance gains across several of the tasks. They find that the domain specific pretraining is less helpful for the legal summarization task. The data and language model are accessible through HuggingFace.The authors present a new benchmark for three different types of legal tasks (classification, judgement prediction, and summarization) in Korean. Existing legal tasks are largely English-oriented. Having legal NLP tasks in a diverse set of languages is important.This work proposes a dataset (LBox-Open) containing 150k Korean legal precedents. It proposes two classification tasks, two legal judgment prediction tasks, and a summary generation task based on part of the data in this dataset. And a GPT2 model LCUBE-base is pre-trained using three datasets.The authors propose a new legal corpus for the Korean language, with a total of 5 associated tasks. They release a new language model based on GPT-2 architecture but for Korean legal language. The whole pipeline, from the preprocessing to the evaluation, lacks rigors and makes me doubt the pertinence of the contribution.The authors present a multi-task legal service benchmark in Korean for language understanding measures over judgment prediction. The main contribution is that the authors contribute a large open-source LBOX OPEN dataset in Korean. This is valuable given that legal documents are hard to harvest and annotate by humans.Large-scale corpora, benchmark datasets, and language models in the legal domain are lacking. Legal datasets are scarce, particularly in different languages than the English language. The six datasets that are introduced (and benchmarked) have the potential to provide for a significant breakthrough.	This paper presents a large corpus of Korean legal documents, paired with labels corresponding to two classification tasks, two legal judgment prediction tasks, and one summarization task. Reviewers praised the uniqueness of the new resource.  However, there is some criticism of the results section, with several reviewers bringing up possible comparisons to other models and wondering whether the performance of this model has been analyzed thoroughly.  The authors argue in particular that mT5 and KoGPT-2 can benefit from continued pre-training this corpus, which seems satisfactory, although there is the question raised by 7fpy that this is essentially pre-training on the test tasks and may be too generous when assessing the performance on other legal-domain tasks that arise down the road.  Reviewer 7fpy brings up some valid critiques of the dataset construction: several points about filtering were unclear, as was the strength of the automatic pipeline used to construct the dataset.  There are a few ethical concerns raised by the ethics reviewer, which I believe are serious. Point [1] is resolved. Point [2] about misuse seems valid and harder to argue against.  The data does have value for use in the kind of more benign studies the authors report in the response there. But its very existence could encourage the construction of certain kinds of automated tools for legal judgment prediction, which has been a hotly-debated issue in the NLP community before. Ultimately I will defer to the ethics reviewer on this one, who seems satisfied.  Overall this paper seems like a well-done effort. The main question is whether its utility (modulated by issues with the results) outweighs the risks of putting it out there. Reviewers seem to lean positive on this and I would lean positive as well.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is the first work to design a benchmark for graph neural architecture search. The search space is kind of smaller, and the datasets are only for node-level tasks. The benchmark can provide a database of GNN architectures for quick look-up.It presents its way of efficient architecture using 7 base and typical Graph Neural Network (GNN) layer. Can be very useful for many practical purposes. Not completely clear on the main point of the article which seems to be the different methods.The authors trained and evaluated different GNN architectures on nine graph datasets. The proposed benchmark is compatible with existing GraphNAS libraries such as AutoGL and NNI. The studied problem is novel, but not important/interesting. Different NAS algorithms might have different performances on different problems/tasks/datasets.The paper presents a tabular NAS benchmark for GNN architectures spanning nine graph datasets. Concerns raised from other reviewers are either not convincing to me or adequately addressed by the authors. The code that was used to create the benchmark data is not open sourced.NAS-Bench-Graph is a benchmark for graph neural architecture search (GraphNAS) Includes data points from 26K GNN architectures on nine prevalent graph datasets. Authors conducted detailed analysis such as the visualized distribution of accuracy, latency, the number of parameters, and the compatibility of NAS- Bench-Graph to existing NAS algorithms.The authors provide a framework for GNN NAS unifying the codebase hyperparameters, datasets, and evaluation protocols. They propose a new search space and conduct analysis and visualization on the searched architectures and their transferability.	This paper introduces the first NAS benchmarks on graphs. Ratings were quite diverse, with scores of 4,5,6,7,7,8. The many positive reviewers highlighted that GraphNAS is very relevant and that a benchmark for them would be very useful for the community. The benchmark includes comprehensive evaluations on as many as 9 different datasets, which may also help facilitate research on meta-learning for (Graph)NAS. Initially, the code for creating the benchmark and analysis was not available, which is a no-go for tabular NAS benchmarks, but the authors fixed this in the rebuttal period. The most negative reviewer, 8asK, appears to not be familiar with tabular NAS benchmarks and their usefulness, and did not react to the discussion about it, as well as to the modification of the paper to highlight it. The other borderline negative review, by reviewer LkTe, had as their main ciriticism that NAS methods should be benchmarked on many spaces, but the point of tabular NAS benchmarks is *not* to provide a conclusive assessment of the performance of NAS methods that holds in general across search spaces (as a "horserace" paper might try to achieve), but rather to facilitate the cheap evaluation of current and future NAS methods on a single search space. For these reasons, I do agree more with the positive reviewers and recommend acceptance of this work as a poster.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a new portfolio policy network architecture for DRl to exploit cross-asset dependency information. Such a scheme mainly introduces a permutation invariance property, and it is very interesting. The theoretical characterizations are appreciated.The paper presents a deep reinforcement learning model for portfolio optimization that harnesses both cross-asset dependencies and time dependencies. Experiments are carried out on three datasets using price data and compared to three existing methods to demonstrate the effectiveness of the proposed method.This paper proposed a portfolio policy network that has the permutation invariance (equivariance) property when treating multiple assets’ information. The proposed approach is evaluated on three sets of data from the Canadian and US stock markets. The results show that the proposed approach outperformed baselines in terms of different metrics.The paper develops the first CNN based portfolio optimization network named **WaveCorr** It is capable of capturing both temporal and cross-sectional correlation structure for the training data. By conducting numerical studies using data from both Canadian (TSX) and American (S\&P 500) stock markets, the paper testifies the superior performance of **Wave Corr** in terms of Sharpe ratio and stability.	This paper proposes an architecture of a policy network (WaveCorr) that is particularly effective for portfolio management tasks.  A key observation that leads to the design of WaveCorr is that the dependency across asset should be treated differently from the dependency across time.  The proposed WaveCorr has the property that it is "permutation invariant" with respect to assets, which means that the class of functions that can be represented by WaveCorr is invariant to permutation of assets.  WaveCorr is shown to achieve the state-of-the-art performance in a portfolio management task.  A major point of discussion was the definition of "permutation invariance".  The reviewers and AC understood the difference between the permutation invariance defined in this paper and that studied in the prior work (the output of a network is insensitive to the permutation of the particular values of the input).  With the definition in this paper, however, a fully connected layer is permutation invariant, but the Corr layer proposed in the paper appears to have more structure.  It is unclear exactly what properties of the Corr layer leads to the performance improvement.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper analyzes oversquashing in GNN using geometric methods. The paper proposes a novel rewiring method based on negative curvature. Though the theoretical contribution is strong the empirical evidence is not strong.The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature. This is motivated by a link between negatively curved edges and graph bottlenecks. The paper has a theoretical focus, but also provides a set of validation experiments.The paper provides an analytical explanation for the over-squashing and GNN bottleneck phenomena. The paper provides a thorough analysis using the proposed notations and connects it to further geometric theoretical and classical ideas. The authors show an impressive understanding and familiarity with the related and classical work.The authors work on the over-squashing effect that has been recently observed in the GCN literature. They suggest a concrete rewiring algorithm that they compare with state-of-the-art. The paper is well-written and well-organized.	The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it's impact.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The focus of this paper is mainly on BERT. Authors find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on the study, they propose a new compression pipeline named XtrmC.The paper presents an empirical study of the role of various stages in recent extreme compression of Transformer architectures for NLP tasks. Based on this study, the paper presents a simplified architecture and training process that significantly reduces model size. The paper lacks a detailed analyses of the computational complexity of the various methods.This paper provides an empirical study on the compression of BERT via distillation and quantization. The novelty of this paper is incremental since it mainly discusses the effectiveness of the strategies proposed in the TinyBERT, TenaryBERT and BinaryBERT. The paper contains valuable engineering insights, yet its presentation is largely hampered by the careless writing and lack of basic proofreading.The paper provides a thorough understanding of the low-bit (1-bit and 2-bit) quantization for pre-trained BERT. The experiments show that the compression ratio of XtrmC is much higher than the previous works.	Reviewers agree that this paper presents a systematic study on the impact of hyper-parameters and training strategies of previous works. Based on those empirical observations, they propose a simplified model with layer reduction and single-stage distillation, which do not rely on a complicated and ad-hoc training strategy. Extensive experiments are conducted with thorough comparison with existing works. Authors also clearly point-out their current limitations.  The major concern is that this paper is more focused on discussion of the effectiveness of training strategies in previous methods, while the theoretical contribution is somehow limited. It would be much better if authors could explain their observations (more training epochs is needed while additional distillation stages can be discarded) from a theoretical perspective, although this may be far out of the scope of this work. Nonetheless, this paper presents valuable empirical study over existing Transformer compression methods and may inspire following research; therefore, AC recommends acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper makes the previously unreported finding that contrastive learning produces "group disentangled" representations and outperform other disentanglement models. It offers a nice literature review, exposition is clear and provides many implementation details that should help reproducibility.The paper applies the contrastive BYOL method on a set of datasets, which are mostly well-established in the disentanglement community. Here, they show empirical evidence that BYOL successfully learns disentangled representations.BYOL learns disentangled representations with just a change of normalization method in the encoder. The work also proposes a new concept called "group disentanglement", which is a relaxed version of the original disentangling. I like the simple and clear idea (to study disentangler properties of contrastive methods) and the comprehensive experimental results.This paper makes an interesting empirical observation - BYOL representations have better disentanglement properties than current specialized methods. The selection of the normalization function affects the results. The authors also claim the dimensions are "group disentangled" although this is only shown on one dataset.	The paper provides additional empirical evidence that self-supervised learning methods can help disentangling factors of variation in a dataset. That said, the paper can benefit from better framing and perhaps comparison with existing work (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810). Furthermore, the authors acknowledge that there was a bug in their code, which I believe should at least lead to softening the claims about group disentanglement. Accordingly, please consider revising the paper and re-submitting to other venues.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work proposes a new NAS benchmark based on the results of surrogate models prediction. This surrogate model is able to predict all architectures in DARTS search space, which is about 10^18 possible architectures. The authors have made several significant improvement over the past submission.This work explored how to use surrogate models to expand the existing (and limited) neural architecture search -- NAS -- benchmark. All codes are open-sourced, which demonstrated the good reproducibility of this work. In general, this is a good NAS paper that explored a new direction.The authors propose a method to create surrogate benchmark for NAS. By modelling and predicting the performance of neural architecture in the search space, a much larget search space can be covered without expensive training. This creates a tabular benchmark which is much bigger than NAS-Bench-2.The authors present another benchmark tabular benchmark for neural architecture search. Only few architectures are actually trained while the remaining ones are imputed using a regression model. Overall, the paper is well-written but it does not contain significant contributions.	This paper proposes a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces. Certainly, the work is interesting and useful, with comprehensive studies to validate such approach. It should be credited as belonging to the first efforts of introducing and comprehensively studying the concept of surrogate NAS benchmarks. In AC's opinion, it is a solid paper that will (or has already) inspire many follow up works. The paper is well written.   This paper received highly mixed ratings. Although the authors might not see, all reviewers actually participated in the private discussions. Reviewer 1eb8 indicated hesitation in her/his support. Reviewer yTPb stated that if not considering the arXiv complicacy, she/he "would certainly raise score by one level".  AC also reached out to Reviewer yTPb about her/his mentioned possibility of updating scores, and got confirmed that her/his original opinions wasn't changing after rebuttals. Besides, AC agrees the arXiv/NeurIPS complicacy shouldn't brought into the current discussion, and ignored that factor during decision making.   The main sticking (and considered-as-valid) critique is on the relatively outdated and incomplete selection of baselines. As a benchmark paper, it should capture and diversify the recent methods. For example, the authors might consider adding: https://botorch.org/docs/papers (latest methods in Bayesian Optimization) https://github.com/facebookresearch/LaMCTS (latest methods in Monte Carlo Tree Search) https://facebookresearch.github.io/nevergrad/ (latest methods in Evolutionary algorithms)  Given the above concerns, AC considers this paper to sit on the borderline, and perhaps with pros outweighing the cons. Hence, a weak accept decision is recommended at this moment.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper focuses on injecting backdoors into a trained clean model. The authors provide a theoretical analysis to illustrate why the variations are always AWP or small weight perturbations in this circumstance. In general, this paper is novel and solid, and I recommend a strong acceptance.The paper proposes a novel logit anchoring method to improve the consistency between clean models and backdoored models. Theoretical insight gained from the paper is novel and inspiring. The presentation of the supplementary material could be improved.In this work, the authors first analyze the behavior of injecting backdoors into a well-trained clean model via fine-tuning it on a poisoned dataset. The authors propose to evaluate the consistency of the backdoor performance with both global and instance-wise consistency. They propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee.The method is extremely simple, and I think the theory is a bit extraneous. The motivation of this work seems a little tenuous, and the presentation is overly complicated. Nonetheless, the experiments do a good job of showing that their method accomplishes their goal.	This paper proposes measures of consistency between back-doored and clean models, proposes regularization using those consistency measures, and showcases that such trained models indeed exhibit better consistency. Also, it is demonstrated that the fine-tuned model does not deviate too far from the original clean model. The reviewers' comments are all well addressed. Some concerns related to the notion of consistency and how it relates to the detection of backdoors are still left open, but the reviewers seem to be satisfied with the answers. Given the overwhelmingly positive reviews, I propose accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The problem is shown to be not-submodular, and therefore the most standard methods don't quite work. The authors present a greedy algorithm to get a reasonable approximation. The stronger contribution in my view is the observation that this problem helps in the COVID vaccine design pipeline.A set of experiments suggests the proposed approach outperforms the formulations considered by the authors. The novelty of the proposed work is limited. An expert would likely be able to take the formulations in the paper and turn them into code.The paper introduces a variant/generalization of multi set multi cover problem. The aim is to maximize the weight of the elements covered at least n times by up to k overlays. The objective function is not submodular, hence does not admit a classical greedy approach.The submission is concerned with vaccine design and proposes to model the task as a combinatorial optimization problem. The authors describe two algorithms: A marginally greedy algorithm using beam search and a mixed integer linear programming approach. Experimental results show that the ILP comes close to the optimum solution.	The paper introduces the maximum n-times coverage, a new NP-hard (and non-submodular) optimization problem. It is shown that the problem can naturally arise in ML-based vaccine design, and two heuristics are given to solve the problem. The results are used to produce a pan-strain COVID vaccine.   The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews in the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques. The authors claim the proposed method has better generalization performance. The improvement on test errors does not look significant.Batch augmentation is a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample. This will increase the size of the batch by M.The paper shows that training with large batch size serves as an effective regularization method for deep networks. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch. The transform is executed by a data augmentation method such as Cutout or Dropout.	The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn’t address the reviewers' concerns, and they argue for rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Paper tackles the problem of dynamic inference where the forward graph will depend on the input data. Experiments are performed on image classification and demonstrate latency reduction of 23 to 45% depending on the hardware superiority.This paper presents a dynamic inference paradigm based on selective inference of convolutions on the spatial dimension. For each convolutional block, it uses a masker layer to predict the masked region of the feature map. Such convolution has less computation than a full convolution.This submission introduces a spatially dynamic neural network approach, developed in a latency-aware manner that can generate realistic speed-ups during inference. The proposed methodology allows to dynamically skip computation at different spatial regions of the feature maps at a coarser granularity (of blocks of pixels) This leads to more regular computation pattern and reduces the overheads, to maximise the attainable inference speed gains.	The paper proposes latency-aware spatial-wise dynamic neural networks under the guidance of a latency prediction mode. reviewers arrived at a consensus to accept the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The approach addressed an important challenge in semantic segmentation in medical images: partially labeled images. The overall performance is not so convincing as stated by the authors. It seems the improvement is marginal compared to multi U-Net.The approach is benchmarked against important baseline approaches, U-Net and DeepLabv3. The method has a lot of overlap with the DoD-Net paper so I feel the technical novelty of this paper is limited. Is it possible to publicly release (part of) the data and set up a challenge around this?The problem of having a multiple class segmentation method from partially labeled datasets is of value to the community. No current solution is satisfactory. The authors seem to ignore the multi-organ segmentation literature, where training with partial labels is performed.	While two our of three reviewers pointed out that the method is very similar to a previously published approach, DODNet,  these reviewers still see value in the extensive evaluation presented in this paper, and both suggested weak accept. The first reviewer also increased the score to borderline after the rebuttal owing to additional experiments for evaluation. I also agree with the reviewers that this paper addresses an important problem in the field, i.e.  partially labelled data, and presents extensive evaluation and benchmarking against important baseline approaches, and therefore suggest acceptance of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed learnable audio frontend (LEAF) is a generalization of a mel filterbank, used commonly in machine audition. The proposed LEAF matches the performance of competing frontends in most of the cases.The paper shows a detailed interpretation on the relationship between each component of hand-crafted audio front-ends and learnable counterparts. To do that, they followed the narratives presented from the previous works such as SincNet and improved the model. The results shows that the proposed model outperforms the comparisons for most tasks.This paper presents a new learnable representation fo audio signal classification. It is reported to yield on par or better classification results than the other methods on several of these tasks using single- or multi-task learning. The paper lacks humility in its story-telling and its style.In this work, the authors introduce a learnable front-end (LEAF) for audio. The paper evaluates it on several tasks in the audio domain such as acoustic event classification, speaker identification, keyword spotting, language identification, music classification etc. This paper is well written and easy to follow.	All Reviewers agree that the paper has a clear and solid contribution. Furthermore, all of them highlight that the paper has improved significantly after revision. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.  Pros: - Comparison across network architectures. - Comparison across a broad range of different data sets. - Compactness of the representation (few parameters to learn). - Authors will share code.  Cons: - Role of L2 normalization could be further discussed.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces two online sequential tests of equality and contrasts, for Bernoulli and Poisson process. The new tests show the speed up in decision making and reduce the opportunity cost in data collection process.This paper proposes a sequential multinomial test where Type I error is controlled through the martingale properties and the test is consistent. Then, the paper show how to apply the proposed test to test the data generated from the Poisson counting process and the Bernoulli process. Strengths: The design of thetest is technically solid. Weakness: The assumption on the parameters (line 52 to line 56) about the alternative may not be accurate.This paper proposes new sequential hypothesis tests and corresponding confidence sets and intervals for multinomial data. These tests provide "always-valid" guarantees, i.e. controlled Type I error probability under optional stopping/continuation.The paper introduces a sequential test for multinomial hypotheses using a martingale construction. It then uses this result to develop a test of equality and contrasts in inhomogeneous Bernoulli processes and time-inhomogeneous Poisson counting processes.	While there is no unanimity, the majority is positive and sees the potential value of the approach for machine learning. It is good to mention that the authors did a good job in replying to comments and criticisms, which has clarified a few misunderstandings. Personally I would recommend the authors not to use the confusing terminology between Bayesian and frequentist approaches that are employed in the paper, because it has generated more harm than benefit (at least among those in the discussion here). The theoretical results seem sound and useful and the contribution is good (even if the targeted problem might be seen as too specific by some).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors consider the problem of finding an approximate extensive-form correlated equilibrium (EFCE) of a finite general-sum multiplayer extensive- form game. They propose an accelerated version via optimism of the algorithm by Farina et al. (2019a) The paper is well written (see specific comments and the concern below)This paper proves a faster no-regret learning dynamics for extensive-form correlated equilibrium (EFCE) in multiplayer general-sum imperfect-information extensive- form games. When the game is played for T repetitions according to the accelerated dynamics, the correlated distribution of play for all players is an $O(T^{-3/4})$-approximated EFCE. To me this is a good and important theoretical contribution to the existing literature.This paper presents an uncoupled no-regret learning dynamic provably converging to the extensive-form correlated equilibrium in general-sum n-player extensive- form games. The paper is concluded by an experimental evaluation of the algorithm that shows that it is superior or performs on par with the algorithm of Farina et al. instantiated with regret matching.Previous work has identified uncoupled, regret-based dynamics that converge to extensive-form correlated equilibria in extensive- form games at a rate of O(T^{-0.5}) where T is the number of repetitions of the game. This paper provides a new uncoupling, regrets-based Dynamics that achieves O( t 0.75) It does so by combining analysis techniques that have shown such accelerated convergence in other settings with the framework used to achieve the	This paper builds upon existing works to prove that learning (correlated) equilibrium can be fast, i.e., faster than \sqrt{n} even in extensive form games.  Three reviewers are rather lukewarm, and one reviewer is more positive (but seems less confident in his score). The two major criticisms is that this paper is very difficult to read and that the results might seem rather incremental with respect to the literature.  I tend to agree with both points but the paper still as merits: the reason is that extensive form games are intrinsically way harder than normal form games and they more or less all have a burden of notations. We agreed  that the authors actually did some efforts to make it fit within the page limit. but another a conference or a journal would have been better suited than ICLR.  Our final conclusion is that the result is interesting yet maybe not breathtaking for the ICLR community; we are fairly certain that another venue for this paper will be more appropriate and that it will be accepted in the near future (I can only suggest journals based on the large amount of content and notations, such as OR, MOR, or GEB - yet, conferences such as EC should be more scoped too) . It does not, unfortunately, reach the ICLR bar.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.It would have been better to present a more detailed discussion on hyper-parameter tuning. The paper implements the work proposed in Chafer et al CVPR 2021. It will be better to avoid terms like "....mind-blowing..." in a research article.This paper does not focus on reproducing the results in a paper. It studies how to combine the approaches in two papers [1] and [2]. [2] is about weakly-supervised segmentation, which uses the attention analysis (of the classification networks) results as the pseudo label.This submission reproduces the work "Transformer interpretability beyond attention visualization’ by Chefer et al. (2021) The authors then go beyond the work by integrating the resulting relevance scores into the pixel affinity propagation framework of Ahn and Kwak (2018) Although the experiments do not surpass the state-of-the-art, this experiment shows promising preliminary results.	An interesting contribution with relevant results. Some more exploration of hyper-parameter tuning could be a good contribution, and a rephrasing of certain ways of describing results (see reviewer 9G6p's comments).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in "Technical Approach" is quite minimilistic and scarce in details.The paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results.	As both reviewers stated, this paper is a good fit for the workshop, with several positives.  I am not repeating all of them here, since I agree with the reviewers' assessment comments. From the a given a natural language instruction and an input and an output scene, the paper investigates how to  train a neuro-symbolic model which  manipulates a program that can be executed by a robot on the input scenes and generates a goal state. The authors carry out experiments to demonstrate how the neuro-symbolic model is end-to-end and show generalization to novel scenes and instructions.  I recommend acceptance of this paper, as it can lead to relevant directions in neuro-symbolic robotics.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The main strategy is to obtain a low rank approximation of the gradient update and then use Riemannian gradient descent. This approximation allows the proposed method to be essentially a factor of O(n) faster than its competitors. The paper is clear and well written.The paper studies the acceleration of unitary neural network training. The paper is well-motivated and clearly-written. The proposed methods are simple-yet-effective, and the theoretical results are strong. Comparison with prior works looks comprehensive.The paper present two efficient update rules for neural networks with unitary matrices parametrized as full n x n arrays. The methods are based on the hypothesis that the training gradient w.r.t. the unitary matrix can be well approximated as a low-rank matrix.The authors propose two optimization algorithms for training deep networks with unitary weight matrices. They extend their approach to unitary convolutional operators. Unlike existing approaches, they exploit the low-rank structure of gradients that is a consequence of the relatively small batch sizes used in SGD-like optimization.	This paper provides two routines to replace gradient updates with low-rank unitary updates, and provides extensive technical discussion and experiments.  Reviewers are uniformly positive, and I also voice similar praises, e.g., I too appreciate the extensive discussion in appendices A and B, and the detailed experiments in the later appendices.  As such, it is easy to recommend acceptance, and I will push for this to receive at least a spotlight.  Even so, I urge the authors to make careful revisions for remaining issues raised by the reviewers, and to perform a full pass of their own.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides new algorithmic and complexity results for the problem of computing envy-free solutions to the fair division problem. Here, n agents must each be matched to one of n rooms and split the total rent for the n rooms among them.The paper focuses on the case where there is uncertainty in the valuations reported by the agents. The paper introduces what they call the lexi-slack solution which remains envy free with as large a radius as possible from the agent valuation reports.The paper studies the problem of robust envy-free rent division. In the conventional problem variant, given n individuals and their evaluations for n rooms, the goal is to find a rule to allocate the rooms and fairly split the rent such that everyone is satisfied.This paper proposes three robust approaches for the envy-free rent division problem. The robustness and the computation times of the proposed approaches are demonstrated on user data collected from Spliddit. The models proposed in this paper are very practical.	Reviewers are all positive and excited about the paper: interesting and natural model, novel and robust mechanism with theoretical guarantees, nice sample complexity analysis, experiments on real-world data.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a learning framework for compositional representations of goals for goal-conditioned RL. The goals reside in a low-dimensional representation space that is obtained from high-dimensional sensory data. The paper shows favorable performance over several baselines on multiple environments.This paper proposes to use discretized self-supervised representations for goal encoding in goal-conditioned RL. Specifically, the paper proposes using a codebook of L discrete codes to encode each of G chunks of a state/goal representation vector. The paper evaluates this method on a variety of environments.The paper presents an approach to improve the performance of learned goal-conditioned policies, by discretizing the goals using Vector Quantization. The method is simple to implement in existing architectures and uses established approaches that have been developed for some time now. The paper is overall very well written, and the ideas and results are communicated clearly.	This paper proposes a discrete and compositional representation of goal states for goal-conditioned RL. The idea is to learn a goal representation via self-supervised learning and discretize the learned representation via VQ-VAE, and finally use the learned goal representation for goal-conditioned RL. The proposed method improves performance on several goal-conditioned RL benchmarks.  All of the reviewers found the idea simple and reasonable, and the results on a variety of benchmarks are quite comprehensive and strong. Although there were concerns around why the proposed discretized representation forms a semantically meaningful latent space and where the improvement comes from, the authors addressed them during the rebuttal period with updated results. All of the reviewers became in favor of the paper as a result. Thus, I recommend accepting this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method to achieve binary and ternary quantization for recurrent networks. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers.This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs.This paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering.	This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors' rebuttal addressed the reviewers' concern nicely.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots.The paper uses three techniques to "sparsify" a dense network over training. These techniques are: non-persistent pruning, soft thresholding, and increasing the pruning ratio linearly while training. The paper shows that these three techniques can achieve good accuracy at a high sparsity levels on ImageNet and CIFAR-10.In this paper, the authors proposed a training algorithm for sparse deep neural network. Their key idea is to decouple the forward and backward paths. In this way, some pruned weights have possibility to be re-activated.The paper proposes a single training cycle method to train sparse networks. It adopts a soft-threshold pruning strategy and progressively increase the sparsity ratio of the network along the training iterations. Experiments show its advantage.The proposed method allows for one-cycle unstructured pruning that facilitates rewiring during training. The paper is in general easy to follow. The authors show SOTA performance for the high sparsity-regime on ResNet-50 for Imagenet.	This submission proposes a method for learning sparse DNNs which consists of three components: First, a "dense" network is maintained and updated in each backwards pass, but the forward pass is done via a sparsified version of the network; sparsification is done via "soft" thresholding; and the sparsity ratio is increased over the course of training. Reviewers noted that each of these components had been previously proposed, and that the state-of-the-art baselines are not actually state-of-the-art anymore. They also noted that the paper read more like a draft and needs substantial improvement. The consensus was therefore to reject.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Paper proposes a framework for problems where the output has some validity constraints, for e.g. the output must be a valid python program that must compile. Such problems arise naturally in settings such as pseudocode to program, and there are many more unlabelled valid programs that are easily available.The authors propose a more data-efficient way to train generative models with constraints on the output. For the SPoC task they show an improvement of 3-5% over a simple transformer baseline. I believe that this is an interesting idea, and practically useful in the cases where data is sparse.The paper introduces a “predict-and-denoise” model for structured prediction. This framework allows leveraging of unlabelled output data to train the denoiser. The paper demonstrates the performance of this model on two tasks - font image generation and pseudocode-to-code translation.	This work is well written and easy to follow and proposes a novel framework to utilize unlabeled output data. The authors have also given a detailed proof that the denoiser reduces the required complexity of the predictor. However, ultimately the experimental results are somewhat weak and leave doubts as to how effective the approach is. More convincing experimental results such as significant improvements on a well understood task and acknowledging that the approach is mostly useful when combined with pre-training and back translation would improve the work.  Pros - Well written. - Technically novel approach to the problem of utilizing unlabeled output data. - Interesting proof on the reduced complexity requirement for the predictor.  Cons: - Experimental results are not convincing. Showing significant improvements on a well understood task would be more convincing. - The approach is only really useful when combined with pre-training or back-translation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The main issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data. The authors offered to contact the developers of the publicly available models they are using with the idea of asking for information on the training data used for these models.This paper presents a large-scale and curated dataset in Chinese along with two benchmarks for diagnosis (5 tasks) and pathology mitigation (2 tasks) The authors designed a thorough annotation process including data collection, training annotators in the pre-annotation phase and quality control by the feedback loop. Large language models in Chinese can benefit from training on this dataset to mitigating erroneous sentences.The authors introduce the TGEA 2.0 dataset which is a Chinese dataset where the examples are generated by various pretrained language models. The dataset has been annotated such that the machine-authored texts can be assessed on various tasks within the broad categories of diagnosis tasks and pathology mitigation tasks.This paper proposes TGEA 2.0, the largest dataset for diagnosing typed errors made by pretrained language models. It is an extended version of T GEA, with various large language models and downstream tasks.This paper contributes to understanding and reducing the text generation errors made by large pre-trained language models. Authors have created the largest Chinese language dataset of machine-authored texts. A substantial subset of the texts were manually annotated at a fine-grained level and corrected for text quality issues.The work builds on by releasing a larger and higher quality version of a previously released dataset named TGEA. It is a comprehensive collection of machine authored texts in Chinese language that have been annotated for errors based on a novel ontology of errors. The work goes on to validate whether errors found can be fixed automatically with pre-existing large language models.	The reviewers all liked the paper. The authors' response clarified most points raised by the reviewers. In view of that, the authors are strongly invited to take the feedback on board for the final version. The main ethical issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data. Data cards need to be added to the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies distributed learning in the presence of Byzantine workers. The writing of the paper could use some proofreading. Theorem 1 and Theorem 2 leave something to be desired. The plots corresponding to the experimental results are too small.This work proposes a method for Byzantine learning in a parameter-server setting using asynchronous updates. They provide theoretical guarantees and some experiments on small-scale tasks. The motivation for avoiding storing instances on the master node is not well fleshed out in my opinion. Even with iid data and fully synchronicity, the algorithm still does not converge to a stationary point.The paper proposes a practical asynchronous stochastic gradient descent for Byzantine distributed learning. The server temporarily stores gradients on multiple  (namely $B$) buffers and performs a proper robust aggregation to compute a more robust from them.BASGD uses buffers to perform asynchronous Byzantine learning. In each SGD step, all workers compute gradients and send them to the server. When all of the buffers are updated, the server performs an model update. In this case, BASGD resembles SBL with larger batch size but at cost of tolerating less Byzantine workers.	The authors present BASGD and asynchronous version of SGD that attempts to be robust against byzantine failures/attacks.  The papers is overall well written and clearly presents the results. Some novelty is present as there have been limited work in asynchronous algorithms for byzantine ML.   However, there have been several concerns raised by the reviewers, on which I agree, and they have not been fully addressed: 1) the tradeoff between asynchrony and robustness, as BASGD cannot handle the case of a buffer being straggler, which limits some of the novelty in this work 2) issues with the definition of privacy leakage has not been fully addressed 3) some reviewers mentioned the theoretical results being of limited importance, but arguably this is true for other related work in this area. Perhaps a general criticism is valid as to what is the operational value of the proposed guarantees. That is convergence does not exclude a model that has undesirable properties, eg has bad prediction accuracy for a small subset of tasks. 4) Finally, the motivation of the system model of the paper ( eg storing gradients as opposed to instances) paper is of unclear practical relevance, as was raised by multiple reviewers.   Overall the consensus was that the paper does have merits, however, some of the most major concerns were not properly addressed. This paper can potentially be improved for a future venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies post-training quantization, and propose to leverage inter-layer dependency. The authors introduce a number of approaches: AR, ASoftmax, improved initialization and AMixup, and verify the effectiveness of each part accordingly.This paper proposes a Network-Wise Quantization (NWQ) approach. NWQ arises overfitting and discrete optimization problems. To solve the combinatorial optimization problem, the authors use Annealing Softmax and Anealing Mixup.This paper proposes a post-training quantization (PTQ) technique that aims to search for a network-level quantization policy. The insight of this paper is straight-forward but very sensible. The proposed method seem to contain too many hyper-parameters.The authors present several training strategies based on the previous Post-training Quantization (PTQ) methods. They conduct thorough ablation studies and experiments on several mainstream networks to fully verify the superiority of the proposed methods.	This paper studies post-training quantization by proposing Network-Wise Quantization (NWQ) an end-to-end quantization approach that takes into account relationships between layers rather than treating layers independently. Using this approach, the paper demonstrates compelling empirical gains across a number of architectures and compression factors. Reviewers recognized the practical success of the approach as demonstrated by these empirical results and praised the clarity of the manuscript. However, there were concerns regarding the novelty of the approach and whether the proposed method is simply a composition of previous methods. While I understand these concerns, I think there is a significant delta between this work and previous approaches, especially when taking into account the markedly improved performance and the challenges of determining how to apply these lines of thinking to end-to-end training. The authors also expanded their discussion of these works in their updated manuscript, clarifying the differences. There were also concerns regarding the hyperparameter tuning, but the authors clarified in their response that the large majority of experiments used a constant set of hyperparameters, suggesting that these results are not simply the effect of tuning. Altogether, I think this paper makes an impactful contribution and will be a valuable addition to the conference.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work is about layer-wise training of networks by way of optimizing the IB cost function. The paper attempts to overcome this problem by using a noisy version of the latent compression. Not too surprising, this additive noise works as a ridge-type (or weight-decay) regularizer.This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance. The paper itself does a very good job of citing recent relevant work. While overall the writing quality of the paper is high, the paper itself is a strong rejection.This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE) By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IBfunctional training for DNN which is shown to have better prediction accuracy.	This paper does two things. First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+ε), where ε is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X. Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L) − βI(X;L+ε), layerwise instead of using cross-entropy and backpropagation. Experiments on MNIST and CIFAR-10 show improvements for the layerwise training over cross-entropy training. The penalty on I(X;L+ε) is described as being analogous to weight decay. The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong. In the AC's opinion, R1's critique that "[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?" is critical, and the authors' reply that "this quantity is in fact a more appropriate measure for “compactness” or “complexity” than the mutual information itself" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information. The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise. Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers' receiving several reminders that the discussion is a defining feature of the ICLR review process.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper shows that although learning from label comparisons does not benefit the sample complexity in the passive learning, label-comparison-based query can improve the query complexity in active learning setting. The studied problem is very interesting and the proposed approach is technically sound.The work needs to be motivated better. The use cases of the findings need to be outlined better. Figures are not necessarily close to their description. Algorithms are referenced in the text as a whole, without a line by line clear cut descriptions.Theorem 4.3 shows that in the 1D setting the $\arg\max$ oracle has the same complexity as the label comparison oracle. Theorem 5.2 for the active pool-based setting shows that the sample complexity of label comparison is actually smaller than the lower bound.	Meta Review: Generally, the idea of querying the label comparisons to is an interesting and natural choice to enable active learning in real-world tasks. Both theoretical analyses and empirical studies have been reported in this paper. The whole paper is well organized and easy to follow.  The expeirmental studies performed in this paper can be improved, such as considering performance comparison in passive learning, the inclusion of more SOTA baselines, etc.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Counterfactual Generative Network (CGN) can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism, i.e. independent factors in the structural causal model of the data. CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting.This paper proposes a new generative model that generate images from 3 seperate aspects: masks (shapes), forground texture, and backgrounds. By doing so, they can vary each aspect individually without changing other aspects, enabling the model to generate counterfactual images.The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al. in Causalgan: Learning causal implicit generative models with adversarial training, ICLR'18. The causal structure that is used in the generation of data is not different than a conditional GAN. The authors should definitely cite this work.The proposed approach is motivated by the assumption of independent mechanisms where different modules of the causal data generating process are independent of each other. Once the decomposition of a training image into shape, texture, and background is obtained, any component can be swapped.	The paper presents a "conceptual  advance connecting causality, disentangled representation learning, invariant representations and robust classification". Authors propose to decompose the image generation process to independent mechanism that can be composed (foreground masks (shapes), forground texture, and backgrounds), allowing for a specific image to generate counterfactuals , by changing some variations factors, while keeping other fixed. One can use interventional data to augment classifiers, this can lead in certain cases to improvement in accuracy and in other in improving the robustness.   There was concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples ) and some missing references. The rebuttal of the authors and their updated paper reflected comprehensively all those concerns and addressed them, highlighting limitations of the method and adding more examples of its failures.   I liked the ideas and concepts in  this paper , and it will be exciting to generalize such generative approach to other domains, this  work is a first step. I think it will be good addition to ICLR program
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed that three data augmentation methods (along with different types of regularization losses) can be applied to boost the performance of existing deep neural network models for knowledge tracing. The experiments are rather extensive, i.e., four widely-used datasets were employed in the experiments.The authors show that various forms of augmentations can improve the performance on knowledge tracing. The experiments are conducted on ASSIST2015, ASSISTChall, STATICS2011 and EdNet-KT1. Data augmentation leads to a certain amount of improvements, but consistency training provides more significant improvements.The method for the most part makes sense. The experiments are reasonably thorough (4 benchmark datasets are tested) and non-trivial accuracy gains are demonstrated. The authors say question is chosen as a replacement if it has some skill overlap with the original question (page 4)The paper investigates how well data augmentation can help improve the performance of contemporary deep learning models for Knowledge Tracing. Disclaimer: I am not familiar with the educational AI field. With strong argumentation, or if one of my co-reviewers is an expert in the field, I could be persuaded to change my score.	The paper proposes new techniques for improving the generalization ability of deep learning models for Knowledge Tracing (KT). Instead of designing more sophisticated models, the paper investigates simple data augmentation techniques that can be applied to train existing models. In particular, three different augmentation strategies are proposed based on replacement, insertion, and deletion in the training data. These strategies are then applied with appropriate regularization loss ensuring consistency and monotonicity in the training process. Extensive experiments are performed using three popular neural models for KT and four publicly available datasets. Overall, the paper studies an interesting problem in an important application domain of online education. The results are promising and open up several exciting follow-up research directions to explore more complex data augmentation techniques for KT.  I want to thank the authors for actively engaging with the reviewers during the discussion phase and sharing their concerns about the quality of the reviews.  The reviewers generally appreciated the paper's ideas; however, there was quite a bit of spread in the reviewers' assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers' feedback to better position the work w.r.t. the existing literature on data augmentation and state of the art results, better motivate the data augmentation strategies in the context of educational applications possibly through additional data analysis, and add more ablation studies w.r.t. the hyperparameters associated with data augmentation. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers' feedback when preparing future revisions of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Authors study the problem of finding (learning) the best neural network initialization. They propose GradCosine - a measure of fitness for neural networkInitialization. This measure can then be optimized using and iterative procedure denoted as NIO.The paper first introduces a quantity, the cosine similarity of sample-wise local optima to evaluate the model performance at the initialization. They theoretically proved that their proposed quantity is the upper bound of both the training and generalization error under certain assumptions.This work proposes a new initialization method for neural network. The authors first use Fig. 1 to illustrate drawbacks of the sample-wise local optima density. Extensive experiments are conducted to verify the efficacy of this approach.This paper works on the initialization of neural network parameter based on theoretically inspired optimization algorithm. It is to substitute the previous network initialization algorithm, such as Kaiming's method, [52], etc. The proposed approach is closely related to [52] by introducing the cosine similarity of sample-wise gradient for optimizing the initial network parameter.	The paper introduces a new procedure to initialize the optimisation in training process of DNN models, including the recent ViT architecture. All the reviewers recommend acceptance and appreciate the promising empirical results backed by the strong theoretical foundations. AC recommends acceptance as well.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new type of generative models with a new inference method of latent variables. Based on this, the authors generalize the propose model to implicit and variational versions. The proposed method is easy and straightforward to implement.The paper proposes GONs which seek to build a generative model with an “implicit” encoder. The main idea being that existing generative models with an encoder are “redundant” in that the decoder itself has the ability to compute the gradient.The paper is very-well written and interesting. The method seems to be getting very good results,. Still, the paper seems to have been rushed. The results are only on small scale and toyish datasets, and there are very few baselines.	This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns.   Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper pursues the research question of how to evaluate representation learning for echocardiographic data (ultrasound images of the heart) The paper shows how to build representation learning benchmarks for many tasks of interest by remixing 3 open-access datasets. Strengths are: Tackles an important problem for an exciting application area (echocardiography)The paper proposes a benchmark approach for representation learning of visual data in cardiac ultrasound. It provides 25 adaptation benchmarks in which a source task is performed on a source dataset, and then learned representation is employed for a target task on a target dataset. The proposed benchmark suite captures a degree of adaptation on multiple datasets.This paper provides an echocardiographic task adaptation benchmark for the various clinically-relevant task using publicly available datasets. This paper shows powerful usage in specific clinical tasks like Cardiac struct identification, estimation, and clinical predictions. Translating the clinical evaluation protocols into ML-friendly tasks will bridge the gap in deep learning applications in the medical domain.This is a very well written paper and I believe that this benchmarking setup can later be used for other medical specialties. The use of AI in medicine requires a higher threshold of security and testing than other use cases because the difference between right and wrong output can mean life or death.The work presents a benchmark suite of tasks for evaluating the performance of learned visual representations for use in echocardiography. This benchmark enables evaluating a number of backbones and other representation learning approaches. The paper proposes an evaluation strategy that covers a large number of model configurations.	The consensus among the reviewers was that this work covers an important topic and a broad number of tasks. There were concerns about the documentation, reproducibility, and accessibility of the dataset. But the authors have done a good job in addressing most of these concerns with documentation, an API, and example notebooks.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a new graph-based Twitter bot detection dataset. TwiBot22 is significantly larger than other similar datasets, with almost 1 million users. The authors use an annotation method by employing several human annotators and using this manual annotation in a weak supervised manner.The paper tackles the problem of bot detection on Twitter with graph-based methods. The dataset released is heterogenous with 4 different node types and 14 different relation types. Extensive experiments are conducted with a range of state of the art methods, including graph neural networks.This paper proposes a new graph-based Twitter bot detection benchmark dataset. The authors evaluated the proposed dataset, along with 8 more existing datasets, using 35 twitter bot detection baseline algorithms. This dataset appears to be a direct extension of a previous dataset “TwitBot20”[1]This paper utilizes a graph based approach to create the largest Twitter bot detection dataset. The primary strength of the paper is the dataset which contains over 100,000 bot users. The authors perform extensive evaluation using latest methods and re-implemented previous baselines.Twibot-22 is a heterogeneous information network, including 4 entities and 14 relation types. It also uses a weak supervision learning strategy to generate high-quality annotations. Extensive experiments are conducted, and the results provide insight into future work.The paper provides a new, larger and more comprehensive dataset for Twitter bot detection. It benchmarks a large number of existing methods on the new dataset and 8 others from the literature. There are also some experiments on the importance of graphs in several models.	In this work, the utilize a graph based approach to create the largest Twitter bot detection dataset, with over 100K bot users identified. The dataset is of great use for the social media mining community in many different aspects. The work was greatly improved after a very interactive and fruitful discussion period, making the contribution a lot more refined and useful.  Pros: - Largest dataset available. - Thorough evaluation - Addition of most reviewer's suggestions have strengthen the details in the paper  Cons: - Minor statistical rigor elements are missing, but not completely necessary.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present an image dataset of everyday household items from homes around the world. The inclusion of lower-income households and geographic regions not typically represented in large image datasets is the biggest strength of this work. The lack of standardization in the photos (regarding angle, lighting, etc), potentially very few examples of a single type of an object may not make this a very suitable dataset for training machine learning algorithms.This paper presents the Dollar Street Dataset which includes images with geographic and socioeconomic diversity. A variety of different image recognition models are evaluated using the dataset. The main finding of the paper is that, without fine-tuning, models do substantially worse on images which are linked to lower socioeconomic status.The paper addresses the well-known problem that widely established machine learning datasets have a significant bias towards higher-income countries. It is based on a pre-existing image database collected by the Gapminder foundation, which was pre-filtered based on formal factors.The dataset includes 38,479 images of 289 common household objects photographed from 404 homes across 63 nations. Each image has full demographic information such as global region, country name and monthly income. The raw data for this dataset comes from the Gapminder foundation.The presented paper introduces a new image dataset presenting household items with their respected income value. The dataset contains a new collection of images. Their income tag is estimated by the country income for this demographic group. Afterwards some comparisons with fine tuned networks have been done.DollarStreet is a dataset that of everyday household items from around the world with a variety of countries and incomes. The paper addresses the issue of an existing amerocentric and eurocentric bias in widely used datasets leading to bias in many machine learning applications.	The AC has taken into account all strengths and weaknesses mentioned by reviewers in making a recommendation for this paper. Here is the summary of what was considered in carefully considering a final decision:  * Reviewers agree that the presented resource, the Dollar Street dataset, addresses a need for models that can be held more accountable in terms of geographic diversity and demographic diversity.  * There is generally no issues in terms of methodology or soundness of the proposed resource. * Unfortunately, experiments on demonstrating the usefulness of this resource are limited, and largely replicate what was already found in Devries et al. 2019, a workshop paper at CVPR from three years ago which also used the same source of data -- and at the same scale. The analysis in this previous paper in some way go further by matching image categories using human evaluators individually for each image -- as pointed out by reviewer dcyB. * The dataset was downloaded from a third party's website and as such the authors had no control in how the images were collected, nor there was any effort to enhance this resource in terms of annotations, diversity or size. * The dataset size is 38k images which is a rather small dataset for training or enhancing a model, and its use would be limited mostly as a benchmark dataset. As a benchmark however experiments seem limited as pointed by reviewer eAoC. An experiment on training on this dataset and evaluating on this dataset is performed only with a Resnet network, however it is hard to assess what is the significance of this experiment.   There are various suggestions made by reviewers below that would enhance the quality of this paper. Despite shortcomings, reviewers are still enthusiastic to see this dataset in an easy to use format as a benchmark. I also strongly suggest the authors to revise their section on licensing where it says "Training machine learning models on public domain work is widely accepted legally." as it does not seem appropriate for this paper to be making determinations or recommendations as to what is considered legal or not.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper considers standard linear algebra problems with two twists on the input. It is a dynamic setting where, at every point of time, exactly one of the matrices undergoes arbitrary changes. They show that in this dynamic setting, by maintaining a binary tree of sketches of tensor products, their algorithms can achieve runtimes $q$ times better than the naive algorithm.This paper studies a lightweight computation of a special case of linear regression. It considers the case where one of the factor matrices temporally changes. The authors propose a tree-based data structure to maintain the sketch of the data.This paper studies a dynamic version of the problem, where at each time stamp some matrix $A_i$ gets updated. The goal is to quickly update the tensor product and also quicklyUpdate the solution $x$ (without recomputing it)	My main concern, that was addressed by the reviewers and was not answered by the authors, is that the improvement is q times faster algorithm for an algorithm that takes time exponential in q. In addition, the missing experimental results makes this a very theoretical paper.  Still, I recommend to accept the paper due to the significance of the problem, and conditioned on the promise of the authors to update the requested changes in the final verison.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors have proposed a well-motivated contrastive learning method for solving goal-conditioned RL tasks. The proposed method simplifies the representation learning + planning in RL problem, by using the inner product of learnt representations as a correlate for the Q-value function.This paper proposes leveraging contrastive learning to action-labeled trajectories. The learned representation will connect to the goal-conditioned value functions. This work may have the potential to benefit the RL, representation learning, and robotics communities.The paper presents a novel view of how contrastive learning can be used by itself, and without any additional RL training on top, to learn goal conditioned policies from pre-collected data. This is achieved by using the similarity learned as a Q-function, that is then used to improve a policy in the policy gradient step. The authors well motivate their approach, compare it extensively with related work on learning goal-conditioned policies, as well as works that use an additional representation learning	How to design RL algorithms that directly acquire good representations? This paper gives an answer that contrastive representation learning can be cast as a goal-conditioned RL using the inner product of learned representations. The technical novelty of this paper is sound, with the thorough theoretic motivation of the proposed method and solid experiments. The presentation of this paper is also satisfactory. All the reviewers provided positive feedback on this paper. I also enjoy reading this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper considers open-world SSL settings where the model recognizes previously seen classes, and detects novel classes which are not present in the labeled dataset. The method contains three losses to train a model in this setting.The method includes a self-supervised pretraining step and a finetune step. The supervised loss, which is the main contribution of the method, overcomes the imbalance problem caused by BCE. The authors validate this loss with empirical results.The study is well-motivated and tackles an important problem that would occur in real-world applications. The design of the proposed method seems reasonable, and it works well in the experiments with several datasets. Some points remain unclear or are not convincing, which makes my score a bit conservative.The authors propose Open World Semi-Supervised Learning (ORCA) The method learns to classify previously seen classes in the labeled data and novel class in the unlabeled data. The proposed method is end to end and achieves significant improvement on ImageNet dataset compared to baseline methods.	This work addresses a novel and important real-world setting for semi-supervised learning – the open-world problem where unlabeled data may contain novel classes that are not seen in labeled data.  The paper provides an approach by combining three loss functions: a supervised cross-entropy loss, a pairwise cross-entropy loss with adaptive uncertainty margin, and a regularization towards uniform distribution.    The authors were responsive to reviewers’ comments and have respectively improved their paper by adding experiments, including an ablation study of each component of the objective function, study of the effect regularization on unbalanced class distributions, reporting accuracy on pseudo-labels.  While two reviewers have slightly increased their scores, some concerns still remain.  This is a borderline paper, and after some discussion and calibration, we decided that the work in its current form does not quite meet the bar for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work claims to propose a general methodology for approximating offline algorithms in online settings. To achieve this, the author prosed a multi-tasks-based method to learn from the datasets created by the offline algorithms. The motivation of bridging the gap between offline algorithms and their online counterparts is clear and practical.This paper makes use of **offline algorithms** (i.e. algorithms that can view entire time-series) to produce outputs which are used to train an online algorithm. The online algorithms are not trained to produce the outputs of the offline algorithm directly. Instead windows of the outputs are mapped to class labels using a hand-crafted mapping specific to the domain.The paper presents a novel method for approximating offline time-series algorithms in an online setting. Results on one-dimensional synthetic and stock-market data show that the predictive behavior of this method matches our intuitions, where it is most accurate when explaining the data and least accurate when predicting into the future.This paper considers the problem of an offline algorithm that operates on a time-series X to obtain sequence of decisions in an online setting. They pose this as a multi-task learning problem, where they slice the input into windows of size d. The goal is to map each d dimensional window to one of the k possible structures in the dataset. They propose a MTL algorithm and use simulations and real-world stock market data to study the effects of their approach.This work studies a methodological framework to transform/approximate offline algorithms into their online counterparts. The main methodology is to predict an offline algorithm’s actions in the real time future via learning behavioral structures of the offline algorithm. The work presents several experiments using both synthetic and real (stock market) data.	## A Brief Summary This paper uses offline algorithms that can see the entire time-series to approximate the online algorithms that can only view the past time-series. The way this is done is basically, the offline algorithm is used to provide discrete class targets to train the online algorithm. The paper presents results on synthetic and historical stock market data.  ## Reviewer s1H9 **Strengths:** - Practical problem. - Novel approach. - Clear presentation. **Weaknesses:** - No other baselines. - No theoretical guarantees behind the approach. - Writing could be improved.  ## Reviewer EgW9 **Strengths:** - Clear writing. - Interesting research direction. **Weaknesses:** - The primary claim seems incorrect and unclear.  - Due to the unclarity about the primary claim of this paper, it is difficult to evaluate the paper.  - Lack of baselines. - The lack of discussions of the related works.  ## Reviewer gii5 **Strengths:** - Interesting and novel approach. **Weaknesses:** - Difficult to evaluate, with no empirical baselines or theoretical evidence. - The datasets used in the paper are not used in the literature before. Authors should provide experimental results on datasets from the literature as well. - The paper needs to compare against the other baselines discussed in the related works. - More ablations and analysis on the proposed algorithm is required. - Unsubstantiated claims regarding being SOTA on the task, since the paper doesn't compare against any other baselines on these datasets. - The paper can be restructured to improve the flow and clarity.  ## Reviewer zoKR **Strengths:** - Novel and interesting research topic. - Bridging classical algorithms and ML. - Clearly written.   **Weaknesses:** - Lack of motivation for the problem. - The approach only works with offline algorithms that work on time-segmented data.  ## Reviewer aaFn **Strengths:** - Novel algorithm.  **Weaknesses:** - Potentially overfitting to the offline data. - Data hungry approach. - Confusion related to the occurrence moments of predicted future actions. - Section 2 is difficult to understand.  ## Key Takeaways and Thoughts Overall, I think the problem setup is very interesting. However, as pointed out by reviewers gii5 and EgW5, due to the lack of baselines, it is tough to compare the proposed algorithm against other approaches, and this paper's evaluation is challenging. I would recommend the authors include more ablations in the future version of the paper and baselines and address the other issues pointed out above by the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed a method to decide when to ask for expert’s help to improve the task performance of embodied agents. Instead of using heuristics, e.g. model confusion, or expanding the action space of the embodied agent, this paper learns a separate policy. The experiments on RoboTHOR show that the proposed method can achieve higher success rates using fewer expert queries.The ask4help policy first measures the agent's uncertainty at finishing the given task based on its internal belief state using a pretrained Success Prediction Model (SPM) Then, the ask4Help policy decides if it should ask for the expert's help (i.e. the next action to do in the environment) or use the pretrained embodied agent's predicted action. Experiments were conducted on two different environments: the RoboTHOR Object Navigation and the Room Rearrangement inAsk4Help learns a base policy-agnostic approach for learning when to ask for help. Formalized as a separate policy that takes in a set of user preferences, the “belief” (hidden state) of the embodied base policy.This paper introduces a method, Ask4Help, for incorporating expert knowledge for embodied AI. The switching policy is trained by RL given  the reward from both success rate and the portion of expert knowledge used in the episode. The resulting human-in-the-loop policy achieves performance improvement on several common embodied AI challenges.The paper introduces Ask4Help, a method for augmenting an existing policy with the ability to fall back to an expert policy during an episode. This is achieved without retraining the existing pre-trained agent by introducing a meta-controller that will select whether to follow the agent or the expert at every timestep. The proposed method is evaluated on two tasks, namely object navigation and room rearrangement.	I thank the authors for their submission and active participation in the discussions. This paper introduces a method for learning a policy that can ask an expert for help, i.e., to obtain the expert action. On the positive side, reviewers found the method to be general [uya8], original and significant [gw2r], intruiging in terms of being able to reuse an existing policy [HGan], and tackling an important problem [rsmr,bRWC], and the paper to be clear [uya8,gw2r,rsmr,bRWC]. In terms of negative points, reviewers were concerned about the novelty [bRWC], unimpressive qualitative results despite strong quantitative results [bRWC], and issues with the range of baselines [bRWC,rsmr] and ablations considered [uya8]. Overall, the paper is borderline. However, bRWC indicated they would raise their score but I don't see this being reflected. Furthermore, in my view reviewer rsmr's concerns regarding baselines and ablations has been addressed by the author rebuttal. Thus, I am siding with reviewers gw2r and HGan, and recommend acceptance. However, I very strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular the points raised by reviewer bRWC regarding the importance of the Success Prediction component of the method.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studied developing robust algorithms in presence of an attacker who can corrupt a portion of the training set in the classical version space learning scenario. In the realizable scenario, the paper provided impossibility result that shows if the attacker can perturb eta fraction of training samples, the test accuracy cannot be lower than eta*d with high probability.This work shows that under the realizable case the adversarial risk bound is tight in terms of corruption fraction and VC dimension of the hypothesis set. Under agnostic setting, for any deterministic learner, a lower bound is present.This work studies learnability under a targeted poisoning attack. It is fundamentally based on the ensemble-based robustness proposed by Levine and Feizi [2020] The writing overall was clear, the number of typos was a problem.The paper studies the possibility and impossibility of learning from poisoned data. The authors prove matching upper and lower bounds, up to a multiplicative constant. The paper is well-organized, but there are many typos which make the reading challenging at times.	All reviewers agree that this paper comprehensively studies a fundamental question of PAC learning under instance-targeted poisoning, including the study of realizable, agnostic, deterministic learning settings; overall this paper makes a nice contribution to the field of robust machine learning.   The authors are strongly encouraged to incorporate the comments by the reviewers, including revising on motivating examples, terminologies, etc.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proses a new adversarial (poisoning) defense based on a known graph reweighting scheme known as the ricci curvature. It is not clear to the reader if the sampling scheme/training scheme is the major reason for adversarial robustness. The paper lacks clarity at some points and has inconsistencies in notation.The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems.In Ricci-GCN new graphs are resampled in each iteration of the training phase based on the Ricci flow metric. Compared to e.g. spectral embedding it is more robust to structural perturbations. This leads to improved robustness against adversarial attacks.	The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews.  However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper aims to solve the rigid registration of 3D point clouds using a deep neural network. The key difference from previous methods is that this paper proposes a region-conditioned transformation. This paper also has many flaws, such as suspicious implementations, insufficient experiments and unclear descriptions.This paper studies point cloud registration with deep neural networks. The key insight is learning a region-based transformation is more robust than learning a per-shape transformation. The paper is not well organized and presented. Many proposed modules are not technically sound and are not clear to follow.This paper addresses the problem of global rigid registration which is a fundemental problem in computer vision or graphics. They proposed a region-aware decoder and fuse the transformation predicted for all those regions. It seems they have got a big improvement over the DeepGMR and DCP method.This paper proposes a method for the registration of 3D point clouds using a trainable cascade of MLPs. The pipeline first computes per-point features, then proposes a transformation based on those features. The transformations are weighted using a predicted weight and combined into a final global transformation. The method is evaluated on ShapeNet40, showing good results w.r.t. prior art.This paper addresses the problem of point cloud registration. Given two point clouds with the same rigid shape, they aim to determine the relative pose (R and T) between them. For this purpose, they propose to predict several regional poses (determined by some subregions of the point cloud), weighted, and fuse them together.	This paper proposes a learning-based method for shape registration that conditions on regions of the shape rather than learning from the entire point cloud in one shot.  The reviewers point out several questions about the method, thanks to expository issues as well as missing comparisons/ablation studies.  As the authors have chosen not to submit a rebuttal, I will refer them to the original reviews for details here for additional points of improvement.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper analyzes and evaluates several different measures for evaluating reward learning. The authors provide a theoretical worst-case analysis showing limitations of common measures and provide empirical tests across several domains. I still struggle with the novelty of this paper as it relates to well known theoretical and empirical work.The paper brings up some of the limitations of existing reward learning approaches. While there is a very rich literature in this space with major claims and cool demonstrations, it looks like the evaluation methodologies followed are often fundamentally flawed.This work serves to point out that the current methods of evaluating reward function algorithms do not appear to be sound. In particular, the paper shows both parameter and reward based comparisons to ground truth reward functions can fail to give a reasonable sense of how good a given solution is. Both theoretical and empirical arguments are provided to support their case.The paper is an unusual contribution for a robotics conference. It does not directly contribute to advancing understanding of robot control or learning. It presents a critical analysis of the measures used to assess the quality of policies learned via HRI driven reward learning.	Post discussion update ------------------------  The authors completed a rebuttal that addressed all of the reviewers' critiques, and also uploaded a revised and improved manuscript. The most significant improvement seems to be the addition of a small related work section in the introduction and a limitation plus future work section at the end of the paper. However, since the final paper has an 8 page limit, the current 9-page manuscript will need to be shortened.   The clarifications seemed to sway two of the reviewers toward increasing their ratings. Generally, it's clear that although this is a non-standard paper, it raises interesting and potentially provocative points that would be great for the CORL community to discuss. Therefore, I am recommending acceptance.    Original review ----------------   This paper addresses the HRI challenge of measuring how well a robot has learned a user's reward function. It describes two main types of measures---parameter-based and reward-based---and provides both theoretical and empirical examples of the shortcomings of each.    Strengths * This seems like the type of paper that would be useful to assign in an HRI course or give to a new grad student, because it clearly explains the different measures and provides a comparison of them.  * Reviewers noted that the issues brought up by this paper are important and very relevant to current approaches. The paper is timely.  * Reviewers agreed that paper balances well between theory and empirical examples.   * The paper was well written and easy to understand.   Weaknesses  * The main critique is that reviewers feel that the paper doesn't go deeply enough into the discussion of what to do with these findings. How should robotics researchers use this information to design better measures? In what cases are these measures likely to fail, or to be accurate? Section 6 poses such questions, but it would add substantially to the novelty of the paper if there was some suggestion of where to go from here.    * Although the paper bills itself as a survey, reviewers noted that it doesn't provide much of a discussion of related work. Reviewers also provided additional citations that address some of the issues brought up regarding current measures of performance.   * A couple of reviewers questioned the connection to robot learning in particular.  * There is no limitations statement, which is a required component.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.SwinTrack adopts the classic siamese structure, and uses Swin Transformer for both feature extraction and fusion of template and search region. A special motion token which encodes the past trajectory is introduced to further boost the tracking performance. SwinTrack is evaluated on major SOT benchmarks and achieves SOTA performance.The paper addresses the problem of general object tracking, where the target object is given during test time. The authors propose a simple but strong baseline approach based on a Swin Transformer image encoder and a standard transformer decoder. The paper introduces a motion token, which encodes past bounding boxes in order to improve the final prediction.The authors propose an interesting Siamese architecture for single object tracking based on the Transformer model. Instead of CNNs the authors suggest SwinTransformers as branches with shared weights. The experiments show state-of-the-art results on large-scale tracking datasets. The idea to use SwinTransformer instead of a CNN in the branches seems to be novel.This paper proposes a fully-attentional tracker where both the representation learning and the feature fusion modules are based on the Transformer. A motion token is introduced to embed the historical target trajectory, further improving the tracking performance. Extensive experiments and ablation study on several large-scale tracking benchmarks are performed.	Authors present a method for single object tracking (SOT) that is entirely comprised of transformers. The architecture is simple:   1) Swin is used to generate embeddings for both template and search region 2) Embeddings are concatenated 3) An encoder transformer performs MHSA of the embeddings. 4) A decoder performs cross-attention from search tokens to template tokens and a special "motion token" which is constructed from a linear operation over the prior motion trajectory relative to the frame.  5) Output token is fed to final layers that perform IoU aware classification and bounding box regression.  Evaluations are performed on 5 SOT datasets, achieving SOTA on all of them.   Pros: - [AC/R] Important problem, technically sound, and new SOTA on this task. - [AC/R] New motion token approach is novel and provides significant improvement. - [AC/R] Simple and elegant architecture - [AC/R] Insightful discussions - [AC/R] Clearly written and easy to follow - [AC/R] Interesting ablations - [AC/R] High frame rate  Cons: - [R] Low novelty of transformer approach, but this is negated by the novelty of the motion token. - [R] Details regarding pretraining are missing. Authors provide in response. - [R] Motivation of motion token design is not clear. Authors provided further ablations of different implementations of the motion token, showing the current form performs best. - [R] Provide additional details regarding where and how SwinTrack outperforms other approaches on the benchmarks. Authors provided additional granularity of performance stratifications within the LaSOT benchmark. - [R] Add more recent high performing trackers. Authors added several methods published in 2022. - [R] Some additional questions about various details were posed by reviewers, which will all answered by the authors.   The single reviewer with reject recommendation changed to accept in their comments after the discussion period but did not update their score. Given unanimous agreement on accept, AC recommendation is accept.   AC Rating: Strong Accept
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the robustness of deep neural networks based on the perspective of representation similarity. Such a perspective provides an interesting direction to delve deeper into the properties of robust representation learning. According to the observations, the author introduces several ways to design and train better robust networks.This paper presents a probing analysis on clean (non-robust) vs. adversarially trained robust models. The paper introduces a lot of defence mechanisms (mainly centred around the min-max idea) It only employs PGD asdversarial training for obtaining a "robust" model.This paper uses an existing method for comparisons of intermediate neural network activations (CKA) for a comparison of robust and non-robust networks. The authors analyze the similarities in different aspects and try to deduce insights in adversarial training.The authors examine the effects of adversarial robustness training on representation. They use CKA to compare robust vs non-robust networks, benign vs. adversarial inputs. They find the following results: Robust representations are less specialized and distant layers are more similar in robust networks.The paper contrasts representation similarities of networks trained to perform image classification with and without adversarial noise. Early layers converge faster and later layers overfit to local minima. The paper is well-written and experiments clearly described.	The authors study representations obtained from image classifiers and contrast the classic training with adversarial training, so-called non-robust and robust networks, respectively. The authors primarily use the CKA metric on CIFAR10 and subsets of ImageNet2012 provide several novel insights on "salient pitfalls" in robust networks which suggest that robust representations are less specialized with a weaker block structure, early layers in robust networks are largely unaffected by adversarial examples as the representations seem similar for benign vs. perturbed inputs, deeper layers overfit during robust learning, and that models trained to be robust to different threat models have similar representations.  The reviewers agreed that these contributions are interesting to the larger community and that the presentation of the results is clear and straightforward. The main issues raised by the reviewers were carefully addressed in the rebuttal. Please update the manuscript as discussed.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies a fundamental issue in curriculum design for a complex reinforcement learning problem (target task) When the target task is complex to solve with direct training on it, the learner can be trained over a sequence of tasks with increasing difficulty. The authors propose to use the sum of residuals trained on each task for knowledge transfer and develop a novel curriculum RL method called Boosted Curriculum Reinforcement learning.The paper tackles the problem of curriculum learning and presents a new technique (boosted CRL) that provides a tighter bound on the approximation error of the action-value function than standard curriculum learning. The idea of using the sum of residuals to model the state-action function approximation was interesting.The paper proposes a new curriculum for RL method. The basic idea is to "learn" a residual for each task, modeling in this way how the tasks differ. The manuscript contributes interesting ideas and the proposed method seems to have good performance.This paper proposed a curriculum value-based reinforcement learning algorithm. Inspired by boosting methods, the algorithm learns on Bellman residuals between the value function of the current task and that of the previous task. The method is tested on four different problems and is compared with two 3 baseline methods.	The paper proposes a novel curriculum learning method for RL based on the concept of boosting. The proposed method builds on the curriculum value-based RL framework and uses boosting to reuse action-values from previous tasks when solving the current task. The method is analyzed theoretically in terms of approximation accuracy and convergence. Moreover, extensive experiments demonstrate the effectiveness of the method. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The idea is certainly interesting and novel, as it allows to bridge two distinct worlds (VAE and GANs) However, I am concerned about the message (or lack of thereof) that is conveyed in the paper.The assumption made by the authors that "generator is injective" is problematic or even wrong, as it is well known that GAN suffers from mode collapsing problem. The authors failed to discuss their paper with "ON THE QUANTITATIVE ANALYSIS OF DECODERBASED GENERATIVE MODELS", which uses AIS to estimate the likelihood.The paper shows that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While there are some doubtful statements, overall the paper is well written and easy to read.	The paper's strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it's not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs --- it seems that the only reasonable instance that allows efficient generator is the case where Y = G(x)+\xi where \xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a graph network (called HGN), aiming to better leverage commonsense knowledge graphs. The proposed model is tested on several tasks: CommonsenseQA, OpenbookQA and CODAH.The paper builds on the following two observations — (a) KGs are incomplete often lacking facts that would be needed for reasoning to answer a question. Current methods over-retrieves facts (edges) from the KG leading to a lot of unrelated facts that potentially makes reasoning noisier and harder. The proposed model then sparsifies the graph by learning edge weights via a two-step message passing process.This paper is well-written and well-evaluated. The proposed method is also relatively simple and intuitively motivated. The experiments, however, only show modest (yet still positive) empirical gains. Perhaps not a game-changer for commonsense QA, but still a reasonable contribution that I would recommend for acceptance.	The paper proposes an interesting step in the direction of neuro-symbolic reasoning. While there is no consensus among reviewers about the key novelty of the method, all acknowledge the interest of the direction. All of them also recognize that the submission improved greatly during the discussion phase: clarification of motivations, of experimental settings and results, of discussion with previous work.  However, despite those improvements, the submission is not yet ready for publication at ICLR. We encourage the authors to use the very detailed reviews and comments to improve the work. In particular, we encourage them to pay attention at three aspects:  1/ Comparison with large language models: the discussion wrt T5 is important. A key motivation for the proposed model is that it is bringing information and elements for QA (or other reasoning tasks) that purely scaling up language models can not bring. Or maybe they can bring the same kind of improvement but at a much lower computational cost. In any case, this is a very important point to justify the interest of such approach, and neuro-symbolic reasoning overall, empirically.  2/ Using GPT2 (or equivalent): the discussion on using GPT-2 for generating new facts is key too. It is essential to bring this description from appendix to the core of the paper. But more discussion are expected.  For instance, what if GPT-2 generates facts that are false and lead to answering and justifying a wrong answer? In other words, how does it impact the integrity of the contextualized KG? This is an essential point that needs to be worked on more thoroughly.   3/ Overall there have been a lot of discussion to improve the motivations and the contributions. But they are not reflected in the paper necessarily. Following R2, we encourage the authors to "refocus the existing version (e.g., from vague discussion about neural-symbolic models towards establishing solid comparison to the most related previous work in various sections of the submission)"
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper focuses on an interesting and important question: how to perform distributed multi-agent deep reinforcement learning? The author first proposed three challenges to be considered: 1) Demanding data transfer. 2) Inter-process communication. 3) Effective Exploration.This paper introduces a new distributed value-based multi-agent reinforcement learning framework. It divides the system into two parts, multiple containers, and one centralizer. Containers are trained with trajectories generated by their own actors interacting with the environment. In contrast, the centralizer is trained with high-priority samples sent by all containers.This paper proposes a distributed containerized multi-agent reinforcement learning(CMARL) framework that addresses three challenges in MARL. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time.The authors propose a framework for distributed multi-agent reinforcement learning. By grouping actors, replay buffers, learners, and a queue manager into containers CMARL can be used to scale learning to the requirements of the multi- agent setting.	This paper proposes a distributed containerized multi-agent reinforcement learning(CMARL) framework that addresses three challenges in MARL: 1) Demanding data transfer. 2) Inter-process communication. 3) Effective Exploration. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time as compared to state-of-the-art benchmarks.  Although the reviewers acknowledge that the paper addresses a relevant topic, proposes an effective method, and is well written, after reading the authors' feedback and discussing their concerns, the reviewers reached a consensus about rejecting this paper in its current form. They feel that the contribution is too incremental and that the experimental comparisons are somehow unfair.  I suggest the authors take into consideration the reviewers' suggestions while preparing an updated version of their paper for one of the forthcoming machine learning conferences.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper designed a hypothesis testing procedure for detecting changes in episode sequential data. The method is demonstrated based on a non-iid and non-Gaussian setting for reward signals. The strength of this paper would be significantly boosted if the proposed method can be used to solve an non-stationary RL problem.The authors propose a sequential testing procedure for a fixed RL agent reward drift. I believe that a paper requires a notable refactoring to improve the presentation. The provided theoretical grounds looks as non-enough for publication @ ICLR.The authors present a novel change detection test for non i.i.d. data motivated by applications in RL. At first, they provide an offline version of the test, then they extend it to the online setting. The paper is not self-contained.This paper considers the drift detection for episodic data, where data episodes are assumed to be i.i.d. but data within each episodic can be correlated. It is assumed that the pre-change (nominal) mean and covariance of each episody is perfectly known or can be accurately estimated from reference data. The Uniform Degradation Test (UDT) and Partial Degrades Test (PDT) are proposed to detect the mean shift.	The paper's initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline.   The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers' and authors' remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty -- as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper -- even a two time-step trajectory with normal rewards per state transition can exhibit a mixture-of-Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method.   I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper propose new quantization scheme, called SGQ (Stripe-wise Group Quantization) It captures both channel and spatial-level similarity in pixels, and hierarchically encodes the features in these two levels.Theoretical analysis is provided to calibrate gradients of both features and CAG blocks in the backward process. Experiments demonstrate higher traffic reduction and image processing speed while keeping comparable accuracy. The number of clusters $k$ is tied to the bit, then the performance degradation of extreme cases including 1-bit or 2-bit.Network bandwidth poses challenges to the collaborative learning system. The existing communication-optimized methods either are insufficient in reducing the traffic or may degrade the model performance. This work proposed a hierarchical compression algorithm for feature data, called Stripe-wise Group Quantization.This work proposes a structured strip-wise quantization method, named Strip-Wise Group Quantization (SGQ), to implement the scalable collaborative learning systems on tiny edge devices. The novelty of capturing such channel-dimension structured information is good, which is different from the viewpoint of the conventional methods.	This paper proposes a novel communication-efficient learning method that significantly reduces feature size and communication traffic. The rebuttal solved the reviewers concerns about the dataset size and accuracy / latency trade off.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces a library that preprocesses tabular data called Automunge. I feel that one of two criteria must be met: The software implements a scientifically novel algorithm, framework, model, etc.; or the software package is so complex that a well-designed implementation in itself is of scientific significance.AutoMunge converts NLP into features suitable for NNs. The authors of the library have put a lot of thought into its construction, and it looks very useful. However, ICLR is about /learning/ representations, not about feature engineering."String theory" is a sub-field of physics. It is a term used to refer to a type of data analysis. This is a tool to help people with this type of analysis. It can be used for other types of analysis as well.	This paper presents "Automunge" a python library for pre-processing tabular data.  The authors develop a useful library that can be used by practicioners for data engineering in NNs applications.  The reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the  performance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives.  In the response the authors have done a rather thorough job of addressing the reviewers comments and adding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don't think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function. The authors demonstrate their method on 3 Atari games and use A3C as the training algorithm.The paper introduces an attention mechanism into A3C-based reinforcement learning agents. The identified attention regions are intuitive and action-conditional. The motivation and goal on the inverting gaze area are less clear. The paper has some confusing details.This article proposes to include an attention mechanism to Deep RL to provide a visual "explanation" of the learned policy. There is solid evidence that attention is an important aspect in perception and learning. The approach is straightforward and similar to attention maps used in, eg, image captioning.This paper applies the mask attention mechanism on the DRL model (actor and critic), to make the learned policy explainable. The empirical results in Atari 2600 show that the performance of A3C is further improved by implementing mask attention on the actor and critic network separately.	The paper proposes a method to generate attention masks to interpret the performance of RL agents. Results are presented on a few ATARI games. Reviewers unanimously vote for rejecting the papers. R1, R3 give a score of 5, whereas R4, R5 give a score of 4. Their concerns are best explained in their own words:   R1 says, "The use of attention maps to analyze and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either."  R3 says, "the analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze-like, and ball-and-paddle) may shed light on the learned attention's general property."  R5 says, "I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive"  In their rebuttal, to address R1's concern authors suggested that the use of attention on both value and policy networks is novel. This is not sufficient, because it does not show why such attention maps are more useful than ones proposed by prior work. As suggested by reviewers, a systematic study or a human study clearly showing that the proposed method adds more interpretability is critical. However, this is missing.  In response to R3, the authors provided experiments on more games. But this is not the point -- because it's not about the number of environments in which experiments are provided, but rather the nature of the analysis that is performed. Finally, R5 comments that it's unclear whether attention actually provided interpretability or not.   Due to the lack of convincing analysis that demonstrates the utility of the proposed method in advancing the understanding of decisions made by RL agents, I recommend that the paper be rejected.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new algorithm on learning noisy datasets by transforming class labels into pairwise similarity labels. The paper empirically demonstrates that the proposed method works well on several synthetic datasets and a large-scale real-world dataset. The baselines on CIFAR seem too low compared with the SOTAs, e.g. [1].The paper addresses the problem of learning with noisy labels by transforming the original category classification task into a semantic-similarity prediction task. The new task takes pairs of samples as input and predicts if the two samples are coming from the same category or not. It is theoretically shown that the similarity pairs have lower noise rates compared to the original categorization.The idea to deal with label noise by transforming noisy class labels into noisy similarity labels seems to be novel. In most cases the proposed Class2Simi can improve the accuracy compared with baselines. But the improvement is not significant in many cases like those on MNIST and CIFAR10.This paper presented a working framework for learning a robust classifier with noisy labels. It proves that if the number of the classes is more than 8 then the noise rate in the similarity matrix is less than that in the noise rates in labels. After learning classifiers from noisy labels, it updates the classifier.In this work the authors propose a method to learn from noisy labels. The propose method converts the noisy labels to similarity labels which are more robust to noise. The proposed method has been evaluated on multiple datasets with consistent improvement across all noise ratios.	The paper's stated contributions are:  (1) a new perspective on learning with label noise, which reduces the problem to a similarity learning (Ie, pairwise classification) task  (2) a technique leveraging the above to learn from noisy similarity labels, and a theoretical analysis of the same  (3) empirical demonstration that the proposed technique surpasses baselines on real-world benchmarks  Reviewers agreed that (1) is an interesting new perspective that is worthy of study. In the initial set of reviews, there were concerns about (2) and (3); for example, there were questions on whether the theoretical analysis studies the "right" quantity (pointwise vs pairwise loss), and a number of questions on the experimental setup and results (Eg, the computational complexity of the technique). Following a lengthy discussion, the authors clarified some of these points, and updated the paper accordingly.  At the conclusion of the discussion, three reviewers continued to express concerns on the following points:  - *Theoretical justification*. Following Theorem 3, the authors assert that their results "theoretically justifies why the proposed method works well". The analysis indeed provides some interesting properties of the reduction, such as the fact that it preserves learnability (Appendix F), and that the "total noise" is reduced (Theorem 2). However, a complete theoretical justification would involve guaranteeing that the quantity of interest (Ie, the clean pointwise classification risk) is guaranteed to be small under the proposed technique. Such a guarantee is lacking.    - This is not to suggest that such a guarantee is easy -- as the authors note, this might involve a bound that relates pointwise and pairwise classification in multi-class settings, and such bounds have only recently been shown for binary problems -- or necessary for their method being practical useful (per discussion following Theorem 3). Nonetheless, without such a bound, there are limits to what the current theory justifies about the technique's performance in terms of the final metric of interest.  - *Comparison to SOTA*. Reviewers noted that the gains of the proposed technique are often modest, with the exception of CIFAR-100 with high noise. Further, the best performing results are significantly worse than those reported in two recent works, namely, Iterative-CV and DivideMix. The authors responded to the former in the discussion, and suggested that they might be able to combine results with the latter. While plausible, given that the latter sees significant gains (Eg, >40% on CIFAR-100), concrete demonstration of this point is advisable: it is not immediately apparent to what extent the gains of the proposed technique seen on "simple" methods (Eg, Forward) would translate more "complex" ones (Eg, DivideMix).   - In the response, the authors also mentioned that (at least the initial batch of) the experiments are intended to be a proof-of-concept. This would be perfectly acceptable for a work with a strong theoretical justification. However, per above, this point is not definitive.  - *Creation of Clothing1M*. The authors construct a variant of Clothing1M which merges the classes 3 and 5. Given that prior work compares methods on the original data, and that this potentially reflects noise one may encounter in some settings, it is advisable to at least report results on the original, unmodified version.  - *Issues with clarity*. There are some grammatical issues (Eg, "is exact the"), typos (Eg, "over 3 trails"), notational inconsistencies (Eg, use of C for # of classes in Sec 2, but then c in Sec 3.1), and imprecision in explanation (Eg, Sec 3.2 could be clearer what precise relationships are used from [Hsu et al. 2019]).   - These are minor but ought to be fixed with a careful proof-read.  Cumulatively, these points suggest that the work would be served by further revision and review. The authors are encouraged to incorporate the reviewers' detailed comments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper is the first to achieve the optimal convergence rate under the NTK regime. They show that smooth target functions are learned rapidly at faster convergence rate. The paper is technically sound and the proof techniques are different from existing literatures.The paper is well-written, and the result looks very interesting. This paper is a theory, so experiments are a plus. If the authors can address some of my comments, I am tending to increase the score of the paper. Here are some comments about writing.The authors show that under the Neural Tangent Kernel Regime achieves optimal rates in the attainable case. This result is not a *plug and play* based on the current kernelized-SGD litterature. The quality of the paper is undeniable and fills the gap between optimality of NTK literature.This paper considers the optimization of a wide two layers neural network (for a regression task) using averaged SGD. The authors consider the Neural Tangent Kernel (NTK) regime. The result is well explained but some precision could make the paper even more insightful.The paper focuses on the understanding of neural tangent kernel (NTK) NTK plays an important role in characterizing the generalization ability of artificial neural networks. The paper is well written and interesting to read. Overall, I vote for accepting.	The paper presents some exciting results on the convergence of averaged SGD for overparameterized two-layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper addresses contact prediction for PPI by proposing a geometric deep learning framework. They use a k nearest neighbor representation for each protein and compute geometric-based attention scores to convolute the messages over this graph. They validate this approach on the PPI task, using the DIPS-plus dataset.This paper proposes a state of the art deep learning architecture for predicting mutual contacts between proteins. Strengths: The strengths of this work stem from the architecture used, consisting of attention mechanisms and invariant representations. Weaknesses: The performance metrics and benchmarks are lacking.The proposed method is validated on two benchmarking datasets, i.e., DIPS-Plus and CASP-CAPRI. The key contribution of this paper is not clear. The proposed architecture seems not novel, and I have some serious concerns regarding the experiments.This paper proposed a novel geometry-evolving graph transformer for protein interface contact prediction, named DeepInteract. In particular, it predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input.	This paper presents a novel neural network architecture to predict interacting residues among two interacting proteins, and evaluates its performance on benchmarks. While the reviews were initially mixed, there has been a productive discussion and significant improvements in the paper during the discussion, including in particular much needed clarifications about the proposed methods, and more experimental results with an ablation study to better assess the benefits of various design choices. While no reviewer is willing to champion this paper as a "strong accept", due to the relatively modest novelty compared to existing methods, there is a consensus towards "weak accept" given the final quality of the work presented and potential usefulness of the method for the problem tackled.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Experiments are done in DeepMind Control Suite (DMControl) and Atari suite to show improved performance and data efficiency.The authors propose to overcome limitations by using a particle based entropy estimate in the learned representation space. The method is novel and the experimental results are strong. However, it would benefit from a more detailed analysis in order to understand where the reported gains come from.The paper proposes a method to simultaneously learn effective representations and efficient exploration in a reward-free context. The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward. Authors empirically evaluate the method over a set of visual Mujoco tasks and Atari games.	The authors propose a particle-based entropy estimate for intrinsic motivation for pre-training an RL agent to then perform in an environment with rewards. As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, "A Policy Gradient Method for Task-Agnostic Exploration", Mutti et al, 2020--MEPOL. What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments.  The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin. Unfortunately this work does not meet the bar for acceptance relative to other submissions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is well written, and clearly explains the background material and concepts. The main technical section (4) feels a bit rushed. The experimental section has some room for improvement. I liked the comparison of decoupled and coupled CRF training, but I didn't get much out of the synthetic experiments.The authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The experiments seem to show performance in par with the FullCRF on decoupled training. The authors base on a different network than that of the CRFasRNN baseline.The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below.	The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.  In practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.  The ratings before the rebuttal and discussion were 7-4-6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said "I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3's writing/claims issues are fixed, neither were my additional experimental requests, not even R1's typos." There is therefore a consensus among reviewers for reject.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper examines the use of STE for learning simple one-layer convolutional networks with binary activations and non overlapping patches. In this setting, the gradients are 0 almost everywhere ( gradient of sign(x) hence it is not clear how to use gradient descent. The approach studied here is to instead use the gradient of an alternative function such as ReLU or identity.The paper presents an analysis of training single-layer hard-threshold (binary activation) networks for regression with a mean-squared loss function using two different straight-through estimators. The paper demonstrates that training with the latter against the population loss is guaranteed to converge to a critical point.This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties the identity STE. The discussions for most theoretical results are very short or not organized well.	The paper contributes to the understanding of straight-through estimation for single hidden layer neural networks, revealing advantages for ReLU and clipped ReLU over identity activations.  A thorough and convincing theoretical analysis is provided to support these findings.  After resolving various issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Valid criticisms of the presentation quality were raised during the review and response period, and the authors would be well served by continuing to improve the paper's clarity.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper study the relation between role diversity of tasks and the MARL model performance. The paper provides an interesting perspective (role-diversity) to measure the difference between MARL tasks. The discovered guidelines about the choice of training strategies will benefit the community.This work looks at defining role diversity as a means for analyzing multi agent dynamics. The paper does not do a good job at motivating why we should care about analyzing multiagent systems from the perspective of roles. The theoretical analysis is not well-motivated.This paper proposes to use role diversity as a metric to describe MARL tasks. They also find that the error bound in MARL can be decomposed into three parts that have a strong relation to the role diversity. To evaluate the proposed method, they conduct some experiments on MPE and SMAC environments.The authors propose a role diversity metric to quantify the difference between agents in multi-agent reinforcement learning. The metric is used to inform the use of communication, parameter sharing, and other MARL algorithmic decisions.	This paper proposes a Role Diversity metric, meant to quantify how different roles are in a multi-agent RL setting. There's actually three versions of this metric, or three aspects (the distinction is not entirely clear to this area chair).  The reviewers are generally not very enthusiastic about the paper, with scores hovering at or just below the acceptance threshold. There has been extensive discussion between reviewers and authors, but there a sense that there is confusion about the exact purpose and contribution of the paper. This is reinforced by the authors' "letter to area chair", which outlines several ways the reviewers have not gotten the message. Reading the paper, it appears to me that the root cause is that the authors are indeed not communicating clearly what the paper contributes and why. It is, after all, the authors' responsibility that the reviewers understand the work. My own impression is that the text is dense and not particularly easy to get through. Perhaps the authors are simply trying to cram too many contributions into a single conference paper? This is a classic error which leads to hard-to-read papers. In addition to this, there is a lingering concern about the generalizability of the proposed methods.  I think the authors need to work more on their presentation, and perhaps reconsider which parts to include in their paper and exactly which measure they want to send, before they submit to another venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a novel factorisation approach and uses recurrent networks. The qualitative experiment is interesting, but there is no information given about the level of musical training the participants had. The paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.The introduction is quite well written and it is easy to follow. It provides a good review that is nicely balanced between older and recent literature. At the technical parts, the paper starts to suffer due to sloppy notation. The cross entropy definition is missing important details.In this article, we look at a new approach to polyphonic music generation. We also look at the quality, clarity, originality and significance of the work. There are a few points that could be better clarified. We conclude with a list of the pros and cons of this work.	This paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. The technical parts in the original write-up were not very clear, as noted by multiple reviewers. During the review period, the presentation was improved. Unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.OmniVL is a vision-language foundation model that supports both image-language and video-language pretraining in a unified framework. The model allows for evaluation of vision-only tasks, multimodal retrieval, and multimodals based tasks like captioning and VQA.The authors argue they achieve a good performance on many different types of tasks. The motivation is good. The so-called decoupled paradigm separates the image-language pretraining and video-language joint pretraining. End-to-end pre-training is easier to train.The paper proposes a unified transformer based model for Image-Language and Video-Language tasks. The model is pretrained on image-language and then on video-language different from prior works. The limitations are briefly discussed while the potential societal impact is left for future studies.	After the authors’ rebuttal and long discussion between reviewers and authors, the paper unanimously receives positive rates thanks to reasonable proposed ideas and thorough experiment evaluation. The camera-ready version may need to be updated to fully reflect reviewers’ comments and authors’ answers to them.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a deep-learning framework to model shape deformations. The motivation is that prominent methods like ARAP are restrictive in their results. The authors demonstrate their method on the DeformingThing4D-Animals dataset.This paper deals with the problem of surface deformation by training a Transformer network on point cloud. The key idea is, they adopt a canonical model or space as often used in human modeling and design a backward and forward deformation network.This paper presents a neural deformation method, which utilize the deformation priors in a large-scale dataset. The proposed method first deforms the input model back to the canonical space, and then deforms it to the target that satisfies the constraints given by the user.In this paper is introduced an approach to learn mesh deformations of dynamic bodies from user-provided handles. To manipulate the source meshes into different poses, the shape deformations are learned via canonicalization. Two backward and forward deformation fields are considered, that are learned by transformer deformation networks.	While some of the scores on this paper are mixed, even the negative reviews highlight the quality and interest of the work and have specific (and somewhat debatable) technical concerns.  Overall, the AE recommends accept, especially in light of the detailed and thoughtful responses during the rebuttal phase.  In the camera ready, the authors are encouraged to see if they can squeeze some of the new results (e.g., transfer learning attempt in Figure 6 and comparisons to Shapeflow) in the main body of the paper, where they're more likely to be noticed.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the problems of classification and object detection from natural image datasets in a continual learning setting. It is assumed that a memory buffer of a certain size is available to store data in. The strategy used by the paper is to fill this memory buffer with compressed data samples.Memory replay with data compression is both an important yet neglected baseline and a promising direction for continual learning. Using a naive technique of data compression with a properly selected quality, the proposed method can achieve the SOTA performance in a time-efficient and plug-and-play way.In this paper, the problem of catastrophic forgetting for continual learning is explored. The core idea is to benefit from data compression to reduce the space to store data samples and then replay the reconstructed data points. JPEC method has been explored empirically to determine an optimal compression rate.In this paper, the authors propose to compress the data for memory-based continual learning. Extensive experiments show that with the proposed method, a naive compression method can achieve the SOTA on several continual learning benchmarks.	This works considers limitations of rehearsal-based methods in the context of continual learning (classification and object detection). Rehearsal-based methods provide a strong baseline, but a loss in predictive performance arises when the memory is limited in size. The authors propose to leverage compression (JPEG) to increase the number of data (images) stored in the memory. The approach is evaluated in the context of an autonomous driving application.  The additional experiments conducted by the authors were highly appreciated and helped clarify open questions (e.g., class-incremental learning set-up, DPP objective to determine size of the memory, quantity vs quality of compressed data, etc.). The authors addressed the issues raised by three out of four reviewers, who did not have further comments. The remaining reviewer found that the methodological contributions of this paper, namely of using compression in the context of CL, was pretty straightforward. However, the authors addressed the concerns raised by the reviewer regarding the selection of the compression quality q as far as I am concerned and conducted additional experiments to further demonstrate the usefulness of the approach. I would encourage the authors to include this discussion in the final version of the paper. I would also encourage them to include the additional experiments they conducted with fixed memory size and amount of memory that can be saved.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a self-supervised approach for the task of surround-view depth estimation. 2D image features are un-projected onto a shared 3D volume, which is later queried for decoding depth maps at target views. The proposed model has the unique capability of predicting depth maps for views not among the inputs.This paper proposes a novel surround-view depth estimation, and project features of different views into a volumetric feature spaces. The pipeline also enables novel-view Depth synthesis. The problem setting may be useful for autonomous driving.The proposed method consists of three parts. First, the surround-view feature fusion module generates a unified volumetric feature. Second, the depth fusion module reconstructs the depth map given an input camera viewpoint. Last, the global motion of the canonical camera is estimated by assuming static camera extrinsic.	Initially, the paper had mixed reviews (455).  The major concerns from the reviews were:  1. missing refs about unprojection. (K314) 2. quality advantage is not convincing, slightly better than FSM, while qualitative results show not obvious improvements. (K314) 3. visualize the depth maps as point clouds (K314) 4. what is the trade-off between resolution/memory, computation, and depth estimation? (K314, ZBww) 5. insufficient experiments (9Ef1) 6. comparison with monocular methods (9Ef1) 7. cubic vs spherical space? (9Ef1) 8. can it be trained on real data w/o GT supervision? (9Ef1) 9. how to handle collision of multiple pixel rays? (ZBww) 10. why use different MLPs to fuse "overlap" and "non-overlap" features? No ablation study on this. (ZBww)  The authors wrote a response to address these concerns, providing more qualitative results and ablation studies, as well as further explanations.  The reviewers were satisfied with the response, and K314 upgraded their rating to 6, while other reviewers maintained 5s. The reviewers appreciated the novel problem and the solution that can produce more consistent depth maps across views, and also synthesize depth maps in novel views. After reading the paper, the AC agrees with the reviewers, noting that the paper  addresses the limitations of the problem setup of previous work [13], thus developing a new line of research.  Thus, the AC recommends accept. The authors should prepare a revised version of the paper according to the reviews, rebuttal, and discussion.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The platform can be very useful for the research community as it supports audio-visual rendering and can be used in any environment. The authors have shown results for multiple down-stram tasks such as far-field speech recognition.The authors present a second-generation of the soundscape tool. The tool is designed to simulate realistic acoustic reverberation from 3-dimensional room/scene models. Such a tool facilitates large-scale rendering of audio for simulated scenes.The authors introduced SoundSpaces 2.0, a simulation platform that supports audio simulation with a number of improvements over existing platforms. The dataset and the benchmarking results are also good to have for the community. The methods used in benchmarking are limited.This paper proposed a new simulator for simulating acoustic effects. The major difference compared to the prior work is that this work can compute sound properties on the fly and thus avoid using precomputed data. The new platform can support arbitrary geometry and continuous outputs.SoundSpaces 2.0 is a platform for on-the-fly geometry-based audio 2 rendering for 3D environments. It allows continuous spatial sampling and demonstrates the effectiveness in two downstream tasks. The paper is clearly constructed and presented.The paper introduces SoundSpaces 2.0, the first geometry-based real-time acoustic simulation platform for 3D environments. The main difference from its previous version (SoundSpaces) is its continuous acoustic simulation (as opposed to discrete)	This paper was reviewed by six experts and received all positive scores. AC feels this work is interesting and deserves to be published on NeurIPS 2022 dataset track. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.FlowSelect builds on existing literature of conditional randomization tests, as well as normalizing flows from the deep generative modeling literature. The empirical results are quite promising, FlowSelect provides better results than several competing knockoff methods across two synthetic tasks.This paper proposes a new variable selection method called FLOWSELECT to perform controlled feature selection. Asymptotically,  the proposed method computes valid p-values. Empirically, FLOW SELECT consistently controls the FDR on both synthetic and semi-synthetic benchmarks.The paper considers the problem of multiple hypothesis testing with the FDR control. The work is situated in the model-X setting. The proposed method is evaluated with synthetic and semi-synthetic data. The presentation of this paper can be improved.FlowSelect is based on normalizing flows to select features in a controlled fashion, meaning that the false discovery rate (FDR) is limited. The authors prove that their estimates of the p-values are asymptotically correct, i.e. that the estimates converge to the true value almost surely.	This paper describes how to use normalizing flows for selecting features in a way that controls the type-1 error by using a normalizing flow along with MCMC to sample from the null distribution. The majority of the reviewers were positive, however the most confident reviewer was negative. From taking a look at that reviewers concerns, I tend to agree with most of them.  The paper is titled knockoff-free, which means in the context of this paper that both 1) 1-bit p-values are not used and 2) The full knockoff property is not required, only sampling from complete conditionals are required. Most of the experiments compare knockoff methods to the proposed approach, so it's not clear if 1) 1-bit p-values are not great or 2) the model-X process/complete conditional sampling process is better with normalizing flows. The former point is known and the latter point on the best way to sample from the complete conditionals is really the value.   If we take the paper as,   1) complete conditionals are 1-D 2) MCMC can be used to sample from a 1-D unnormalized density 3) Simple MCMC won't be bad because the problem is 1-D  -> Any likelihood based deep generative model can be used to sample complete conditionals  then it's a solid paper.  On the other hand, the belief that flows are the correct choice versus other likelihood-based deep generative models is harder to take as there's only a comparison with a mixture density network used in the original HRT paper. Also from other uses of these models, different models are better in different situations. I'd suggest a heavy discussion in the paper on this point at the minimum. Maybe even a reframing of the paper is needed.  Finally, for the test statistic, the HRT may not be the best choice for work like this paper that studies the problems with estimating X-distribution. The  paper "CONTRA: Contrarian statistics for controlled variable selection" at AISTATS 2021 shows that the HRT test statistic is more sensitive to model-X estimation errors than a simple mixture statistic that doesn't give up much power. The choice of test statistic also merits some discussion in step 3.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The Spectrum Random Masking method is a novel way of performing data augmentation in image-based reinforcement learning by perturbing regions in the frequency domain of the observed images. The method improves upon state-of-the-art data augmented methods both during training and in generalization.The proposed method achieves higher performance than SVEA on video background generalization tasks. The proposed method is applicable only when the observation has a wide spectrum. This explores new possiblities about using strong and useful augmentations for RL.Spectrum Random Masking (SRM) is a data augmentation method by random masking in the frequency domain. SRM can be plugged into existing policy generalization algorithms like DrQ and SVEA. The only benchmark demonstrated in the paper is Distracting DMControl, which is a set of toy environments.This work presents a new perspective of data augmentation for policy generalization in image-based RL. The experiments and ablation studies are sufficient to show the effectiveness of the proposed method. The authors have claimed some limitations.	The authors have introduced a method of data augmentation for image-based reinforcement learning that performs masking in the frequency domain, combined with techniques for stabilizing Q-learning, to achieve improved performance on a number of DMControl Generalization Benchmark tasks.  There was agreement among the reviewers that this work is novel and technically sound, and their concerns were mainly related to the breadth of tasks explored in the initial submission. During the review process the authors have gone to considerable effort to introduce new tasks (e.g. DrawerWorld, Robosuite and CARLA) and the improved performance of their method appears to generalize well. I believe that this work will be of broad interest to the RL community and recommend it for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words for that topic.This paper presents a topic model based on adversarial training. The paper adopts the framework of InfoGAN to generates the bag-of-words of a document. The latent codes in InfoGAN correspond to the latent topics in topic modelling.This paper proposes a new framework for topic modeling, which consists of two main steps: generating bag of words for topics and then using RNN to decode a sequence text. The author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss.	This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag-of-words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words.  Pros:  It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation.  Cons:  There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component-wise evaluation (e.g., text classification from topic models) and insufficient GAN-based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong.  Verdict: Reject. Many technical details require clarification and experiments lack sufficient comparisons against prior art.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors investigate a very interesting and important problem that lies in the intersection between perception and statistics. Interesting (but debatable) observations between image prior, image quality, and perceptual distances. See above for detailed comments.Perceptual sensitivity is correlated with the probability of an image in its close neighborhood. Distances induced by AEs are correlated with Prob(Trng data), as well as with human perception. The paper specifies that perceptual distances do not always lead to noticeable gains in performance over Euclidean distance.The authors present few relation between statistical learning and perceptual distances. The most interesting contribution is the training of auto-encoders with noise and a perceptual similarity distance. I think it is more a review paper and there are not important enough contributions.This paper presents a mainly theoretical explication of the relationship between natural image statistics and perceptual distances. While the paper is dense, I found it nevertheless to be well structured and the figures to be generally helpful. I believe that the paper draws out the value of making these connections explicit.	All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods.  I believe this paper will be of broad interest to different communities at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper develops methods to construct predictive intervals on regression for cross-sectional time series data that come with valid coverage guarantees. The underlying problem studied in the paper is extremely relevant to the machine learning community. The authors give error bounds for coverage and also support their claim empirically.The paper considers the task of predictive inference with panel data consisting of a time dimension and sample dimension. Two types of coverage guarantees are considered here--- longitudinal coverage and cross-section coverage. In contrast to previous works that focus on either of the coverage guarantees, this paper aims at achieving both (at least empirically)Temporal Quantile Adjustment (TQA) is proposed for time-series data with a focus on constructing efficient and valid prediction intervals. TQA adjusts the quantile to query in Conformal Prediction at each time t. The theory part supports the validity of coverages.This paper proposes a new approach called temporal quantile adjustments (TQA) TQA can improve longitudinal coverage and preserve cross-sectional coverage for the prediction interval built for regression on cross-section time series data.	In this paper, the authors propose the temporal quantile adjustments (TQA) that can improve longitudinal coverage and preserve cross-sectional coverage for the prediction interval built for regression on cross-sectional time series data. While previous works focus on either of the coverage guarantees, a major contribution of this paper is to achieve both. The research questions addressed in this paper are of critical importance for practitioners in relevant areas. The paper is well written. The presented approach has solid empirical support and decent theoretical guarantees. Including a simulation study to demonstrate the validity of the proposed approach under specific (and controlled) setup will further improve the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work proposes LECO, i.e. to get the state visitation count via learning the VQ-VAE model. The count-based reward is combined with the task-specific modulation, which is learned through the LIRPG framework. The experiment is conducted on procedurally generated environments in MiniGrid benchmark and Deepmind Lab.This paper introduces a method for count-based intrinsic motivation in Reinforcement Learning. The counting mechanism may drive the exploration towards states that are known to be useless for maximizing the extrinsic reward. The proposed method LECO modulates the intrinsic reward such that exploration of useless agents is not encouraged.This paper studies a learnable hash function for count-based exploration. The associated intrinsic reward is further modulated by a second reward that learns to focus on task relevant extrinsic reward. Experiments in MiniGrid environments and two DMLab tasks shows that their method generally outperforms or is on par with several baselines.	Reviews were mixed here and all quite borderline. There are legitimate points raised for why this is being consistently given borderline ratings, with two in particular resonating with my own reading (novelty and comparisons with other methods). However, despite these issues the paper itself is a solid contribution, and I think could easily lead to others building on the core ideas and approach. The paper has also improved notably during revisions and at present does not have any fundamental flaws that should preclude publication. Therefore, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Federico Monti, Michael M. Bronstein, and Xavier Bresson. Geometric matrix completion with recurrent multi-graph neural networks. CoRR, abs/1704.06803, 2017.This paper aims to tackle the matrix completion problem by drawing connection from prior work in image completion domain. It seems to be a combination of prior work: Multi-graph convolution combined with Dirichlet energy on row and column graph laplacian. The writing and presentation is significantly below par Iclr acceptance in the current form.The grammar and style of writing are not as high as expected. Several typos and lowercase/uppercase misuse throughout the whole paper. Use of the adjectives “surprising” and “spectacular” for the state-of-the-art methods is uncustomary language.In this work,  the authors addressed the problem of Matrix Completion on Non-Euclidean domains. They mainly adopt the method Matrix Data Deep Decoder. Here are my main concerns: The technical contributions of this paper is limited. The motivation of this work is also not very sound.	The focus of this paper is to analyze an end to end network to reconstruct matrices originating from non-Euclidean data which are corrupted. The authors present an untrained network for this task. In the review period the reviewers raised a variety of concerns including concerns about novelty of the paper with respect to existing work, technical depth and clarity. The authors did not respond to these concerns. Therefore, I recommend rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The research field of video TAP is under-studied. This paper introduces a companion benchmark it would benefit the community. The data collection method provided in this paper shows a standard pipeline for the following research.The paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video.TAP-Vid aims to expand keypoint tracking in video with a greater diversity of points and longer duration point tracks that account for occlusion. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets.This paper constructs a new dataset for arbitrary physical point tracking. The proposed task, i.e., tracking any points (TAP), can benefit the development of embodied agents. Although optical flow-based methods can perform well in short-term videos for arbitrary point tracking, the drift accumulation problem hinders their application.This paper formulates the problem of ‘tracking any point’ on video data. The formulation differs from related problems by aiming to track points starting from any pixel in any frame. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks.This paper introduces a new benchmark dataset for the task of tracking any point (TAP), i.e., tracking an arbitrary 2D point defined at the first frame over the entire video. The dataset is composed of a mixture of real and synthetic data. For real data, human manually annotates the ground truth for a number of 2D salient points, and for synthetic data, perfect ground truth labels are provided.	TAP-Vid presents a benchmark that will be highly useful for tracking research for tracking arbitrary physical points on surfaces over long video clips. The reviews are all positive, with one reviewer raising ethical aspects in preparing the benchmark.  I find the rebuttal sufficient and adequate and hence recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors note that the stationarisation of time series removes information from the time series that can be used to aid in prediction. Given that we want stationarity to improve model performance, but do not want to lose the information, motivates the "Non-stationary transformer"This paper study the non-stationary problem for transformer based forecasting model. They find that the nomarlization step in pre-process will lead transformers to generate indistinguishable temporal attention. The proposed strategy can be easily integrate with different transformer backbone. Empirical results show consistent improvement.This paper proposes Non-stationary Transformers as a generic framework to tackle over-stationarization problem. The motivation is clear and the writing is easy to follow. The proposed Series Stationarization technique share lots of similarity with section 3.3 Scale handling in [1].This paper focuses on forecasting and introduces a new method for scaling attention. The proposed procedure is quite simple to implement. The theoretical part is not very rigorous. The authors are a bit pretentious at times, but other than that it is all right.	The paper introduces a transformer-based method for non-stationary time series forecasting.  This research addresses a clear need, as acknowledged by the reviewers. Also, most reviewers found the method clearly described and the experiments compelling, demonstrating an improvement of the state of the art.  The reviewers asked questions about the baselines, evaluation methods and ablation studies. They also made requests related to clarifying the wording and some of the theory. The authors put in significant effort in addressing the comments, offering detailed responses to every reviewer. Only one of the reviewers responded during the discussion period, and the response came very late in the discussion period. However, I read the authors' response and concluded that they adequately addressed most issues raised by the reviewers.  As the model is in the Transformer space, and transformers have previously been shown to be state of the art on a number of tasks, I do not find it necessary to compare against other 'families' of methods. So I will consider that issue addressed as well.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the problem of learning the optimal threshold policy for Markov Decision Processes (MDPs) is considered. Using the monotonicity property of threshold policies, an off-policy actor-critic algorithm is proposed to learn the optimal policy. Extensive simulation results are presented to demonstrate that the proposed algorithms outperform other algorithms in the literature.The paper consider a subset of dynamic problems, in which the optimal policy is a threshold-policy. The authors use this attribute to formulate tailored off-policy actor-critic algorithms. They empirically compare their method to SOTA methods in three MDP domains and three RMAB parametrizations. The results show that their method, DeepTOP, performs better than the compared methods in all the experiments.The paper presents an algorithm to compute an optimal threshold policy in MDPs and RMABs. The main contribution is based on the use of threshold policy and simplification of the policy gradient. The proposed method is evaluated on three domains and compared with other RL-based baselines.The paper considers threshold policies problem. The authors show that the gradient for these problems has a simple expression. They propose a rephrasing of Whittle index policies for restless multi-armed bandits in the form of threshold policy.	The paper considers a subset of dynamic problems, in which the optimal policy is a threshold-policy. The authors use this attribute to formulate tailored off-policy actor-critic algorithms, for both MDPs and RMABs which are gradient-based, so can utilize neural networks. They empirically compare their method to SOTA methods in three MDP domains and three RMAB parameterizations, the results show that their method, DeepTOP, performs better than the compared methods in all the experiments.  The paper is well written and the claims are correct.  The performance of DeepTOP compared to the other methods is impressive.  All four reviewers were on the positive side for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is a nice study that sheds some light on the capability of transformers model to deduce what function the context refers to and execute/compute the value of that function given an input in the context. All the model is doing is doing sequential modeling that given a series of inputs and the corresponding function values, the model predicts the function value of the previous input. I think the paper can easily be expanded to include complex functions.Transformers are capable of in-context learning all of the simple function classes tested in the paper to a surprising extend. Carefully designed experiments show that this capability isn't explained by simple memorization of training examples. The paper is very well scoped, and the goal and the contributions are very clear.This paper tries to study the in-context learnability of function classes in transformers. The paper is well written, with a clearly defined (and well motivated) experimental protocol. The empirical results are intriguing and will be of interest to the broader ML community.The paper studies in-context learning in transformers by training models to predict the output of a linear function given a few input/output examples. The main findings are that transformers trained in this way are able to match the performance of the optimal estimator in various settings.	This paper demonstrates compellingly that transformers are able to in-context learn simple function classes (e.g., linear functions), to the extend that they can recover solutions from algorithms like LASSO. The experiments are well designed and executed, which lead to surprising and intriguing results. While the paper does not provide any explanation for why transformers exhibit such capabilities, it will spur both empirical and theoretical work studying how transformers learn algorithms from in-context examples. Congratulations on a nice work!
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper considers the problem of $L_p$-regression. In particular, the focus of this paper is on the case of large $p$ and when the matrix $A$ has structure. Theoretically, the results in the paper are solid and the result for $p=\infty$ is new. The paper is pretty well-written (some minor typos at the end of this section)The paper creates a coreset for solving Lp regression on structured input. The algorithms, theorems and lemmas are very well written. The motivation towards this technique is not well presented.This paper studies ways of subsampling tall-and-dense p-norm regression involving structured Vandermonde matrices. It shows that in this setting with additional structure, sampling by Lewis weights produces poly(p) * d row sized samples for all values of p.	Dear Authors,  The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted:  - The new result about query complexity of regression problem that the authors have added. Along with the result on   for (noisy) Vandemonde matrix, these make the paper lie above the accept bar. - The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores.  The current consensus is that the paper deserves publication.  Best AC
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Overall the paper is well written, and it was easy to follow for me. The theoretical advantages of the proposed framework such as asympototic unbiasedness seem interesting. There is a lot hyperparamters or customiazations possiblei in the framework.The paper is easy to follow and is ordered in a logical way. The importance weighting may not work in some cases, and some conditions may be required to be satisfied. The method proposed is based on the importance of reweighting.The reviewer reviewed an earlier submission of this paper for another conference. The authors claim to derive an active learning framework that is asymptotically unbiased. The paper is based on the assumption that an instrumental distribution Q exists.Theoretic analysis is conducted to show the convergence of the proposed AL framework when the number of iterations approaches infinity. It seems that in this framework, all sampled data should be saved and used in the calculation of P, Q, and a_t.	Meta Review: I find this paper to present an interesting perspective, analyzing the sampling bias in active learning and correcting for it.  There was some debate with Review YKx9 about the assumptions and rigor of the theory, but I found the author responses adequate.  I recommend acceptance.  The authors should take care to improve the readability of Figure 1.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper derives accelerated gradient flow formula in the space of probability measures from the view of optimal control formalism. The resulting algorithm from the derived PDE seems not having much practical advantage over SGHMC (a stochastic version of 2nd order Langevin dynamics)The articles adapt the framework developed in Wibisono & al to the (infinite dimensional) setting. The proposed methods either rely on strong Gaussian assumptions, or  "density estimations" No comparison with other existing method is provided.This paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The basic idea is to define accelerated gradient flows on the space of probability distribution. The experiments are a proof-of-concept on simple illustrative toy examples.	This paper developed an accelerated gradient flow in the space of probability measures. Unfortunately, the reviewers think the practical usefulness of the proposed approach is not sufficiently supported by realistic experiments, and the clarity of the paper need to be significantly improved. The authors' rebuttal resolved some of the confusion the reviewers had, but we believe further substantial improvement will make this work a much stronger contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies multi-task generalization in model-based reinforcement learning (MBRL) One of the standard ways to approach the problem is given by inferring a latent variable Z encoding each task. The authors propose to encode segments of state-action trajectories into Z vectors and maximize similarity between Zs from same trajectory.This paper considers the unsupervised dynamics generalization problem. The authors build an approaches that use past transition segments to estimate an environment-specific vector $Z$, which is then intended to act as a contextual input to the dynamics function. In practice the z's do end up being predictive of the environment id.This paper proposes a new method to tackle the problem of generalization to unseen dynamics for model-based RL. Previous methods learn to predict a vector $Z$ that characterizes a particular environment dynamics from past transitions. However, as the environment id or label is not available, this vector inevitably contains redundant information, which might hurt the generalization of the model. The paper therefore proposes an interventional approach to estimate the probability that two vectors belong to the same environment. As a result,This work tackles the problem of unsupervised dynamics generalisation in model-based reinforcement learning. They introduce a set of auxiliary losses based on relational intervention and causal reasoning to encourage the inferred context to be the same. The method shows improved prediction error and test reward on a range of continuous control tasks.	Description of paper content:  The authors propose a dynamics model that can generalize to novel environments. The train and test MDPs have the same state and action spaces but different dynamics. Environment specific inference is achieved by estimating latent vectors Z that describe the non-stationary or variable part of the dynamics. These Z-s are inferred from trajectory segments in unlabeled environments. The Z-s are learned contrastively: Z-s from the same trajectory are pulled together, and Z-s from separate trajectories are pushed apart. However, to mitigate the error of distancing Z-s from different trajectories but the same environment, Z-s on trajectories with similar transitions are also pushed together using a soft clustering penalty. These losses are justified based on ideas from Pearl’s causal inference.  Summary of paper discussion:  The reviewers concluded that the contributions are conceptually interesting and “somewhat” novel. The reviewers felt that the empirical performance gains of the method over baselines were demonstrated but not extremely impressive.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a different look at why neural networks generalize despite optimizing to zero training error, over-parameterization, etc. The paper focuses on classification with CIFAR10 and ImageNet with AlexNet, VGG, and ResNet models. In particular the results are specialized for ReLU networks training with SGD.The paper analyze the dynamic of “image functions” and show that analyzing the dynamic can explain the generalization. The idea could be potentially interesting, but I found the writing and presentation is a bit confusing.The authors present empirical results about the correlation between the activations in a neural network across time for a fixed pair of images. The authors claim that this provides insight into why neural networks generalize, by arguing that the network uses different features at different epochs during training. This is an entirely empirical work. The empirical findings reported in the paper are not insightful and the paper makes claims that are heuristic and not justified.This paper analyses how the representations of neural networks evolve during training, in particular which ReLU nodes are activated. The authors attempt to show how this leads to a diverse set of functions being learnt. The paper is poorly written, poorly structured, and the results are extremely difficult to parse.	In an attempt to understand generalization, this paper aims at understanding the dynamics of functions presented by the network for different images in the training set. Authors look at activation patterns (whether a ReLU activation is on or off) as a way of characterizing the active paths in the network and approximating the function presented by the network for each image. Authors study different related statics (eg. correlation) and how they evolve during training including.  Pros:  - Understanding the dynamics of training, how diversity is encouraged by the training procedure and its relationship to generalization is an important problem. - This paper takes an empirical approach and tries to make interesting empirical observations about the dynamics of the training.  Cons: - The paper is poorly written in terms of structure, making clear arguments with enough evidence, notation, etc. - Some empirical trends are shown but their connections to the main claim of the paper about generalization is very weak. The main attempt to connect the observations to generalization is Fig. 7 which shows model accuracy correlated with the ratio of early to mid overlap. This is problematic both because it only has 6 data points and also because a simple correlation analysis is not enough to establish this claim which is more about the cause of generalization.  Reviewers have pointed to various concerns including but not limited to clarity of the paper, lack of rigorous arguments, not providing enough evidence for the arguments, etc. Unfortunately, authors did not participate in the discussion period.  Given the above concerns, I recommend rejecting the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network.The paper presents a new characterization of generalization error bound for general deep neural networks. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018. The RHS of the equation  in Lemma 2 misses terms related with B_{d,2}.This paper proposes a generalization error bound for DNNs (and their generalizations) based on the depth and width of the networks, as well as the spectral norm of weight matrices. I have increased my rating to 7. After rebuttal: The authors have nicely addressed my comments.	I'm quite concerned by the conversation with Anonymous, entitled "Why is the dependence...". My issues concern the empirical Rademacher complexity (ERC) and in particular the choice of the loss class for which the ERC is being computed. This  class is obviously data dependent, but the Reviewers concerns centers on the nature of its data dependence. It is not valid to define the classes by the Jacobian's norm on the input data, as this _structure_ over the space of classes is data dependent, which is not kosher. The reviewer was gently pushing the authors towards a very strong assumption... i'm guessing that the jacobian norm over all data sets was bounded by a particular constant. This seems like a whopping assumption. The fact that I can so easily read this concern off of the reviewer's comments and the authors seem to not be able to understand what the reviewer is getting at, concerns me.  Besides this concern, it seems that this paper has undergone a rather significant revision. I'm not convinced the new version has been properly reviewed. For a theory paper, I'm concerned about letting work through that's not properly vetted, and I'm really not certain this has been. I suggest the authors consider sending it to COLT.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces a new normalization method named pixel-wise tensor normalization which improves both accuracy and robustness of the model. The results shows somewhat improvement, but not significant. The paper still needs further polishment and is not ready for publication at the moment.This paper proposes two techniques including tensor normalization and full distribution training to improve model robustness. These two techniques are easy to understand, and bring improved robustness compared with the baseline. Here are some suggestions for the authors.	Reviewer KVLA raises some concerns on the completeness of this paper, suggesting that this paper needs further improvements for publication. Based on the comments of both reviewers, this paper is accepted as a long paper. Please address the reviewers' comments in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This works proposes a scalable way of approximating the eigenvectors of the Laplacian in RL by optimizing the graph drawing objective on limited sampled states. The authors empirically show the benefits of their method in two different types of goal achieving task.This paper proposes a method to learn a state representation for RL using the Laplacian. One use-case of the learnt state representation is for reward-shaping that is said to accelerate the training of standard goal-driven RL algorithms.The authors propose a Laplacian in the context of reinforcement learning, together with learning the representations. The method is also well illustrated and compared with other methods, showing the efficiency of the proposed method. Overall the authors make a nice contribution.	This paper provides a novel and non-trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Eigenvectors of the Laplacian have been used for proto-value functions and eigenoptions, but it has remained an open problem to extend their use to the non-tabular case. This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian.   The paper could be made stronger by including a short discussion on why the limitations of this approach. Its an important new direction, but there must still be open questions (e.g., issues with the approach used to approximate the orthogonality constraint). It will be beneficial to readers to understand these issues.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces “Cross domain schemas” (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the user’s high-level intent. Also introduced is a model which leverages CDS to improve semantic parsing to a meaning representation language.Results are interesting but the paper has some major limitations. The paper totally disregard the work on Semantic Role Labelling and on languages for expressing the general meaning of language. The key idea of the paper seems to be the existence of an intermediate representation language to encode meaning for utterances.This paper describes a two-stage encoder-decoder model for semantic parsing. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset, but is still behind the traditional slot-filling models.	Interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, MRL) for language understanding. Experiments have been performed on the recently released SNIPS corpus and comparisons have been made with multiple recent multi-task learning approaches. Unfortunately, the proposed approach falls short in comparison to the slot gated attention work by Goo et al.  The motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose.  Experimental results could be extended to the other available corpora mentioned in the paper (ATIS and GEO).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work studies fully single-loop stochastic algorithms for bilevel optimization problems where the inner problem is strongly convex and the inner and outer objective are smooth and given by a finite sum. The authors propose 2 methods: SOBA and SABA. SOBA updates $x, z, v$ similarly to single-level SGD and uses two-timescales decreasing step-sizes. SABA uses variance reduction, similarly to SAGA.This paper proposes a simple framework for solving bilevel optimization. The framework involves only 3 unbiased estimation in each iteration. The authors provide theoretical convergence guarantees for both SGD version and variance reduction version.In this paper, the author consider a finite-sum stochastic bilevel optimization problem. The proposed framework is matrix-inversion free and is very convenient to incorporate the SGD or SAGA variance reduction technique. However, the theoretical result does not justify the reason for using variance reduction.	The main topic of this work is stochastic bilevel optimization. It provides an efficient algorithm for this task, and provides theoretical results in this setting.    The reviewers are unanimous that this is well-presented work of high quality and should be accepted, and so do I.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose an approximate MCMC method for sampling a posterior distribution of weights in a Bayesian neural network. While the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance.This paper considers a new learning paradigm for Bayesian Neuron Networks (BNN): learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a "functional space" The approach is demonstrated on various tasks.The idea of extending  Riemannian Langevin dynamics to functional spaces is elegant. However it is extremely hard to follow the proposed method as details are kept to a minimum. The finite approximation of the posterior distribution is a function of the parameters theta, however it displays parameters lambda.	This paper addresses a promising and challenging idea in Bayesian deep learning, namely thinking about distributions over functions rather than distributions over parameters.  This is formulated by doing MCMC in a functional space rather than directly in the parameter space.  The reviewers were unfortunately not convinced by the approach citing a variety of technical flaws, a lack of clarity of exposition and critical experiments.  In general, it seems that the motivation of the paper is compelling and the idea promising, but perhaps the paper was hastily written before the ideas were fully developed and comprehensive experiments could be run.  Hopefully the reviewer feedback will be helpful to further develop the work and lead to a future submission.  Note: Unfortunately one review was too short to be informative.  However, fortunately the other two reviews were sufficiently thorough to provide enough signal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper aims to address the issue of mitigating side effects in policy learning. The authors propose an algorithm SARL, which uses a safe policy to define a regularization term for penalizing the agent's actions. The proposed algorithm is competitive across the experiments presented in the paper.The paper addresses the timely and important problem of how to train RL agents such that they solve desired tasks while not engaging in undesired behavior. Experiments are performed on SafeLife, which provides a suite of tasks in an environment. The main idea of the paper is to co-train an RL policy on this side effect measure with the aim of minimizing side effects.The paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning (RL) agent. The proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects. The authors then empirically investigate the effectiveness of combining the two agents.This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects. The key idea is that a safety policy is learned independent of the task reward. The method is tested on SafeLife Suite, and its performance can match task-specific safe learning baselines.	Based on the paper, reviewers' comments and discussions, and the responses, the meta-reviewer would like to suggest the authors to improve the paper and resubmit.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. The benefits over existing models are the structured, 3D object representations which allows to manipulate objects in the scenes. This paper also shows that the inferred object representations can be used in a visual reasoning task.The paper aims to decompose a scene into objects and infer the representations of 3D occupancy, color, and pose for each object from a single image of the scene without supervision. The experiments show that the proposed model achieves competitive 2D segmentation performance on CLEVR6.This paper proposes a model which is able to segment 3D scenes into objects by a combination of slot-attention and a mixture of object NeRF functions. The method receives a single input image (with the camera coordinates though these are fixed) and extracts a set of slots - one slot for each object. Results are demonstrated on CLEVR data as well as CATER (which is visually very similar) and some downstream tasks.This paper proposes a model to infer structured 3D object representations from a 2D scene in an unsupervised fashion. The model adopts a similar mechanism as Slot Attention to derive object latent code. The rendering part takes the idea from 3D neural rendering to represent all objects excluding the background.	The paper proposes an approach for learning a decomposition of a scene into 3D objects using single images without pose annotations as training data. The model is based on Slot Attention and NeRF. Results are demonstrated on CLEVR and its variants.   The reviewers point out that the method is reasonable and the paper is quite good, but even after considering the authors' feedback agree that the paper is not ready for acceptance. In particular, the key concern is around experimental evaluation - that it is performed on one dataset (and variants thereof) and that the evaluation of the 3D properties of the model is not sufficiently convincing: it does not outperform 2D object learning methods on segmentation and is not compared to those on "snitch localization".  Overall, this is a reasonable paper, and the results are promising but somewhat inconclusive, so I recommend rejection at this point, but encourage the authors to improve the paper and resubmit to a different venue.  (One remark. The paper makes a point of not using any annotation. It is technically true, but in practice on CLEVR unsupervised segmentation works so well that it's basically as if segmentation masks were provided. If the authors could demonstrate that their method - possibly with provided coarse segmentation masks - works on more complex datasets, it would be a nice additional experiment)
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes the algorithm "AdaRL" a transfer method for reinforcement learning for different domains. The method is based on learning a latent representation with "domain shared" and "domain specific" components. The transfer is done by collecting some data in target domain and estimating " domain specific" variable of the target domain.The paper is a good paper but could benefit from some clearer writing as discussed in the clarity section below. Instead of implicitly updating the policy using data from the source domain, learn a particularly structured latent model and the elements of variation and learn a policy that performs pretty well. At test time, estimate the elements and provide them as input to the policy. I think the explicit factorization is great and the results are impressive.This paper proposes a method that learns structural relationships between variables of the RL system so as to be able to adapt to changes across domains. My two main concerns are clarity and the quality of the empirical evaluations. Overall I quite liked this paper as it introduces a well-motivated idea that can be quite useful to the research community.The paper investigates the question of transfer via identifying a small number of changing variables to maximise data efficiency. The method is evaluated on variations of a cartpole and a pong domain and evaluated against a set of recent, competitive baselines.	The authors present a method called "AdaRL" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart-Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the effect of “robustification” (i.e., adversarial training or data augmentation) of models on the accuracy to seen and unseen perturbations. The authors propose a technique to “standardize” the robustification process across different perturbation. It is unclear exactly what contributions this paper is making.The paper studies the degradation in performance of deep learning models under both adversarial as well as natural perturbations or contamination. The driving question of this paper seems to be whether naturalperturbations hurt generalization as much as adversarial examples. It is not always clear if the reported results are over training or testing samples.This paper studies an interesting problem. Natural perturbations have gained increasing interest from the robustness community. In some cases, adversarial training hurts natural robustness. The notation throughout this paper is at times confusing.	The reviewers indicated a number of concerns (which I agree with) which have not been addressed by the authors as they have not provided any response.  Indeed, the paper would be significantly improved once these issues are addressed.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper suggests a simple denoising technique for existing semi-supervised learning methods (with surrogate objectives) The proposed objective is shown to have less variance in some cases, and is now consistent when $n->\infty$.This paper tries to give theoretical guarantee about the safeness of semi-supervised learning methods. The paper's presentation is very poor and very hard to follow. Theoretical ananysis about the generalization are discussed.This paper proposes to debias the training objective for semi-supervised learning (SSL) methods. The paper suggests a simple modification to the training. objective, which de-biases it, by reducing the certainty on labelled datapoints. Then, the authors proceed to prove statistical learning results on the debiased SSL method.The authors propose a method of removing the bias in semi-supervised learning: debiased SSL (DeSSL) The paper provides generalization error bounds for the proposed methods based on the missing completely at random (MCAR) assumption. The paper has good clarity on the theory presented and generally sound approaches and good results.	This paper provides an interesting generalized perspective on SSL techniques and proposes a debiasing technique that can be viewed as decreasing the variance of the risk estimate. The authors argue that this leads to estimators that are better than the purely supervised estimator under a rather weak assumption called MCAR - which assumes that the probability of a missing label is independent of covariate and label.   Although the exposition and perspective are interesting, this paper is borderline for the following two main reasons:  1. A few reviewers were not convinced of the theoretical result that is supposed to show that unlabeled data strictly helps - indeed, whereas usual variance reduction techniques (e.g for optimization schemes such as SGD etc.) lead to a strict gain in terms of convergence rate, there is no clear asymptotic/high probability statement that indicates statistical gain of the corresponding estimator (which is what we really care about - not the risk estimate). The dependence of lambda_opt on theta (which changes every iteration) does not help in providing such a statement. Since this is the primary contribution of the paper, I would suggest the authors follow through with the analysis to show a gain for the actual estimator compared to the "complete case" (only using supervised data).   On that note, the authors claimed in their rebuttal that they have added an asymptotic variance analysis in Appendix I, which indeed would have made a very valuable point - however, I could only find a copy-pasted version of Theorem 3.1. in Appendix I? Similarly, Appendix F does not seem to include the comparison between debasing using labeled and unlabeled data but instead contains the proof of Theorem 3.2. Perhaps the wrong revision was uploaded, but unfortunately, given the current version, this point is not adequately addressed.  2. If the experimental results were more extensive and conclusive, then the current theorem could have perhaps been alright as a mainly methodological contribution. However, as the authors note, extensive experiments require a lot of compute power - however, given the lack of the ultimate theorem, the methodology becomes the primary contribution and would thus require more experimental evidence as the reviewers asked for.    Addressing one of the above points would push the paper above the acceptance threshold which we hope the authors can pursue in their next submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The empirical evaluation appears quite weak. The experiments are carried out using only synthetic datasets. The presentation of the learning algorithms needs additional details. It is not very clear what is the contribution of this paper beyond previous work.The paper is well written and sound; the topic is interesting and fits UAI's domains - it can lead to interesting discussions. The approach is original. Further developments and experiments are necessary, but if they confirm the significance of the approach, the result is valuable.The paper fits the topic of the conference perfectly. I am missing evidence that the problem studied is truly useful. There does not seem to be an experiment that would work on some (semi- realistic data to solve a meaningful problem.The first half of the paper is well written, particularly Sections 2 and 3. I felt the authors covered a lot of ground and also explained many of the complicated ideas fairly clearly. I feel the second half could be significantly improved, in clarity and perhaps also in experimental content.	Meta Review: The paper presents a new means to learn sparse Choquet integral, using for instance classical L_1 penalization techniques. This research participates to a long trend of research aiming at learning preference functions in various settings (here, having only limited preferential information).   All reviewers agree that the paper is well-written and the contribution nicely presented. Two main critics were done by the reviewers:  * The first is the potential significance of the research, i.e., that it addresses an actual real-world problem. After discussion and rebuttals, this critic appears to be of less importance  * The second (with which I would concur) is that the experimental part is limited in different ways. The main critics regarding those are as follows: 1. The authors only use synthetic data that mostly comply with their assumptions, thereby confirming that their approach is valid. THey do not really challenge it. 2. There is no realistic data sets used (which is very difficult to obtain in incremental preference learning, but less in passive preference learning). 3. There is also no real comparisons with other methods intending to learn value functions from observed preferences. Given the large literature on preference learning and the fact that most learning papers require the comparisons with other baselines when possible, one could have hoped to have such comparisons.   Depsite that, the paper appears to be well-made with a relevant algorithmic contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed SCL-WC method outperforms state-of-the-art weakly-supervised whole-slide image classification studies. The application of contrastive learning, i.e., MoCov3, in histopathological image classification is very common.The paper proposed a novel framework for Weakly-Supervised Whole-Slide Image Classification. It uses a self-supervised contrastive representation encoder trained with large-scale data. Ablation studies are conducted to help understand the importance of each proposed component.The paper explores the multiple instance learning problems, i.e. the whole slide image classification task. The main goal of the weakly-supervised cross-slide constructive learning module is to pull WSIs with same disease types closer and push different WSIs away.This paper presents a weakly supervised contrastive learning approach for the classification of whole-slide images (WSI) The proposed method consists of two main steps: a domain-specific self-supervised feature extractor and task-specific feature aggregation modules.	This work presents a method to obtain slide level representations in computational pathology. The specific contributions of this work are a positive-negative-aware module (PNM) and a weakly-supervised cross-slide contrastive learning (WSCL) module and a loss to encourage intra-WSI local patch separation and inter-WSI global feature contrast. The idea is to use the attention weights in a MIL framework as patch level pseudo labels. These are used to compute "weights" for positive and negative patches (the latter of which is assumed to be significantly larger in number for a given WSI). By using these weights in a contrastive manner to push the representations from positive patches away from those of negative patches, the model learns more effectively since it is less susceptible to the noise from the negative patches.  The reviewers found these contributions novel. During the review, the largest source of concern was along the choices made during the empirical evaluation. Specifically, the manuscript in its current form lacked empirical backing for several of the choices made regarding the neural architecture, the algorithm for self-supervised learning etc. In response to this the authors conducted several different kinds of ablation studies (which were incorporated into the supplement) during the rebuttal process, which other reviewers found convincing as a potential explanation of the outcomes.  Overall, I found the main contributions of this work (leveraging patch level attention weights as psuedo labels) to be an interesting use case for computational pathology to better focus the learning signal on positive patches. My additional comment is that I think the additional ablation experiments (and references) are an important part of the contributions of this work and should be incorporated into the main paper rather than in the appendix (several equations can be compressed to make space in the manuscript in addition to the additional page).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper builds upon the work of Luo et al., 2021 and characterizes the phase diagram for different dynamical regimes in the case of a three-layer neural network at infinite width. The paper shows that different layers can be simultaneously in the linear, critical or condensed regime.This submission presents a detailed numerical study of gradient flow of feedforward neural networks. Different layers of the three-layer architecture exhibit changes to their parameters. To the best of my knowledge, this submission gives new insight into training dynamics of MLPs with more than one hidden layer.This work studies the effect of initialization scalings on the dynamics of three layer neural networks. Despite a few typos the paper is well presented and easy to read. The most important part of the paper, where the parameters gamma2 and gamma3 are derived, is rushed.	This work studies the effect of initialization scaling on the gradient flow training dynamics of three layer (two hidden layer) MLPs. . Extending Luo et al. 2021's analysis, the authors identify "linear", "critical" and "condensed" regime depending on initialization scaling parameters  Strength pointed out by the authors include, "new insight into training dynamics of MLPS with more than one hidden layer", "elegant and insightful analysis" for a "dauntingly difficult problem" and that "phase diagram is neat and valuable contribution"  Reviewer `PDY9` saw major weakness as treating the second layer as linear regime only, which makes the analysis similar to previous work (Luo et al., 2021). In general, the reviewer believes that the full comprehensive analysis on different phases for each layer is lacking. Although the reviewer did not respond to the AC, this issue has been addressed by author response. Also please do follow the recommendation of the reviewers in terms of improving writing (better description of gamma_2, gamma_3 for example), improving related works, as well as making figures more legible.   Overall, while the paper is somewhat borderline, there are interesting insights and analysis as the reviewers pointed out without critical issues. I recommend accepting this work at NeurIPS 2022.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper extends the variational autoencoder framework with a richer prior distribution to model more complex correlations in the latent variable distribution. The paper tests the method on three datasets, a synthetic 40 dimensional spiral dataset, the venerable MNIST dataset and a scaled down CELEB A dataset.The paper introduces an additional prior-encoder network to autoencoders to learn an unconstrained prior. The autoencoder and prior- Encoder networks are iteratively trained with the sliced Wasserstein distance (SWD) To strengthen SWD, this paper further applies nonlinear transformations with a structural consistency term.The paper addresses the issues of representation learning with VAEs and proposes EPSWAE as a solution. It applies a prior encoder to construct an implicit prior, which is more flexible. The paper also proposes a graph-based algorithm for minimizing the pathwise energy to achieve the manifold walking.	All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a genetic algorithm framework for molecular optimization. It produces valid molecules through the crossover and mutation operations along with appropriate fitness functions. The proposed model outperforms other baselines in the PlogP optimization benchmark.This paper proposes a genetic algorithm for constrained molecular optimization. The method is evaluated on logP optimization task, with minor improvement over previous work GA-DNN. The main claim of the paper (benefit of two-stage procedure) is not supported by empirical results.The authors proposed a genetic algorithm for molecule generation. The reward is a combination of several objectives like Eqn.(3) The genetic operations include crossover and mutation. The authors conduct experiments to optimize the LogP of molecules.This paper proposes to use a two-stage genetic algorithm for similarity-constrained molecular optimization. The experiments in the paper are limited and not very relevant to real-world drug discovery. It would be more interesting to explore a variety of more complex tasks.	The paper describes a genetic algorithm for molecular optimization under constraints. The aim is to generate molecules with better properties while close to an initial lead molecule. The proposed approach is a two-stage one. The first stage aims to satisfy constraints and searches for feasible molecules that are similar to the lead. The second stage optimizes the molecular property. The method is evaluated on logP optimization task, with minor improvement over previous work.  The reviewers point out the following strengths and weaknesses:  Strengths:  - Molecular optimization under structural constraints is an important research direction. - Comprehensive related work section.  Weaknesses:  - Lack of novelty because it is a standard application of genetic algorithm. - The results show that the proposed method did not outperform existing baselines. - The main claim of the paper (benefit of two-stage procedure) is not supported by ablation study. - The authors only conduct experiments on improving LogP, which is a benchmark that is too easy and not challenging. - The objective function and cross-over operation are the same or very similar to previous work. - The experimental evaluation is limited, and the overall setting is not very relevant to real-world tasks.  Overall, all reviewers vote for rejection. It is clear that the paper needs more work before it can be published.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Circulant MinHash (C-MinHash) to approximate the Jaccard similarity in massive binary data. Compared with MinHash, C-Min Hash only requires two (or maybe one in practice) random permutations in a circulant manner for approximation. Extensive experiments validate the effectiveness of C- MinHash.Theoretically, C-MINHASH provides unbiased estimate, and its variance is smaller than MINHASH. Extensive empirical experiments verify the theoretical analysis. The benefits of using only two permutation is not well justified. The storage cost may not be a concern as one can generate random permutations on the fly without storing them.This paper proposes an effective approach for MinHash by permutating data vectors. It first randomly shuffles the data to break structures exhibited in the original data and then performs permutation K-times to obtain K hash values. This paper shows the theoretical approximation error of the proposed approach.The classical MinHash data structure generates K hash values for a binary string by generating K independent random permutations of the binary string. The new algorithm uses the same permutation but shifted by 1, 2, …, K positions. This is an interesting and surprising result as the algorithm is neat and simple.	This was a somewhat unusual submission in that the authors tried to motivate their paper by pointing to a separate anonymous manuscript.  However, the authors didn't seem to want to confirm they would merge the manuscripts when asked about this. It was thought that in fairness the submitted manuscript should be judged on its own. After discussion, it was agreed that the submitted paper on its own, did not generate enough enthusiasm to merit acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is an exciting research problem, and could be of broad interest in robotics. The problem posed, and associated data sets and simulation code, could be an interesting and novel source of challenge to machine learning researchers. However, the paper appears to be a fairly early work in progress.The paper is clearly written although it contains several typos. The proposed task of cross-modal inference is an interesting task. I however hardly find any significance of the proposed method. Overall, the paper seems to require a significant improvement.This paper is poorly written, and looks like it was not proof-read. Authors ought to better define the image description space of the objects and the haptic space. Concepts have to be clearly defined prior to their use.	The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.  The research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper claims to have 3 main contributions. It explains why the two tricks work in zero-shot learning (ZSL) It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL. It also considers continual ZSL (Sect. 4)The paper shows that normalization is critical for zero-shot learning (ZSL) In the ZSL randomization is coming from the two sources, attribute and feature. Normalization of the two source helps to reduce the variance. Paper also extend ZSL framework to the continual learning ZSL setup. Data comes in the form of a task, sequentially.The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning, in the perspective of data variances. It also demonstrates these tricks are not enough w.r.t. normalizing the variance in a non-linear model. The paper proposes a class normalization trick to alleviate the variance inflation/diminish.This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time. It also shows that two normalization tricks are not enough to variance control in a deep architecture. The experiments for CZSL are performed in two datasets, CUB and SUN.	This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Falconn++ keeps only the top-($\alpha B/iProbes$) points (closest to the hyperplane $r_i$) in each bucket. The closeness is defined by the threshold on the inner product distance $t = (1 − r 2 /2) 2 log D$.Falconn++ combines the LSF property with the LSH to achieve a lower query time complexity beyond the vanilla LSH schemes such as Falconn. Extensive experiments validated the superior performance of Falconn++. The paper is well organized and easy to follow.Nearest neighbor search is a backbone of many recommender and retrieval systems. The paper proposes an interesting extension of the special LSH scheme for angular distance (and cosine similarity) nearest neighbor search. The method improves upon the original FALCON and claims to be comparable to HNSW.	The paper provide a good and exciting improvement over LSH based widely used Falcon Library. All the reviewers found the contribution worthy of publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies learning in stochastic games, which are extensions of Markov decision processes (MDPs) from the single-agent setup to the multi-agent one. Here the objective of each learner is to optimize her own reward function. The main contribution is a model-based algorithm called Nash-VI.Algorithm is model-based, based on successive episods of planning and counting. It involves solving a matrix game at each iteration, looking for a notion of equilibria that is computable in polynomial time.The authors study reinforcement learning in two-player (and more) Markov games, providing new algorithms and bounds. They reduce the dependence on horizon H and states S from H^4 S^2 to H^3 S, matching information theoretic lower bound for these factors.The authors consider self-play in tabular zero-sum episodic Markov game. The goal is to learn an \epsilon approximate of the Nash equilibrium while minimizing the sample complexity. This rate matches the lower bound of order \Omega(H^3S(A+B) up to a factor min(A,B)	The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper analysises the loss minimisation in second-order learner for epistemic uncertainty quantification. It shows the proposed loss functions do not incentivise the learner to represent the uncertainty in a faithful way.This paper demonstrates the problem of current approaches while modelling epistemic uncertainty by minimising a given loss function. The message is relatively straightforward: (1) the averaged (proper) loss minimisation shouldn't be used for the level-2 uncertainty. (2) the prior / regularisation setting might also lead to inappropriate uncertainty for a Bayesian setting.The paper analyzes the recent line of work on uncertainty estimation in neural networks. The theoretical results are an important contribution to the literature. The analysis is limited in scope, applying only to a certain losses proposed in prior work.This paper discusses the difficulty in learning a predictor that exhibits the epistemic uncertainty given labeled examples as a training dataset. The strength of this paper is that it is relevant and important in the literature. The weakness is that a limited amount of numerical results may restrict the understanding of readers.	This meta review is based on the reviews, the authors rebuttal and the discussion with the reviewers, and ultimately my own judgement on the paper. There was a consensus that the paper contributes interesting insights on uncertainty quantification, and most reviewers praised several aspects of the submission. I feel this work deserves to be featured at NeurIPS and will attract interest from the community. I would like to personally invite the authors to carefully revise their manuscript to take into account the remarks and suggestions made by reviewers. Congratulations!
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the tail properties of the regret attained by multi-armed bandit schema. It shows that any consistent policy must suffer a heavy tailed regret distribution. The schemes achieving this behaviour are quite simple - successive elimination or UCB.This work proves that an algorithm cannot incur an optimal expected regret and achieves light-tailed meanwhile. It shows the trade-off between these two targets in Theorem 1, and studies the performances of the new SE and UCB algorithms theoretically and numerically.The paper studies tail behaviors of bandit problems. It proves that any algorithm that achieves instance-dependent consistency (logarithmic regret) must exhibit heavy-tailed regret. On the positive side, it shows that it is possible to achieve light-tailed and worst-case optimal regret at the same time.The authors consider a novel problem of characterizing the tail behavior of the random regret for algorithms that are designed to solve the multi-armed bandit problem. They show that the distribution of regret has a heavy tail in the large T regime for the standard policies.	The reviewers came to consensus that this paper makes a good contribution to the study on the tail behavior of the regret of bandit problems. I agree with these opinions and please polish the paper so that the minor concerns raised by the reviewers become clear in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Nocturne is able to generate 2D views of the world that are visible to any actor. The authors train RL and IL agents on the benchmark, investigating the performance of the agents as a function of the number of training scenarios.Nocturne is a new 2D driving simulator, with a focus on multi-agent coordination under partial observability. The simulator is run on real-world data at a high-frequency rate, providing an accurate account of driving situations.Nocturne is a 2D-driving simulator constructed on real-world data and designed for partially observed MARL research. It first reconstructs maps and replay objects' trajectories contained in real- world datasets. After that, traffic vehicles are turned into controllable agents with partial observability.Nocturne is the only simulator that can compute an agent’s visible objects and step the agents dynamics at above 2000+ steps-per-second. Compared to previous works, Nocturne has more efficient environment interaction efficiency.	Overall, this paper provides a great starting point for future benchmarking experiments. The reviewers engaged in a lively discussion with the authors and provided valuable suggestions for future improvements, which the authors have integrated in their submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides a detailed discussion of the main characteristics of harmful text that are, or can be, generated by (large) language models. While the paper offers many critical insights and has many noteworthy merits, it might still benefit from a second round of reviews. The authors fail to provide clear definitions of key terms that are used in the paper such as harm, bias, and toxic.This paper presents six characteristics of harmful (English) text to be considered when assessing language generation models. The authors focus on large language models, but this study could be related to any language generation benchmark. The proposed characteristics are important and should be considered by future studies.This is a very interesting paper. It focuses on the issue of potential harms that can arise from the usage of modern large scale language models. The paper restricts to autoregressive models, as those are often used in generation. Overall, the paper has merit, as it promotes a discussion about important issues.The submission draws upon existing literature on machine learning fairness to create a framework of analysis of harm benchmarks for language models. The analysis framework it presents is exhaustive and sheds light on distinctions that might be overlooked. The submission does not introduce new benchmarks or datasets.The author introduced six characterizations of harmful text to benchmark language models in this paper. The paper further showed the characteristics that are missing from the existing benchmarks. These characteristics have been applied to Perspective API as a case study.The paper outlines six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. The authors then use these characteristics as a lens to identify trends and gaps in existing benchmarks and conduct case study of the Perspective API.	The paper proposes a framework for characterizing harmful text generated from LLMs. This is a timely and extremely important topic, and the paper engages with the complex socio-technical questions around it. The reviewers are split in their opinions (three in favor of acceptance; three opposed). The key argument raised against the paper is that it does not introduce a new benchmark or dataset, and does not make falsifiable claims. The meta-reviewers consider the paper to be well in-scope for the track: despite lacking empirical results, it provides a "framework for responsible dataset development" and clearly identifies "significant problems with existing datasets."  The authors successfully address the other concerns. The paper makes an important contribution to the community and deserves to be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work studied an interesting topic of training-free Neural Architecture Search (NAS) The experiments show the proposed approach has improvement in searching time. The major contributions of this work, the two network measurement methods, were proposed by previous works.This paper introduces a searching framework of neural architectures ranking the candidates with two different metrics: the spectrum of NTK and the number of linear regions in the input space. These two metrics do not require the training of neural networks lightening the computational burdensome. Authors support their method by providing results from CIFAR-10, ImageNet, and NAS-Bench-201 benchmark.The authors proposed TE-NAS to rank architectures by analyzing the two indicators. The method can search within 30 minutes (on CIFAR-10) and 4 hours (on ImageNet) On NAS-bench-201 and DARTS search spaces, the propose method’s results are extremely promising.This paper aims to speed up NAS with a training-free performance estimation strategy. It estimates an architecture’s performance from two perspectives: (1) trainability, and (2) expressivity. The metrics for trainability and expressivity seem to be the direct application of deep learning theories.	The authors propose training-free neural architecture search using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure "trainability" of the architecture), and the number of linear regions in the input space (to measure "expressivity"). These two heuristics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training-free Neural Architecture Search. It is certainly not the first training-free NAS proposal, but achieves competitive results with much more expensive NAS methods.  A few reviewers mentioned limited novelty of the method, a claim with which I agree. The contribution of the paper, however, is something different than how it was presented. The core message seems to be that the two proposed heuristics can greatly speed up NAS, and should be a baseline method against which more expensive methods should test.  I feel like this is a borderline paper, but may be of interest to researchers in the field.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a modification to existing information theoretic feature selection algorithms which adds a strong relevance term estimated using a k-nn MI estimator. It's a slightly expanded copy of a paper published at IEEE International Symposium on Information Theory 2020, referenced as "Exploring unique relevance for mutual information based feature selection" Liu & Motani 2020. This paper contains the same experimental results, same plots, same theoretical description.In this paper, the authors recognized the function of unique relevance (UR) of features for optimal feature selection. As a result, they proposed a new criterion called MRwMR-BUR. Experimental results show that MIBFS with UR consistently outperform their unboosted conterparts.This work suggests improving mutual informaton based feature selection methods with an extra term (i.e., the unique relevance (UR) The work is easy to follow. However, the perspectives and methods are not novel. And there is a technical flaw in the analysis.The paper presents an investigation of Mutual information based feature selection methods and the use of unique relevance (UR) with mutual information. The results indicate that the method improves the baseline. Using the UR measure can be beneficial in many domains.	During the discussion among reviewers, we have shared the concern that this work has a significant overlap with [Liu et al. 2018] and [Liu & Motani 2020]. Although the authors tried to address this concern by the author response, I also think that the difference is not enough. In particular, the reviewers pointed out that Figure 1, Table 1, and Figure 3 are exactly the same with those in [Liu, 2020], and Proposition 2 in [Liu & Motani 2020] is Proposition 1 in this paper. Since these overlaps are not acceptable, I will reject the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors define memorization as any situation in which the model predicts the correct word as maximum probability given its context. They find that larger models memorize faster under this definition. They then study memorization paper according to part of speech, suggesting that rare words tend to lead to memorized sequences.The authors show that larger-sized language models memorize training data faster. This memorization happens before the overfitting of language modeling. Specific part-of-speech-tagged tokens are memorized faster like nouns and numbers.Larger models also tends to forget less, and they seem to memorize unique parts of speech tokens such as nouns and numbers. The authors have not discussed the limitations of their work thoroughly.This paper investigates the memorization and forgetting dynamics of large language models during training. The paper measures memorization as the proportion of correctly predicted labels (which corresponds to predicting the correct next/masked token) and forgetting as the decline in memorization.	This paper studies the underlying training and memorization dynamics of very large language models. The main take aways are that larger-sized language models memorize training data faster, and that this memorization happens before the overfitting of language modeling. Tokens with certain part-of-speech tags (nouns, numerals) seem to be memorized faster during training.   Overall, most reviewers feel positively about this paper, agreeing that it tackles an important problem and that it provides a solid contribution. The experimental results are detailed and use reasonable metrics for data memorization, including the forgetting identifier experiments. Some of the weaknesses that have been pointed out (e.g. regarding the significance of the part-of-speech tags experiment, clarifying the criteria for memorization, etc.) seem to have been well addressed during the author response. Therefore, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The Iterefin method is able to the predicted types as supervision to finetune the embeddings (ComplEx and ConvE) The experiments show substantial improvement on datasets with rich ontologies (not wordnet) The effects of iteration are minimal, so it is not clear that they are useful.The paper addresses the KG refinement task, aiming to improve KGs that were built via imperfect automated processes. The key insight is to augment KG embeddings not only with implicit type information, but also with explicit types produced by an ontology-based system. The resulting algorithm, TypeE-X, leverages the benefits of structured (often human-crafted) information and the versatility of continuous embeddeds.The authors propose a new method for identifying noisy triples in knowledge graphs. They combine Probabilistic Soft Logic with an embedding method, such as ConvE or ComplEx. They show that combining these two approaches improves performance on four different datasets with varying amounts of ontological information provided.This paper tackles the task of knowledge-base refinement. The described method uses an approach based on co-training using a combination of two models. The experiments are conducted on four datasets based on NELL, YAGO, FB15K, and WN18.	This paper proposes a novel method, IterefineE, for cleaning up noise in KGs. This method combines the advantages of using ontological information and inferences rules and KG embeddings with iterative co-training. IterefineE improves the task of denoising KGs on multiple datasets. While the importance of multiple iterations is mixed, reviewers agree that the combination of two significantly different types of reasoning is a promising direction.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors show that certain complete neural network verifiers can be mislead by carefully crafted neural networks that exploit round-off errors. Such a construction can be obfuscated by taking advantage of the compounding effect when there are many layers of the network.The paper shows that it is possible to fool exact verifiers using numerical instabilities. The problem is first put into light on a very simple architecture and then on more complex ones. A defence to this behavior is proposed, making all network parameters slightly noisy.Adversarial neural networks can cause the complete verifier to produce imprecise results in floating point arithmetics. They also showed it is possible to insert a backdoor to the network such that the backdoor is missed by the verifier while it can trigger some behavior desired by the attacker.The paper presents a method to create neural networks that, due to floating-point error, lead to wrong robustness certifications on most input images. The authors show how to make their networks look a bit less suspicious. The paper has several weaknesses related to presentation.	The authors demonstrate that complete neural network verification methods that use limited precision arithmetic can fail to detect the possibility of attacks that exploit numerical roundoff errors. They develop techniques to insert a backdoor into networks enabling such exploitation, that remains undetected by neural network verifiers and a simple defence against this particular backdoor insertion.   The paper demonstrates an important and often ignored shortcoming of neural network verification methods, getting around which remains a significant challenge. Particularly in adversarial situations, this is a significant risk and needs to be studied carefully in further work.  All reviewers were in agreement on acceptance and concerns raised were adequately addressed in the rebuttal phase, hence I recommend acceptance. However, a few clarifications raised by the official reviewers and public comments should be addressed in the final revision: 1) Acknowledging that incomplete verification methods that rely on sound overapproximation do not suffer from this shortcoming. 2) Concerns around reproducibility of MIPVerify related experiments brought up in public comments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a deep state space model for videos. The dynamics are defined by linear Hamiltonian Dynamics. A latent variable z is introduced for explaining content and kept fixed for all frames. Experiments are carried out on Sprites and MUG to demonstrate the efficacy.The paper proposes Halo, a novel type of variational autoencoder with structured latent space. It demonstrates its applications to different types of (controlled) video generation tasks. The main contribution is a principled decomposition of the latent space into a content space and a motion space.This paper deals with the task of generating image sequences. The authors propose a method called Halo that allows to disentangle the content from the motion in image sequences, in the VAE framework. They do so by separating the latent space in two spaces: 1) the content space, a global content vector that summarizes the image sequence, and 2) the motion space.	This paper proposes a novel type of variational auto encoder, referred to as HELO. The latent space is decomposed into a content space and a motion space, and the main contribution is the proposal to model the motion space using Hamiltonian dynamics. All reviewers agree that the idea of using Hamiltonian dynamics is interesting and novel. One main critique, that the authors agreed on, was that the operator does not contain any stochasticity and that this might be a limitation when applying the idea to model more complex data. Another remark was that the experiments are limited and experiments on less constraint data are missing. A quick look at the baseline methods revealed that they also use the same kind of data sets to evaluate their methods, so this latter concern might be of minor importance.  All in all, the potential positive outcomes of this paper outweight its current limitations, so we recommend acceptance at this point, while urging authors to address the remaining concerns in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Authors experiment on polyphonic music modeling and language modeling. Authors propose a diffusion model that adds a mask as an additional inputs to noisy data. The mask controls which positions in the input should be denoised, to provide control for partial inference.This paper introduced a new generative modelling method for structured data. It demonstrated its effectiveness on symbolic polyphonic music and character level and word-level text modelling. This method also offers the opportunity for subtle control at inference time, which could be useful for downstream human-in-the-loop applications.This paper proposes SUNMASK, which models discrete sequences without ordering assumptions. The organization of the writing is very loose, making it very confusing for the readers. The analysis is not very convincing both quantitatively and qualitatively. The authors did not really cover too much about limitations...This paper proposes a new non-autoregressive generative model for discrete sequences which combines a recent work SUNDAE with masking. The mask is provided to the network as additional conditioning - unlike e.g. BERT which masks out the tokens themselves.A generative sequence modelling based on masked unrolled denoising autoencoders is proposed in this manuscript. Compared to the baseline, the authors claim SUNMASK has a better performance on music modelling. This idea is very close to SUNDAE proposed by DeepMind this year, which is claimed on page 6.	This paper introduces SUNMASK for modeling discrete sequences. It builds upon previous works such as SUNDAE, Coconet and order-agnostic NADE, but uses a masking scheme that enables fine-grained or human-in-the-loop control during the generation. The qualitative experiments about musical inpainting and masking terms in language modeling do support this motivation to some extent. However, the reviewers are mainly concerned with both the algorithmic novelty and experiments of the paper.   Regarding the algorithmic novelty, some reviewers are concerned that the method is a straightforward combination of SUNMASK and Coconet. I tend to agree with this. Also the reviewers are concerned that the paper could have done a better job in the introduction and background section by putting the method in a better context and better describing related methods such as SUNDAE. This could help highlight the novelty of the paper.   Regarding the experiments, some reviewers are concerned about language modeling (Fig. 2) experiments, and that they do not show much improvement over SUNDAE. I tend to agree, and this is also shown in the results of Table 3 where it is clear that the quality of the language model is not on a par with the recent developments.   Also some of the motivations of the paper, especially the arguments about "high trust / low trust" interpretation of the mask, was unclear to me.   In short, I believe the paper should be clarified and improved by addressing the above concerns.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper defines an entity-monitoring problem where the goal is to identify the distinct objects see in an episode where the agent/model moves through the scene/observes partial state. The paper proposes the OBM-Net model architecture to address this problem.The paper proposes an end-to-end system for the data association and filtering (DAF) problem. The architecture is built to mimic components typically found in DAF systems to provide a sort of algorithmic prior to the network. The resulting system is evaluated on several different tasks using synthetic data.This paper addressed the problem “entity monitoring” Based on the problem formulation, it is very related to the object re-identification and instance retrieval problem. All experiments are performed with synthetic dataset. This does not expose much real-world challenges.	This paper proposed a long-term object-based memory system for robots.  The proposed method builds on existing ideas of data association filters and neural-net attention mechanisms to learn transition and observation models of objects from labelled trajectories.  The proposed method was compared with baseline algorithms in a set of experiments.  The initial reviews raised multiple concerns about the paper. Reviewers nrGQ and  V7qP commented on the conceptual gap between the problem proposed in the introduction and the extent of the experiments. Reviewer qPet understood the paper to be a form of object re-identification and was concerned about the limited comparisons with related work.  The author response clarified their goal of estimating the states of the objects in the world, which they state is different from the goals of long-term tracking and object reidentification mentioned by the reviewers.  The authors also clarified the relationship to other work in slot-attention and data association filters.    The ensuing discussion among the reviewers indicated that the paper's contribution remained unclear even after the author response. Two reviewers noted the paper did not clearly communicate the problem being solved (all reviewers had a different view of the problem in the paper).  These reviewers wanted a better motivation for the problem being addressed in this paper.  The third reviewer remained unconvinced that the problem in the paper was different from long-term object tracking.  Three knowledgeable reviewers indicate reject as the contributions of the paper were unclear to all of them. The paper is therefore rejected.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Theory could just be presented intuitively. It's not clear how much the details add given that you don't give a big empirical exploration of the tradeoffs you derive. "Suppose that data $x, y, \lambda$ follows the model in (1)." is a big assumption.Lack of motivation for problems with weak supervision signals. Class Balance in Eq. 1 needs to be defined. Need to clearlydefine all the notations. The paper is somewhat presented in atelegraphic style.An interesting improvement method for weak supervision techniques via combination with foundation model embedding space. Extensive experiments in six different tasks to demonstrate the advantages of the proposed method. Inconsistent ideas and motivation in the paper, e.g. the title “shores up the foundations”The proposed method outperforms six benchmarks with a large margin. The major concern of the proposed method is the label model. The authors claim that they model the P(y,\lambda|x) as a probabilistic graphical model. It is suggested that the authors should provide the graphs.	Meta Review: This paper proposes LIGER to better use the foundation model in a weakly-supervised setting. LIEGER has two key components: 1) a finer estimator of weak source quality by dividing the embedding space and learning the source accuracy for each part, 2) source voting expansion  in the embedding space.  The reviewers agree that the proposed work is innovative and makes a significant technical contribution in using pretrained models under weak supervision. There were some concerns raised by reviewers, but most of them have been well addressed by the authors.  I recommend acceptance of this paper given its novelty and significant technical contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper propose to sample effective hard negative samples in contrastive learning. Authors proved that their new objective is a more biased lower bound than InfoNCE, but with less variance. Experiments on several datasets as well as transferring datasets show some promising results.The paper tackles a typically disregarded problem in contrastive learning. It shows the importance of selecting difficult negatives to obtain stronger unsupervised representations. The Ring model results in a simple method that can be applied to any contrastive algorithm resulting in a better representation.This is an interesting paper that discusses the negative sample mining in visual representation learning. The authors discuss the theory and method to conditionally select the negative samples based on the dot product of representations in noise constructive estimation (NCE)This paper adopts semi-hard negative mining, a sampling strategy widely used for metric learning. The paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles. This sampling strategy improves the contrastive learning methods (IR, CMC, MoCO)	This paper addresses the problem of how best to sample hard negatives during contrastive learning, a topic of importance for the recently resurgent field of metric learning / contrastive loss-based unsupervised representation learning. Backed by theoretical results for a new low-variance version of the NCE, the paper proposes an easy-to-implement "Ring" method for selecting negatives that are at just the right level of difficulty, neither too hard nor too easy.  Happily, this is a paper that has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the authors. Perhaps the result that tipped this paper over the line in my assessment: the new experimental results now show significant gains from applying the "Ring" approach for hard negative sampling to near-state-of-the-art implementations of the MoCo-v2 approach, which is among the leading unsupervised visual feature learning approaches.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision. ODConv combines two prior ideas, i.e. filter recalibration with attention in SENet and additive kernels in CondConv/DyConv. The method is overall quite efficient without too much overhead, due to separate attention weights for different dimensions.The paper is well written and properly structured, and the extensive experiments prove the effectiveness of the proposed method. The proposed method can be plugged into most existing CNN architectures and has good performance on public datasets, such as ImageNet and MS-COCO.This work presents a dynamic convolution method based on Dynamic Conv (dyconv) and Coordinate Conv (cordconv) It incorporates more attention weights to the multiple convolutional kernels to mix kernels more effectively. The results look promising.This work mainly focuses on designing a new dynamic network for large-scale image recognition problems. Extensive experiments confirm the superiority of the proposed new framework. But the weakness of this paper is too experimental-oriented without thorough analysis on the results.	This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor. The paper is well motivated and well explained, easy to follow. This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns. This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented as a spotlight.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose an evolutionary ensemble method for decoding neural signals. The key challenge that this work addresses is the fact that the underlying function between the variable of interest and the signal changes over time. The authors suggest that their method can discover and adapt to functional changes.The work addresses the task of neural decoding (estimating intended user action from observed neural brain signals) for a changing observation model ($h_t$ in eq. 1) over time. The method involves initially training an ensemble model via minimizing a squared error loss on training data. The overall model prediction is then made from a weighted combination of ensemble members.This paper proposed a neural decoding method to process non-stationary data for brain-computer interface. The main advantage of the method is that the model can be adaptive to the variation of neural signals through evolution in the Bayesian framework.The method seems potentially interesting. Unfortunately the extremely confused description makes it impossible to understand, much less reproduce the method. This applies both to the description of the method itself, as well as the experiments, as explained below.	The review ratings/confidences were 5/2, 6/2, 4/4, and 5/2. Although the average rating of 5 was just above the acceptance threshold, I think that it should somehow be discounted by the lower confidence levels. Although I myself does not have expertise in the field of BCI, as for the reviewers' evaluation, I think that they basically agreed on the following points: - The problem is well motivated. - The proposed method was built on DyEnsemble with some empirically-motivated extensions to have decoder models dynamically evolving. One could then argue that the proposal is not groundbreaking but somehow incremental. - The authors showed experimentally that the proposed method works well compared with a number of other existing methods.  I also noticed that the authors made revision (adding a paragraph at the end of Section 2: It can be observed in the August 10 revision), which would have improved readability of this paper. I would thus recommend acceptance of this paper, provided that there is room for it.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper aims to make strides towards a theoretical understanding of Deep neural networks, which remains elusive to date. They show that their proposed Lyapunov loss converges faster than the L1 and L2 losses. Can a similar loss function for MLPs on classification tasks be easily derived?This paper presents a Lyapunov based analysis of the loss function in neural network training. It derives a priori upper bounds on the settling time of the training, which somewhat complements existing studies. The supervised neural network learning problem is formulated as a control problem with the weight parameters being the control input.This work studies the finite-time convergence for neural networks. Supervised learning is then reformulated as a non-linear control problem with a Lyapunov based loss. The weight update is then transformed to be the control inputs. Finally, convergence results are obtained with standard theory from non- linear systems.The authors in this paper make an attempt in providing finite time convergence guarantees of the training process of neural networks. The idea is to come up with a weight update rule which guarantees the convergence rate with some assumptions on the inputs. The extension to multi-layer case is an extension of the back propagation algorithm.	This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers:  1. The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides. 2. There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works. 3. The experimental part is weak. It only involves small data set and very simple networks.  Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides a solution for multi-cloud configuration problem. It tries to provide cloud customers with an optimal configuration to minimise runtime and cost. The paper also presents a dataset, for offline benchmarking, comprising of 60 different multi- cloud configuration tasks across 3 cloud service providers.This pager presents CloudBandit to solve the multi-cloud selection-configuration problem. Overall, the paper is well written but several aspects of the problem were ignored. The dataset presented could be meaningful but currently it lacks the rigor in generating the dataset.The paper proposes a method to find the best cloud provider for multi cloud compute selection (time, cost) to avoid vendor lock in. They achieve this using a best-arm identification algorithm where each arm runs an BBO on each provider and eventually choosing the best arm. They test this approach on 60 different configuration across 3 providers using a proposed dataset.In this paper, the authors try to solve the multi-cloud configuration problem with an algorithm from the AutoML domain. The results show the proposed method can find best cloud provider with best instances more cheaper or faster, compared to BBOs.	This paper studies the problem of choosing the best cloud provider for a task. The problem is formulated as a bandit and solved using algorithm CloudBandit. The algorithm is compared to several baselines, such as SMAC, and performs well. The evaluation is done on 60 different multi-cloud configuration tasks across 3 public cloud providers, which the authors want to share with the public.  This paper has four borderline reject reviews. All reviewers agree that it studies an important problem and that the promised multi-cloud optimization dataset could spark more research in the area of cloud optimization. The weaknesses of the paper are that it is not technically strong and that the quality of the new dataset is not clear from its description. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper identifies that traditional datasets used for learning disentangled representation have several shortcomings such as no correlation between variables and simple structure. It proposes a new dataset that has 1M higher-resolution simulated images along with 1K annotated real-world images.The authors proposed a unique learning scheme for representation disentanglement. Unlike infoGAN or ACGAN which explicitly learn disjoint feature representations for describing the attributes of interest, the authors chose to address this task in a questionable "weakly supervised setting" Most real-world images would be complex and contain multiple attributes.Dataset contains dependencies in FoV, not artificially induced but present due to the attempt at realism in the simulation. The work has sufficient quality, and is sufficiently clear and original. For detail, please see the below pros and cons.The paper presents a new, more complex, dataset for the use of disentangled representation learning. The dataset is based on real and simulated images of the trifinger robot platform. The authors also present a new neural architecture to scale disentanglement on more complex datasets.	This paper introduces a new dataset for evaluating disentanglement and its impact on out of distribution generalization based on the trifinger robotics platform. Using this dataset, the authors rigorously investigate the performance of beta-VAEs in this setting under a number of conditions, finding that weak supervision is necessary to induce disentangled representations, and that, perhaps surprisingly, disentanglement does not help for sim2real settings despite the similarity between the simulator and the real data. Reviewers were divided on the work, but had a number of concerns related to the claims of novel architecture, comparisons to baselines, and issues with the clarity of the paper, some of which were addressed in the authors' response. I agree with some of these concerns, particularly with respect to the claims of novel architectures since the modifications could simply be viewed as tweaking hyperparameters and are not rigorously compared to baselines. However, I think the novelty of the dataset and the rigorous evaluation of OOD generalization settings is likely to be valuable enough to the community to merit acceptance. I'd encourage the authors, however, to tone down some of the claims regarding the architecture (or provide sufficient baseline comparisons), and instead focus on the dataset and the OOD results. I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots.This paper provide a large visual-tactile dataset with GelSight tactile sensor and camera, containing diverse senarios and objects. Various applications are presented on the proposed dataset, including a novel tactile-driven image stylization task. For touch prediction only one model is evaluated.The dataset empowers multiple tasks, including tactile feature prediction, newly proposed tactile-driven image stylization, and future touch prediction. The newly proposed dataset is much more diverse than previous efforts. The visualization of tactile sensors is a little confusing.The paper presents a new dataset that helps to understand the material properties of objects in real-life. The authors use human data collectors to probe objects in the wild (both indoors and outdoors) and collect both video and information received on tactile sensors. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown.The paper introduces a new dataset that contains both the visual and the tactile modalities for visuo-tactile learning. The new dataset is collected in natural environments with diverse objects. Experiments on three tasks have demonstrated the usefulness of the dataset.A paper introduces a database with videos of a person probing objects with a tactile sensor. It is more diverse than existing databases, and provides multi-modal information. How these benefits touch and sight association tasks are unclear.	Overall, all reviewers see the novelty in the dataset that provides an original new set of tactile data, accounting for a wide range of objects and scene, collected in the wild. The paper is overall well written, clear and easy to follow, and provides a good demonstration of the importance and usefulness of the dataset, going beyond datasets collected by robots in the context of material properties. It also includes initial steps towards benchmarking, although it is not the core contribution of the paper, which lies in the creation of the dataset itself.   Two reviewers recommend acceptance, two stand marginally below acceptance, and one does recommend rejection. The authors have provided detailed responses to all reviewers’ comments and concerns and updated their manuscript. The main concerns of those reviewers sitting on the fence seem to be answered and limitations acknowledged however the element of bias in the data collection was still considered as a possible concern. Based on my reading and accounting for the authors changes, there approach itself is novel and original and hence provides already a relevant contribution to NeuroIPS; while the authors acknowledge limitations and future work avenues. The one reviewer recommending rejection didn’t engage in the discussion but from my reading of both the paper, the comments, and the authors responses, I believe the main points were addressed by the authors and misunderstandings clarified. Overall, I believe the papers offers a novel, non-existing dataset that can inspire interesting future works; all the dataset is shared and presented clearly for others to use.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors considered that a kernel function is consisted of a set of "basis" kernels in the space of kernel functions and a number of operations. The proposed method uses a Gaussian process with this kernel- kernel as the covariance function, and Bayesian optimization is used to select the kernel model.This paper develops a novel way to compare two kernels, based on optimal transport between feature representations of the tree representation of the kernel. The paper comes up with a good idea for kernel search, inspired from references [10,4]The paper proposes a method for kernel selection (search) wherein kernels are built from base kernels using fundamental operations. Kernels so constructed are represented as trees, allowing their similarity to be evaluated based on their grammar rather than distance between GPs.This paper proposes a new way to select a kernel (covariance function) for Gaussian processes (GP) The authors borrow ideas from NN architecture search to propose a so called symbolical-optimal-transport (SOT) kernel. The SOT kernel is then used for Bayesian optimization for model selection.	This is a strong submission that benefitted greatly from productive and clarifying discussion between the authors and reviewers, after which the reviewers reached a unanimous stance in favor of acceptance. I recommend the authors to revise the manuscript accordingly in light of these discussions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new way to model discrete state diffusion models. The method is based on a 'continuous time' model. It uses a technique called 'tau-leaping' to test the model.This paper proposed to extend the continuous-time diffusion/score-based modeling idea to discrete state space. The forward process needs to be replaced by a discrete-state CTMC, which is characterized by the infinitesimal generator. Experiment results show it outperforms prior discrete diffusion models.This paper study diffusion models with discrete states. This paper considers continuous time by introducing the transition rate matrix. While there is still a (relatively small) performance gap between this work and continuous diffusion models, I think it is an important work. I'd like to further raise my score if my concerns are addressed.The paper proposes to sample from the CT diffusion model via tau-leaping, a well-known technique in chemical physics. The author test their method on various experiments, outperforming existing DT diffusion models on discrete data.	The work proposes a continuous-time generalization of diffusion models on a discrete space. The description uses continuous-time Markov chain (CTMC), in parallel to the existing stochastic differential equation description for continuous space. Reverse CTMC and modeling and ELBO objective are described. Some practical considerations and inspirations are also discussed, including avoiding exponentially large model in high dimensions, efficient reverse (generation) process simulation, and a corrector technique that further exploit the model to improve simulation (generation) quality. An error bound on the learned data distribution is also presented that shows a mild dependency on data dimensionality.  All the reviewers agree that this work presents the very right way to describe the continuous-time version of diffusion model on discrete space, and thereafter inspired techniques make a desired contribution to the community. Some concerns are raised, including still inferior performance than the continuous counterpart, and on the independence among dimensions. The authors provide reasonable remarks on them. Hence, I recommend accept to this paper.  One minor point: In Sec. 4.2, it would be clearer if the independence is specified both among the random variables $x^{1:D}$ in “output” and between each $x^d$ in “output” and $x^{1:D\backslash d}$ in “input”. Conventionally independence refers to the former, in which case the size is only reduced to $S^D \times D S^2$.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes an optimization-based algorithm for bit-flipping a limited number of bits in a quantized / binarized deep-learning model. Experiments CIFAR-10 and ImageNet show that the proposed method outperforms the SOTA.The paper describes a bit-flipping white-box attack on deployed neural network classifiers. Given a model, find a perturbation of the parameters bits such that the model with misclassify one specific example, while maintaining high accuracy on other examples.Adversarial weight attack is an interesting research direction with important practical importance. The proposed method is mathematically sound. It empirically outperforms or at least is comparable with previous state-of-the-art methods on undefended models.The paper proposes a bit-flip attack where model parameters weights are altered such that a certain sample is misclassified to a target class. Comparisons in the experiment section show that prior methods are performing on par with the proposed method in terms of the attack success rate and benign accuracy.	The major concerns about this paper are that (1) There are too many hyper-parameters, such as those needed for ADMM. I'd point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper-parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it’s unclear why flipping fewer bits is an important objective - an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought-out approach that improves the state of the art by some metrics.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal of this paper is to evaluate methods for detecting out-of-distribution samples in a more comprehensive fashion than prior work. It distinguishes between three different cases of OOD samples. Using a pre-trained network improves OOD detection (in line with previous results), but one particular method does: cosine similarity.The paper empirically analyzes the evaluation framework of the current OOD detection systems for the image recognition task. They motivate the paper by the necessity of having better evaluation to be reflective of real world scenarios. Many recent and relevant papers are not cited or compared against.This paper is about a comparison of methods for out of distribution detection on image classification. Irrelevant inputs, novel classes, and domain shift are compared. The results show that cosine similarity consistently outperforms other methods across all sub-tasks.The paper provides an empirical evaluation of various OoD methods. There is no real discussion or insights into why certain methods work in some scenarios and not others. Fine-tuning/pre-training gives a strong performance boost for OoD detection.	This paper proposes an OOD evaluation framework under three categories: irrelevant input detection, novel class detection, and domain shift detection. As with several reviewers, the AC recognizes the importance and effort to distinguish between different cases of OOD detection, as well as the amount of experimental comparison across several prominent methods in literature (MSP, MC-dropout, cosine similarity, ODIN, Mahalanobis).    Despite being well-motivated, three knowledgeable reviewers find the paper not ready yet for publication at ICLR. The AC recommends a rejection, given the standing major concerns from the reviewers. The AC is hopeful that the paper can be significantly improved by   - sufficiently discussing and highlighting the novel insights of the results.  - a more rigorous definition of  "novel" vs. "irrelevant" inputs. There seem to be overlapping definitions between what Hsu et al. considered vs. this paper. In particular,  Hsu et al distinguish i) samples of a novel class but in a known domain, called semantic shift (S), and ii) samples of known class in a novel domain, called non-semantic shift (NS), both of which are reconsidered in this paper. Therefore, the novelty of this submission is more precisely to distinguish within the category of semantic shift. The AC agrees that this might deem some more rigorous measurement and definition of "semantic closeness".  - The AC also finds the evaluation of domain shift in Section 3.3.2 may be potentially misleading the community, as it falls out of the standard OOD scope. The notion of common corruption is closer to the robustness problem (which is how ML model predictions changes w.r.t some delta changes in the input space). The changes may not be substantial enough to be "out-of-distribution".
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper explores how maximizing the loss influences the optimizedneural network's weights. Adversarial training in linear neural networks is a corollary to Theorem 5. The paper's results place a milestone in the theory of adversarial robustness. The proof of Theorem 2 may have some potential errors.This paper characterizes the bias of adversarial training toward specific minimum-norm solutions or KKT points of a particular optimization problem. Their results generalizes the work of Li et al 2020 by proving the directional alignment with the adversarial max-margin solution for deep linear models.This paper focuses on the convergence behavior of adversarial training methods. The paper tries to show the implicit bias in a simplified setting with a deep linear neural network and linearly separable data for a binary classification problem. Under this setting, the paper proves the gradient descent algorithm will converge to the max-margin solution.This paper aims to understand the training results of adversarial training. The results, to be honest, are not surprising, given previous works on standard training. But I believe the rigorous justification presented in the paper is of importance.	The paper is a nice addition to the developing theory of implicit bias in neural training. While the results are somewhat expected, the technical aspects are fairly involved due to the adversarial component.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper considers the problem of building a composite network from several pre-trained networks and whether it is possible to ensure that the final output has better accuracy than any of its components. The analysis done in the paper is that of a simple linear mixture of the outputs produced by each component.This paper studies composite neural network performance from function composition perspective. In theorems 1, 2 and 3, the authors essentially prove that as the basis functions (pre trained components) increases (satisfying LIC condition), there are more vectors/objects can be represented.The paper aims at justifying the performance gain that is acquired by the use of "composite" neural networks. The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., "components") in the input of a network then you have "more information", and this cannot be bad.	Dear authors,  All reviewers pointed out the fact that your result is about the expressivity of the big network rather than its accuracy, a result which is already known for the literature.  I encourage you to carefully read all reviews should you wish to resubmit this work to a future conference.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors proposed a Sparse Low-Dimensional Decision "SLDD-Model" for image classification. The model ensures a low-dimensional feature space by performing feature selection prior to the last (linear) layer of a neural network. To understand the meaning of the selected features, the authors used posthoc feature alignment with human interpretable concepts.This work proposes the SLDD-Model, which reduces large, unstructured inputs down to a smaller subset of semantically meaningful features. A smaller classifier then operates over the semantic features, producing classifications. The results are strong, the discovered features are interesting, and the qualitative examples are sensible.This paper argues that because humans can only consider around 5-10 aspects at a time when making a decision, models used for interpretability should similarly be sparse. They propose a model that, for increased interpretability, has a very sparse last layer (5 neurons) which should make it feasible for a person to visually analyze each feature and understand its meaning. I really liked this paper and this it's a great fit for the workshop, well done!	This paper proposed an interesting method to induce a low-dimensional feature space for interpretable image classification. The proposed method is supported by comprehensive and strong experimental evaluations. Please consider the reviewers' comments in the final version. One common concern is it is unclear if the paper has novel technical innovation compared to the prior works.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A more general variant of the predictive coding (PC) algorithm for learning deep neural networks is introduced. Unlike traditional PC, this generalized PC is also effective in training highly complex neural architectures such as transformer networks. The paper is well written with impressive results, although some notation and exposition can be improved.This paper proposes a modification of the predictive coding (PC) paradigm in the context of variational inference with artificial neural networks. While the canonical PC framework assumes Gaussian form of the underlying generative model, the proposed PC modification extends PC to arbitrary distributions. It is comparable to backpropagation in the classification of MNSIT and CIFAR-10 datasets.The paper is somewhat original, in that it generalizes previous approaches to training neural networks with predictive coding. The authors apply this to supervised classification, VAE training, and training transformers. The results are typically better, in terms of performance and efficiency.This work addresses the problem of generalizing Predictive Coding to arbitrary distributions. PC has so far been established as a credible alternative to backprop only when considering Gaussian activations. This is extremely unfortunate, as it limits the applicability of PC to virtually a small fraction of modern deep nets.	Motivated by advancing the applicability of backpropagation alternatives, the paper extends predictive coding to non-Gaussian distributions, so it can be used to train effectively complex architectures such as transformers.  The reviews are divided: three reviews give a score of 7 (accept) whereas one review gives a score of 4 (borderline reject). The positive reviews cite the following strengths: clearly stated motivation, technical soundness, potential for impact and convincing experiments. The negative review cites lack of clarity as the main weakness (something also mentioned in one of the positive reviews), while the reviewer is not convinced about the originality of the method given similar advances in variational inference and generative modelling.  On balance, given the potential of impact in the field of predictive coding and the technical soundness, I'm happy to recommend acceptance, even though clarity is somewhat lacking.  Some reviewers requested more details on computational complexity, memory consumption and scalability. I encourage the authors to use the extra content page in the camera-ready version to discuss these aspects further.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network. In general, the paper is addressing an important problem but I still have several concerns.The novelty is limited and not well explained. Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement. It is not clear how much data is used to train the super-resolution model.This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty.	This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non-medical datasets.   Reviewers raise multiple concerns around the following:  - Novelty (all reviewers) - Inadequate comparison baselines (all reviewers) - Inadequate citations. (R2 & R3)  Authors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper aims to improve the performance of the Nondeterministic Stack RNN (NS-RNN) Using unnormalized positive weights instead of probabilities for stack actions and allowing the model to directly observe the state. The paper also uses the new NS-Rnn for a language modelling task on the Penn Treebank by introducing a memory-limiting technique.This work continues a recent work on nondeterministic stack RNNs. The authors propose two changes to the architecture which create an RNS-RNN. The goal/effect is that the score of each (current state,current stack) pair being maintained in the N-PDA does not tend to zero as the input sequence length increases. The proposed changes are simple and appear to improve the model on some synthetic tasks.The paper proposes a new stack-augmented RNN, RNS-RNN that includes two modifications to the Nondeterministic Stack RNN. The ideas are technically sound and empirically proved. Overall the paper proposed useful improvements to the NS-Rnn. But the novelty and impact of this paper are not significant enough.This work presents various improvements over the differentiable non-deterministic push-down automaton. The gains are not large, but meaningful improvements are observed in the synthetic data set. The model is not space efficient in that the stack is unbound, and thus, consumes large memory.	This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS-RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks. Each of these requires substantial algorithmic innovations.  The reviewers all agree that this is a strong paper worthy of publication. The paper includes a useful review of previous differentiable stack models which nicely sets up the rest of the paper where the contributions are well motivated and clearly presented. The reviewers had a number of clarification questions, partly due to the author's use of overly concise citations for key algorithms rather than inline descriptions. This situation has been improved by updates made to the paper. The evaluation includes a series of synthetic experiments which are clear and provide a good elucidation of the various stack models properties. The practical evaluation on language modelling is more limited and serves mostly to demonstrate that the nondeterministic model can be scaled to a basic language modelling task.  Overall this is a strong paper with a well motivated and clear hypothesis. It provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Review of SLDVBF, a paper showing how an RNN-based inference procedure fits complex dynamical systems. The paper draws on many ideas from recent literature for inference, and incorporating these ideas is a good start. The authors need to better motivate the use of RNNs.This paper proposes a new model for switching linear dynamical systems. The standard model is very well described, formally and graphically, except for the dynamic model of the switching variable, and its dependence on z_t-1. The proposed model has a clear graphical representation, but its formal counterpart is more difficult to grasp.This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches.	The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes to solve high-dimensional nonlinear parabolic partial differential equations by using an Actor-Critic neural network architecture. The performance benefits are well articulated although a bit repetitive. The potential for the proposed idea is difficult to realize because the paper lacks essential details.The paper proposes a neural network-based model to learn solutions to parabolic backward stochastic differential equations. The proposed work claims to decrease the number of trainable parameters, hyperparameters, and convergence rate. While the general approach seems promising, the paper does not seem to articulate the addressed research gap.The authors introduce the abbreviations D(BSDE) without ever defining them of explicitly explaining what they refer too. The introduction has no reference to previous work, and the first time any relevant previous work is referenced is on page 3. The discussion of the method itself could be made clearer.	Reviewers were mixed about this paper. The stand-out concern is clearly the fact that this paper does very little to set itself in context. I would echo these concerns, and this is clearly the single greatest weakness of the paper. For example I would highlight lines 54--62, in which the proposed technique is compared only against some nebulously-defined "other".  Despite these concerns, I am inclined to agree with reviewer QEo4 that the paper is acceptable for a workshop, and would strongly encourage the authors to address the concerns raised by reviewers.  In addition to the review comments, a few comments of my own:  - Equation (2) assumes that the SDE is solved via the Euler--Maruyama method. Whilst analytically convenient, I believe it is more elegant to treat the general case for as long as possible (without reducing to just the EM method at this early stage). If the noise has particular structure then it may become desirable to solve the SDE via some more efficient numerical method, e.g. Milstein if using commutative noise or Heun if using additive noise (Heun converges to the Stratonovich solution, but for additive noise then Itô and Stratonovich are identical). - I think equation (2) may be missing a $t_{n+1}-t_n$ coefficient for the $f$ term. - The "not too far" of line 67 is usually referred to as a "trust region".
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper discusses linear interpolations in the latent space, which is one of the common ways used nowadays to evaluate a quality of implicit generative models. Researchers often tend to judge the quality of the model based on these interpolations. The authors present some set of experiments, where DCGAN generator is trained on the CelebA dataset with the Cauchy prior.The paper asks how we should interpolate in the latent space. The key idea is to derive a natural interpolant from the prior distribution p(z), where z is the latent variable. I have some concerns about the idea itself (see below), yet find that the paper brings an interesting new idea to the table.The authors study the problem of when the linear interpolant between two random variables follows the same distribution. In the paper, the authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property.	All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn't provide a very convincing new evaluation to replace the linear interpolation. However, given that it's largely unclear what are the right evaluations for GANs, the AC thinks the "negative result" about linear interpolation already deserves an ICLR paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors hope to explore a more efficient/reasonable masking strategy for training masked autoencoders (MAEs) Their intuition is that current masking strategies (e.g., random masking) cannot utilize semantic information contained in images, as what have been done in masked language modeling.A semantic-guided masking modeling method is proposed in this paper. To achieve this, an iBOT-pretrained ViT is used to extract the features, and then StyleGAN-based decoder is trained to learn semantic parts.This paper proposed a new semantic-guided masking strategy for unsupervised vision transformer training. The paper claims that semantic parts of objects are the visual analogue of words, masking semantic parts from intra-part patterns to inter-part would help the vision transformer learn better representations.	Authors present a method attempting to perform Masked Auto-Encoding (MAE) using semantic knowledge, to try to better approximate the semantic MAE seen in language domain. To do this, they leverage an iBOT framework and add some embeddings of the class token to create "part tokens", which are then compared to patch tokens from iBOT to produce attention maps. The objective for this process is a StyleGAN-based image reconstruction.  Once the part attention maps training is done, the network is then used to guide semantic based part masking based on the generated attention maps, for semantic MAE.    SemMAE pretrained networks are then compared against other forms of SSL pretraining on ImageNet 1k, iNa, CUB, Cars, and ADE-20K, demonstrating improvements in all domains.  Pros: - [R] Idea is interesting / novel - [R] Well written - [AC/R] Results improve over baselines     Cons: - [AC/R] Pipeline is complicated. - [R] What about starting from random MAE and then adapting based on parts knowledge? Authors respond that this is future work. - [AC/R] Not convinced parts are visual analog of words. Authors provide benchmark improvements in performance, and qualitative visualization of the parts. However, there is no quantitative assessment of the parts and whether they have true semantic meaning. - [AC/R] Some improvements are marginal. Authors respond that although marginal in some cases, they are consistent.  - [R] Paper does not discuss more how to deal with background. Authors respond that this is future work.  Overall, all reviewers have changed their assessments to accept, including the one reject reviewer. AC recommends accept, though would be preferable if quantitative assessment of the quality of the semantic parts (for example, by segmentation masks) could be provided.  AC Rating: Accept
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is a well written paper proposing a quantitative framework to expand the inferences that can be done in the study of the causal effects of task stimuli on brain signals. The originality of the proposal is based on the explicit modeling of a unique and shared task causal effect (which the zone generalization metric aims to capture)The considered problem is interesting and important. The proposed metrics are intuitively reasonable. Although the considered problem has some "causal flavour", I feel that the proposed method is rather irrelevant to causality. I am not sure whether this paper would be a good fit for this conference.The paper proposes an intriguing question, to study if two causal effects indeed share a common stimulus. The proposed metrics to answer the question are simple and easy to follow. The review below focuses only on the weaknesses of the paper.	The authors develop two metrics for evaluating how different brain zones may be affected by the same stimuli. The framework created shows promising results when applied to actual and simulated data. This is a well-written paper with a particular application in mind. It designs a framework for discussing causal effects for applications in neuroscience. This paper's developed metrics and framework are potentially impactful and present a promising start for future research in this direction. The reviewers and I found the author's replies and rebuttal especially helpful in reaching our decision, so I hope some of these comments and arguments are included in the final version of the paper.  Minor comment: In addition to the comments made by the reviewers below, I would encourage the authors to make the references more consistent. At the moment some authors' first names are included and others are not.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a new GAN procedure. The update in the generator is carried out l times and takes into account points generated in the previous iteration. The paper is head over heels. It can be caricatured as extreme obfuscation.This paper considers natural gradient learning in GAN learning, where the Riemannian structure induced by the Wasserstein-2 distance is employed. The natural gradient is expected to be efficient when the underlying coordinate system is non-orthonormal (Amari, 1998) The proposal provides an easy-to-implement drop-in regularizer framework.The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. The motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to. I encourage more simulation studies and take more GAN structures into consideration.	Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with "revise and resubmit".
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed a method, called REM, that generates makes a dataset "unlearnable" to adversarial training. Experiments are conducted to validate the effectiveness of REM under various settings. This paper is well-written and the experiments are extensive. However, the proposed REM has the following issues: Unrealistic assumptions.The paper proposes a min-min-max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The paper has shown that the proposed method can successfully reduce the test performance of the model (adversarially) trained by generating robust-error-minimization noise.Error minimization noise was first proposed so that when a dataset is released to the public, conventional empirical risk minimization cannot learn a good model from the perturbed dataset. However, error minimizing noise is only effective to the extent that adversarial training is not used. Adversarial training was identified as an effective method of overcoming the error minimization Noise in prior work. In this paper, the authors proposed a variant of the error minimize noise which continues to be robust even in the presenceThis paper studies how to make data unlearnable towards adversarial training. In prior works, including dataset poisoning, no methods can prevent data from being learned. This paper proposed a robust error-minimizing noise to solve this task.	To address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. Based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. In addition, considering the vulnerability of error-minimizing noise in adversarial training, robust error-minimizing noise is then introduced to reduce the adversarial training loss. The authors have tried to respond to reviewers' comments along with adding more experiments. Overall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of error-minimizing noise against (simple) image processing operations was not verified. In comparison with other manuscripts I'm handling that got consistent positive comments, this manuscript is still recommended to be accepted (poster) with a further study of robustness under simple image transformations in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The results of the work are convincing and interesting. The authors provide a detailed and helpful appendix. The paper is written quite well in most parts, but some parts (e.g. the introduction, or the second part of 3.1.) lack clarity.The method is well organized and well written. Only weakness that I can think of is that improvement may be only in a context of 2.5D vs 3D. The method uses a randomized scheme for extracting patches, self-enforcing and spatial ensembling.The paper is clearly written; and with good motivation. Methodological development is well motivated and clearly written. Experiments are well defined, and results are presented well. The paper is missing comparison against a V-Net architecture trained on patches used for learning global representation.	The paper presents a novel idea of a 2.5D segmentation approach considering the characteristics of medical imaging data. The practical importance and novelty of the proposed approach are well received by all the reviewers. Clear description and illustration are included appropriately in the paper. Some experimental details-related questions were raised by the reviewers, and they were well addressed in the rebuttal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In theoretical reinforcement learning (RL) literature, it has been understood that directed (optimistic) exploration strategies (provably) improve sample complexity over random exploration techniques. In practice, people use heuristic-based (exploiting domain knowledge) reward shaping to demonstrate successful results. This paper conducts a formal theoretical investigation on the impact of such reward shaping methods on the sample complexity of RL algorithms.This paper attempts to shed light on reward shaping (RS) in reinforcement learning (RL) from a theoretical perspective. In terms of quality and scientific soundness, this paper is difficult to fault. The paper is also eminently readable and clear. My central issue with the paper is its significance.This paper investigates the regret analysis of a particular form of reward shaping. The shaping function is an approximation of the optimal state value function. This reward shaping is integrated into an extension of the UCBVI algorithm.The paper addresses a very relevant and useful question, and it provides a convincing and competent answer. The theoretical investigation in this paper is relevant and timely. Empirical evaluation on three domains is also provided, and the experiments confirm the merit of the contribution.	The reviewers carefully analyzed this work and agreed that the topics investigated in this paper are important and relevant to the field. Overall, the reviewers had a generally positive impression of this paper. One reviewer argued that this paper addresses a relevant and valuable question and makes an important step towards a better understanding of regret when reward shaping is used. Even though this paper makes assumptions that were of some concern to other reviewers, this reviewer argued that the paper is nonetheless an important milestone for the community. Another reviewer acknowledged that this paper conducted a formal theoretical investigation of the impact of reward shaping methods on the sample complexity of RL algorithms and argued that all proofs seem to be sound. This reviewer had a few technical questions, which were all addressed by the authors. Post-rebuttal, the reviewer encouraged the authors to incorporate the corresponding details (such as those discussed in the rebuttal) in the updated version of their draft. A third reviewer emphasized that this paper shed light on reward shaping from a theoretical perspective. They argued that the quality and scientific soundness of the paper are objectively excellent, that the paper is original, and that it deserves merit. The reviewer pointed out one main weakness, however, regarding the assumption of the type of the shaped reward function. They wondered whether this assumption could limit the impact and applicability of the paper's results. After reading the authors' thorough rebuttal, however, the reviewer stated that they were satisfied with all responses and updated their score accordingly. Finally, a fourth reviewer also had an overall positive view of this work but pointed out, as a weakness, the seemingly strong assumption that the shaping signal is an approximation of the optimal value function. After reading the authors' response, however, the reviewer stated that the assumptions made in this paper were not as restrictive as they initially thought, and updated their score. Overall, thus, it seems like most reviewers were positively impressed with the quality of this work. They look forward to an updated paper version addressing the suggestions mentioned in their reviews and during the discussion phase.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The problem of testing the closeness of distributions asks to distinguish two cases for input distributions $P,Q$ The most natural setting for a testing algorithm is to only allow black-box sample access to $P$ and $Q$. However, it is known that this requires the sample complexity to be exponential in $n$. In this submission, the authors present an algorithm, Pacoco, that has sample complexity $\tilde{O}(\sqrt{n} \log nThe paper is concerned with probabilistically validating if samples are close to high-dimensional distributions. It leverages several oracles called COND, PCOND, and DUAL. The empirical results show the implementation's number of samples for a wide array of problems. The new approach works significantly better.This paper considers a slightly nonstandard notion of tolerant testing. They give an algorithm with query complexity scaling up to polylog factors. Notably, this avoids the exponential scaling that one would get from traditional closeness testers. The authors have adequately addressed the limitations, and I don't see any potential negative societal impact from this work.	This submission studies (a somewhat non-standard version of) tolerant closeness testing of distributions over the n-dimensional hypercube. Instead of only iid samples, it is assumed that the tester is able to efficiently evaluate the probability mass at any point in the domain and to sample from the distribution conditioned on any subset of size two of the domain. The main result is an algorithm with query complexity scaling near-linearly in the dimension. Using only iid samples, one would need exponential dependence on dimension. The algorithm is evaluated on synthetic and real-world datasets. It is experimentally shown that their algorithm outperforms a previous baseline, which in the worst case has complexity scaling exponentially in the dimension. Overall, this is an interesting work that appears to meet the bar for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method to perform signal processing of a signal represented via a neural field directly, without the need to first sample the underlying MLP. To this end, the authors propose to express operators on top of the neural field as functions of the first- and higher-order derivatives of theMLP.This paper proposes training an additional network which operates on the high-order spatial derivatives of the original INR previously fitted for some image. The paper discusses tasks such as denoising, deblurring, inpainting, and classification. It is helpful for the community to be aware of the huge potential of INR of images.This paper studies implicit (and continuous) neural representations (INRs) of data defined on discrete lattices. In particular, the authors present a way of performing different operations on these INRs directly without the need for "decoding" them.The paper introduces a novel signal processing framework named INSP-Net that directly operates on INRs without explicit decoding. Theoretically, the authors prove continuous convolution filter can be uniformly approximated by a linear combination of high-order differential operators. The paper is well organized and easy to be understood.	The paper proposes a framework to perform signal processing tasks on a signal represented with an implicit neural representation directly in the representation space, without the need to instantiate the signal.   After the rebuttal period, all reviewers recommend acceptance.   In particular reviewer 1Yx6, an expert on the topic, finds the idea original, and the quality and clarity of the paper to be high. The reviewer finds that while for now the significance is limited (since working as proposed in the representation space is computationally prophibitavely expensive), this is not a major issue, since the the paper is likely to inspire work that will push the idea further.  Reviesers edqT and qGk1 also liked the general idea of the paper and find the proposed method to directly perform operations on the representation space of implicit neural representations to be novel and interesting  Reviewer Uh51 initially identified a few issues regarding experimental design, the validation, and on the included literature. The main concern regarding the experimental design of the reviewer was that the paper focuses on images (and not signal processing tasks more broadly), which I don't consider a shortcoming, due to the importance of image processing tasks. The concerns on the validation issues have been addressed as well, and the reviewer raised their score.   I recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a novel VIO method which estimates 5 DoF poes (with the elevation fixed) from monocular camera and IMU. The proposed method adopt Unscented Kalman Filter (UKF) which is natually differentiable for the state estimation.This paper proposes a visual inertial odometry system based on kalman filtering. The main idea of this work is to learn the noise filtering parameters by making the filter differentiable. The approach is evaluated against a set of baselines on the KITTI dataset.This paper proposed an interpretable architecture for VIO, which admits differentiable training. Experimental results on two domains demonstrate that the proposed approach is effective and competitive against recent deep learning-based architectures. It is not clear in what ways the proposed model is exactly interpretable.This work introduced a novel visual-inertial odometry system, which is learning-based but interpretable without deep neural networks. IMU data is used by a differentiable UKF to estimate the roll and pitch, which are subsequently used to project images to bird-eye view (BEV) The BEV images are transformed to the frequency domain to estimate SIM(2) camera motion.	The paper proposes a differentiable approach for monocular VIO estimation based on BEV, without relying on deep neural networks. The reviewers find the paper well written and the idea of using BEV to be interesting. This paper received highly mixed reviews. The major concerns raised by the reviewers include empirical evaluations of the model interpretability, justification for relying on algorithmic priors than parameters, results on more challenging datasets, positioning this work with respect to existing work on deep state estimators, and clarifications regarding the claims made, among others. Most of the concerns raised by the reviewers have been thoroughly addressed in the rebuttal. I thank the authors for the engaging discussions during the rebuttal. Some minor concerns still exist. Nevertheless, I agree with the reviewers that the paper is an interesting contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors proved that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors, rather than the dimensionality constraints. In Claim 1, the degree d and the dimension D are mixed. This work is theoretically complete and experimentally sufficient.The manuscript proposes a theoretical bound on the generalization performance of learning graph embeddings. The authors find that the term in the bound that represents the function complexity involves the norm of the learnt coordinates.The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors. The main result from the theorem is that the error gap of norm constrained embeddeddings scales as O(d^-0.5(lnn)0. 5)	This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.I hope that the following questions can be resolved during the discussion period. Some things are a bit unclear to me which is preventing me from providing a more detailed analysis of the work. This work considers a regularized IRL setup, where instead of the entropy regularization used in maximum entropy IRL, an arbitrary convex regularizer $Omega$ is used.The proposed method is based on the relationship between imitation learning and statistical divergence minimization. The experimental results shown in Figure 3 is interesting, but I have a few concerns. In some cases, the averaged Bregman divergence of RAIRL-NSM was larger than that of Random.This paper studies a novel problem, regularized inverse reinforcement learning. The paper proposes several techniques to solve the regularization in different aspects. The experiments are conducted under different settings, but some cons are needed to revise.Regularized IRL is a nice contribution to the field. The mathematical foundation is rigorous and the experimental results are promising. The empirical analysis of the actual divergence is interesting. The paper is a bit notation heavy and that makes it hard to follow.This paper proposes a new method for regularized inverse RL. The paper builds upon work by Geist et al. who studied regularized MDPs with convex policy regularizers. The Shannon entropy is a special case of such a policy regularizer.	This paper studies inverse reinforcement learning through the prism of regularized Markov decision processes, by generalizing MaxEntIRL from the negative entropy to any strongly convex regularizer (as a side note, strict convexity might be enough for many results). The reviewers appreciated the clarity, the mathematical rigor and the empirical evaluation of this paper. They asked some questions and raised some concerns, that were mostly addressed in the rebuttal and the revision provided by the authors. This is a strong paper, for which the AC recommends acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots.This submission investigates the convergence rate of the stochastic proximal point algorithm (SPPA) for convex, finite-sum functions. The authors derive new convergence rates for SPPA under the assumption that each function in the finite sum is convex.This paper shows the arbitrary convergence rate of the stochastic proximal point algorithm(SPPA), under the convex assumption of the objective function. Numerical results show the efficiency of SPPA on some realistic data-sets. The experiments suggest the SPPA outperforms SGD when training DNNs.This paper provides a novel elegant proof of convergence for the Stochastic Proximal Point Algorithm. It proceeds by surprisingly conditioning on future iterates, reversing usual convergence proofs. Experiments show that the SPPA is competitive, including in nonconvex settings such as training neural networks for image classification.The submitted paper proposes a new approach to study the convergence of stochastic proximal point algorithm. The main result of the paper is Lemma 1, which shows backward conditional probability. The extension to the Douglas--Rachford splitting case is also interesting.	The paper considers the stochastic proximal point algorithm, the main contribution is a convergence proof (in addition to arguing that it is a practical algorithm for ERM problems).  However, reviewer EvJ6 and xJB8 pointed out a fatal flaw in Lemma 1 that affects all the downstream results, and the other two reviewers and myself agree that this invalidates the core results. Reviewer EvJ6 gives extensive examples of how the lemma cannot be simply fixed. So while this is an interesting stochastic extension of the famous deterministic method and may warrant further study, this paper doesn't yet provide a rigorous convergence proof.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is well written and clearly explained. The previous literature is adequately discussed and the experimental results are clear. The problem being solved does not seem to be particularly common or clinically relevant. It is always better to include a data table for clarity.The aim of the presented paper is a bit ambiguous and partially unclear. The claimed 264,198 samples in the training set do not match 90% of the full dataset. It is not clear why only subsamples of the validation and test data were used.There is substantial interest in encoding ordinal relationships between classes into the training process of neural networks. The approach is validated with almost 200 CT scans. Results for different training set sizes and different values for the parameter s would have been valuable.The paper addresses a common challenge faced when training models with labels which are coarse and could benefit from domain knowledge to more correctly define them. The method seems overly complicated in it's presentation. The evaluations are not very convincing.	This is a borderline paper -- while the underlying idea is good and relevcant, the authors don't do a very good job of selling it; their experiments are performed on a very specific task with limited clinical relevance. The reviewers had a number of questions regarding experimental setup, which were largely answered in the rebuttal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The experimental results are convincing, though they could be made more complete. It is enlightening to observe such performance of from a spectral based methods. An interesting paper which could open further avenues of research at the intersection of spectral methods and neural-based approaches.This paper presents a comprehensive analysis of spectral clustering based approaches for node classification in graph-based data. The clarity and care the authors put into understanding each component of the algorithms is a key merit of the paper. There are several concerns regarding missing forms of analysis such as deeper investigations into why certain methods work well on certain datasets.The paper argues for refocusing the efforts of the graph neural network community onto previous work on spectral clustering. The main weaknesses of the paper are: (1) a lack of direction and (2) an insufficient distinction from previous works.The paper proposes a study about spectral clustering techniques. The experiments show that such a simple approach can produce state-of-the-art results on social graphs. While I like the general idea and I appreciated the effort in explaining the underlying theory, I think there is no significant methodological novelty.This paper reviewed the application of spectral clustering embedding on node-level classification problems. Strengths: It is very clear and easy to follow and understand. It conducted solid experiments to show the performance differences for each method.Weakness: Though it concretely introduces lots of methods, it does not propose any new methods or insights.	There was some discussion on this paper, both with the authors and between reviewers. On the one hand, there is a general agreement that the empirical results suggesting that spectral clustering-based method can be competitive with SOTA methods on node classification benchmark is an interesting result. One the other hand, reviewers did not find a significantly novel contribution in the methodology proposed, and found that the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work or not on some benchmarks). There is therefore a consensus that the paper is not ready for ICLR in its current form, but we hope that the reviews and discussion will help the authors prepare a revised version in the future.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.I find that this paper has interesting ideas, and it addresses an important question in learning self-exciting processes in the presence of non-stationarity. However, I think the authors missed an extremely important influential paper by Feng Chen and Peter Hall (2013) I would like to see how they contrast their work with Chen/Hall, which I think they missed.This paper propose a point process with a non-stationary kernel to model complex event data. To learn the model parameters they used stochastic gradient to maximize the resulting likelihood function. Authors compared their method against state-of-the-art and baseline on synthetic and real datasets.The paper propose to replace the kernel involving in Marked Point Processes by the product of two neural networks. Theoretical results appears to have very few links with empirical results. Limitations of the proposed approach are not investigated. The paper could be a good paper with more complete work.	This paper proposes a self-exciting temporal point process model with a non-stationary triggering kernel to model complex dependencies in temporal and spatio-temporal event data. The kernel is represented by its finite rank decomposition and a set of neural basis functions (feature functions). The proposed model has superior performance in comparison to other state-of-the-arts methods. All the reviewers recognized that the model is interesting and advances the state of the art in a meaningful way. While they were some concerns regarding the experimental evaluation, particularly in terms of real data, and the presentation, the rebuttal/revision by the authors cleared up these concerns.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper aims to deal with the learning of noisy/corrupted labels based on the small loss criterion. This construction crucially relies on the use of the Wasserstein gradient flow associated with Gaussian distributions. A series of experiments show that the new methodology proposed in the paper leads to state-of-art results.The authors' approach is to map the distribution logits of uncertain samples onto the 2-Wasserstein ball centered on the measure of certain samples. To overcome the dilemma of selecting the ball radius, the authors propose a surrogate objective, namely Wasserstein Normalization. An SDE grad flow is proposed for solving the normalization.The paper builds on the idea of small loss criteria, which favors learning on certain samples in the beginning of the learning process. It proposes a novel type of distributional normalization based on Wasserstein distance. This process is done with a particle based stochastic dynamics.This paper propose a computationally efficient Wasserstein distributional normalization algorithm for accurate classification of noisy labels. Empirical results on CIFAR-10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches.The paper aims to give a better way of handling the noisy label data. The proposed method is as follows: select the "lowest-loss mini-batch" among the samples. The clarity of the manuscript is not quite good.	This paper proposes a potentially very interesting and original approach to handle label noise.  The numerical experiments suggest that the method works very well. But the paper  itself has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and  Markov theory.   Note that due to the low confidence in several review an additional emergency review by an expert was asked and it confirmed the global opinion from  other reviewers that the paper is interesting but needs a major rewriting before acceptance in a ML conference. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation of the method before resubmitting.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces a new intrinsic reward for MARL, representing the causal influence of an agent’s action on another agent counterfactually. This reward is related to maximising the mutual information between the agents’ actions. The behaviour of agents using this reward is tested in a set of social dilemmas, where it leads to increased cooperation and communication protocols.The paper is a novel extension of ideas from single agent RL to multi agent RL. The causal influence term can be thought of as a form of reward shaping. The paper focuses on cooperative environments which is an underfocused area in RL.This paper proposes an approach to model social influence in a scenario-independent manner. Agents are operationalised as convolutional neural network, linear layers and LSTM. Using these base mechanisms, different abilities are inferred based on counterfactual actions. The architecture is explored across two different sequential social dilemmas.	The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper has a few issues. The sampling from hindsight experience is partially based on the likelihood of the corresponding state-action pair under the current policy. The reviewer is not sure that this strategy makes sense when the developed approach was applied to off-policy RL methods.The paper introduces an extension of Hindsight Experience Replay (HER) called Hindsight Curriculum Generation (HCG) HCG is demonstrated to learn faster in multi-goal RL benchmarks. The paper seems to be rushed, making it hard to follow the ideas presented in the paper.The authors evaluate the proposed method on 3 simulated robotics manipulation tasks and compare against HER-EBP and CHER as baselines. The authors also provide ablation studies for the Lipschitz constant hyperparameter. The proposed method relies on assumptions that might not hold true for many goal conditioned environments.Theorem 1 is not trivial and there is indeed doubt in my mind. A proof should be provided upon revision. The word usage and sentence structure is unnatural, and I find myself guessing at what exactly the authors mean. The related works section can be greatly improved.	This paper extends the idea of hindsight experience replay (HER) to learn Q functions with relative goals by constructing a distribution over relative goals sampled from a replay buffer using a clustering algorithm. This approach is evaluated on three multi-goal RL environments and is shown to learn faster than baselines.  ${\bf Pros}$: 1. Faster convergence as compared to baselines 2. Interesting use of clustering in the context of HER but this choice is made without strong justifications or formal arguments  ${\bf Cons}$: 1. Some of the key choices made in this paper are not justified or explained property, e.g. - the goal sampling strategy, choices made in the clustering algorithm and associated heuristics, implicit assumptions (e.g. R1 raised the question of using L2 distance in measuring metrics between two states)  2. There are several choices made without sufficient formal arguments, verification or guarantees.   The paper studies an interesting problem but could be made stronger by incorporating feedback received during the discussion period.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors convert a differential to a contour integral in the complex plane and then interpret this integral as a time integral of an oscillating teaching signal. They show that this holomorphic EP surpasses EP in performance especially in the presence of noise. However, the authors do not provide any pointers as to how this may be implemented in biology except mentioning that there may be connections to theta neurons.Equilibrium propagation is a promising framework to learn neural networks in a biologically plausible way. It is sensitive to noise, which makes it hard to scale to large machine learning tasks. The authors solve those two problems by introducing a new variant of equilibrium propagation that relies on complex analysis and holomorphic functions.The authors present a beautiful extension of equilibrium propagation for energy-based models with holomorphic activation functions. The problem of solving the credit assignment problem using local learning rules is an important challenge in neuroscience. It is not clear if the results here are actually useful for understanding neural computation.The authors' toy experiment shows that holomorphic EP is capable of learning. The cosine similarity of the approximated gradients is significantly higher than with classic EP. The paper presents a very novel idea which greatly increases the power of a local learning rule using the elegance of complex analysis.	Equilibrium propagation is a biologically plausible form of backpropagation based learning where the true gradient of an energy based model is computed for infinitesimal perturbations. In this innovative work, authors extend EP using complex analysis that links contour integrals for finite perturbations with the oscillatory dynamics in time. This not only allows better gradient estimates, but also applications to related theories of learning in neuroscience as well as neuromorphic engineering. It represents a significant advance that opens new doors.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies an online allocation setting where arriving items are allocated among $n$ agents. As opposed to the standard setting, the planner does not know the realized valuation at each period. The paper proposes an interesting mechanism to overcome the ignorance of the valuation distribution. However, this approach is hampered by its reliance on the iid assumption.The paper studies a repeated online allocation of a single type of good to agents who have the same but unknown valuation distribution for the good. Unlike the prior work, the paper assumes the agents are strategic and no monetary payments can be used to incentivize truthfulness.This work considers an online allocation problem with learning and strategic agents. Previous works have either used payments/transfers between the players and the central planner, or assuming that the planner knows beforehand the valuation distribution of the players. This work is thus the first to propose an approximate BIC policy, with sublinear regret, when neither of these conditions hold.	Executive summary:  The authors study the repeated allocation of an identical good over T rounds to n strategic buyers in a "no monetary transfers" setting. The buyers have i.i.d. valuations drawn from an unknown distribution, and the algorithm must work with reported valuations. The goal is to maximize social welfare (= sum of valuations) under the constraint that each buyer receives a pre-specified fraction of the total number of goods.   The main result is an algorithm for this problem that ensures two things:   (a) approximate Bayesian incentive compatibility (approx-BIC) (Definition 2 and Theorem 1) and   (b) low individual regret (Definition 3 and Theorem 2).   The key idea of the algorithm (is to exploit the iid-ness of the problem) and detect misreports from the underlying CDF using Dvoretzky-Kiefer-Wolfowitz type bounds.  Discussion and recommendation:  After some initial set back on the problem motivation, the reviewers bought into the motivation for studying this online allocation problem "without monetary transfers" (adding examples such as the foodbank example might be good).    There was some discussion around "assuming iid valuations" limiting the generality of the result, but there is in fact a history of papers that studies learning with strategic agents under this assumption (eg Kanoria and Nazerzadeh 2021).  The main difference of the current work is that it's working in a setting without money.  The idea behind the algorithm is maybe "the obvious think to do" - but of course it still requires some work to formally prove that it actually works.   I think one thing that could strengthen the paper would be to add some discussion around the tightness/non-tightness of the approximate BIC and individual regret bounds.  Weak accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation. The teacher ensemble is achieved by concatenating features from each of the teacher model and then conducting dimension reduction.The paper proposes a knowledge distillation method for face recognition. The method inherits the teacher’s classifier as the student’S classifier and then optimizes the student model with advanced loss functions. ProxylessKD adopts a shared classifier for two models. Shared classifier yields better aligned embedding space. Since optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart.	This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated “Ok but not good enough - rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors address a very relevant problem for medical applications of deep learning. To my knowledge this is a quite unique approach. I felt that the explanations in 2.2. NormGrad were not really sufficient to understand the proposed approach.NormGrad is a saliency detection method used in medical image analysis. The evaluation was carried out on a fairly large dataset. Insufficient clarity in the description and the discussion of the method. I need to read the original paper in the reference to get a more comprehensive understanding of the approach.The paper has a clear aim and focus, is technically sound and mostly easy to follow. The evaluation of hyperparameters as well as the comparison of NormGrad to other methods is sound and extensive. One downside of the paper is a lack of qualitative results. It would be great to see more examples in the appendix.The results and message of the paper are very clear. The authors intended to demonstrate the superiority of normGrad for IQA in chest x-Rays versus other techniques. The paper is generally well written, the literature review is expansive and covers the state of the art well.	The paper receives unanimously positive reviews from four knowledgeable experts. They all agree that, though the saliency detection method NormGrad is not invented by the authors, its use in the context of medical image analysis shows promising results, which is clearly demonstrated by the authors. They also express some concerns, which are largely addressed by the authors in the discussions. I, therefore, recommend the acceptance of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents an interesting mathematical derivation that drives the subsequent approach based on reducing classifier discrepancy. The manuscript has a number of typos and grammatical errors. Despite those problems, the logical flow is good so the paper is acceptably readable.This work investigates the cause of error in few-shot classification and finds an upper-bound for it. The proposed method, Reducing Classifier Discrepancy, has two training phases. Empirical results show the benefits of the proposed method on three few- shot datasets.This paper analyzes the upper bound of error rate on novel classes in few-shot learning theoretically. It shows that classification error is mainly caused by classifier discrepancy. The reviewer rates the paper 4/10 because the third concern is critical.This paper seeks to understand theoretically the current bottleneck in few-shot learning and address it with a new way of training the embedding. The theoretical parts of the paper are very poorly written and peppered with unexplained notations and incorrect claims.	This paper proposes a contribution aiming at understanding the cause of errors in few-shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. Hence, I propose rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Traditionally, most trojan detection methods focus on reverse-engineering a fixed trojan pattern. This paper proposes to reverse-engineer the trojan in the feature space of DNNs. The proposed method can be used to detect feature-based trojan or dynamic trojan that is input dependent.This paper proposes a trojan reverse engineering method by exploiting the feature space input constraint. Results show much better trojan detection performance than prior works. The approach can also be used to mitigate trojan attacks and achieves the best mitigation performance.This paper designs a novel reverse-engineering method that exploits the feature space constraints to reverse-engineer Trojan triggers. Results on four datasets and seven different attacks demonstrate that the proposed method effectively defends both input-space and feature-space Trojans. The authors have addressed the limitations of the work.This paper provides a unique insight that Trojan features will form a hyperplane in the high dimensional space. Based on such insight, the paper extends an input-space trigger detection method to cover feature-space triggers. The extended solution is formally defined as an optimization task, and the algorithm is clearly listed.	This paper proposes a new reverse-engineering method for trojan attack detection. The idea is to focus on feature representation space so that the detection is more robust to dynamic / input-dependent attacks and other feature-based attacks. The reviewers consider the idea generally novel and effective, and the experiments thorough. Some reviewers hope to see more visual analysis that can provide better insights into the effectiveness of the method.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces a reinforcement learning algorithm with temporal abstraction using the options framework. It provides empirical results in a variety of domains, demonstrating that the algorithm can improve data efficiency. The paper can be improved by providing additional analysis to better understand the behaviour of the algorithm.This paper introduces a novel option-learning policy gradient method, HO2. The method learns a parameterized joint distribution over options and actions and uses a soft-continuation based approach to interrupt or "switch" between options before option termination. The paper demonstrates the performance of the proposed algorithm on a handful of 3D virtualized environments and on robotic simulation tasks.This paper studies an important area in RL, hierarchical RL, which improves data efficiency by incorporating abstractions. In this paper, the authors proposes an efficient option learning algorithm, which utilizes a TD(0) type objective. The method is well-motivated in general, but the notation is a little bit unclear.The paper considers the Hierarchical Reinforcement Learning setting, Options in particular. It proposes an algorithm that allows to learn both the high-level and low-level (option) policies at once. The paper is quite difficult to follow, and requires several attentive reads to be understood.	There was a fair amount of discussion about the paper.  Several reviewers felt that the paper would have been stronger if it tried to do less but better.  The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements.  Doing this requires a better understanding of the algorithm's behavior and a valid ablation study, a new concern raised during the discussion with the authors.   The reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the prediction problem on tabular datasets and proposed a differentiable multi-layer fern-based architecture. The approach is computationally efficient on CPUs and can learn sparse representation. The prediction performance is on-par with state-of-the-art methods.The paper constructs Sparse Hierarchical Table Ensemble (S-HTE) based on oblivious decision trees for tabular data processing. The motivation is to benefit from the expressiveness of multi-level processing based on a differentiable framework. The experiments are conducted on the commonly used tabular datasets.This paper modifies from a differential-tree architecture (NODE) for tabular data that has lower theoretical computational complexity to be suited for lower-end CPU devices. They find this sparsification, with other modifications like summing across the outputs for each layer before passing to the next layer, can enjoy much smaller operation counts. They show that their performance is somewhat comparable to other commonly used methods like CatBoost, TabNet and NODE. And they do an ablation studyAuthor proposes a variant of differentiable multilayer tree ensembles with annealing mechanism. They use a specific types of trees, called ferns, which are constrained to have the same boolean function for all nodes at the same level. Training is done by softening non-differentiable parts of f Ferns.	The submission introduces the sparse hierarchical table ensemble (S-HTE), based on oblivious decision trees for tabular data. The reviewers acknowledged the clarity of the presentation and the importance of the computational complexity analysis. However, they also raised concerns regarding the novelty of the proposed method and the significance of the results compared to competing methods (e.g., CatBoost). Given the consensus that the submission is not ready for publication at ICLR, I recommend rejection at this point.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this work, the authors propose various task-sampling strategies for an episodic meta-learning setup and compare their performances against a standard uniform sampling. The list of sampling strategies include simple methods like only using a fixed set of tasks in each episode and more complicated strategies like online-hard-negative-mining. The paper does not do a good job in explaining why some of the sampling methods work better than others.In this paper, the authors investigate the effect of task diversity in the training process of meta-learning. They evaluate the performance on four few-shot image classification datasets. The findings indicate that increasing task diversity during the meta-training process does not boost performance.The paper studies how the diversity of tasks in the training phase affects the performance of meta-learning algorithms. The paper finds negative evidence, which is consistent with Setlur et al. (2021) Compared with the existing work, the paper performs more extensive experiments with different algorithms, task samplers, and different datasets.	This paper set out to show that increasing task diversity during meta-training process does not boost performance. The reviewers mostly  agreed (only reviewer wVFn dissented) that the empirical set up of the paper was convincing, but they also felt it over-emphasized empirics over a deeper understanding of the phenomena observed. In turn, this resulted in discussions around how the experiments and the explanations didn't fully prove that increasing task diversity does not help. Overall, the discussion and the additional analysis tools provided by the authors (such as the diversity metric) will greatly improve the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes to employ a CVAE structure to generate pseudo OOD samples by providing some synthetic conditional information. They show that their approach outperforms other SOTA methods in the task of out-of-distribution detection.This paper suggests to model outliers using a CVAE. It then suggests simple scoring functions for incorporating these simulated outlier into a classifier. A numerical comparison shows this method is able to outperform CSI on a couple of datasets.The authors proposed to use a Conditional VAE in order to generate pseudo OOD data, which can be used to improve OOD detection. The paper is well written, covers the related works, and importantly, is well motivated.The paper proposes CGA (CVAE-based generative data augmentation for out-of-distribution (OOD) detection) The module consists of a classification module (classifier and feature extractor) and a generation module (encoder and decoder) In the majority of setups, CGA shows competitive performance against the conventional OOD detection methods.	The paper adopts CVAE to generate OOD samples for training an outliner detector. It consists of two phases that train an OOD detector by leveraging the generated OOD data and shows it outperform other methods. According to reviewers’ discussion, there is a concern from the discussion: why CVAE works but other variants or cGAN doesn’t. The paper needs more motivation or evidence or ablations to support the generality of the work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A method BAM is proposed for Bayesian learning in non-stationary environments. At each time step, each previous datum may or may not be incorporated into the new posterior. The method doesn't rely on parametric assumptions. Experiments demonstrate it to work well in various scenarios.In this paper, the authors propose a new framework, Bayes Augmented with Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. The variety of experiments demonstrate the ability of BAM to continuously adapt in an ever-changing world.The paper aims to solve the downside of the posterior shrinkage of Bayesian online learning when applied in a non-stationary environment. While the method seems technically sound, the method still falls short of the following points. An immediate improvement would be to use a fixed memory.Bayes Augmented with Memory (BAM) is a new approach to learning. It uses selection variables to allow an agent to adaptively choose past experiences to forget. The paper is well written and easy to follow. The proposed learning framework is a reasonable solution.	The article introduces a Bayesian approach for online learning in non-stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori.   The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers: * it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine on the examples shown, the approach may not be applicable to larger datasets * it requires to store all the data * it requires the traceability of the marginal likelihood  Despite these limitations, there was a general agreement that this paper offers a novel and useful contribution, and I recommend acceptance.   As noted by reviewer o4TK, I also think that the title is not very accurate. Bayesian methods naturally allow recursive updates of one's beliefs, and therefore have "memory". Maybe change the title for "Bayes with augmented selective/adaptive memory"?
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a novel approach to improve self-attention through by bridging the attention maps from different layers via a chain of convolution-based prediction modules. The main contribution of this paper is the introduction of CNN-based attention prediction to enhance model predictions.This paper's main topic is the enhancement of Transformer models for improving performance in a task-independent way. The experimental results on several different datasets from different domains show that the proposed method consistently improves the performance. The authors then hypothesize that the performance could be improved by integrating an additional module.This paper proposed a modification to the classical transformer architecture. It demonstrated significant performance gain on multiple benchmark tasks. However there are many unclear expressions and claims in the paper. Some ablations are missing which might be critical to better understand the module.The idea of predicting attention weights (or generating them) is not new. The difference here is that there is a 2D CNN to model relationships between N previous layers. The experiments on GLUE are also weak and could be a result of variance over the existing BERT model.	Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer. However, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper extends RML Mapper with access to Excel data. It represents a solid piece of Java engineering work, although the chosen semantic representation approaches are not optimal. A demo page http://www.dfki.uni-kl.de/~mschroeder/demo/excel-rml/ is provided.This paper describes extensions on top of RML to support Excel spreadsheets. It was not clear to me whether this was a system/demo paper or a short research paper. The paper does not give many insights except for introducing those extensions.This is an interesting approach for dealing with spreadsheets written in Excel rather than CSVs. The authors have created the corresponding code to deal with this, and added some specific functions that are not common in RML and its related tools.	The paper presents an extension of the RMLMapper tool to cover Excel features. The three reviewers agreed that the contribution is relevant to the workshop and presents a solid work. Please take into account the comments provided to include them in the camera-ready paper, try to be clear if the work presents a demo or a short research paper.  The recommendation is to accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a class of model called monotone deep Boltzmann machines. The underlying potentials are parameterized such that they obey some monotonicity constraint. This ensures that the inference problem has a global optimum, which can be found using some generalized variant of parallel mean field.In this paper the authors propose a restricted parameterization of the Boltzmann machine that guarantees that for any set of observations, the mean field objective has a single global optimum. To turn inference into learning, the model is treated as a supervised learning model.This paper theoretically shows that the mean-field equation for a certain family of Boltzmann machines with hidden variables can be modeled as the recently proposed monotone Deep Equilibrium (DEQ) model. The significance of this paper is not high, and evaluation is weak. I am happy to increase my score if the above my concerns are properly addressed.This paper proposes a new family of monotone deep Boltzmann machines. The pairwise potentials satisfy a monotonicity condition, giving rise to efficient mean-field iteration with provable convergence guarantees. Small-scale experiments are done as proof of concept. The paper is very well-written and easy to read.	This is an interesting contribution to the Boltzmann machine (BM) literature that makes a nice connection to DEQ models. On a positive note, reviewers found that it was well-written, clear, and interesting. Unfortunately, there were significant concerns with the manuscript that were not fully addressed in the revision: inappropriate or incomplete baselines, insufficient credit given to previous works, and the fact that this model is limited as compared to its BM relatives.  I would recommend that the authors take into account the reviewers' feedback in a revision of the work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a rigorous approach to generating discrete data by introducing the hitting time on the target range as the stopping time. With the help of the h-transform, one can approximate the desired distribution by modeling the drift term. The method seems hard to scale to large and complex images.First Hitting Diffusion Models (FHDM) are a class of generative models that generalize the fixed time diffusion probabilistic models. Learning and inference are achieved by approximating the drift term in the corresponding Ito diffusion process with a neural network. Experiments show that FHDM is able to generate high-quality samples for general data distributions.This paper proposes FHDM (first hitting diffusion model), which differs from diffusion models based on infinite time. The authors also propose a fast sampling algorithm for the rotational symmetric domain. The proposed method shows good performance compared to the different genetive models on various domains.The authors propose a generative model based on the simulation of SDEs that terminate at random hitting times. The strength of the paper is the introduction of a new (to the best of my knowledge) data generation methodologies suitable for arbitrary domains.	The paper introduces a new approach for generative modeling: a diffusion process is run until it first hits a target set, and then outputs the first point that is hit.   Three reviewers generally praised the originality, technical quality, and empirical results of the paper. They found the idea very interesting and novel, and technically sound. The numerical results were judged to be compelling and fair. One concern was clarity of exposition. There seemed to be two issues: (1) there were more typos and rough edges than expected, (2) more significantly, there was some difficulty in following all details of the main method given the notational complexity and significant amount of mathematical background on diffusion processes. Reviewer FUwB gave a number of concrete suggestions for improvement.  Reviewer wpmu had a negative overall opinion and critiqued the originality, quality, and clarity. On these issues: (1) the quality concern was based on a misunderstanding that was later resolved, (2) the originality concern does not seem justified to the meta-reviewer (it is based on a shared technical tool with a not-yet-published paper), and (3) the clarity concern is similar to those raised by other reviewers (especially FUwB). Overall, the meta-reviewer does not feel that the low score (3 = “reject”) was fully justified.   In summary, overall reviewers found the paper sound and novel, with the main area for improvement being clarity of exposition about diffusion processes; one reviewer considered originality a weakness, but the meta-reviewer did not find this position well justified.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes reducing slip during dynamic motions by adapting the trajectory, specifically decreasing the velocity. This is in contrast to previous papers which normally focus on moderating gripping force. Reactive Slip Control (RSC) outperforms PSC, since it can plan to prevent slip before it occurs.The goal of this work is to develop a slip control mechanism for robot manipulation tasks where objects being manipulated may move relative to the gripper due to insufficient shear force. The key idea is to learn a slip prediction model that takes tactile sensor data and planned future actions as input and outputs an estimation of whether the object is going to slip. The proposed method achieved a comparable success rate compared to reactive slip control.This paper applied machine learning to the problem of slip detection and avoidance in manipulation tasks. They implement two solutions: one a purely reactive method which uses an LSTM to detect slip events triggering the controller to slow it's velocity. The second implementation is also an L STM but takes in both past data from the tactile sensor and future planned actions to predict slip.The paper develops a novel algorithm for contact slip avoidance in robotic manipulation of objects. The algorithms consists of an offline procedure of learning a slid detection and prediction model and online trajectory adaptation to prevent slip.	This paper presents a method for preventing slips in point-to-point motions while grasping an object using LSTM-based models for detecting and predicting slips. This approach is effective when contact force cannot be increased. Reactive and proactive controllers are presented, and the latter demonstrated better performance.   The paper proposes an interesting approach to slip prevention especially useful for handling delicate objects. The authors successfully addressed most of the concerns raised by the reviewers, while expanding the dataset and examples will be addressed in a future journal paper. Nevertheless, the paper will make a valuable contribution to the conference.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is extremely clear, and results on DBP5L show significant improvements over the baselines. Authors propose AlignKGC, a system for jointly learning to complete multiple monolingual KGs, and aligning their entities and relations.AlignKGC is a neural model that performs knowledge graph completion (KGC) on multilingual knowledge graphs that have entity and relation overlap. The resulting model demonstrates strong empirical results on DBP5L for all three tasks.This paper explores how entity alignment (EA), relation alignment (RA), and knowledge graph completion (KGC) can benefit each other. It proposes a model which jointly optimizes the three tasks using a linear combination of their losses. KGC is evaluated on each of 5 languages for which a KG is available.	This paper introduces AlignKGC, a neural model that performs entity alignment (EA), relation alignment (RA), and knowledge graph completion (KGC) on multilingual knowledge graphs. The model jointly optimizes for the linear combination of losses for all 3 tasks. The authors present a convincing set of experiments to show that each of these tasks helps the other two. The paper is well written and demonstrates strong empirical results on DBLP5L containing 5 languages on all 3 tasks. Results are reproducible as both the code and data are made public. No major concerns after the author's response.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper study the effects of adding random features (RF) to graph neural networks (GNN) Theoretical aspect seems novel and quite surprising, especially since it shows that adding RF makes GNN more expressive than k-GCN (or k-IGN) for any k.The paper analyzes the model of Random features in GNNs as suggested by Sato et.al., in the paper called RNI. The authors design two datasets wich require 2-WL distinguishing power (which is higher than the ones MPNNs have)This paper studies the power of message passing neural networks (MPNNs) with random node initialization (RNI) The main result of the paper is to prove that RNI makes MPNNs universal. The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability.The paper studies the how random initialization of node states can improve the expressivity of message passing graph neural networks. Theoretically the paper shows that RNI makes MPNNs universal approximators for invariant functions over graphs.	In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well-founded. The key concern was that the novelty over the Sato's paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer's comments and resubmit it to a future venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper asks the interesting question of whether you need individual neuron (or even population level) class selectivity at intermediate stages in order to have good classification performance. The authors introduce a regularization term to the loss that controls the amount of selectivity in the units of the network. They find that the selectivity of the units in standard networks can be reduced while maintaining classified performance.This paper examines the impact of forcing units in a CNN to be more or less “class-selective” The approach taken is to include a regularizer in the loss that directly penalizes or encourages class selectivity in individual units. The authors report that penalizing classSelectivity at intermediate layers has little-to-no effect on classification performance, and in some cases mildly improves performance.The authors attempt to ascertain whether single-neuron class selectivity is beneficial or harmful to overall network performance. They observe that discouraging selectivity can have a small benefit, and generally doesn't harm performance even at very high values. Conversely, encouraging selectivity dramatically decreases performance.	This paper has received three positive reviews. In general, the reviewers have commented on the importance of the question related to how much selectivity is needed from units of a neural network for good classification -- from both the neuroscience and ML perspectives. The reviewers also commented on the thoroughness of the experiments and the general readability of the paper. This paper should be accepted if possible.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Batching of similar and independent operations in a neural network computation graph is a common way to improve efficiency through computational parallelism. The proposed solution is an automatic batching strategy that work for dynamic computation graphs. The results (for both CPU and GPU) show that the proposed method improves on top of two existing batching strategies across three different tasks.This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits. The paper provided a detailed analysis of time consumption on only a success-case.The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups. The potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth)	This paper describes a new batching strategy for more efficient training of deep neural nets. The idea stems from the observation that some operations can only be batched more efficiently in the backward, suggesting that batching should be different between forward and backward. The results show that the proposed method improves upon existing batch strategies across three tasks. The reviewers find the work novel, but note that it does not properly address the trade-offs made by the technique - such as memory consumption. They also argue that the writing should be improved before acceptance at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper describes a few-shot learning approach that takes into account the semantic relatedness of the ground truth labels. In principal adding textual data that is semantically less ambiguous than image data helps. The method (TASNet) is built over Protonet (Snell et al) method.In few-shot learning for image classification, visual features alone may represent multiple objects in an image. The label text provides additional information that could serve as useful inductive bias. The paper proposed a reasonable model to use the text labels in image classification.This paper proposes a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. The visual and semantic features are learned separately, thus preserving the structural heterogeneity of different modalities. Experimental results are provided on four benchmark datasets that demonstrate that the proposed approach outperforms existing approaches.This paper proposes a few-shot learning method that incorporates semantic features (i.e., class label embeddings) for both support and query samples. The experiments are unconvincing. The effect of using a pre-trained feature extractor is unclear. The explanation of the proposed method seems unnecessarily redundant.	This paper proposes a few-shot learning method that learns task-adaptive semantic features that can incorporate for both of the support and query sets. Two approaches for modality combination are developed. The additional experiments in the author response addressed some concerns of the reviewers. However, the technical novelty of the proposed method is high enough since the proposed method uses existing techniques.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a multi-task paradigm for deep transfer learning via cross-domain mixup. Experiments on six datasets show the efficacy of the proposed method over existing alternatives. I vote for rejecting the paper due to limited novelty and lack of convincing experiments.This paper proposes a method to address the transfer learning in the multi-task learning setup. The proposed method has two main components: (1) finding a one-to-one mapping from source dataset classes to the target dataset classes (2) applying mixup between the samples from the source and target class pairs.XMixup is a strategy for improving transfer learning in neural networks. It consists of mixup applied between target samples and source samples from the class pre-determined to be closest to target sample’s class. Experiments conducting transfer learning from pre-trained ImageNet to 6 smaller image classification datasets demonstrate XMixup to outperform the baseline approaches.This paper proposes a simple variant for the mixup training mechanism for transfer learning problems: cross-domain mixup (XMixup) The key idea is to mix up the training samples from both domains where the samples are generated by nearest-center assignment in each class.	All reviewers recommend rejection due to limited novelty and insufficient experimental analysis. The author’s response has addressed several other questions raised by the reviewers, but it was not sufficient to eliminate the main concerns about novelty (as the method is a combination of existing techniques) and missing comparisons to justify the effectiveness of the proposed approach.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces a novel connection between adversarial attack on graph neural networks in a restricted black-box setup via node feature perturbation. An analysis shows that the objective function of the corresponding IM problem is submodular under assumption. Experiments show such attacks are effective compared to baselines.The authors propose an adversarial attack strategy for graph neural networks based on influence maximization. The attack is (claimed to be) black-box (does not have direct access to the model), evasion-based, and limited to perturbations of the node attributes. The paper is well-written and generally easy to follow.The paper shows that the attack design can be reduced to the influence maximization problem under the threshold model. The entire analysis is based on Assumption 1, but the paper does not provide a formal description of the data flow in the network under such an assumption.The paper studies the problem of attacking GNNs in a restricted black-box setup. The authors draw a connection between the restricted attack problem and the influence maximization problem. Experimental results on attacking three GNN models demonstrate the effectiveness of the proposed attack.	This paper relates the problem of influence maximization and adversarial attacks on GCNs.  The paper, and its formulation and assumptions stirred up quite a discussion among the reviewers and the authors. I do appreciate the thorough rebuttal that the authors provided, and the reviewers did take it into account (and revised their scores).  However, all in all, I am afraid that there are just a few too many concerns with this paper.  If the authors take the reviews to heart, they should be able to improve the manuscript and submit a stronger and improved version to the next conference.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A new framework leverages information generated from counterfactual predictions of optimal joint action selection. The proposed method overperforms many baselines including QMIX. Experiments based on StarCraft II show the improvement of the proposed method.The paper proposes that in partially observable MARL problems, the agent's ordering of its own action may impose concurrency constraints on the representation function. To address this problem, the authors propose that PAC uses counterfactuals selected from the best joint action.This paper leverages Assistive information by Counterfactual Predictions of optimal joint action selection, which use a novel counterfactual loss. The training assistive information is generated using the variational inference method to expand the representational ability of value functions.This paper aims to improve the joint-action Q-values learnt in a cooperative MARL setting under the paradigm of CTDE. The authors aim to do so by first identifying the shortcomings of existing methods, QMIX and WQMIX in particular, under significant partial observability. To remedy the errors they allow for agents to communicate and explicitly encourage them to send messages.	After reading the reviews and feedbacks, I lean towards acceptance. A majority of reviewers gave a positive score after the rebuttal period and some concern were answered in the authors response. Specifically authors have shown that the recent baselines they use outperform other baselines and therefore those do not need to be added in the paper, they have also clarified some proofs and some notations. Overall, the reviewers found the method presented interesting, the paper well written and appreciated the comparison to other methods of the literature. Finally, experiments shows interesting results on large scale domains which is a sign that the proposed method could scale up.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tries to prove that there is a bottleneck in feature learning for long-tailed classification. Data augmentation can help relieve the issues in long-tail feature space. The weakness: Only positive effects from data augmentation were shown, the reasons and mechanisms were not fully discussed.This paper poses an interesting and important question - where are the bottlenecks in long-tailed classification. Authors use empirical experiments to show their observations: representation is more critical than classifier, and data augmentation is helpful.The authors study the long-tail dataset problem to determine the true bottleneck for the task. After performing many ablations and experiments on 3 benchmark datasets they establish that contrary to common belief the bottleneck is in data representation rather than the classifier itself.Based on extensive experiments, the authors propose that representation learning is the bottleneck in long-tailed classification. This work shows the effectiveness of intra-class compactness and inter-class separation on long-tails representation learning. The experiments would have been more persuasive if the balanced training set is a variant of the long-tail training set.	This paper investigates the role of representation learning when the distribution over the feature space has a long tail. The main motivation is to determine how much of the overall learning, in this case, is bottlenecked specifically by representation learning. The main findings are that vanilla learning gives brittle long-tailed representations, harming overall performance. The paper suggests a form of data augmentation to remedy this. Reviewers acknowledge that this investigation is worthwhile. However, many concerns were raised as to whether experiments support the drawn conclusions. A more principled approach to the data augmentation methodology is also needed. The authors address some of these, providing further experiments, but these were not enough to sway reviewers. Since results are fundamentally empirical in nature, this shortcoming indicates that the paper is not ready to share with the community just yet. Stronger experiments with clearer evidence are needed to fully support the thesis of the work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the role of activation functions (via their multiplicity at the origin) in the condensation of neural networks at the initial training stage. Theoretical analysis is limited to the case of one-dimensional inputs and activation functions of multiplicity one. Some experimental demonstrations are provided to support the claim but the theory is somehow limited.The paper studies the relation between condensation of neurons activations and the multiplicity of the used non-linearity. The paper presents several typos and unclear sentences in the current form. I am going to increase slightly my evaluation of the paper.The authors offer empirical evidence that the complexity of the function initially learned by nets is related to the multiplicity of the activation function at zero. Broadly, this work is intriguing, but could stand to benefit from a few improvements, suggested below.This paper investigates the condensation of weights of neural networks during the initial training stage. It showed theoretically and empirically that the maximal number of condensed orientations is twice the multiplicity of the activation function. This condensation restricts the capacity of NNs at the beginning, working as implicit regularization.	*Summary:* Study isolated orientations of weights for networks with small initialization depending on multiplicity of activation functions.   *Strengths:*  - Interesting analysis of properties in early stages depending on activations.   *Weaknesses:*  - Reviewers found the settings limited.  - Reviewers found experiments limited.    *Discussion:*  In response to ejGJ authors reiterate scope of covered cases and submit to consideration that their experiments should be adequate for basic research. Reviewer acknowledges the response, but maintains their assessment (limited scope of theory, limited experiments). KucV found the experimental part limited in scope, the settings unclear (notion of early stage, compatibility with theory), and review of previous works lacking. KucV’s sincerely acknowledged authors for their efforts to address their comments and improving the manuscript, and raised their score, but maintained the experimental analysis is not fully convincing and unclear, and the comparison with prior work insufficient. zuZq also expressed concerns with the experiments and the notions and settings under consideration. They also raised questions about the comparison with standard initialization. Authors made efforts to address zuZq concerns. zuZq acknowledged this but maintained initial position that the article is just marginally above threshold. jDJ5 found the paper well written and the conclusion insightful. However, also raised concerns about the experiments the settings under consideration. Authors made efforts to address jDJ5’s concerns, who appreciated this but was not convinced to raise their score.   *Conclusion:*   Two reviewers consider this article marginally above and two more marginally below the acceptance threshold. I find the article draws an interesting connection pertaining an interesting topic. However, the reviews and discussion conclude that the article lacks in several regards that in my view still could and should be improved. Therefore I am recommending reject at this time. I encourage the authors to revise and resubmit.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is one of the first papers doing BERT-style pretraining for vision transformers. The whole model and approach works with 2 augmented views of the same image. The paper presents state of the art results in kNN (retrieval), image classification with linear probing and fine-tuning.This paper presents a new strategy for self-supervised training of vision transformer models by combining the self-distillation-based method DINO with mask image modeling. The momentum teacher model in DINO is taken as an online tokenizer.The paper proposes the iBOT method. This approach is inspired by the contrastive self-supervised learning approach like DINO and the mask modelling approach like BeiT. The paper is quite well written, the method is simple and gives better performance than DINO. Nevertheless, the comparison between the different approaches does not seem to be complete enough.	The paper is interesting, and its focus is timely and important, given the continuing rapid rise of transformers (and their dependence of tokenization of images). All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Using O(n polylog ) images one can recover all private images in the InstaHide challenge. However, the running time of the algorithm is exponential in the number of private images. The paper makes reasonable improvements to the sample complexity of the challenge.In this paper, the authors examines the sample complexity for recovery private images in the InstaHide setting. The authors provide a solid solution for a problem with limited scope. In the current form, I believe the value for the general readers are limited, therefore I would recommend for weak reject.The paper studies the weaknesses of InstaHide and provides an algorithm for retrieving all private images using near-linear size samples from the distribution of the private images. Although the time needed to design the attack is exponential in the number of private images, the theoretical guarantees are favorable.	All reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. The authors suggest in one of their response that their technique could apply to "any data mixing method with “batched k-sum” structure". Such a larger level of generality might make the paper more interesting, but at the moment it is an extremely niche result.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The current structure of the paper is very unusual what makes the presentation of the key results very unclear. As far as I understand, the authors study Lyapunov chaos that occur in learning in non-zero-sum games. The main text chops on Theorem 11 without any discussion on this theorem after.This paper provides tools for classifying the payoff dynamics in general-sum n-player games as Lyapunov chaotic. The paper could benefit from additional discussion and explanation of the claims (see below)This paper studies the chaos phenomena of learning in general normal-form games beyond zero-sum and coordination games. Building upon the previous works by Cheung & Piliouras, the authors apply the canonical decomposition of a general bimatrix game. The authors further devise two new techniques: matrix domination and linear program to help analyze the game dynamics.This paper studies Lyapunov chaos in learning algorithms for matrix games. It appears to extend earlier work by Cheung and Piliouras to more general-sum settings. The paper presents an interesting notion of matrix domination which is a necessary and sufficient condition for chaos.	This paper looks at chaos in learning in games, extending a line of work in two players zero-sum games (that I found quite restrictive in the past). It somehow reduces the class of more general games to zero-sum and cooperative games (this decomposition is already known) so that the techniques can be transposed here.  The paper is interesting, yet sometimes difficult to follow, and I am not certain that it gives many new insights.   Nonetheless, we believe its quality justify acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The work introduces memUP, a training method that allows to learn long-term dependencies without backpropagating gradients through the entire sequence. The idea behind the method is clear and interesting, but could have make a better job exploring it further.In this paper, the authors proposed a method called MemUP to help recurrent neural networks learn long-term dependencies better. The authors leverage a memory model to predict future outcomes with high uncertainty. By skipping all states with lower uncertainty, the training process takes less computational cost in backpropagation.MemUP is a method to learn long-term dependencies for recurrent models. It learns to keep useful past states for future use, so it saves both computation and memory. Empirical experiments on supervisied learning and reinforcement learning tasks show that MemUP significantly boost the results over its baselines.	The reviewers found the ideas presented in the paper interesting -- the use of mutual information to train memory for a model, and the clear presentation. Some questions were raised about demonstrating on a more elaborate set up such as NLP tasks -- the main experiments aside from the toy experiments of copy, etc algorithmic tasks, seem to be on RL experiments, but the method has been advertised more broadly in the motivation. Another reviewer raised the question of the complexity of training multiple networks. Nevertheless, the reviewers found the paper interesting enough to recommend a weak accept and I support that recommendation.  From a reviewers lens, I was a little surprised that the paper made no mention of prior works on maximizing mutual information between features of neural networks to improve results. As an example, see the following paper [1] that uses a mutual information regularizer between states at different steps of a recurrent neural networks. There is also a rich literature of doing so for convolutional neural networks. It would have made sense to compare how the idea in the paper performed in comparison to these methods (and in a sense the ablation study which looked at randomly choosing time steps, k, (regardless of the uncertainty estimator) is an experiment in this direction). I understand that part of the paper deals with the choice of time points to increase mutual information between, and so its probably more efficient than the other alternatives, but a comparison (or discussion in related works) would have made the paper stronger.  [1] Better Long-Range DependencyBy Bootstrapping A Mutual Information Regularizer. https://arxiv.org/pdf/1905.11978v1.pdf
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots.This paper provide a large visual-tactile dataset with GelSight tactile sensor and camera, containing diverse senarios and objects. Various applications are presented on the proposed dataset, including a novel tactile-driven image stylization task. For touch prediction only one model is evaluated.The dataset empowers multiple tasks, including tactile feature prediction, newly proposed tactile-driven image stylization, and future touch prediction. The newly proposed dataset is much more diverse than previous efforts. The visualization of tactile sensors is a little confusing.The paper presents a new dataset that helps to understand the material properties of objects in real-life. The authors use human data collectors to probe objects in the wild (both indoors and outdoors) and collect both video and information received on tactile sensors. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown.The paper introduces a new dataset that contains both the visual and the tactile modalities for visuo-tactile learning. The new dataset is collected in natural environments with diverse objects. Experiments on three tasks have demonstrated the usefulness of the dataset.A paper introduces a database with videos of a person probing objects with a tactile sensor. It is more diverse than existing databases, and provides multi-modal information. How these benefits touch and sight association tasks are unclear.	Overall, all reviewers see the novelty in the dataset that provides an original new set of tactile data, accounting for a wide range of objects and scene, collected in the wild. The paper is overall well written, clear and easy to follow, and provides a good demonstration of the importance and usefulness of the dataset, going beyond datasets collected by robots in the context of material properties. It also includes initial steps towards benchmarking, although it is not the core contribution of the paper, which lies in the creation of the dataset itself.   Two reviewers recommend acceptance, two stand marginally below acceptance, and one does recommend rejection. The authors have provided detailed responses to all reviewers’ comments and concerns and updated their manuscript. The main concerns of those reviewers sitting on the fence seem to be answered and limitations acknowledged however the element of bias in the data collection was still considered as a possible concern. Based on my reading and accounting for the authors changes, there approach itself is novel and original and hence provides already a relevant contribution to NeuroIPS; while the authors acknowledge limitations and future work avenues. The one reviewer recommending rejection didn’t engage in the discussion but from my reading of both the paper, the comments, and the authors responses, I believe the main points were addressed by the authors and misunderstandings clarified. Overall, I believe the papers offers a novel, non-existing dataset that can inspire interesting future works; all the dataset is shared and presented clearly for others to use.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method VAEL, which combines VAE and ProbLog. The main idea is to interpret a subset of the latent representation z of VAE as symbolic representation of the input. A neural network maps z_sym to probabilities of facts in a ProbLog program. VAEL is tested on two image generation tasks and is shown to have better generalization.The presented paper bridges the gap between the idea of a VAEs and its applicability to neuro-symbolic systems. To this end, the authors derive a VAE model that internally makes use of a Probabilistic Logic Program.This work propose to unify unify VAE with probabilistic logic programming, which is an original problem to investigate. The proposed method is still an early-stage one. The experiments are limited and the proposed method should have been better motivated. These weaknesses make it hard for a reader to draw inspiration from this work.This paper studies an interesting field of neuro-symbolic methods, conditional generation. It proposes to integrate VAE with ProbLog to take advantage of the extra symbolic-program information for conditional image generation with better data efficiency and generalizability.	The paper proposes to include a component enforcing logical constraints on top of a variational autoencoder (VAE). The resulting method, VAEL. does so by leveraging ProbLog in addition to a neural encoder and decoder. VAEL is employed for simple generative tasks with constraints over the outputs, such as conditional image generation with MNIST and small Mario levels.  The reviewers have appreciated the direction where VAEL is heading and the importance of integrating constraints in deep generative models. Some concerns are still open. Specifically, the motivation behind some architectural choices (and the specific choice of VAEs as deep generative models) and the small-scale nature of the experiments. The complexity and scaling campabilities of performing symbolic reasoning with ProbLog are not discussed in depth.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new sequential recommendation model with behavior pathways. The pathway attention using learned binary routes can effectively remove unnecessary items for a given user sequence. Experimental results show that the proposed model significantly outperforms existing models on seven benchmark datasets.The Recommender Transformer (RETR) can dynamically plan the behavior pathway specified for each user. It can also sparingly activate the network through this behavior pathway to effectively capture evolving patterns useful for recommendation.This paper proposes a recommender transformer with a pathway attention mechanism. It is characterized by its ability to capture three types of user pathways and predict user action sequences with high accuracy. The paper demonstrates the usefulness of the proposed method in comparison with several state-of-the-art methods.The authors propose Recommender Transformer (RETR) with a Pathway Attention mechanism. The key design is a learned binary route to prevent the behavior pathway from being overwhelmed by trivial behaviors. The paper is generally easy to follow and the idea of using pathway in recommendation algorithms seems to be new.	This paper presents Recommender Transformer (RETR) with a pathway attention mechanism that can dynamically zeroing-out the interactions (e.g., the trivial/noisy ones) in transformer-based sequential recommender systems. Extensive experimental results demonstrate the effectiveness of the proposed architecture.   Overall this paper received mixed reviews with borderline scores. The reviewers raised concerns around baselines and evaluations, some of which the authors promptly addressed in the revision during the rebuttal period. I also read the paper in details myself. I do agree with some of the concerns from the reviewers but I don't think a method needs to beat every other published papers to be published (and I think the current baselines are more than thorough enough). My biggest complaint about the paper is around the writing, specifically, how the proposed idea is presented.   This paper tries to tackle an important question, which is that in sequential recommendation, not every interactions are useful in helping predict future interaction. The self-attention mechanism in transformer kind of addresses this problem but in a more "softer" fashion with attention weights. This paper presents a simple yet effective method to introduce a pathway mechanism that adaptively zeroing-out some of the interactions via a binary pathway router. In order to train such a model end-to-end, Gumbel-softmax sampling is utilized.  The most important part of the contribution to me is that this is an improvement to the transformer architecture, as opposed to a new model which is what this paper's writing suggests -- the proposed approach is effectively model-agonistic and doesn't marry to a particular loss function or finer-grained architectural choices (number of layers, etc.). Currently there are many baselines in the paper, but each made some different model/architecture choices, which could contribute to the difference in performance (or not, but we wouldn't know). An ideal evaluation should have been to take all the transformer-based baselines that are currently in the paper, add this pathway mechanism without changing anything else, and show that the results improved over the transformer architecture. In this way, we know the improvements are exactly coming from introducing the pathway. The authors might argue some of the current results are already supporting this argument, but my point is to emphasize this point very explicitly rather than leaving it for the readers to infer.   From what I read in this paper, I truly believe this pathway idea has its potential. Therefore, I would especially want the authors to further refine the presentation to better convey the idea, which in turn will hopefully increase the impact of this paper once it is eventually published.   Some minor comments: * The way the paper is currently written seems to suggest there are only three types of pathways and the network is capable of capturing all of them. I am personally not a big of fan of over-interpreting what a neural net is trying to do. Therefore, I wouldn't overly focus on the characterization of different pathways and only show the qualitative examples at the end as a high-level demonstration. * In Eq 2 "softmax" should really be "sigmoid" if a 0-1 prediction is made there. Then the following line "logit" is probably not the right word here.  * The qualitative examples at the end (figure 3) can be more carefully examined/labeled. For example, the current categorization is quite ambiguous -- "Indie" refers to the type of developers while "JPG" refers to the genre of the game, they are certainly not mutually exclusive.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces a new method, Recursive Gradient Optimization (RGO), for continual learning in the task-incremental scenario. This method modifies the direction of gradients on a new task in order to minimise forgetting on previous tasks. The authors introduce a Feature Encoding Layer to achieve this. The experiments seem comprehensive in range (many benchmarks, many architectures)The paper addresses a long-standing problem of deep neural networks. New training data for new tasks arrive continuously and data does not stay forever. The network has to learn to cater for all the tasks without forgetting what it learnt for an older task. The experiments show great improvement over sota approaches on benchmark datasets.The paper proposes a continual learning approach based on recursive gradient optimization. A projection matrix is derived for the gradient modification. This matrix, P, is computed incrementally and updated by integrating the Hessian on each task locally.Recursive Gradient Optimization (RGO) modifies the gradient direction at each update by multiplying it with a projection matrix P. RGO is theoretically designed to prioritise performance on the current task and, among the optimal solutions, find the one that causes least interference with previous tasks. Experiments are run on a number of standard continual learning image classification benchmarks, demonstrating an extremely strong performance of RGO.	This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the problem of distribution shifts as out-of-distribution generalization. It formulates the OOD problem as invariant risk minimization under different environments. The relation between these two has been extensively discussed in the paper.The paper adapts a recently proposed invariant risk minimization approach for tackling distribution shift on node-level predictions on graphs. The main idea is to decompose the graph into a set of ego-graphs rooted at each node. Since the learning objective requires data from different environments they introduce auxiliary context generators trained to maximize the variance loss.The importance of out-of-domain (OOD) generalization has emerged. The proposed approach, "Explore-to-Extrapolate Risk Minimization", that adopts K context generators solve the problems in a smart way. Please see the weakness and questions section.This paper tackles the out-of-distribution problem for node-level prediction on graphs from the invariance perspective. It presents a novel approach in which the whole graph is divided into n ego-graphs where n is the number of nodes. In empirical experiments on multiple datasets, the approach (EERM) outperforms its counterpart baseline ERM.	The reviewers have improved their scores after the rebuttal, and I agree that the work has value. It proposes a model-driven data augmentation approach to environment-invariant graph representation. Just like most data augmentation works in graph representation learning, the approach relies on graph proposal generator. The work has an implicit mechanism hidden in the graph generator (the authors' reply to a reviewer that "we target the extrapolation via a new invariant learning approach that is agnostic to specific GNN models" is a misunderstanding of why & how these types of methods work). The submission could significantly improve the introduction by properly describing the work w.r.t. other OOD efforts in graph representation learning. While the tasks of different works may be different (e.g., graph classification vs node classification), it is important to emphasize what each different approach brings to the table (rather than just state that the tasks are different). I hope the authors take this opportunity to improve the introduction. This work is more similar to the former OOD graph representation works than IRM & REX, since (without modeling assumptions) both IRM and REX require the support of the environments in test to be a subset of the ones in training. The present work does not need this support assumption since it uses an implicit mechanism in the graph generator.  The toy example in Section 3.1 is informative, thank you. The theory looks solid and easy to understand. The technical novelty is limited, since once the input graph has been decomposed into a set of ego-graphs the definitions, formulations, and theory are all straightforward adaptations from the respective versions for IID data.   Minor: - In Assumption 2, m() should be defined inside the assumption.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a way to integrate a translation memory into a neural machine translation model. They show improvements of up to 2.64 BLEU over a reasonable but not SOTA baseline for English-French task. The model and the experiments are not adequately described.This paper describes several improvements on using information from a Translation Memory (TM) in Neural Machine Translation. Experiments are conducted on benchmark French-English data, showing consistent improvement over classical baselines. There are a number of issues discussed below, related mainly to modeling and to the experimental evaluation.The paper argues that the existing way of using Translation Memory (TM) in neural machine translation (NMT) is sub-optimal. It proposes TMG-NMT, namely Translation Memory Guided NMT, which consists of two parts, a universal memory encoder and a TM guided decoder.The idea itself is not novel. Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me. The authors should conduct experiments beyond English-to-French. More languages pairs should be verified.	This paper presents a way to use a translation memory (TM) to improve neural machine translation.  Basically the proposed model uses a n-gram retrieve matching sentence （or pieces） and takes advantage of the useful parts using gated attention and copying mechanism.  Although the idea of leveraging TM in the context of NMT is not new,  this work seems to be a fair contribution. My major concerns are the following 1. The retrieval part  is not clearly presented, raising questions about  complexities and the noise brought by the common words. The authors should give a better exposition on the ranking mechanism.  2. The experiments are not convincing enough since the proposed model is not compared to the SOTA and the competitive models described in the prior work.  In conclusion I would suggest to reject this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a new loss function for survival analysis based on proper scoring functions to less then penalty wrong predictions. The benefit over the traditional maximum likelihood estimator is small. I would also like to see an eval on data with far less censoring.The proposed approach is only marginally better than MLE-based model fitting. In particular, as mentioned by the authors the WTTTE-RNN has a similar architecture. I would have liked to see an evaluation on more datasets.The claim that that the proposed approach constitutes the first time a scoring rule other than maximum likelihood seems too strong, unnecessary and irrelevant to the value of the presented work. It is not clear how did the authors handle the irregularity (in time) of EHR encounters in the context of an RNN specification.The paper proposes the use of Survival Continuous Ranked Probability score instead of maximum likelihood estimation for personalised probabilistic forecasts of time-to-event data. The authors describe the evaluation their method using (1) proper scoring rule objectives and (2) evaluation of calibration using sharpness as a metric.	All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes two datasets: VCE (samples containing emotion label distributions) and V2V containing pairwise comparative labels. These datasets can be used to predict emotional responses, and wellbeing of viewers. Authors train standard CNNs on these datasets, and observed that while these networks perform well, they do leave a significant performance gap to be fulfilled.The dataset contains 60,000 videos with human annotations for 27 emotion categories. The background of 400 annotators are not mentioned in the paper. If the videos contains mixed emotions like first sad then happy, such type of videos will have very subjective label.The authors introduce two datasets, Video Cognitive Empathy (VCE) and Video to Valence (V2V) The datasets contain over 60,000 short videos that were annotated with emotional content and general emotional valence by crowdworkers. The authors train machine learning models on the datasets and demonstrate that some models reach good performance.	Overall, the paper is capturing a really timely and increasingly important topic. Although the authors mention that they want to make their dataset particularly useful for academics/future research, I believe, that the topic is of great interest for companies, making the raised ethical considerations even more important. A critical reflection upon possibly ‘unintended uses’ can’t be emphasised enough, as well as possible biases in the data. However, I appreciated that the authors have done a fantastic job in their response and strengthening their article. Moreover, all reviewers recognise the utility and relevance of the proposed dataset and the authors did provide satisfactory responses to all raised concerns/comments. I believe this paper will stimulate some interesting and hopefully critical reflections on how to use/or not use the dataset and future improvements to overcome dataset biases.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method to estimate the multi-dimensional intensity function of a Poisson process with a lower-dimensional projection. The performance of the proposed method is extreme through synthetic examples and a real data analysis. The proposed approach has long existed in the statistics literature for decades, under the context of generalized additive models.This paper considers the problem of estimating the intensity function of a multi-dimensional Poisson process. Inspired by the generalized additive model, the authors propose a certain non-parametric form for the log-likelihood. Experiments on synthetic data as well as real data (New York taxis) are included.This paper proposes a new model for estimating the intensity of a high dimensional Poisson process. By representing the intensity as a sum of lower dimensional projections, the method can deal with sparse data. The assumed intensity form is motivated by the Kolmogorov-Arnold theorem. The estimation procedure has some formal guarantees of convergence.The paper propose to apply generalized additive models (GAM) to learn the intensity of the multi-dimensional Poisson processes. This approach is interesting but it would deserve more works to further develop the idea. The paper is not motivated and clear enough.	The paper proposes a novel approach for estimating the high-dimensional intensity function of a Poisson process. The proposed approach builds on generalized additive models, using lower-dimensional projections.   The reviewers noted that, although the paper is well written, the position of this paper compared to earlier related work is unclear, and the empirical evaluation of the method should be strenghtened. The authors clarified some points in their response, but the paper would still require some more modifications to be ready for publication. I therefore recommend this paper to be rejected.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors proposed an improved deep-learning-based representation learning method. The integration of a softmax-formulated orthogonal constraint is able to provide more stable latent feature representation. I was deeply impressed by the far above state-of-the-art values of evaluation metric.This paper proposes a clustering-friendly representation learning method. Instance discrimination loss and feature decorrelation loss are combined to optimize the network. The paper is well qritten and experimental results are good.The evaluation is fairly detailed. The authors used datasets that are somewhat larger than often used in the literature. The inclusion of the study of the temperature parameter also helped clarify a few questions I had when reading it.	This paper received mostly positive reviews. The reviewers praised the strong performance when compared with previous work. Also, the evaluation clearly shows the benefit of the proposed contributions in terms of performance. Most concerns raised by reviewers were properly addressed in the rebuttal.  Lack of comparison to several previous works has been noted in a comment, but the authors clarified this concern, stating that the current work is a “large deviation from prior works”. The authors promised to include the missing references into the comparison.  Given the reviews, comments, and author's answers, I suggest acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents the imperceptible warping based back-door attack technique. With the poisoned image generated from the technique, the network will be fooled. I am not confident in the review because I'm not familiar with the attack algorithms.This paper proposes a new backdoor attack method on vision classifiers, where a warping-based trigger is adopted instead of patched triggers. To bypass backdoor detectors, the authors also propose a “noise” mode of training poisoned classifiers. I would like the authors to address the issues I raise.This paper proposes a backdoor attack method using warp-based triggers, which distorts the global structure of the image ( input) making the attack undetectable/un-noticeable by humans. The authors have addressed my comments and revised the manuscript accordingly.	The paper presents a new method for generation of backdoor attacks against deep networks. The new method uses global warping instead of noise patches which makes the attack much more stealthy than previous approaches. The attack effectiveness is demonstrated on 3 benchmark datasets. A small user study is carried out to demonstrate that the attack is stealthier than conventional backdoor attacks.   The new attack is a novel and original contribution which is likely to advance the understanding of backdoor attacks. There were some issues with respect to clarity in the original manuscript but the authors adequately addressed the critical remarks raised by the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes matching problem-independent (worst-case) upper and lower bounds for the tabular version of the problem described above. The paper is well-written with a clear technical exposition. The observation that variance-aware algorithms lead to bounds that are order-wise better in this setting is an interesting one.This paper studies the cascading bandits in both tabular and linear cases. The paper's main contribution is to find out that the hardest problem instances (where means are small) yield small variance, meaning that the estimation for these instances is naturally more accurate.The cascading-bandits problem considers a scenario where the agent aims to choose K out of L items. Each item corresponds to an independent Bernoulli random variable. In each iteration, the agent chooses an ordered list of K items, and the underlying variable is revealed in the same order.This work studied the regret minimization in cascading bandits. It closed the gap for the upper and lower bounds on the regret in the tabular setting. It also studied the linear setting and evaluated the algorithms with numerical experiments.	All the reviewers were generally happy with this paper. There were some comments about a better experimental section and maybe a better discussion of results and extensions (e.g. gap-dependent bounds, what happens in the K->L regime), but everyone felt that the manuscript as written was solid enough to merit acceptance. I encourage the authors to incorporate the discussions on these points in the final manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed a unified sequence interface (Pix2seq++) for computer vision tasks. The proposed method achieves state-of-the-art performance on 4 benchmarks. The keypoint tokenization is not clear to me.The work proposes an unified framework/interface for multiple vision and vision+language tasks. There are some concerns about novelty and whether the work is just incremental compared to previous work of pix2seq[6]. In addition training(pretraining) setup is slightly confusing and performance on few tasks are not so encouraging.This paper presents a unified sequence-to-sequence generation interface for 4 core vision tasks: object detection, instance segmentation, keypoint detection and image captioning. The model is designed with a single encoder-decoder architecture, with no task-specific heads.This paper addresses the unification of 4 specific vision and vision-language tasks (namely object detection, instance segmentation, human pose estimation, and image captioning) The experiments show that their method achieves competitive results compared to well-established task-specific models. In my opinion, the results are promising as it could initiate a new research line unifying different computer vision tasks.	Four reviewers provided reviews for this submission. Several reviewers felt that the idea to unify core vision tasks into a sequential output format is interesting and an entirely new approach and can have a large impact on how we train vision models in the future. There were a few concerns discussed between reviewers and authors. One concern was the comparison to past works that pre-trained on ImageNet vs the proposed model that was pre-trained on Objects365. The second concern was differentiating this work with Pix2Seq. In my opinion, the authors were able to answer both questions well. Overall, given the positive reviews, novelty of the work, potential to cause a significant shift in the approach of future modeling and discussion, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. Unlike in dropout masks, the masks are kept fixed throughout training/inference. This shifts the focus of the network away from texture and towards shape features.The authors of this paper propose a simple yet effective method "defective convolution" that randomly "disables" neurons on the convolution layer. The authors argue that by doing so, the CNN is encouraged to learn less from object texture and more on features such as shape.The authors present a modification of a convolutional layer that they claim forces their model to be more robust to texture attacks. In none of their experiments do they explictly show that the model is focusing on shape outlines.	The paper presents a method to make CNN focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation.  The paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. Tables are not readable and results tend to be cluttered and confusing.  Some comparisons seem to be cherry picked as pointed out by some reviewers. Although the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach (that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others). I strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. In my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes to combine Normalizing Flows with generative Diffusion Models. The target data is first non-linearly transformed via the flow and then the distribution over latent embeddings is modeled with the diffusion model. Since the flow is invertible and defined by a deterministic function, it is possible to formally relate the combined flow and diffusion process to a non-linear diffusion directly in data space.The paper proposes parameterized diffusion model (PDM) that combines a normalizing flow on top of a linear diffusion model, effectively modeling a nonlinear diffusion. Empirically, such nonlinearity results in a tighter variational gap compared to the linear diffusion and other baselines.This paper is set up in the emerging toping of generative diffusion models. The paper comes with a strong theoretical flavour and the proposed approach is well grounded. However, all these remarkable developments come with an unacceptably low quality of english usage.The paper is clearly written and easy to understand. The structure of PDM is very similar to LSGM mentioned in the related work. From the experiment, it seems that LSGM produces lower FID scores compared to PDM. Since the PDM produces better NLL, I guess it can produce better distribution coverage.	This paper presents a simple approach called PDM for composing non-linear and complex normalizing flows with score-based generative models. Since score-based models can be considered as a special form of continuous-time normalizing flows, PDM corresponds to a composition of different classes of normalizing flows.   Pros:  * Combining generic normalizing flows with score-based models is an interesting direction as they have different characteristics and can be complementary to each other. * Using Ito's lemma to show that the model learns a non-linear SDE in data space is valuable.  * The authors show that the variational gap can be reduced using normalizing flows.  Cons:  * The proposed method does not exhibit a clear advantage compared to the diffusion baseline without the normalizing flow component. On the CIFAR10 dataset, the best NLL and FID results are obtained by the diffusion baseline.  * Theorem 2 makes a very unrealistic assumption that a flow network is flexible enough to transform $p_r$ to any arbitrary distribution. If this holds, we wouldn't need the score-based generation model anymore. We could simply train the normalizing flow to map the input data distribution to a Normal distribution.  * This submission chooses to discuss differences with the recent LSGM framework. However, in doing so, several inaccurate claims are made. The lack of inference data diffusion in LSGM is mentioned as one of its drawbacks. However, it is not clear what is the value of having such a mechanism and what implications it may have on the expressivity of the model. Note that mapping from data space to latent space in VAEs can be considered as a stochastic inversion rather than an exact inversion. Ito's lemma does not require invertibility and it can be easily applied to the forward and generative diffusion in LSGM. The authors argue that applying it to the forward diffusion in LSGM will result in $\hat{p_{r}}\ne p_{r}$. But, $\hat{p_{r}}$ would be only considered for visualization of the forward diffusion and it is not used for training or any other purposes. LSGM, the proposed PDM, and score-based models are all trained with a reweighting of ELBO (see [here](https://arxiv.org/abs/2106.02808)). It is not clear if the drawback mentioned above has an impact on the training or expressivity of the model.  * The presentation in the paper requires improvement. The motivation on why invertibility plays a key role is not clear beyond generating the visualization in Figure 2.   In summary, the paper proposes an interesting idea and explores directions very relevant to the current focus in generative learning. However, given the concerns above, we don't believe that the paper in its current form is ready for presentation at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a way to algorithmically construct Gaussian Process functions in such a way, that realizations strictly follow a given ordinary differential equation. The resulting LODE-GPs are compared to classic GPs in three experiments. The paper is very well-written and the method is presented in a clear way.The paper describes how to constrain realisations of a Gaussian process to satisfy $A f = 0$, where $A$ is a linear, matrix-valued differential operator with constant coefficients. The Smith normal form also restricts the class of problems that can be tackled to linear, homogenous, ordinary differential equations with constant coefficient.The authors introduce a new method which tackles the problem of learning the solutions to systems of ordinary differential equations from noisy data. In particular the authors attempt to solve the problem in the case of linear differential operators for systems which are underspecified and uncontrollable. The proposed method relies on the symbolic calculation of the Smith normal form of the differential operator representing the system.This paper proposes a method, based on Smith normal form, to construct Gaussian processes indexed by R. The entire noise free data is not shown in fig 2. It woudl certainly be interesting to see how well the method extrapolates.	The reviewers found this paper novel, significant for the community, and well written. All four reviewers recommended accepting the paper. I also appreciated the numerous illustrative examples in the paper. You addressed many of the remaining questions/concerns in your rebuttal and in the author-reviewer discussion. Please, go through the reviews once more for the camera-ready and take these into account.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper describes a novel implementation of RED, regularization by denoising, which better leverages multicore architectures to achieve a significant speedup. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy.The paper proposes asynchronous type of parallel algorithms by combining the pre-trained deep denoisers. Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in terms of running time. However, the novelty of the methods look incremental.This paper proposed for the first time the asynchronous variants of deterministic and stochastic regularization-by-denoising (RED) algorithms. These new variants are aimed to fully utilize the multi-cores structure of computational devices.Theorem 2 shows that the "convergence" is not variance-reduced. I would appreciate a discussion on whether this is unavoidable or if this is an open question for future work. The DnCNN architecture is used in the experiments: does it satisfy Assumption 4? More generally, are all assumptions met so that the theorems apply in the experiment?	The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Authors study the problem of large-batch optimization for various "dense" computer vision tasks, such as object detection or instance segmentation. They propose Adaptive Gradient Variance Modulator (AGVM) - an optimizer-agnostic technique. While i have many low-level concerns about the paper positioning and quality, I am strongly in favor of paper acceptance.This work aims to the heavy performance drop that takes place in the large-batch training regime for object detection and segmentation. They propose an algorithm named Adaptive Gradient Variance Modulator (AGVM) that is able to work with a very large batch size.This paper proposes an Adaptive Gradient Variance Modulator (AGVM) to achieve large-batch optimization for dense visual tasks. The paper clearly claims the motivation, which is the high gradient variance in the different modules. The AGVM has a theoretical guarantee, and sufficient experiments have proved its effectiveness.The paper presents a new approach called Adaptive Gradient Variance Modulator for large batch-size training. The approach is well motivated with simple implementation. Experimental results on different kinds of dense prediction tasks like object detection, instance segmentation, panoptic segmentation validate the effectiveness of the approach.	The authors describe a new method of large-batch optimisation for dense prediction computer vision tasks. The reviewers appreciate the simplicity of the method, convincing experiments and the potential practical importance. AC recommends acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This article investigates emphasis effects in information visualisation. It focuses on three emphasis strategies : varying colour, size, and blur/focus. The authors found that blur was the most effective strategy to improve prominence.The new revision of the paper has clarified several issues. The addition of past work was not well integrated. It would be good to further add the number of trials seen by participants. The choice of visualizations in study 2 was based on their resemblance of scatterplots.The authors do not really tackle the difficult issue of distance in the users' reaction metrics to the different effects and magnitudes. The authors claim they clarified how visualizations were chosen. They used 16 visualizations. Which ones??? I'd like to know exactly which forms were used.	The reviewers found that the article tackles of problem of relevance in formation visualisation, and that the studies bring relevant insights to the community. They nonetheless highlight of number of problems remaining in the article that should be addressed before being published.  1. Clarify the presentation of the study conditions (all reviewers). 2. Better integrate of the literature in relation to the problem tackled in the article, both in the related work section (R1) *and* in the discussion (R2). 3. Clarify the visualizations used in study 2 (all reviewers). 4. Expand on the limitations of the experiments and their analysis (R2) 5. Reflect and contextualise of the results may apply in the wild (R3)
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Video Text Visual Question Answering (ViteVQA) extends the previous text-based visual question answering task into the video domain. The proposed task aims at answering questions by reasoning texts and visual information spatiotemporally in a video.My main concerns is the create dataset, especially the collected questions as well as the annotated answers. According to the statistic and analysis results in Page 5, the lengths of the majority questions and answers are under 14 and 10. It seems that the questions as demonstrated in Figure 2 is very simple with a very simple format, namely “waht is XXXX”. In such a situation, the vqa task becomes very simple.This paper propose a new multi-modal dataset, namely M4-ViteVQA, to challenge the research community with a difficult Text + Video question answering task. Different from previous video-QA tasks, the model is expected to understand "visual text" in the scene, rather than text from human dialogue.	The paper presents a new video text VQA dataset, along a new baseline. The reviewers’s key concerns are around the size and difficulty of the dataset; the authors successfully address those during the discussion and revision period. The dataset is likely to be of interest to the community and was non-trivial to construct.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tries to verify a hypothesis that language grounding DO help to overcome language drift when two agents create their own protocol. There are several constraints to enforce: 1) naturalness, say "Englishness", 2) grounded in visual semantics.The paper presents an approach to refining a translation system with grounding (in addition to LM scores) in the loop to manage linguistic drift. The intuition is straightforward and results are clearly presented, but the gains are unfortunately much weaker than I would have hoped for.This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system.	This paper proposes a method to resolve "language drift," where a pre-trained X->language model trained in an X->language->Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.  The main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre-trained X->language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).  A concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high-profile conference such as ICLR may help re-popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy's seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow-up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.  Thus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper targets at weakly supervised semantic segmentation for both 2D and 3D domain. The paper doesn't demonstrate if the learned model is practical and can generalize to images or scenes outside the chosen dataset. The proposed method is effective and improves existing methods by a large margin.This paper proposes a method to do weakly supervised semantic segmentation with 2D-3D information. Existing methods have studied the use of class activation maps (CAMs) on 2D images and 3D point clouds. This paper proposes to leverage 2D and 3d CAMs jointly to learn a network for better 3D semantic segmentated under weak supervision.This paper proposes a novel 2D-3D Weakly Supervised Semantic Segmentation framework that jointly targets both domains of image and point cloud. To bridge the gap between 2D and 3D domains, the 3D outputs are projected onto the image plane.The paper proposes a new task formulation and a method to jointly train semantic segmentation on images and corresponding point clouds. To achieve that, the authors introduce new loss functions to match the pre-pooling activations and the statistics on intermediate features of pixels and projected features of 3D points. That leads to significant improvement in SOTA in both 2D and 3D segmentation tasks.	The paper proposes two constraints, transferring the knowledge from 2D domain to 3D domain and vice versa, to do weakly supervised semantic segmentation in 2D and 3D.  During inference the model doesn't require paired data.  The paper is clearly written, and explains how the 3D geometrical information complements the redundant dense 2D information.  The reviewers mention limited experiments.  However, the ablations are thorough, and show significant improvement over SotA performance.    The authors addressed the reviewers' comments during the rebuttal.  The reviewers agree that the benefits of the proposal outweigh its flaws.  I recommend the paper for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method to prune deep neural networks. The aim is to reduce the computational cost and inference time while maximally maintaining the classification performance. The motivation of this work is to consider second-order structured pruning (SOSP)This paper address the problem of neural network single-shot structured-based model pruning. The key challenge is to find a good objective that can be efficiently calculated to make the approach scalable to various modern convolutional neural networks.This paper presents saliency-based second-order structured pruning methods. The proposed methods are designed to capture the correlations among all structures and layers. In particular, SOSP-I employs Hessian approximation while SOSP -H employs exact Hessian. Overall, this paper has its merits.This paper develops two novel saliency-based pruning methods for second-order structured pruning. The proposed method can also be used to find and remove architectural bottlenecks, which further improves the performance of the pruned networks.	Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Author: Authors actually perform a prediction task: using the 4D CTA scan to predict the infarct core segmentation on a follow-up scan that is acquired at a later date. The use of a multi-centre data set is another strength.The paper includes a thorough prior work section of previous techniques that used CTP data for estimating follow-up lesion. Since the proposed method is simpler than existing methods it is interesting to know the segmentation performance. It remains unclear with what software and what settings the perfusion maps were computed.The paper presents this need clearly and sets out to tackle the problem with a custom CNN architecture. There is a sufficiently large dataset available for this research, including an annotated target. The paper is written with a clarity that makes it sufficiently easy to follow the path (apart from the weaknesses addressed next).The results fail to convince me that the method actually works. The results show that using longer sequences improve predictions, however there is no justification that even the best performing results are good enough. The clinical challenge is relevant, and described very solidly.	While the reviewers raised some concerns about the experimental results, I agree with the overall sentiment that this paper proposes a promising temporal convolutional network for segmentation in 4D CTP data. The authors may want to consider updating the title of the paper as the "outcome prediction" term seems to be a source of common confusion.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies boosting-like RL algorithms using the most boosting techniques from online learning. It advocates the advantage of the proposed algorithm as the sample complexity's independence of the number of states. The novelty looks limited as the techniques are very similar to Hazan and Singh 2021.In this paper, the authors study boosting in RL, i.e., how to convert weak learners into effective policies. The algorithm uses a variant of the Frank-Wolfe method to overcome the non-convexity of the value function. Weighing the strength and weakness of the paper, I am recommending weak accept.This paper attempts to deploy the boosting technique in the supervised learning on policy learning in the RL setting. It is difficult to catch the main idea of this paper, and it is dubious if the proposed algorithm can be implemented in practice. I suggest the paper be added more clarification of notations and interpretation of main results.The paper proposes boosting (using Frank-Wolfe) weak RL learners to get a strong (in a concrete sense) policy. The main theoretical bound does not depend on the number of states, which indicates that the method should be competitive in very large state-space MDPs. The paper is extremely dense and hard to read in many places.This paper proposes a new approach for solving RL problems with sample complexity independent of the number of states. Rather than imposing structural assumptions, the authors consider access to weak learners and propose a way to combine these weak learners effectively to generate a near optimal policy. The paper is very tersely written and not easy to understand.	The paper proposes a boosting algorithm for RL based on online boosting. The main advantage of the result is that the sample complexity does not explicitly depend on number of states. Post rebuttal, some of the reviewers have changed their opinion on the paper. However, overall the reviewers still seem to be on the fence about this paper. Seems like the paper combines the techniques from Hazan Singh’21 along with a frank-wolfe algorithm to deal with non-convex sets but the reviewers seem to view this as not as significant a new contribution.   I see the paper as being interesting but do agree with some of the comments of the reviewers and am leaning to a reject.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a method for efficiently automatically generating a pool of “risk scores” (sparse linear models with integer coefficients) They evaluate both the speed and accuracy of their approach on multiple benchmark datasets. The main improvement seems to be speed (which wasn’t very well quantified with respect to baselines)This study proposed a novel method to accurately and efficiently generate a collection of high-quality risk scores. The whole methodology was solid and efficient. It achieved SOTA performance with less time cost in some datasets.This paper aims to provide a fast algorithm to derive sparse risk scores that scales to high-dimensional datasets. The algorithm is implemented in stand-alone Python code, which is advantageous over competitors that rely on mathematical programming solvers. My major concern is the scope of this study, which affects its quality and significance.The paper is focused on risk scores learning which are simple but efficient (in terms of performance) models. The main idea is to produce a pool of almost-optimal sparse continuous solutions using a beam-search algorithm.	Thank you for submitting your paper to NeurIPS! This paper makes a valuable contribution to the scoring model literature, providing a fast and scalable algorithm to derive sparse risk scores. The reviewers uniformly appreciated the methodological approach (integrating beam search with logistic regression, diverse feature selection, and star search for choosing integer coefficients), and noted that the stand-alone Python implementation is also advantageous over competitors that rely on mathematical programming solvers. I am pleased to recommend acceptance of this practically relevant work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A relevant geometrical criterion is proposed despite fuzzy or qualitative medical definition to discriminate both types of white matter. The medical argument is convincing. The problem is interesting since it requires to account for intensity and geometry at the same time.There are several weaknesses found when I went through the paper. The weight for the loss function is not mentioned. Results need to be polished. Metrics are confusing when putting two-class (whole) results and three-class results together.Results may be en par with a MICCAI 2017 challenge winner. the paper tries to align with clinical practice. It is unclear what actually has been achieved beyond yet another WMH segmentation algorithm. The paper seems to overall propose a 1:1 implementation of some arbitrary clinical criteria in a processing pipeline.The distinction between deep and periventricular WMH is not made using a neural network, but by a relative simple rule-based processing. The authors mention the different rules that are used in clinic and which one they chose to implement.	all reviewers agree that the weaknesses of this work outweigh the few positive aspects and thus the paper cannot accepted in the current form. No rebuttal or revision was provided.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal of the work is to associate semantic data types to table attributes. The authors claim that existing methods use regular expressions or dictionary lookups. They train their models on 686,765 data columns extracted from the VizNet corpus.This paper develops a semantic type detection model DCoM. It takes column values as text in addition to auxiliary features extracted by Sherlock. Experimental results on the VizNet corpus show thatDCoM models with DistilBERT/ELECTRA outperform Sherlock and other non-Deep Learning models.This paper studies the problem of semantic type detection that takes a column of texts then predicts a header name for the column. Existing methods mostly adopt regular expressions or dictionary-lookup or feature engineering. The paper introduces a new method, called DCoM that is different from existing methods. It adopts deep neural networks to process raw text tokens directly.	The paper studies semantic type detection.  The problem is of practical significance to  i  tabular data.  However, in its current form, there are concerns about  the scope of novelty and technical significance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a principled method for incorporating successor feature into an actor-critic algorithm. The performance of the proposed method is very similar to the goal conditioned SAC baseline on meta world tasks. The environments used in the experiments are limited (only reacher and door close tasks are considered)This work looks at the use of successor features for solving simple continuous control tasks. They show their approach outperforms a goal-conditioned SAC baseline on these control tasks including generalizing to target locations that were not used during training.The paper proposes a method to incorporate Successor Features (SFs) in domains with continuous state and action spaces. It proposes an actor-critic architecture that learns disentangled representations for the environment dynamics and the tasks. The main contribution of this model is the enablement of the SFs for continuous domains without relying on the costly inference mechanism.	The paper presents an actor critic type of method consisting of two types of features -- dynamics and tasks, in the multi-task continuous control setting. While the topic of the research is interesting and relevant to ICLR, the reviewers have concerns with the novelty and technical significance of the work. Specifically, the proposed method is very similar to several other works leading to an incremental novelty. In addition, the method is evaluated only on simple environments. The concerns remain after the discussion period.   In the next version of the manuscript, the authors are encouraged to pursue more difficult settings and modify the method to work on those problems. That would make the paper stronger, and lead to a more novel method evaluated on harder problems.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors analyze the convergence of a proximal gradient descent ascent (GDA) method when applied to non-convex strongly concave functions. They show that proximal-GDA admits a novel Lyapunov function that monotonically decreases at every iteration.This paper studied the convergence properties of the proximal-GDA algorithm for solving nonconvex-strongly-concave optimization problems. It develops a novel and comprehensive theoretical understanding of the variable convergence and rates of the algorithm under the KL geometry.In this paper the authors analyze a proximal gradient descent-ascent method for nonconvex minimax problem. I have two main concerns. The first one is that required assumptions are quite restrictive. The second concern is that a big part of obtained results is very natural if not known already.This paper discusses the convergence of the proximal gradient algorithm in the case of KL geometry. It focuses on understanding the simplest (although not the best) algorithm that can be applied to the saddle point objective. The minmax problem has gained considerable attention recently but is not yet fully developed.	The paper studies nonconvex-strongly concave min-max optimization using  proximal gradient descent-ascent (GDA), assuming Kurdyka-Łojasiewicz (KŁ) condition holds. The main contribution is a novel Lyapunov function, which leads to a clean analysis. The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis. Nevertheless, the paper was overall viewed favorably by the reviewers, who considered it a worthwhile contribution to the area min-max optimization.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper exploits depth information by estimating ground plane and fuses that information with image features via attention layers. Experimental results on KITTI are very strong, outperforming MonoDETR by 2-4points. The proposed method is intuitive and clearly presented.The method refines the camera pose relative to this plane via horizon/vanishing point estimation in the images. It is claimed to be top on monocular 3D image detection on KITTI.This paper utilizes ground depth as geometry cue to improve monocular 3D detection. The ground depth is estimated by camera pose and vanishing point. And the ground depth information is further integrated into the network by the transformer.This paper proposes a framework, namely MoGDE, to consider ground depth cues for more accurate and robust monocular 3D object detection. It estimates the camera pose at first and constructs ground depth map to guide the depth estimation of objects. The overall framework achieves state-of-the-art results on the KITTI benchmark.	The paper received positive leaning reviews (2x borderline accept, 1x weak accept, 1x accept). The meta-reviewer agrees with the reviewers' assessment of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents an entirely unsupervised method for taking as input a corpus of reviews about a set of items and building as output a directed graph. Each node represents an opinion phrase (extracted from text) and edges indicate implication relationships.The authors propose a method to automatically build a Knowledge Base of opinions and implications between them. The KB is realized as a directed graph where nodes correspond to opinions in a canonical form (modifier, aspect) It is built by factorizing a matrix of item-opinion frequencies, and finding the top k neighbors of an opinion.This paper addresses the problem of building KBs from datasets like product reviews, that identifies implications between opinions from reviews. The authors claim that opinions and their implications often do not co-occur within the same review and the annotation for such implication relation is expensive.	This paper addresses the task of unsupervised knowledge base construction. The reviewers like that the authors present a novel unsupervised approach, and are happy with the thorough experiments. However, they also point out that the approach could be motivated better, and that it makes many assumptions that are not explained properly.  We recommend acceptance but nudge the authors to consider the reviewer suggestions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The consecutive position in sequence is often referred to as “span” in NLP filed, which is identical to what the authors call “area’ in this paper. The idea of utilizing spans currently becomes a very popular in N LP field. Some important related studies are missing.Paper “AREA ATTENTION” extends the current attention models from word level to “area level”, i.e., the combination of adjacent words. Every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn. (3) and EqN. (4)Attention mechanisms are designed to focus on a single item in the entire memory. I prefer the idea of using some statistics (such as variances) of multiple items for attention. Gains on BLEU and perplexity are limited.	although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors' effort at improving the result further during the rebuttal period. this may be due to a less-than-desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The results are sensible but confirm what we already strongly suspected. The most interesting experiment is the final one in Section 6. This experiment seems like it could be the germ for a far more interesting paper getting at how these pretraining objectives help with downstream tasks.This paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other Pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data.This is nicely written paper analyzing the effect of various pre-training methods. The experiments are well motivated and well described. Language models are very effective on sequence tagging tasks (POS, CCG)	Strengths:    -- Solid experiments  -- The paper is well written  Weaknesses:  -- The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already  suggested that LM objectives are preferable and also using LM objective for pretraining is already the  standard practice (see details in R1 and R3).   There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies  the local asymptotic stability of a specific class of solutions points, referred to as strict local minmax equilibria. The time-scale separation (\tau) is being captured by the ratio of the step-sizes between the min and max agents respectively.This paper studies the stable points of gradient descent ascent with different step-sizes. The paper's main result is that for any fixed point for the GDA dynamics, that point is stable for GDA with a large enough timescale separation if and only if it is a local Stackelberg equilibrium.The main result of the paper states that a strict local minmax point is a stable critical point of t-GDA for some large enough t. It seems to me that the authors have not qualitatively improved over (Jin et al. 2020)This work proves that we can get the same stability guarantees while keeping the learning rates of both players finite. This is crucial for practical applications where using unbounded learning rates in not an option. The theoretical findings are complemented with empirical evaluations both on small min-max problems.	This paper treats the problem of running gradient descent-ascent (GDA) in min-max games with a different step-size for the two players. Earlier work by Jin et al. has shown that, when the ratio of the step-sizes is large enough, the stable fixed points of GDA coincide with the game's strict local min-max equilibria. The main contribution of this paper is an explicit characterization of a threshold value $\tau^*$ of this ratio as the maximum eigenvalue of a specific matrix that involves the second derivatives of the game's min-max objective at each (strict local) equilibrium.  This paper generated a fairly intense discussion, and the reviewers showed extraordinary diligence in assessing the authors' work. Specifically, the reviewers raised a fair number of concerns concerning the initial write-up of the paper, but these concerns were mostly addressed by the authors in their revision and replies. As a result, all reviewers are now in favor of acceptance.  After my own reading of both versions of the paper and the corresponding discussion, I concur with the reviewers' view and I am recommending acceptance subject to the following revisions for the final version of the paper: 1. Follow the explicit recommendations of AnonReviewer3 regarding the numerical simulations (or, failing that, remove them altogether). [The authors' phrase that "The theory we provide also does not strictly apply to using RMSprop" does not suffice in this regard] 2. Avoid vague statements like $\tau \to \infty$ in the introduction regarding the work of Jin et al. and state precisely their contributions in this context. In the current version of the paper, a version of this is done in page 4, but the introduction is painting a different picture, so this discussion should be transferred there. 3. A persisting concern is that the authors' characterization of $\tau^*$ cannot inform a practical choice of step-size scaling (because the value of $\tau^*$ derived by the authors depends on quantities that cannot be known to the optimizer). Neither the reviewers nor myself were particularly convinced by the authors' reply on this point. However, this can also be seen as an "equilibrium refinement" result, i.e., for a given value of $\tau$ only certain equilibria can be stable. I believe this can be of interest to the community, even though the authors' characterization cannot directly inform the choice of $\gamma_1$ and $\gamma_2$ (or their ratio).  Modulo the above remarks (which the authors should incorporate in their paper), I am recommending acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a new framework to describe and understand the dynamics of RNNs inspired by quantum physics. The authors also propose a novel RNN architecture derived by their analysis. Despite this paper is mainly a theory paper, it would have a lot more strength if the authors provide some experiments to demonstrate the strength of the proposed architecture.This paper attempts to do three things: introduce a generalization / formalism for describing RNN architectures, demonstrate how popular RNNs fit within the proposed framework, and propose a new RNN architecture that overcomes some limitations in LSTMs. The ultimate goal of this work is to develop an architecture that is better able to model long-term dependencies.The authors make connections between RNN dynamics and those of a class of ODEs similar to RNNs (ORNN) that has different orders of nonlinearity and order of gradients in time. They go on to outline a RNN structure inspired by this insight that has time-dependent activations to increase the scale of temporal dependence.In this paper, the authors relate the architectures of recurrent neuralnetworks with ODEs. They further propose QUNN, a RNN architecture that is more stable and has less complexity. The paper is well presented and the categorization method is well defined.	although the way in which the authors characterize existing rnn variants and how they derive a new type of rnn are interesting, the submission lacks justification (either empirical or theoretical) that supports whether and how the proposed rnn's behave in a "learning" setting different from the existing rnn variants.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Tiling over whole slide images (using small patches) is considered explicitly in this method to adopt the optimal transport to histopathology slides. The method facilitates using a publicly available dataset from different tissues for pre-training a model to apply to a small (local) dataset.The paper proposes a method to select a more suitable dataset for transfer learning in histopathology. The hierarchical generalization of optimal transport distance is novel. The paper is well written and is mostly easy to follow.The paper is well written and easy to understand. The proposed distance is theoretically novel and interesting. I believe there is potential that the proposed distance can be useful in more difficult tasks and rare cancer datasets. The transfer learning study does not seem to support the authors' claim strongly.	All reviewers agree that the paper has value and proposed a fairly novel idea. This method is solely applied to digital pathology images and deserves future work, but it is interesting for the MIDL community.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides a uniform generalization bound for overparameterized neural networks. The approach is based on so-called maximum information gain, which is neat. Overall, the paper is well-written and technically sound.In this paper, the authors provide a more generalized equivalence of the NT and NNGP kernels of MLPs and Matern kernels for activation functions of form $max(0,x)^s. They show that for a fixed x, $| f_{ntk}(x) | < Poly(n, log(1/\delta)$ with probability greater than $1- \delta$.The main contribution of this paper is to characterise the eigenvalue decay of the Neural Tangent Kernels associated with fully-connected neural networks. The paper is well-written and the results make non-trivial extensions over the state-of-the-art. The exposition of the concepts is very good in general, though some references and some (minor) aspects appear to be missing.Theorem 1 gives relations between the RKHSs generated by the NTKs and those generated by Matern kernels. Theorem 2 gives bounds for the maximal information gain of theNTKs. The theorem 3 provides some uniform generalization bounds for kernel ridge regression with the NTks.The paper studies over-parametrized neural networks when the size of the network goes to infinity. It is known that training the network via gradient descent type algorithms is equivalent to learning via the Neural Tangent Kernel (NTK)	The paper provides a uniform generalization bound for overparameterized neural networks using the notion of maximal information gain. The analysis relies on the eigendecay of the eigenvalues of the NTK, which has recently been the object of a lot of work in the literature, including the work of Bietti and Bach (the proof actually uses one of their key lemma).  The paper originally received a set of reviews with a large disagreement between the reviewers (including two reviewers with a negative opinion and three reviewers being more positive). After the discussion period, two reviewers kept a very negative opinion, while other reviewers slightly lowered their score. Some of the problems raised by the reviewers include the restrictions imposed on the data, a missing proof (which was eventually added by the authors), the discussion of prior work being inadequate (including for instance the differences with more classical generalization bounds), and the novelty of the analysis.  Overall, the paper clearly has some merits but some of the concerns above are too important at this stage to accept the paper. I recommend the authors address the concerns mentioned in the reviews before re-submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method for learning robust binary neural networks from random initialization. The proposed training scheme includes an adaptive pruning approach and an edge pop-up algorithm for training the binary model to obtain a binary mask.For a robust binary model built with Strong Lottery Ticket Hypothesis, the paper proposes a model compress method with three strategies: adaptive layer-wise pruning, a binary initialization scheme, and adding a last batch normalization layer. Experiments on CifAR-10 and CIFAR-100 show that the proposed method outperforms previous competitive methods.The paper leverages the existing edge-popup algorithm for pruning untrained networks via learning important scores instead of model weights. The paper proposes adding a last layer batchnorm to stabilize the training of their compressed robust models. The authors claim that their work is generic and does not suffer from limitations or potential negative societal impact.This manuscript extends the Strong Lottery Ticket Hypothesis to binary networks. It proposed a new method to obtain compact binary networks with better robustness. The proposed method consists of three main parts: a) adaptively pruning different network layers; b) an effective binary initialization scheme; c) adding the last batch normalization layer.	The reviewers agree the paper studies an interesting problem on training robust binary neural networks and the paper does a good job in evaluating the proposed approach on multiple datasets and compares well with baselines. However the paper also has some drawbacks such as the proposed method has limited novelty and is a combination of existing techniques, missing comparisons to standard training based approaches, evaluations limited to ResNets.  Overall the paper is on borderline. I suggest acceptance and encourage the authors to include all the changes that came up during discussion in the final version and discuss the limitations.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Many SOTA models usually overfit w.r.t. spurious correlations in the data. The proposed method tries to maximally filter out task-irrelevant information in the feature vectors. Experiments on several datasets show improved performance on both in-domain and out-of-domain datasets.This work applies information bottleneck as a way to compress the pre-trained representation so that only meaningful features are employed for the target task. It is applied for the number of GLUE tasks especially focusing on low resource settings. Experiments are performed extensively on various tasks and demonstrates its effectiveness in generalization.The paper is presented well, and it's a good read. My major concern is on the novelty of the proposed method. How would the VIB framework work with a different PLM, e.g., XLM-Roberta, XLNet or T5?The proposed architecture shows state-of-the-art results compared to other recent regularization methods. The novelty mainly stems from applying the DVIB to a new specific setting. I'm rather concerned about the general concept of "fine-tuning across random seeds"	The paper shows the success of a relatively simple idea -- fine tune a pretrained BERT Model using Variational Information Bottleneck method of Alemi to improve transfer learning in low resource scenarios.  I agree with the reviewers that novelty is low -- one would like to use any applicable method for controlling overfitting when doing transfer learning, and of the suite of good candidates, VIB is an obvious one -- but at the same time, I'm moved by the results because of: the improvements and the success on a wide range of tasks and the surprising success of VIB over other alternatives like dropout etc, and hence I'm breaking the tie in the reviews by supporting acceptance.  Its a nice trick that the community could use, if the results of the paper are an indication of its potential.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.It would be good if the authors could at least mention that “Boltzmann rational’ is a specific model of “systematic” bias for which much experimental support eith humans and animals exists. Not all examples in the introduction are necessarily biases but can be modeled with reward functions, where reward is given to specific states other than finishing work by the deadline. The authors are strongly encouraged to review the literature on IRL.This paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. The main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation. The proposed approach can work effectively in larger scale domains with more difficult biases.This paper has proposed algorithms for inferring reward functions from demonstrations with unknown biases. The experimental results are not as convincing as I would have liked. It begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias.	The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.  To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors provide a long-time data created by using honeypots to detect anomalous traffic. The characteristics of the data have been studied by using TSNE and optimal transport. This is quite a useful datasets especially because data is collected over a long time period.This paper proposes a distribution shift benchmark for unsupervised anomaly detection. The dataset is built over a network traffic data Kyoto-2006+ and the test data is split into three distributions IID, NEAR, and FAR chronologically. The proposed problem is practical and meaningful.The authors curated an anomaly detection dataset from a popular network intrusion detection dataset named Kyoto-2006+ and the proposed dataset focus on the distribution shift of the data over time. Extensive analysis of the dataset demonstrates its non-stationary property from multiple perspectives. Some preliminary experiments also show the performance degradation of classic anomaly detection algorithms due to the data distribution shift.The Kyoto-2006+ dataset covers a span of 10 years with naturally occuring distribution shift. The authors also empirically validated the performance degradation of models due to distribution shift using proposed split strategies. The dataset and code are available and pro-processed version of the data is also easily accessible.This paper proposes a chronology-based benchmark to enable a better estimate of the model’s performance in an environment close to the real world. This benchmark splits the test data based on its temporal distance to the training set and introduces three testing splits IID, NEAR, and FAR.	Four out of five reviewers recommend acceptance of the paper with a score of 7. These reviewers praise the quality and importance of the dataset. Moreover, all reviewers acknowledge the quality of the writing.  The authors have constructively engaged into the discussion with the reviewers, and have put a lot of effort into drastically improving their submission based on all reviewers' comments. The weaknesses raised by the reviewers have been successfully addressed.  In particular, the reviewer (dm8f) who gave a score of 4 raised the following concerns:  1. There exist other time series anomaly detection datasets. 2. More anomaly detection methods should be considered in the experiments.  3. The finetune mode and distillation mode are not detailed, their pseudocodes should be provided.  The authors have addressed these concerns as follows:  1. In their answer, the authors went through an extensive list of existing benchmarks and justified why they are not appropriate to solve their focused problem, and how their dataset differ from those. The main added value of the proposed dataset is its timespan and the fact that its occurring distribution shifts are natural (as opposed to synthesized). 2. The authors have expanded their experiments with more anomaly detection models. The new results corroborate the previous conclusions. 3. The authors have added the pseudocode algorithm in Appendix and have added code to replicate their experiments in the git repository.  I believe that the authors have sufficiently addressed the reviewer's concerns (and more).  For these reasons, I recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. In the context of this paper, the two domains are the clean and adversarially perturbed images. To achieve domain invariance, the authors propose a domain classifier that discriminatesDANN is leveraged to generate domain invariant and robust feature representation. Authors claim that the proposed method outperforms other methods when the target domain is the adversarial examples. The experiment section is not comprehensive. More recent SOTA methods are missing.This paper proposes DIAL to learn domain-invariant representations for clean and adversarial examples to improve model robustness and clean accuracy. The main idea is to treat the problem as a domain adaptation problem by considering the data shift between adversarial and clean distributions.This paper proposes a simple and effective adversarial learning method DIAL, which brings the idea from domain adaptation for robust representation. The model is equipped with a domain classifier that constrains the model not to discriminate between natural examples and adversarial examples.	The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. Although the experimental results are solid and technically sound, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as "unsupervised domain adaptation by backpropagation". On the other hand, more recent SOTA methods are missing and only smaller scale datasets are used for evaluation. During the discussions, the major concerns from three reviewers are novelty.   I totally agree that the simplicity of the method should be a virtue. However, the idea of domain-invariant representation learning is already established well, and its application to adversarial training is quite intuitive to the community. Also, the similar methodology already exists in domain adaptation. According to the top-tier conference culture in the ML community, what most valuable is the novelty and insight, not the performance. In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes, WPipe, a technique for reducing the memory overheads of weight versioning and improving freshness of weight updates in pipeline parallelism. The paper has following strengths. It tackles an important problem of how to fit rapidly growing model sizes into slower growing device memory.WPipe is a novel pipeline parallelism technique for training large DNN models. Method aims to improve upon existing pipeline methods such as Pipedream-2BW. Key idea is to divide the model partitions into two parts groups and make the forward pass of the second part.This paper introduces a novel pipeline training strategy WPipe. WPipe divides model partitions into two groups and updates each group alternatively, which eliminates half of the delayed gradients and memory redundancy. The experimental results show that WPipe can achieve higher throughputs and reduce memory footprint with similar final model accuracy.The paper addresses the problem of very large scale deep neural network training through model parallelism. A new model parallel pipeline called WPipe is proposed, that builds on previously existing schemes. Experimental evaluation shows that WPipe can achieve significant acceleration (1.4x) and lower memory footprint (36% smaller) when compared to state-of-the-art PipeDream-2BW.	The paper proposes a new pipeline-parallel training method called WPipe. WPipe works (on a very high level) by replacing the two-buffer structure of PipeDream-2BW with a two-partition-group structure, allowing resources to be shared in a similar way to PipeDream-2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream-2BW is impressive.  In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow. And while these were valid concerns, and the authors should take note of them when revising their paper, I do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. As a result, I plan to follow the majority reviewer opinion and recommend acceptance here.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces four EEG signal transformations to model the real-world variability observable during deployment. Then, the paper proposes a multi-pronged approach to evaluate the robustness of healthcare ML models. It is well-organized and easy to follow.The authors motivate the need for better evaluation methods for medical deep learning models. They propose using latent space integrity methods and predictive uncertainty as two ways of evaluating the behaviour of a model. The paper is quite clear, but could do with some additional figures showing correlations between in-the-wild performance and the proposed measures.This work aims to provide a holistic analysis of the robustness of EEG data representations after domain-guided data shifts. The overall idea is well-motivated and addresses a unique and important challenge in ML for health. The highlights in Table 2 is misleading - when several numbers are equal, they should all be highlighted.The paper provides a description how four types of feasible and realistic dataset shifts can be used to estimate their effect on robustness of the classification and regression tasks in EEG analysis. The paper is a excellent step forward to address the data shifts in the medical domain.	The paper introduces model-agnostic ways of quantifying predictive uncertainty and latent space differences in the situation when the distribution of data encountered during deployment differs from what the system was trained on and there is no access to the data itself. The model is evaluated on large scale EEG data.  The paper solves an important problem, in particular to the field of healthcare which has previously seen instances of models underperforming significantly at the time of deployment. As mentioned by the reviewers, this direction has not been sufficiently explored, so there is novelty in the problem itself, as well as in the solution. The experiments on EEG data were seen as convincing by the reviewers, though some questions were raised about the scope of the paper.  Overall, there is considerable merit in the work and recommend acceptance of this paper. In the camera ready, the authors should make sure not to overstate the applicability of their method. While this work could, in theory, be applied (or adapted) to other data, the merits of it outside of models trained on EEG data have not been demonstrated and should therefore not be stated as a given.  Reviewers x119 and PL6S have not engaged in the discussion although the authors responded to the issues they raised, which I kept in mind when issuing my recommendation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors consider Byzantine-robustness of a distributed learning system in the setting of non-iid data distribution. The bucketing technique is applied to reduce the heterogeneity across the workers. Overall, this paper has publication merits, but some issues need to be addressed.This paper focuses on Byzantine-robust federated learning, a learning paradigm where a centralized server coordinates learning across a data set partitioned across multiple worker nodes. Typically this is done via robust aggregation schemes which ensure that adversarial nodes do not hinder learning. In this setting, the main focus is on studying the setting where honest worker nodes own heterogeneous data sets.This paper proposes a bucketing scheme for robust distributed learning with heterogeneous data. Gradients are more homogeneous after bucketing, which will increase the robustness of existing algorithms. There are weaknesses in this work, considering the challenges from heterogeneity, I think this work is slightly above the threshold.The paper provides a systematic and deep theoretical study of the problem of Byzantine-robustness in the heterogeneous setup. The paper is well-motivated, clearly written, and contains solid contributions. There are some minor inaccuracies in the proofs, small typos, and other minor weaknesses.	This manuscript proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The manuscript shows how existing Byzantine-robust methods suffer vulnerabilities when the devices are non-iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine-robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information-theoretic lower bound for certain settings.  During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and have satisfied the reviewers. After discussion, reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are reminded to make the final changes agreed in the public discussion e.g., discussion of the reduction to SGD when  $\delta=0$
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Paper presents empirical analysis Multi-headed Self-Attention (MSA) as part of Vision Transformer (ViT) and its variants. Based on these observations, paper proposes a new architecture -- AlterNet, where CNN block is combined with MSA block. AlterNet is shown to outperform pure CNNs in both large and small data regimes.The paper shows experimentally that contrary to popular belief ViTs do not directly benefit from their more expressive underlying representation, but instead smooth the loss landscape which aids training. The paper then presents a hybrid CNN-ViT architecture that combines some of the benefits of ViTs with advantages of CNNs.The authors show that the robustness and better performance of ViTs is attributed to their property of flattening the loss surface. The paper interprets deep models as a series of independent blocks, and then proposes a new architecture, in which the last conversation block is replaced with multi-head self-attention.This paper attempts to explain the behaviour of multi-headed self-attention in vision transformers using a series of empirical observation. Some main observation are that the poor performance of ViT in the small data regime is not due to overfitting but to issues induced into the optimization procedure. Based on the observations a new model is proposed that combines convolution and MSAs.	The paper presents an empirical analysis of Vision Transformers - and in particular multi-headed self-attention - and ConvNets, with a focus on optimization-related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance.  Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses.  Overall, it's a good paper on an important topic and I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The experimental results in Figure 1 show the sample efficiency and show an improvement over the relevant baselines, using 5 trials. I see the value being optimized in Eq 14 as nearly identical to the one optimized in MAAC (S4.1 there), but some of the baseline results (without the gradient loss) in Figure 4 are better than the MAAC baseline.This paper seeks to improve performance of model-based policy optimization methods. It starts by showing theoretically that error in estimating the model's gradient contributes to bias in the learned policy. The paper then proposes directly optimizing the jacobian of the world model using a sample-based strategy. I am recommending to accept this paper because its algorithm construction is well-reasoned.The authors present a new approach for model-based reinforcement learning by training two models and using them separately. One model prioritizes accurate transitions and another has a weighting term for value- gradient, which they combine in a new algorithm directional derivative projection policy optimization.The paper proposes a method for model learning which explicitly tries to estimate the Jacobians of the underlying dynamics. The theoretical results and experimental protocols seem sound, although the core change proposed by the method does not provide very large improvements over baselines.	The paper considers model-based RL, and focuses on approaches that benefit from the differentiability of the model in order to compute the policy gradient. It theoretically shows that the error in the gradient of the model w.r.t. its input appears in an upper bound of the error in the policy gradient computing using the learned model. Motivated by this, it suggests a MBRL approach that learns two models, one of them minimizes the next-state prediction error (as commonly done) and the other minimizes a combination of prediction error and the gradient error.  The paper empirically studies the method through extensive experiments.  Reviewers are generally positive about this work. They believe that the paper is insightful and the method is original. At first, there were some important concerns raised by the reviewers, but the authors revised their paper in the discussion period, and it appears that the reviewers are all satisfied now. I also read the paper during the rebuttal phase, and I should say that I have some concerns myself, especially on the theory part of the paper. Given that the authors did not have an opportunity to answer my questions, I do not put much weight on my concerns (and I believe most of them can be addressed with some clarifications). Considering the positive response of reviewers and promising results, I am going to recommend **acceptance** of this paper.  I strongly encourage the authors to consider the comments by reviewers, as well as the following ones, in the revision of their paper.   **Comments**  1) The true dynamics $f$ is defined as a stochastic one, i.e., $s_{t+1} = f(s_t, a_t, \epsilon_t)$ (just before Eq. 1), and similarly for the learned model. Here $\epsilon_t$ is the noise causing the stochasticity of the model. But later, when the errors on the model and its gradient are introduced (i.e., $\epsilon_f$ and $\epsilon_f^g$), the role of stochasticity becomes unclear. For example, we have $\|| \tilde{f}(s,a) - f(s,a) \||  \leq \epsilon_f$.  What happened to the noise term?  The same is true for Eq. (5). The next-state s' (either according to the true dynamics or the learned model) is random. In that case, it is not obvious how to interpret Eq. (5). Is it the error of the expected gradient of the next state? Or is it something else?  In case the dynamics is assumed to be deterministic, this should be clarified early in the paper.  2) The upper bound in Theorem 1 might be vacuous if the Lipschitz constant $L_f$ of the model is larger than 1. To see this, consider Lemma 1. The constant $C_0$ is $\min [D/\epsilon_f, (1-L_f^{t+1})/(1 - L_f)]$. If $L_f$ is larger than 1, for large enough t, the term $(1-L_f^{t+1})/(1 - L_f)$ blows up and $C_0$ becomes $D/\epsilon_f$. Therefore, the upper bound of Lemma 1 becomes $D$. Here $D$ is the diameter of the state space, which is assumed to be bounded.  This carries to in the next lemmas. In Lemma 4, $C_5$ would be of the same order as $C_0$ (multiplied by an extra $L_1 L_f / (1 - \gamma) )$, so the upper bound of this lemma becomes proportional to D too.  The $C_0$'s appearance continues in the proof of Theorem 1, in which $C_8$ is proportional to $C_0$ and $C_5$. So, $C_8$ is also become proportional to $D/\epsilon_f$. When we have $C_8 \epsilon_f$ in Eq. (34), we get a constant term $D$. A similar dependence appears in the proof of Theorem 2, where B_3 is proportional to $C_8 \epsilon_f$, which can be as large as $D$. And in Eq. (47), we have $B_3^2$. So the upper bound in Eq. (47), which seems to the be upper bound of Theorem 2, is proportional to $D^2$. This means that if $L_f$ is larger than one, the upper bound does not go to zero, no matter how small the model error $\epsilon_f$ is (unless it is actually zero). This makes the bound meaningless.  This might be unavoidable. I am not sure about it at the moment. But it definitely requires a discussion.  3) Assumption 2 has a term in the form of $E[\frac{s_{t_2}}{ s_{t_1}} ]$ (I have simplified the form). The states $s_{t_2}$ and $s_{t_1}$ are vectors in general. How is the division defined here?  4) Please improve the clarify of the proofs. For example, in Lemma 2 it seems that a negative sign is missing in Eq. (49). Also how do we get Eq. (50) and Eq. (52)? (I couldn't easily verify them).  5) I believe the "periodicity property" used in Assumption 1 should be "ergodicity property".  6) The paper still has a lot of typos, e.g., "To optimize the objective, One can ..." (P3), "argument data" (instead of augmented) (p4), "Superpose" (p5), "funcrion" (p6).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is written clearly with no substantial formatting errors. The dataset involves instance-based annotations which would be valuable for the community. The authors commit themselves to open-source the dataset and codes for reproducibility.	The reviewer praises the creation of this potentially very valuable dataset and while there are some shortcomings in the presented benchmark/baseline results, I believe these will be quickly addressed by the community if the data becomes more widely used. MIDL aims to give some space for the presentation of open source data and I therefore also recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.HEDjitter behaves differently from the other techniques. DC-GAN (Radford et al., 2015) cannot be considered state-of-the-art anymore. Certain aspects of the method are not described in sufficient detail.The paper tackles a very relevant problem in histopathology image analysis. The proposed methodology (combination of classical data augmentation and GANs) is very straightforward and logical. The paper is not very clearly written and is not easy to follow.The paper is correctly motivated, indeed getting vast quantities of annotated medical data is a non trivial task. The application the authors attempt to analyse is both an important one and a yet unsolved one. The presentation of the protocols the authors propose could have been clearer and better explained. The discussion relating to the results is somewhat superficial.	The reviewers are in agreement that the paper should be rejected. Furthermore, the authors have declined to rebut the arguments of the reviewers. As such, I agree with the suggested rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work investigates how pre-training distributions induce robustness in image-text models. They use Contrastive Language-Image Pre-training (CLIP) as a case study and evaluate it with six publicly available datasets. The authors call for further study into dataset design.This paper investigates the effects of training data distributions for vision-language pretraining. The empirical results suggest that no single training data provides consistent out-of-distribution performance. The authors also analyze the results of ensembling model outputs trained on different datasets.This paper studies the role of training data on the robustness of large image-text models like CLIP and ALIGN. I think this paper, in its current form, already matches the quality of a typical publication at NeurIPS conference.	This paper studies the effect of pre-training data on the robustness of pre-trained vision-language models such as CLIP. Both empirical and theoretical results are provided to show that simple scaling may not always improving robustness. This is a timely results that shed light on how to perform better pre-training to improve robustness to distribution shifts. All reviewers agree that the work is technically solid and the contribution is significant.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provide nice discussions of a simple modification of ERM paradigm. Mainly, it consists in applying an exponential smoothing to the loss function. The success of Term heavily rely on a *magical* tuning of the parameter $t$ depending on the application.This paper considers a unified framework named TERM for addressing a bunch of problems arising in the simple averaged empirical minimization. By leveraging the key hyper-parameter t in the TERM loss, it can recover the original average loss and approximate robust loss.This work analyzes the LogSumExp aggregated loss. It provides several general properties of the loss, such as its relation to min/avg/max-loss. Empirically, it is shown that TERM can be applied to a diverse set of problems. The current manuscript does have some novel interpretations.The paper presents a modification of the empirical risk minimization (ERM) framework in order to obtain more robust or fairer results, using tilted objective. Strong points: the article is entirely reproducible as the author furnishes a well documented code. Weak points: Use of 80% outliers in Table 1 without further explications.	Dear Authors,  Thank you very much for your detailed feedback to the initial reviews and also for further answering additional questions raised by a reviewer. Your effort has been certainly contributed to clarifying some of the concerns raised by the reviewers and improving their understanding of this paper.  Overall, all the reviewers found a merit in this paper and thus I suggest its acceptance. However, as Reviewer #2 suggested, investigating the convergence in the stochastic case is very important. More discussion on this would be a valuable addition to the paper, which the authors can incorporate in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a new algorithm and analysis for multi-fidelity best-arm identification. It builds on the ideas of previous algorithms that do successive elimination of arms until only one is left. The paper is well written and covers the prior work well.The paper studies multi-fidelity best arm identification (BAI) On each round, a decision-maker should choose which arm to pull and which fidelity to pull the arm at. The lower bound given in the paper is quantifiably better than the lower bound for regularBAI.This work studies the multi-fidelity variant of the best-arm-identification problem. It first proposes a new performance measure called cost complexity. Then it gives a lower bound for the cost complexity of this problem.The authors tackle establishing the best arm in a multi-arm bandit problem. Lower fidelity evaluations are cheaper to evaluate, but provide a more biased estimate of the objective function. They present and analyse a new algorithm, IISE, that theoretically outperforms basic bandit algorithms that do no exploit the multi fidelity structure of the problem.	This paper considers the multi-fidelity variant of the best-arm identification problem. I recommend its acceptance and I strongly encourage the authors to take the several fantastic points raised by the reviewers while crafting their next draft. For instance, please include a discussion about (and clarification of) Assumption 1 that reflects the discussion with the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method named zero-shot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. This problem is tackled by first decomposing the seen attributes into basic attributes, and then recombining these basic attributes into new ones.The paper proposed to train the model by decomposing overlapping concepts from labeled annotations and then combining them to create new concepts. After training, the model can be used to provide new annotations which can be further used for supervised training.This paper develops methods to automatically annotate novel attributes for a dataset. The proposed method can detect the unseen attributes via a decompose-and-reassemble manner. Results are demonstrated using the CUB dataset alone.The paper proposes an approach for estimating new detectors of visual attribute conjunctions on images. The new attribute synthesis can be used to solve Zero-Shot Classification (ZSC) problems. The approach is compared to 4 other ZSC algorithms on the CUB dataset.	This paper has proposed a method named zero-shot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. The reviewers have many questions in the intial round. After the rebuttal, the authours clarify most unclear points, and some reviewers raise the score. In general, all the reviewers agree with the acceptance of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposed a very interesting training algorithm for auto-regressive models. It does not require any MLE pre-training and can directly optimize from the sampling. The idea should also apply on many “incremental rewards”, for instance, BLEU scores in machine translation.The authors propose an alternative approach to training seq2seq models, which addresses concerns about exposure bias and about the typical MLE objective being different from the final evaluation metric. The authors conduct a number of experiments, and show that this scheme allows them to attain state-of-the-art performance on end-to-end speech recognition.The experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art. I would, however, prefer more clarity in the presentation of the approach. It might not be straightforward for a reader to figure out how the tilde-sequences are obtained.	This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state-of-the-art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is written, provides good description of the state-of-the-art and comprehensive experimental results. The paper is for the expert audience mostly and is difficult to grasp without a good background on deep learning for speech enhancement.This paper tackles one of important speech enhancement issues of how to predict phase information. The authors work on this problem based on three novel techniques, one is to use complex U-net, second is to propose a new complex mask representation, and the last is an objective function motivated by SDR. The paper is well written, and also shows the experimental effectiveness of the proposed method. My major concern is that this paper is a little bit too specific to the speech enhancement applications.This paper used a complex-valued network to learn the modified complex ratio mask with a weighted SDR loss for the speech enhancement task. The total idea is good, but the novelty is not much. A human listening test using MOS or preference score should be conducted.	The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. This is done by adapting UNets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric.  Strengths - Modifies existing techniques well to better suit the domain for which the algorithm is being proposed. Modifications like extending UNet to complex Unet to deal with phase, redefining the mask and loss are all interesting improvements. - Extensive results and analysis.  Weaknesses - The work is centered around speech enhancement, and hence has limited focus.   Even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The paper is well written with interesting results and analysis. Therefore, it is recommended that the paper be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a multi-document extractive machine reading model and algorithm. The model is composed of 3 distinct parts. First, the document retriever and the document reader. Then, the paper proposes to use a "multi-step-reasoner" which learns to reformulate the question into its latent space.The authors improve a retriever-reader architecture for open-domain QA by iteratively retrieving passages and tuning the retriever with reinforcement learning. They first learn vector representations of both the question and context, and then iteratively change the vector representation of the question to improve results.This paper introduces a new framework to interactively interact document retriever and reader for open- domain question answering. The idea of multi-step & bi-directional interaction between the retriever & reader is novel enough. The paper contains enough literature studies on existing retriever-reader framework in open-domain setting.	pros: - novel idea for multi-step QA which rewrites the query in embedding space - good comparison with related work - reasonable evaluation and improved results  cons:  There were concerns about missing training details, insufficient evaluation, and presentation.  These have been largely addressed in revision and I am recommending acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The current paper considers the relation between convolutional neural networks and Gaussian processes. The theoretical justification provided is basically heuristic argument and, speaking rigorously, is not a theorem. The practical applicability is very limited as the kernel obtained has very high computational complexity.This paper extends an argument for the GP behaviour of deep, infinitely-wide fully-connected networks to convolutional and residual deep neural networks with infinitely many channels. It provides a computationally tractable approach to compute the corresponding GP kernel. This kernel has few hyper-parameters, and achieves state-of-the-art results on the MNIST dataset.This paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) The paper uses similar ideas to those proposed by Matthews et. al (2018a) and Lee et al (2017) The method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and no theoretical analysis is done.	This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.  The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: "the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own" and R2: "I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks").  All the reviewers found the contribution of the covariance function to be novel and exciting.  Some cited weaknesses of the paper were that the authors didn't analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.  In the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.  Weighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.  The authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.DaSGD is an algorithm for large-scale large-batch training of deep neural networks. The algorithm combines Local SGD with delayed averaging steps to hide the communication overhead. However, workers still synchronize their forward/backward passes in each iteration.In this paper, authors consider the Distributed SGD with delayed gradients and bounded delays. This assumption on delays allows algorithm to converge with the same rate as Local SGD algorithm. Authors decided to decrease the font size of all important equations and plots to fit the conference size requirements.In this paper, the authors present DaSGD, which overlaps the computation and communication of distributed training. The paper is well-written and easy to understand. However, I have some concerns in the novelty.The paper proposes to combine local sgd with overlapped communication. I did not find the improvement regarding the wall clock time by hiding the communication in the experiment. I believe the proposed method is not novel and [2] has proposed a very similar method.	This paper proposes a variant of stochastic gradient descent that parallelizes the algorithm for distributed training via delayed gradient averaging. While the algorithm (DaSGD) proposed is sensible and seems to work, it also seems to miss a lot of related work. As pointed out by one of the reviewers, the class of asynchronous decentralized methods already seem to cover the space of DaSGD, and it's not clear how DaSGD differs from the existing methods in this space. As a result of this lack of comparison to related work, the reviewers recommended that the paper not be accepted at this time, and this evaluation was not challenged by an author response. I agree with this consensus.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies decentralized optimization algorithm over general communication weight matrix. The weight matrix has to be fixed over time, so it does not seem to apply to decentralized algorithms where the communication pattern is different in different steps. When the matrix is fixed, the paper proves the optimal convergence rate for nonconvex objectives.In this paper, the authors investigate the optimal convergence rate for smooth and nonconvex decentralized stochastic optimization. Strengths: A tight lower bound for any \beta within (0,cos(\pi/n) An algorithm that matches the lower bound. Weaknesses: The degree of a 10-node complete graph is 9, not 10.The paper considers decentralized stochastic non-convex optimization problems. The main innovation is the new structure of the connection graph, which can be used to revisiting the lower bounds. The article is quite well written and easy to follow.The paper advances the state of the art on SDGD-like algorithms for smooth, nonconvex problems. The overall presentation is good and the paper is easy to read. I do not find the proof under PL particularly more difficult than under strong convexity.	This paper presents some new results on near-optimum algorithms for distributed optimization, nearly matching lower bounds. Most of the reviewers are positive about the contributions of this work. However, one issue that came up is the assumption of bounded gradient dissimilarity, which is essentially a gap between upper and lower bounds. While I am recommending to accept this paper, I believe this gap should be more prominently discussed in the abstract and introduction.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tackles the problem of controlled text generation by converting it into a conditional text generation similar to (Keskar et al.19). It proposes an architectural modification to the transformer LM used in GPT2. A CoCon layer is added as a separate transformer block in the middle allowing self-attention to be performed.The paper proposes a self-supervised technique for controlling the productions of a Transformer-based pretrained generator. It consists in augmenting the architecture of the pretrained model with a special "content-conditioner" (CoCon) block which is able to exploit a contextual condition.The paper proposed a way to control the content output of a DNN-based language model. It places an layer (CoCon) that can take an arbitrary phrase as the hint after generating the embedding but before generating the text. Experiments showed that the control is effective at directing the generated text.The paper proposes a method for controlled text generation with pretrained (unconditional) language models. The novelty of the CoCon method lies in its ability to condition on entire input sequences. The paper is well written, the method is intuitive and all components are well motivated.	The paper aims at controllable generation by introducing an additional "content-conditioner" block in the Transformer models. The paper further provides 4 different variants of a pre-training task to train the content-conditioner model.   While the proposed approach seems an incremental contribution over CTRL and PPLM, certain reviews praised the approach being novel while keeping the architecture changes minimal. Overall, reviews indicate that the overall proposed method of fine-grained controlled generation with self-supervision is valuable, and empirical results support its effectiveness.   All reviewers initially raised concerns regarding clarity and lack of human evaluation. However, clarity issues seem to be resolved through author/reviewer discussions and the updated revision.  R3 had important concerns regarding topic and sentiment relevance evaluations.   While the reviewer remains unconvinced after discussions with authors, after carefully reading the revised paper and discussions, I feel that the authors tried to address this point fairly  through their additional experiments and also edited their contribution statement accordingly.  Overall, at least two reviewers sounded very excited about this work and other than R3's concerns, the general sentiment about this work was positive. Therefore, I recommend weak accept.    There are still some writing issues that I strongly encourage authors to carefully address in the future versions. Quoting from reviewer discussions:  > Differentiability of the adversarial loss. Authors just added one statement saying " Through continuous approximation.." without any more details are given, which continuous approx was used (Gumbel softmax?) and how they overcame the problem of its training instability.   > Table 6, can be misleading, authors bold the results when cocon+ is performing better than baselines (mostly in content similarity) but not the other way around topic/sentiment accuracy. The latter is arguably more important.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper considers the problem of continual learning on semantic segmentation. Experiments have been conducted on the PASCAL VOC dataset. The proposed solutions, although simple, does not give too much insight, more like some tricks.The authors firstly reveal the overcompression of features causes model degradation when training the backbone of the continual segmentation model. Then they retrain the backbone combined with the extra Dropout layer and train the classifier with a greater number of features than SSUL. The experimental results show the effectiveness of their proposed methods.This paper proposes to tackle continual semantic segmentation by addressing the overcompression issue in model learning. The authors draw inspiration from previous studies in the field and introduce wider convolution for final feature extraction as well as apply dropout to the output of the feature encoder.The authors focus on an important topic of overcompression, which has a negative impact on continual learning model training. They show an increase in performance in specific setups using two proposed improvements. The work is not self-contained. Limitations of the proposed improvements were not described.	This paper deals with continual learning in semantic segmentation.  Authors introduce wider convolution at final feature extraction layer and apply dropout to limit the overcompression issue.   No reviewer was convinced by the approach and they have raised many issues, including model design choice, training protocol and missing experiments.   No rebuttal has been provided by the authors.   As it is, this submission is not ready for publication, and we encourage the authors to consider the reviewers feedbacks for future publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed idea is based on the observation that skilled programmers write and read source codes in a hierarchical way. The paper proposes constructing a pyramid-shaped input representation based on a pyramid attention mechanism for efficient feature aggregation and distribution across different hierarchies.The paper introduces pyramid attention for source code summarization andintegrates it into a transformer architecture. The pyramid attention combines three levels of looking at the source code (statement, token, and subtoken level) The levels are combined bottom up from subtoken to statement level, and also with a cross-granularity interaction.This paper proposes a multi-granularity method for the code summarization task. One code snippet is first hierarchically broken into three granularities: statements, tokens, and sub-tokens levels. Then a new mechanism named pyramid attention mechanism is proposed for the aggregation of multi- granularity features.The paper presents the method to tackle the source code summarization task. The proposed method is elegant, does not overcomplicate the process, and seems easily reproducible. Authors do not provide limitation section nor analyze the results qualitatively.	The paper presents a multi-granularity input represenaion and a pyramid attention mechanism for code summarizaiton tasks. After extensive discussion, the reviewers still cannot agree on accepting or rejecting this paper. The key discussion points and my opinion are summarized in below.    1. Performance improvement -- a few reviewers point out the performance improvement is relatively small (about 1%) compared with baseline. With the additional error bar provided by the authors, it seems to me the improvement is statistically significant. The authors also provide sufficient ablation study to justify the improvement. Although it's arguable if the proposed appracoh is substantial, the progress of AI is often driven by incremental improvement in terms of performance. Therefore, I'm less concerned by this issue.   2. Comparison only on 1 language and 2 datasets. I partially agree with the authors and reviwers Xtyo that the paper already conducted extensive experiments and the merits of the proposed approach are justified. However, I disagree with the attritue that the comparison on 1 language is sufficient given the recent progress of code summariziton. As the proposed approach is mainly justified by empirical comparison, conducting results on a limited dataset raises the concern whether the proposed approach is generalizble to other languages and datasets. It also makes future work harder to compare with this work. I especially disagree the point that some earlier papers only compared on limited datasets. These papers are published earlier than the benchmark CodeXGlue has been released. As most recent baselines are compared on CodeXGlue, there is a need to justify the missing of results on this dataset. Besides, the argument of the dataet is noisy the performance is low do not seem rigorous and reasonable to me.  3. Human evaluation. The authors provide a preliminary study of human evaluation on the generated outputs. On one hand, it shows the proposed approach indeed improve the quality of the summary, but on the other hand, the study requires more rigorous design. I would suggest including the human evaluation on the main text rather than in appendix.   4. Presentation. The paper is mostly well-written and provide nice intuition behind the propose method. However, I also agree with Q2Zz that some statements might overclaim and require justification. The later part of the paper has significant number of typos and require a careful proofread. It's pity that the authors do not take the opportunity during the rebuttal period to revise the paper.  Overall, I think the paper has sufficient merits but still have room to improve.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a method that incorporates multi-way relationships between action dimensions in the estimate of action-value functions. This is an interesting approach, but there are presentation and experimental issues preventing me from endorsing its publication.This paper incorporates a concept called hypergraph network into reinforcement learning. The idea of hypergraph is to extend edge to hyperedge where a set of vertices can be considered at the same time. This seems natural for scenarios like continuous action control.This work focuses on learning action representations for problems involving high-dimensional action spaces. The aim is to build a flexible and general methodology for learning representations of multidimensional actions that can be combined with existing architectures. The paper is clearly written and well-organized.This work considers the idea of adding a representation for action space. This is on top of the usual representation of state-space. Their approach is based on hypergraph representation. But in general, I find the technical contributions (borderline) incremental.The paper considers the problem of representation learning of actions, i.e., learning a decomposition of action-value function in multidimensional action spaces using hypergraphs. Empirical results on a few atari games (29) and simulated physical control benchmarks clearly demonstrate the superiority of the approach.	After reading the reviews and rebuttal and looking over the paper, I feel that the results are indeed strong, and the paper could have an impact in terms of exploiting the relationship among action dimensions. Maybe the only detail that I would add is that going through the example given in Fig 1 completely could be useful as it might not be perfectly obvious how (e.g. considering a simple mixing function like summation) one retrieves the q values for someone not familiar with this particular topic.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is clearly organized and the logic flow is good. Discovering the low-dimensional feature representation in retinal neuronal firing (the time feature) with the combination of a ML based method is interesting. Yes, the authors addressed the limitations.The authors modify a U-net with a ResNet encoder and variational sampling layers for skip connections to identify salamander retinal population response space-time features. The finding that static features and dynamic features are encoded synergistically in the retina is novel. Given that this finding helps to elucidate how the retina processes visual stimuli, this is a significant contribution.The authors use a deep-learning pipeline to understand the nature of encoding in the salamander retina. They collect PSTHs from neurons in response to short natural movies, then the PSTHS are converted to images and used as the input to a U-Net. Curiously, the latent representation can be used to accurately decode time in the movie, even for completely novel movies. This suggests that the model learns a low-dimensional representation of elapsed time.The authors try to apply a U-net architecture to predict future movie frames from salamander retinal cell responses to the previous frames. The main finding is a signal for the time since the start of the movie in the latent representation. The plan to train a large DNN architecture on the reconstruction of a few seconds of movie is doomed from the start.	The authors analyze the latent representation of visual features from natural movies in the salamander retina using a U-net. They train an encoder to learn a compressed latent representation from a large population of salamander retinal ganglion cells responding to natural movies, while a decoder samples from this compressed latent space to generate the appropriate movie frame. They characterize its representation of “time in the natural scene” in the latent space of their model.  Overall, the reviewers expressed a lot of interest in the topic and valued the novel application to salamander retinal data. There were some questions about the significance of the finding, and through the rebuttal period, the authors provided a number of experiments to compare against other variants of their baselines (and ablations), with some reviewers increasing their scores in favor of acceptance.   At the same time, there was concern that the U-Net architecture could potentially reconstruct the movie without any retinal data. Thus, it was not entirely clear whether the model was truly leveraging the retinal data to obtain meaningful outputs. Unfortunately, this concern was not fully addressed in the revision, leaving the reviewers overall with mixed views but the majority in favor of acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a model, called PoET, for single-image pose estimation. They feed the image to a backbone object detector (Scaled-YOLO-v4), which produces multi-scale features and bounding boxes. An attention-decoder model subsequently processes each detected object, using an architecture based on Deformable DETR. The model then predicts rotation and translation, using geodesic loss and L2 distance. They evaluate the model on the YCBThis paper proposes a system for object pose estimation from RGB images. The only input to the system is the RGB image, it does not use depth maps or 3D models. The system first uses an object detector to generate feature maps and bounding boxes. The output of the transformer is then passed to a translation head and a rotation head.This paper tackles the problem of 6D pose estimation object instances. It is a transformer based architecture that takes in the features at different scales of the object detector as the input along side the positionally encoded information for each detection. The method is evaluated on YCB-V dataset.The paper presents a framework for single-view, multi-object 6-D pose estimation. The approach is novel and achieves state-of-the-art results on the YCB-V and LM-O datasets for RGB-only methods.	Dear Authors,  Thank you for submitting your manuscript to CoRL. I'm happy to inform you that your paper is acceptable for publication. We have completed the review of your manuscript and a summary is appended below.  The reviewers have advised accepting your manuscript as a poster after improvement of the quality of the manuscript presentation based on the comments. Please note it is crucial to incorporate all provided explanations from authors to convince the reviewers and recommended editing into the final manuscript.  Regards,
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This model is fundamental and useful. I believe there should be many applications. This work already experiments on many. I cannot find any clear weakness in this work. The organization of this paper could be improved. The experiment part can be shrunk.Lack of explanation on how this novel method can be applied in real-world scenarios. Good references to other research overall, but references missing at times. More explanation on prior methods, who derived them, what applications they have nowadays, and where they are being used in the AI world.The paper is not easy to follow, which makes it hard to pinpoint the exact contribution. The proposed approach is an interesting contribution to network Granger causal modelling. The evaluations are extensive and provide some insight into the hyperparameter h.	Meta Review: In this paper, the authors proposed a new method for forecasting and other prediction  analyses for multiple time series dynamic networks. All the reviewers consider that the proposed method is fundamental and useful.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is concerned with selective classification in a stylised 'realisably noisy' data model. It is approached by designing a new loss for learning a selector given a predictor $f$, denoted $W(g;f, \theta) The proposed method has remarkably better selective risk compared to the underlying classification literature.The paper considers a setting where the label is random noise if the input is in an uninformative subspace. It provides theoretical analysis for the estimated predictor/selector that minimizes classification risk. It proposes a practical iterative algorithm to approximate the estimators based on MWU.This paper proposes a novel formulation of supervised learning with abstention. It assumes that feature space can be split into an informative region, and a completely unpredictable region. Under this model, the authors derive an oracle-based algorithm which can recover both the informative-partition and a good model on it.The paper is well-written, clearly organized and easy to follow. The problem of identifying uninformative data is interesting and very relevant in modern machine learning. The theory of the paper---though highly stylized and is based on $0-1$ loss that cannot be optimized efficiently---is revealing and also a good starting point.	This paper studies a learning scenario in which there exist 2 classes of examples: "predictable" and "noise". Learning theory is provided for this setting and a novel algorithm is devised that identifies predictable examples and makes predictions at the same time. A more practical algorithm is devised as well. Results are supported by experiments.  Reviewers have raised a number of concerns (ranging from how realistic this settings is to missing references). Overall they found this work interesting and relevant to ML community and appreciate the effort that authors have put in in their thoughtful response. However, after a thorough deliberation conference program committee decided that the paper is not sufficiently strong in its current form to be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue.The paper claims to propose a "theory" for spread divergences (conditioning a f-divergence by a "third-party" conditional distribution on supports which makes supports match) The paper focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties (including downsides, such as variance increase)spread divergences are obtained by taking the divergence between smoothed/noisy versions of two distributions. Adding noise to distributions to ensure overlapping support is intuitive and has been used to stabilize training of GANs. The paper seems hastily written with some grammar issues, typos, and sloppy use of LaTeX.	This manuscript proposes spread divergences as a technique for extending f-divergences to distributions with different supports. This is achieved by convolving with a noise distribution. This is an important topic worth further study in the community, particularly as it related to training generative models.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work, or expressing issues about the clarity of the presentation. Further improvement of the clarity, combined with additional convincing experiments would significantly strengthen this submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors develop a Transformer model for language-agnostic code summarization. The Transformer is provided both sequential tokenized code and a parsed AST as inputs, and tasked with generating the name of the corresponding code function as output. The authors show that the inclusion of this added structural AST information improves performance on the task, and also improves the cross-language transfer learning abilities of the model.The paper proposed to add additional structural information about distances of tokens in the abstract syntax tree. This positional information gives an edge of this model on several code summarization datasets. The simplicity of the proposed model puts the work in the state-of-the-art category in machine learning for code.This paper wants to combine sequence (called Context) and AST (called Structure) representations of source code in a Transformer encoder model. Different pairwise relations based on ASTs and sequence ordering are computed and each of them is encoded as a separate distance matrix. This model is evaluated on the task of code summarization and compared against code2seq and GREAT models. The results show that Code Transformer achieves results better than these models.This paper presents a representation of source code based on the AST. By adding positional relational information, such as shortest path length, and ancestor distance the transformers learn to better represent code, without language specific features. The representations learned seem to share semantic similarities among languages.	The paper gives an extension of the transformer model that is suited to computing representations of source code. The main difference from transformers is that the model takes in a program's abstract syntax tree (AST) in addition to its sequence representation, and utilizes several pairwise distance measures between AST nodes in the self-attention operation. The model is evaluated on the task of code summarization for 5 different languages and shown to beat two state-of-the-art models. One interesting observation is that a model trained on data from all languages outperforms the monolingual version of the model.  The reviewers generally liked the paper. The technical idea is simple, but the evaluation is substantial and makes a convincing case about setting a new state of the art. The observation about multilingual models is also interesting. While there were a few concerns, many of these were addressed in the authors' responses, and the ones that remain seem minor. Given this, I am recommending acceptance as a poster. Please incorporate the reviewers' comments in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents an empirical study of language pre-training (GPT-2), image pre- training (iGPT), and training from scratch for offline RL. The study looks at the differences in activations, parameters, and gradients as well as the impact of the context length.Previous studies reported that, for offline RL tasks, pre-training a Transformer-based model on image data either maintains or helps performance. Pre-trained models also do not acquire more knowledge about the input or the label during fine-tuning.Pretrained Transformer models trained on two different modalities (GPT-2 for language, and iGPT for image) are applied to Mujoco offline RL tasks - Hopper, HalfCheetah and Walker2D. The paper examines how the representations change during finetuning.	The paper unanimously receives positive rates thanks to strong motivations and interesting results. As the reviews show satisfaction on the authors’ feedback, the final draft needs to respect it accordingly, for example, about the limitations of this research.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper establishes generalization bounds for a broad class of losses over linear predictors by utilizing Moreau envelope theory. The setting is fairly broad, as it allows for models misspecification and more general loss functions.The main issue I have with this paper is that some of the assumptions are not transparent and seem to be difficult to verify. The main results in the paper are novel due to their generality. The connections to existing literature are very interesting and show the generality of their results.Moreau envelope analysis is an approach to studying generalization via Moreau envelopes. The main result is that ERM leads to small expected Moreau loss. The results are not really novel, but the literature review is quite thorough. Some results are limited to linear multi-index models and gaussian covariates.	The manuscript proves a new (finite-sample) generalization bound for generalized linear models, using Moreau envelope theory. The paper also provides experimental validation.  While the results only hold for Gaussian data, I believe there is some interesting novel results which might inspire some future work in the learning theory community. (In comparison, several novel frameworks have been created in the past for other problems, in which the initial versions assumed Gaussianity, e.g., the work on support recovery on sparse linear regression, for instance.)  Several technical clarifications regarding the assumptions (e.g., surrogate distribution, comparison to prior results) were asked by the reviewers, which the authors thoroughly and successfully addressed during the rebuttal phase. I recommend adding this to the camera-ready version of the paper, as well as other discussions and clarifications raised by all the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an algorithm for Heteogeneously Observable Imitation Learning. It is expensive to obtain paired samples that depict corresponding imitator and demonstrator states. The proposed method is a form of GAIL that uses importance sampling to reweight the demonstrator/novice observations from data source (2)The authors propose the Importance Weighting with Rejection (IWRE) to solve the dynamics mismatch with importance sampling. They conduct experiments on Mujoco and Atari games and their method gets the best imitation performance.The authors consider the problem of imitation learning. They propose methods for overcoming the challenges of the setting. The work has a good level of originality on the theoretical side. Overall the structure of the manuscript is very good, with a clear storytelling.	In this paper, the authors tackle the problem of demonstrators and learners having different observation spaces, by proposing an importance-weighted learning algorithm to bring the support of the imitator state marginal closer to that of the expert demonstrator's state marginal.   All reviewers have voted to weak accept, but it would seem that this year's NeurIPS mechanism of reviewer assignment has resulted in a much higher acceptance rate than typical, with 80%+ of the AC batch having accept votes. As such, I am tasked with the tough job of rejecting some papers that reviewers were only lukewarmly excited about. It pains me to do this to a paper that reviewers have found methodologically correct. I will be recommending this paper be rejected based on calibration against other papers I'm AC'ing, mostly for the following reason:  The HOIL paper assumes a problem setting where:  1. expert observations with more privilege observations than learner:  2. access to expert observations being high cost and invasive  3. importance-weighting the data to close the support between learner and expert state distributions mitigates (1) and (2)  However, it's not demonstrated that (1) and (2) is an actual problem in practical applications. Is sensor mismatch between human and autonomous vehicles the actual problem for learning? Do self-driving cars even utilize a policy formulation explored in the HOIL paper? How big are these state / support mismatches in practice, and couldn't they be mitigated by simpler methods? This paper is the first I've heard suggested that sensor mismatch between humans and autonomous vehicle sensors is the bottleneck for performance. If we remove this motivation from the paper and only consider the importance weighting algorithm used for removing out of distribution examples, then it doesn't feel quite as novel as there are many works that propose some kind of distribution-projection step as a way to mitigate state marginal differences between teachers and learners (e.g. CQL).   In fact, the setting (1) is not only avoided, but actively exploited by some learning papers (Asymmetric Actor Critic, Guided Policy Search, and other ideas) to *boost* the sample efficiency of learning, with the idea that the asymmetry in state support allows experts to provide useful learning signal in a way that the learner cannot adversarially overfit easily.   From reviewer pD56,  ``` I didn't find the authors' response to be particularly convincing (the self-driving example doesn't seem like a good fit for what they're doing, and I think they're conflating differences in perceptual accuracy with differences in sensing hardware). ```
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The idea is to use the matrix exponential as a layer in a neural network. The paper is written in a nice easy to read way. The robustness result is something, but it follows from a very basic result on matrix exponentials.This work proposes a novel machine learning architecture (or M-layer) that is in the form matrix exponential. This architecture can effectively model the interaction of feature components and learn multivariate polynomial functions and periodic functions. Theoretical discussion is generally clear although some places are a bit hard to follow. Experimental study supports the claims.M-layer is a neural network "layer" that consists in affinely transforming the input to a matrix, applying its matrix exponential, and applying another affine transformation. A single M-Layer compares favorably to neural ODEs as well as a very simplistic RELU network.The idea of using matrix exponential as a non-linearity is an interesting idea that is perhaps not explored in the context of neural networks so far. The paper provides several scenarios where matrix exponential could be an interesting non- linearity to use. Experiments are provided on synthetic examples, and standard image recognition benchmarks in a limited setting and show some promise.	This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re-submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors have built a platform for assisting medical residents and their supervising doctors in keeping track of their progress toward different milestones. The system was designed over multiple iterations with feedback from the residents and supervisors and was finally deployed and monitored for a period of four months to collect in-the-wild usage data.This submission reports on the creation of a system to help medical residents and their reviewers to assess their learning using an information visualization dashboard. The motivation and context is sound, with references on how information visualization and dashboards support learning analytics or educational data visualization. The proposed methodology of design and development relies on well established practices.This paper presents a visualization system for tracking and reviewing resident physicians’ performance in a medical training program. The design of the system was through four focus groups and the evaluation was through a four-month deployment study. User behavior changes were observed after introducing the system.	The reviewers are in agreement that this is a well-motivated paper and should be accepted. As R1 mentioned, the contribution does not lie in a novel visualization but rather in the process, insights, and patterns learned during the design and evaluation process.   The reviewers also agreed that the design implications section lacked depth. This is the one area where the paper has the biggest scope of improving. The reviewers have offered suggestions for different approaches to addressing this shortcoming.   Some other comments worth highlighting: R1 has raised some concerns regarding how the 5 questions were selection and would like added details regarding the process.  R2 would like some discussion around the prior approach or set-up that this system replaced.  R3 has provided detailed feedback on minor changes which will improve the overall readability of the paper and can be accomplished prior to the camera ready submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a discrete, structured latent variable model for visual question answering. In comparison to the existing approach, this paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty.This paper proposes a variational neural module networks (V-NMN), which is formed in a probabilistic aspect. The authors compare the performance of V- NMN and NMN on SHAPES dataset. I find the technical part is hard to follow.The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.Experiments performed on the SHAPES dataset show good performance compared to neural model networks.	This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.  This paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2's feedback, but unfortunately due to the competitive nature of this year's ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.  As a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides an empirical study of binary weight networks (BWNs) They find that there exists a subnetwork that stabiles early in training. They propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs.The authors show empirically that weight signs are more important than weight magnitudes. They exploit the latter to propose a new quantization method that increases the compression of binary networks. The experiments include numerical results for a wide range of data sets and networks.This paper proposes some interesting observations for training BWNs. However, the paper seems to be incomplete and needs to be further improved. The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWN's and maybe BNNs.Binary kernels in the convolutional layers of final models tend to be centered on a limited number of fixed structural patterns. They propose a binary kernel quantization to quantize the binary kernel, which effectively reduces the number of all possible kernels.	## Description The paper discovers interesting phenomena in training neural networks with binary weights: - Connection between latent weight magnitude and how important its binarized version for the network performance -training dynamics, indicating that large latent weights are identified and stabilize early on - Observation that amongst learned binary kernel, several specific patterns prevail, up to the bits who's reversal has very little effect. This is so regardless of the architecture, the layer considered or the dataset.  The paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.  ## Review Process and Decision The reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. The redundancy of consecutive affine transforms is known and not connected to binary weights investigation. The investigation itself lacks a more in-depth analysis. The proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. The AC shares these concerns and supports rejection.  ## General Comments From my perspective, the study undertaken is methodologically „wrong“. An ad-hoc training method is investigated, which is not even clearly defined in the paper (there are many „STE“ variants) and for which it is not known what it is doing, what are the real-valued weights for and whether they are needed at all (as empirically argued by Helwegen et al. (2019)). As such, the investigation makes impression of poking a black box (the training method in this case). At the same time, there are more clear learning formulations, applicable in the setting of the paper (binary weights), in particular considering the stochastic relaxation: * Shayer et al. (2017): Learning Discrete Weights Using the Local Reparameterization Trick * Roth et al. (2019): Training Discrete-Valued Neural Networks with Sign Activations Using Weight Distributions * Peters et al. (2018): Probabilistic binary neural networks  These methods are approximate, but at least the optimization is well posed and it is known what do the real-valued weights represent (e.g. logits of binary weight probabilities). From this perspective, it can be seen that latent weights close to 0 correspond to Bernoulli weights that are almost fully random (and thus only contribute noise) and are fragile to gradient steps. Therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic (corresponding to large latent weight). So one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. Furthermore, there are recent works explaining STE and its latent weights as optimizing the stochastic relaxation: * Meng et al. (2020): Training Binary Neural Networks using the Bayesian Learning Rule * Yanush et al. (2020): Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks.   The authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.  ## Further Details  *  „We show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand-crafted or learnable methods brings marginal or no accuracy gain to final model.“  From theoretical perspective, this is obvious and known to me. Practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. The section devoted to this issue can be given in the appendix but is not justified in the main paper.  * „change of weight signs is crucial in the training of BWNs“  The sign determines the binary weights, so this is by definition.  * „ Firstly, the training of BWNs demonstrates the process of seeking primary binary sub-networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks“  In the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. It is therefore could be of a different nature.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents the memorization effect/behavior in adversarial training in perspective of model capacity, convergence and generalization. They first investigate the difference of memorization behavior between two adversarial. training methods, PGD-AT and TRADES, with random labels. Also they suggest that previous complexity measures are inadequate to explain robust generalization in AT. Lastly, they propose a mitigation algorithm by a regularization term for avoiding the excessive memorization of adversarial examples.The paper presents several interesting insights about the behavior of adversarial training algorithms. The proposed methods are shown to out-perform PGD-AT and TRADES. However, the findings in the paper are disconnected and some of them lack explanations. Memorization has been investigated in deep neural network classifiers but not that much in adversarial training. The authors investigate the ability of adversarialTraining to memorize random datasets. They use PGD-AT and TRADES to do the adversarialtraining and see different convergence properties, which differs significantly from training on true labels. They also discover that robust overfitting can occur with memorization and use temporal ensembling to mitigate it.The paper presents a thorough investigation of the dynamics of adversarial training (AT) when a network is trained on a dataset with random labels. The authors find that PGD-AT is incapable of fitting random labels when trained from scratch, while TRADES or a warmed-up version of PGd-AT can. They also suggest plausible explanations for this behavior.	This paper demonstrates that deep networks can memorize adversarial examples of training data with completely random labels, which motivates some analyses on the convergence and generalization of adversarial training (AT). The authors identify a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback. Experiments on benchmark datasets validate the effectiveness of the proposed algorithm. One of the reviewers is concerned about (1) the validity of stability analysis where 80% of the data labels are noisy, and the perturbation (64/255) is large, (2) the gap between theory and practice, and (3) novelty. The authors have made a great effort to address these concerns. Although there is still no consensus after the author's response, the majority of the reviewers are in strong support. I, therefore, recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method for learning convolutional filters with trainable size. The learnable convolutions are called FlexConv, and a network that deploys them is called FlexNet. Anisotropic Gabor kernels are used within the multiplicative filter networks. The results obtained are higher than those reported by other existing works.The authors propose FlexConv, a novel convolutional operation with which high bandwidth convolutionals of learnable kernel size can be learned at a fixed parameter cost. The authors are expected to demonstrate the superiority of FlexNet on typical and practical tasks. The reviewer will keep the score.Flexconv constructs their convolutional kernels with the product between the CKConv and Gaussian masks so that the size of the kernels can be learned during training. Also, in the network of Flexconv, MAGNets allow the Flexconv to be used at higher resolutions without aliasing.This paper presents a novel convolutional operation named FlexConv. It is able to generate kernels with large kernel size and model long-term dependencies among elements in a sequence or an image. State-of-the-art performance on both sequential and image datasets demonstrates the effectiveness of the proposed method.	This submission proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks.  Anti-aliasing is achieved by parametrization with anisotropic Gabor filters.  The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR.  The authors are encouraged to make use of the extensive reviewer discussion in improving the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method to train high- and low-level controllers to tackle hierarchical RL. By dividing state representation into proprioceptive and task-specific, the reward used to train LL and HL controllers are simplified. The paper is difficult to follow at places.The HRL method uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation. The higher level policy is trained as usual by commanding lower level policies.This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers. Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out.	Strengths   The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well. The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the separation of internal state from external state is a clean principle that can potentially be broadly employed.  The method does well in outperforming the alternative baselines.  Weaknesses  There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses  a policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as "Virtual Windup Toys for Animation"  exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help.  The separation of internal and external state is an assumption that may not hold in many cases. The results are locomotion focussed. There are only two timescales.  Decision  The reviewers are largely in agreement to accept the paper.  There are fairly-simple-but-useful lessons to be found in the paper for those working on HRL problems, particularly those for movement and locomotion.  The AC sees the novely with respect to different pieces of related work is the weakest point of the paper.   The reviews contain good suggestions for revisions and improvements;  the latest version may take care of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution to ICLR 2019.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper improves existing DRL-based CO methods in two terms. It leverage a continuous probabilistic space for the solutions, leading to a REINFORCEMENT-based training method which is more efficient than previous Q-learning or Actor-Critic methods. A MAML-based meta-learning framework is proposed.This paper studies a differential meta solver for CO problems and its performance is demonstrated by using TSP instances (size 500, 1000, and 10000) The difference between the proposed method in this paper and existing DRL-based methods is from the fact that DIMES focuses on a parameterized continuous space.This work proposes a novel DIMES framework (stands for differentiable meta solver) to tackle large-scale learning-based combinatorial optimization problems. The key novelties and contributions are an RL-based approach to train the widely-used GCN model to generate probability heatmap, and a meta-learning approach to finetune the solutions at inference.DIMES is a framework for tackling graph-based combinatorial optimisation problems with reinforcement learning. DIMES runs a graph neural network to produce a heatmap over the candidate variables (edges for TSP, nodes for MIS) This approach seeks to provide more scalable CO solvers.	This paper proposes a differentiable meta-solver applicable to large-scale combinatorial optimization. After a thorough discussion phase, all the reviewers are on the positive side of this paper. The reviewers appreciated the novelty of this paper and the importance of scaling neural combinatorial optimization for large-scale instances. Overall, I recommend acceptance for this paper.  However, the reviewers also showed concerns about the presentation of this paper. The gap between generalization and testing performance is not clearly discussed and the connection to prior works using continuous latent space should be clearly stated. Since scalability is an important issue, it would be useful to clear up time/objective comparison and unify experimental settings as suggested by Reviewer fQdp and fe3B.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper tackles the important problem of generating realistic phantoms. The presented method shows better results than the other methods (SyN and Voxelmorph) The deformation field looks highly irregular and seems to have quite a lot of foldings.	The paper contains interesting ideas and fits well within MIDL's short paper scope of also providing a live discussion space for accepted journal publications. Given that unfortunately only one reviewer was available, whose weak reject is mostly based on formatting, I believe that the paper can still be accepted. However, I strongly urge the authors to reformat their paper to be consistent with the MIDL style when preparing the camera-ready version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper propose a framework to obtain the disentanglement of syntactic roles as latent variables for sentence representations. The model is an attention-driven VAE which maps syntactic role to separate latent variables using an encoder-decoder framework.This paper proposes a probabilistic model called Attention-Driven Variational Autoencoder (ADVAE) The model is composed of Transformers rather than previous neural architectures such as RNN. The authors aim to disentangle the semantics of latent variables according to some syntactic roles.The paper proposes a method for unsupervised disentanglement of text components. They find that their proposed architecture is more successful at disentangling semantic roles into the latent variables than standard VAEs. While the research presented in this study is very exciting, it is not quite convincing enough yet for me to trust that the results are actually due to the modeling decisions presented.This paper proposes a new model ADVAE, which uses a sequence of latent variables which are constructed using cross-attention. The findings show that certain latent variables correlated with different syntactic roles. The work claims that the latent variables, in this case, are able to disentangle content effectively.	This paper proposes improving human interpretability and manipulability of neural representations by obtaining syntactic roles (here, subject, object, prepositional object, and main verb) without supervision by means of them becoming linked to latent variables in a novel proposed attention-driven VAE (ADVAE) model, which provides cross attention between a language transformer and latent variables. The paper argues that syntactic roles are quite central to meaning interpretation and that the ADVAE recovers them better than LSTM or Transformer (with mean pooling) VAEs.  This is a quite interesting direction and paper. There was active discussion with the reviewers, one of whom (9pDc) moved their rating from reject to quite strong support, while the other reviewers either sat on the fence or raised from reject to borderline. Nevertheless, I overall tend to agree that the paper is still lacking in empirical support, a view clearly shared by reviewers WuPD and 7uFL. The SNLI data is very simple descriptive sentences, nearly all in the form of S V O or S V PP. Would this work on more complex data, in other languages, or with more word order variation? There isn't very much investigation, but the new results added during reviewing based on Yelp data seem to offer more concerns than confidence. These are also very short sentences but with more varied structure and some complementation. It seems like D_{dec} is now very low (much lower than for the sequence VAE), the ability to distinguish out grammatical roles seems limited to {subj} vs. {dobj, pobj} in the encoder and none at all in the decoder (Figure 6/7). And then for the examples in Appendix D, the disentanglement abilities barely seem stronger than being able to pick out subjects, though when there are sentences with subordinate clauses, it is perhaps random which subject you get. The resampled realizations in appendix H also seem to show limited disentanglement: resampling the subject usually seems to change the object as well, often markedly. No convincing downstream applications are shown. As such, while I agree that disentanglement is at the heart of representation learning, I can't get on board with reviewer 9pDc feeling that this paper now has convincing results. Reviewer 7uFL also emphasizes that there is no strong reason that the latent variables have to align with syntactic roles. In particular, the motivation in NMT whereby constituents clump and reorder together does not exist here. It may only work for the very simple and regular sentences of SNLI.  Hence, overall, I feel that this method needs more extensive validation on harder, more varied data sets before it becomes a convincing contribution, and so I propose rejecting the paper at this point in time. Nevertheless, I do think the topic is interesting and this approach has the potential to be good.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper addresses the problem of robustness for extracting factual information from large language models. The work evaluates the performance of the variants using a pooled collection of fact-seeking prompts (e.g., LAMA, LPAQA and ParaSel) The results show that different interventions in the input embeddings cause large differences in inter-prompt consistency.P-Adapters is a model that is between the embedding layer and first attention layer of LLMs. It takes LLM embeddings as input and output prompts used to query the LLM. Experimenta results show that P-Ad adapters perform comparably to the more complex MoE models.This paper explores methods for improving the consistency of prompt-based factual probing of pre-trained language models. Methods like P-Adapters for learning a transformation of natural language prompts could be an interesting alternative to standard prompting.This paper tackles one of the most important issues in the large language model: inconsistency results obtained from different prompts. The proposed adaptor, P-adaptor is a simple but effective solution to alleviate the inconsistency.The paper proposes a way to increase the consistency of extracted information from the Large Language Models (LLMs) with respect to the different wordings of the queries that users use to prompt the models. The authors advocate for the use of natural language prompts only. That is they do not require additional relation(s) that one can incorporate into the prompts.	This paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer LMs. The key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. The Reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns, raised by Reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on OOD relations. The former has been addressed in the revised version. While the latter point remains valid, I find that the paper in its current state has enough useful experiments and analysis to warrant publication. The authors have clarified most of the other points raised by the reviewers in their rebuttal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments. The algorithm under study is not described completely. Important hyper-parameters are missing. The basic steps of training the model is also unclear.This paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning. The paper studies various issues including expressivity of policy architecture, regularization, choice of conditioning variables. While the paper presents interesting insights, I am not sure how much these findings can be generalized.This paper studies the importance of the design decisions for supervised learning type reinforcement learning algorithms. Through extensive experiments, find that more complex design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary.The paper investigates different variants of behavior cloning. The methods investigated aim at achieving better policies for use in offline RL through supervised learning than the average behavior contained in the data. The finding that regularization of the learned policy is particularly significant suffers from the lack of a recipe to perform this regularization offline.	The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results.    During the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which removed many of the above concerns (although not all) and convinced the reviewers to raise their scores. As a result, we believe it is fine to accept the paper (although somehow like a weak accept).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies learning zero-sum Markov Games with kernel function approximation. They propose KernelCCE-VTR algorithm based on value-target regression. The paper provides sound and elegant regret bounds in terms of the effective dimension of the kernel.This work proposes a KernelCCE-VTR algorithm for finding the Nash equilibrium of two-player zero-sum MGs. The algorithm uses kernel function approximation in RKHS to approximate the transition kernel and optimal value function.This paper considers two-player, zero-sum Markov games under a kernel mixture MG model where the transition kernel is assumed to be an element in an RKHS. The authors propose KernelCCE-VTR to learn the equilibrium of the game. The work appears to be original and is mostly clear given the technical difficulty.KernelCCE-VTR can be applied to a general class of transiton model. It only requires the transition dynamic belongs to some RKHS. The regret upper bound is provided and it matches the best-known complexity.	This paper considers the problem of online reinforcement learning in two-player zero-sum Markov games. They consider a class of Markov games called kernel mixture Markov games, which extend the linear mixture MDP setting considered in prior work to the RKHS setting. The main result is to propose and algorithm called KernelCCE-VTR, which uses the principle of value-targeted regression to achieve regret bounds that scale with the effective dimension of the kernel. The authors also provide an improved variant of the algorithm based on Bernstein-type confidence bonuses.  The reviewers found the setting to be important and found the paper to be well-written, and agreed that the main results are technically challenging and likely to be a useful starting point for future work on learning Markov games with function approximation. The main issue raised by the reviewers is that the algorithm design and analysis appears to be based on a combination of well-known existing techniques for simpler settings---for the final revision, the paper can be strengthened by more strongly advocating for the novelty required in combining these techniques.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A TIM layer is proposed to replace the Transformer encoder layer, where a mechanism competition is introduced to suppress all but one group. The presentation is terrible: the method description is basically throwing the code to the audience without much explanation.The experiments on a toy image Transformer and speech enhancement demonstrate the effectiveness of the proposed approach. The experiments on BERT pre-training and fine-tuning do not bring much to the table, as the text data is relatively uniform and does not benefit from the introduced structural inductive bias.This paper proposes an independent mechanism that divides hidden representations and parameters into multiple independent mechanisms. The authors evaluate their models on the image transformer model, speech enhancement, and NLP tasks. The main thing that has been missing in this paper is the fine-details of the approaches. For me the significance of the proposed method is minimal.Authors tackle the problem of making vanilla Transformer better by handling different positions (in data) via independent and competing mechanisms. Formulation is interesting but experimental setup has various deficiencies. It is quite heuristical to use first two and last layers of Transformer to be vanilla.	The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. Improvements are seen on only some datasets and the comparisons are not exact. A reject.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present a method for iterative small molecule generation based on an autoencoder framework with graph neural networks. The method specifically focuses on the ability to extend molecular scaffolds (predefined subparts of a molecule) with structural motifs and individual atoms.The paper study the problem of fragment-based molecule generation. They propose a model MoLeR which consists of an encoder of molecular graph using Graph convolutional neural network. The experiments show the model could generate high quality molecules comparison to a few prior methods.This work proposes a generative model for molecules. The approach uses a library of motifs, extracted from the training data by breaking down molecules through acyclic bonds adjacent to a cycle. The method is set up in an autoencoder fashion with a GCN encoder.This paper proposes a flexible framework that can use motifs to generate molecules, as well as support atom-by-atom generation. Using a scaffold as the initial seed can make sure the generated molecules contain the specified scaffold. The decoder is not conditioned on the generation history and it conditions only on the hidden vector.	Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn't respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors studied information-theoretic lower bounds of posterior contraction rate for Gaussian process regression with compositional assumptions. A sharper lower bound is also studied, showing that the performance under the Gaussian wavelet series priors suffers from the curse of dimensionality. The paper is well written and easy to follow.This paper derives a novel lower bound for the GP posterior contraction rate when the true function has an additive structure. For any GP in such scenario, the contraction rate is worse than the minimax estimation rate of deep GP. This is a purely theoretical work and there is no empirical study. The authors have not discussed negative societal impact of their work.In the domain of frequentist guarantees of Bayesian nonparametric models, the paper exhibits a case where Gaussian processes learn slower than the minimax rate. The work is significant enough for publication in this venue in my opinion. At a broad level all work in this area is somewhat abstracted from practical reality.	The reviewers unanimously agree that the theory here exhibiting a particular case where Gaussian process priors are inferior to deep Gaussian processes is interesting, and furthermore that the proof techniques themselves are novel. Indeed, reviewers had minimal or no substantial concerns about the paper, and most of the questions asked by reviewers txpX and sPbe read as simple follow up questions that the authors may choose to include discussion on.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors study MMS and EF1 allocations, and the design of approximation algorithm for optimizing the social welfare. The motivation of this model is that, in a company or a department, workers many need to work in pairs. For the homogeneous setting, the authors provide a greedy-based algorithm that output a 1/8-MMS allocation.The paper considers a new variant of fair resource allocation problem. The motivating example of "pairing up employees" is really weak and sounds made-up to fit the problem definition. Unfortunately, the algorithms and analysis is just a rehashing of existing ideas.This paper deals with a novel variant of the fair division problem. Each agent’s valuations are given by a weighted undirected graph. The value for a bundle of items is given by the total value of the maximum weight matching that can be obtained in the subgraph.This paper studies mechanisms for the graphical resource allocation problem with two types of fairness: approximate maximin share (MMS) and envy-freeness up to one item (EF1) The problem is defined as follows: the resources are represented as nodes in an undirected graph. The goal is to allocate (or partially allocate) the nodes to n agents.	Reviewers agreed that the model is new and interesting and the theoretical results are solid. The main criticisms are about the model: some reviewers felt that it is too specific and there is not enough motivation. Some reviewers liked the technical depth while others felt that it is not enough to compensate for the lack of motivation. Overall, reviewers felt that the paper in its current form is not ready for publication at NeurIPS.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this work, authors dealt with the problem of improving classification performance in the setting of semi-supervised learning. Authors exploit inherent knowledge on the samples distribution via clustering to improve the latent space. The work heavily relies on well-studied concepts including Davies-Bouldin index and maximum margin clustering.The authors argue that SOTA semi-supervised learning methods neglect spatial information in the data. They propose a loss function which combines clustering objectives with classification objectives. Experiments on MNIST and three comparably small datasets show that the proposed method is able to achieve high accuracy with only few labeled data points.The proposed algorithm is not properly put in the context of other semi-supervised learning methods that are closely related. The experiments section of the paper has some shortcomings as well. The datasets used with the exception of MNIST are not standard ones used in recent semi- supervised learning papers.This paper attempts to address the semi-supervised learning topic by proposing a method based on an aggregated loss considering both cross-entry and Davies-Bouldin Index. A correct method may not necessarily be effective, which would require an extensive evaluation. The comparison studies reported in Fig. 4 are not clearly documented and possibly are not fair either.	All reviewers agree that the writing is not precise. It does not help to find any novelty in the ideas, and the limited and too quickly described experiences are not convincing enough to forgive this problem. The authors chose not to oppose or comment on the detailed arguments provided by the reviewers. I agree with the reviewers in recommending the rejection of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed framework of DATASIFTER is interesting and novel with promising results. The method primarily depends on a set selection phase for which no empirical/theoretical evidence is provided. Models considered in the work are small in size as compared to state-of-the-art models.This paper proposes, *DataSifter*, an optimization-based, general-purpose framework for filtering "bad data" from a training set. Overall, I enjoyed reading the paper. It was well-structured, easy to follow, and presented ideas clearly. However, it is my view that the paper still needs more work in particular around the experimental evaluation.In this paper, the authors study the problem of data selection in the presence of bad training data by maximizing the utility function $U$ over all subsets of cardinality $k$. The authors first show that the valuation based approaches can have bad performances, and propose the DataSifter algorithm to solve the combinatorial optimization problem.This paper suggests an algorithmic framework based on utility optimization approach to achieve robustness against general bad data. The method performs well empirically. Theoretically shows that Shapley value based methods has small domination ratio. There is no theoretical analysis of DataSifter.	The paper considers the question of identifying bad data so that models can be trained on the subset of data that is good. This question is formulated as a utility optimization problem. The paper shows that some popular heuristics are quite bad in the framework they propose. They also propose a new algorithmic framework called DataSifter. There is empirical evaluation provided for this. Questions have been raised in the reviews about the size of the models that have been used in the empirical evaluation. The authors have responded to this by suggesting the use of proxy model techniques. There are also questions about learnability of data utility for which some responses are provided in the rebuttal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Paper is on modeling the prediction of ancestor relation between names of science institutions. The proposed approach is set-based models (with neural encodings) where the overlap between two names is measured by set overlap at the unigram level.The paper shows how to infer the organisational structure of an institution. It presents a model for predicting the is-ancestor relationships of institutions based on their string names. The experimental evaluation is on a single dataset only.The paper addresses the prediction of the hierarchical structure of organizations / institutes. The authors develop a new dataset, automatically derived from GRID (global research identifier database), and compare a set-based model against a few baseline approaches. However, the authors mention embedding-based approaches only briefly as future work.	This paper presents a new task and model for predicting the hierarchical structure of organizations / institutes. The model predicts is-ancestor relationships between the institutions by modeling the set operations between the strings. The authors develop a new dataset, automatically derived from GRID, and compare their set-based model against a few baseline approaches.   The reviewers’ comments were generally positive and praised the usefulness of the task, which makes us recommend acceptance. However, there were some concerns about the experimental setup (choice of baselines and evaluation).   We strongly recommend improving these aspects on the final version, as per the reviewers’ suggestions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This submission focuses on the cold start problem of new entities. It combines the strengths of a new entity and the existing entities, and the content features of the new entity. The proposed method outperforms several intuitive naïve strategies as well as MAML.The paper proposes Contextual HyperNetworks (CHNs) as an auxiliary model to generate parameters from existing data, and observations and other metadata associated with new feature. The CHN is applied to P-VAE and some experimental results are provided to demonstrate its effectiveness in some application.The paper proposes a few-shot meta-learning method for recommender system. It uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. The method outperforms a wide range of baselines on MovieLens-1M and a medical synthetic dataset.This work proposes CHN, a framework to extend an existing model to incorporate new features as they become available. The benefits of CHN are demonstrated by utilizing it to extend a P-VAE. There are, however, some points that need to be addressed.	This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature.  Pros - Clear writing. - Detailed hyperparameters to aid reproducibility. - Straightforward model.  Cons - Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017. - Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive. - Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is well written and well organized. Twelve synthetic functions and two practical problems are evaluated. Overall, I think it is an incremental work of DGS (Zhang et al., 2020). The contribution is too marginal.This paper considers the problem of adapting the learning rate and smoothing parameter in the Directional Gaussian Smoothing (DGS) algorithm. The authors claim DGS is particularly sensitive to these parameters and thus attribute the strong experimental results to these changes.The paper is heavily based on [Zhang et al., 2020], where the approach of building a "truly non-local gradient" is brought forward. The default target precision is quite bad: L_{max} / 200, or 0.5% of the diameter of the space.The authors study the problem of global non-convex optimization with access only to function valuations. They propose an approach to automatically control the hyper-parameters of Directional Gaussian Smoothing. The proposed automated tuning approach is supported by a large set of experiments.	The main goal of this paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of DGS (Zhang et al., 2020) method. This paper applies a line-search of the step-size parameter of DGS  to reduce tuning.  A heuristic update rule of the smooth parameter in DGS (Zhang et al., 2020)) is also used.   Pros + The topic of learning hyperparameters on the fly is well-motivated and an important direction to improve many existing methods. + A wide range of tasks are considered in the experiments are interesting, with a wide range of tasks considered.    Cons - Reviewers have found the contribution to be incremental and marginal. Specifically, the reviewers have expressed concerns on the impact of the work since it verifies improvement upon DGS only. The paper could be stronger by showing evidence of improving other algorithms.  -  In the work of [Zhang et. al., 2020], there are two parameters that require careful selection - the learning rate and smoothing radius. However, the proposed approach still relies on some hyperparameters. Although the authors claim that the method is not sensitive to these hyperparameters, it could be better justified.  - The initial version lacks evaluation of the adaptive mechanism, although the authors added the comparison in the revised version.  - The paper could be further improved by comparing against other hyperparameter optimization methods.   We acknowledge the detailed response and the modifications of the manuscript. We believe the paper will make a more profound contribution and impact after addressing some of the major concerns raised by the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new data distillation approach based on neural feature regression that is similar to a truncated backgprop through time using a pool of models. The approach sets a new state-of-the-art results both in terms of accuracy and training efficiency.This paper proposes a new method of dataset distillation called “Neural Feature Regression with Pooling” This work avoids using a surrogate objective like previous works (DSAm DM, MTT) While also circumventing the computational constraints of other methods that optimize the true objective (DD, KIP) Like other methods, FRePo trains teacher networks from which to obtain “ meta-gradients.” Unlike other methods,. FRe po obtains the metaThis paper proposes to use Kernel Ridge Regression on the last layer of a slowly updated feature extractor to perform Dataset Distillation. There are several key contributions in this paper, including demonstrating KRR can work well in DD, using model pools.This paper presents an efficient method for dataset distillation. The goal is to extract/synthesize a small set of images that represent a dataset. The method is evaluated over a series of popular datasets including ImageNet.	The paper proposes a new algorithm for dataset distillation, based on two key ideas:  (1) train a linear layer given the fixed feature extractor, and (2) use a diverse set of modes as feature extractors. The paper has received overwhelmingly positive reviews. Many reviewers find the algorithm effective, the paper well-written, and the results compelling. The rebuttal further addressed the concerns regarding the backbone models and missing experiments as well as provided additional clarifications. The AC agreed with the reviewers’ consensus and recommended accepting the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a framework, called Focal Reasoner, to perform logical reasoning to answer questions. The proposed approach first extracts the facts from raw text. After forming a subgraph, reasoning is performed by a graph convolution module to predict the correct answer.This paper proposes a model architecture for question answering which takes advantage of shallow proposition structure and coreference links using (relational) graph convolutional networks. Subject-predicate-object triples (dubbed "fact units") are extracted from the text using dependency parsing. The resulting representations serve as input to the final classification layer for answer prediction.The paper claims that a lot of previous work in QA-based MRC focused only on entity-aware common sense knowledge. To overcome this, the paper proposes to build a finer-grained local fact, (entity, predicate, entity) triplet, and then connect the triplet nodes with global information such as coreference, entity information. In the experiment section, the authors show that the proposed FOCAL REASONER outperforms the previous SOTA models.This paper presents a novel graph reasoning model based on supergraphs constructed via fact triplets. By associating nodes across different fact units based on coreferences and mentions, a supergraph is built that connects all related information and conducts graph reasoning for answer predictions.	Strengths: * Well-written paper * Strong empirical results on three benchmarks * Interesting approach of producing semantically augmented LMs using dependency parses to extract svo triples, and finding coreferences between them across multiple sentences  Weaknesses: * None of the reviewers seem particularly excited about the paper * Stronger baseline comparisons would have improved the paper * Authors re-define a lot of terminology, but the novelty of the method is more from the type of graph used to initialize their method, which seems to be a function of OpenIE triplets
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes an augmentation of the DDPG algorithm with prioritized experience replay plus parameter noise. Empirical evaluations of the proposed algorithm are conducted on Mujoco benchmarks while the results are mixed. The submission is below the novelty threshold for a publication at ICLR.This paper combines elements of two existing reinforcement learning approaches, namely, Deep Q-learning Networks (DQN) with Prioritised Experience Replay (PER) and Deep Deterministic Policy Gradient (DDPG) The algorithm has a limited novelty with a simple modification of the DDPG algorithm to add the PER component.The paper proposes PDDPG, a combination of prioritized experience replay, parameter noise exploration, and DDPG. Different combinations are then evaluated on MuJoCo domains, and the results are mixed.	The authors take two algorithmic components that were proposed in the context of discrete-action RL - priority replay and parameter noise - and evaluate them with DDPG on continuous control tasks. The different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. There is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. The AC and the reviewers agree that this paper is not strong enough for ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work targets the problem of populating the buffer memory in the Experience Replay (ER)-based paradigm. In ER, we are allowed to store a small subset of the incoming data stream, therefore deciding on a good policy to populate the memory buffer is important. The paper proposes MEMORABLE INFORMATION CRITERION (MIC) which is a combination of the surprisal and learnability of an incoming example.This paper considers task-free continual learning in the context of online memory selection. The author consider the use of information-theoretic principles and propose the surprise and learnability criteria. The authors also propose InfoRS to sample among selective points with high information.This paper considers an online memory selection task for continual learning applications. The paper proposes a new criterion to pick informative points and void outliers. The performance may critically depend on balancing how much we want to “surprise” vs “learn”	One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay.  This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay.  The paper proposes novel information-theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers).  They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance.  Pros:  The paper is well written and organized.  It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting.  It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well.    Cons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed. Reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc.    In the rebuttal, the authors have addressed several of these concerns.  Please make sure to include and incorporate reviewer suggestions in the final revised version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a new image enhancement specific for low light images. They exploit the concepts of geometric and lighting consistency together with a contextual loss criterion. The main weakness of the paper is that none of the individual contributions is individually novel. The work is incremental and uses already existing building blocks.This paper proposes an image enhancement method using an unpaired-GAN based translation network (cycleGAN) It combines geometric, lighting consistency, and a contextual loss criterion. The results do not show any significant improvement over those of the existing methods.This paper proposes an image enhancement model to translate a low light image to a bright image without a paired supervision through CycleGAN. Compared to similar recent GAN-based works, the proposed work relies on cycle-consistency, as well as geometric and illumination consistency. They also separate their discriminators for color, texture and edges.This paper works on a CV task that is low-light image enhancement without paired data. It brings geometric and lighting prior to serve as assistant to the supervision. This paper proposes a low light image enhancement method.	None of the reviewers championed the paper. Many weaknesses were shared across the reviewers: none of the individual contributions is individually novel, paper is not well written and the results do not show significant improvement over the prior state of the art. No rebuttal was provided. The AC agrees with the reviewers that the paper is not ready for publication at ILCR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an alternative deep learning model for use in combinatorial optimization. The attention model is inspired by the Transformer architecture of Vaswani et al. (2017) Given a distribution over problem instances, the REINFORCE update is used to train the attention model.The paper presents an attention-based approach to learning a policy for solving TSP and other routing-type combinatorial optimization problems. An encoder network computes an embedding vector for each node in the input problem instance. The decoder network then uses those embeddings to output a permutation of the nodes which is used as the solution to the optimization problem. The encoder and decoder are trained using REINFORCE to maximize solution quality.This is a very high-quality paper. The writing is clear and sharp. The experimental results are clearly presented and well-illustrated. The theoretical contributions are not ground-breaking. Although it could be quite interesting, as it is it's not very well motivated.	The paper presents a new deep learning approach for combinatorial optimization problems based on the Transformer architecture. The paper is well written and several experiments are provided. A reviewer asked for more intuition to the proposed approach and authors have responded accordingly. Reviewers are also concerned with scalability and theoretical basis. Overall, all reviewers were positives in their scores, and I recommend accepting the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed to solve an interesting problem: how do we perform counterfactual inference for time series data? The paper follows a study of the problem in the static setting. The experiments presented appear promising to the time series inference problem.The authors propose a new method to do counterfactual inference on time series data in healthcare. This is done by modelling interventions in continuous time with differential equations augmented by auxiliary confounding variables to reduce bias. They demonstrate the proposed method on tumor growth simulation and sepsis patient treatment response.The authors present an approach to counterfactual inference for time series based on ordinary differential equations to accommodate for the continuous-time setting. The results in Table 1 are underwhelming considering the error bars of the results relative to the simpler TSD-RMSN.In this manuscript, the authors propose a novel way of performing counterfactual inference in time-series in the presence of hidden confounders. For this, they employ neural ODEs as a latent time- series model, which they augment with additional latent variables. They test their approach on synthetic and real-world data and demonstrate improved performance.	There are some interesting ideas raised on continuous-time models with latent variables in machine learning. However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors studied the problem of training neural networks under data poisoning. They considered two data corruption settings, one allowing both the data x and supervision y to be corrupted, and one with only supervision y corrupted. Their first algorithm, which removes the datapoints whose gradient norm is large when computing the average gradient, applies to the general supervision setting. They showed their algorithm has eps\sqrt(d) error or eps*L error.The related work section misses MANY related results on corrupted data and robust mean estimation. Collaborative learning methods seem to have no solid theoretical understanding and it is unclear why the proposed algorithm build on top of it. The novelty of the theorems may be overclaimed.This paper proposes a robust algorithm for noisy label learning. By keeping the data with a small gradient norm in the training process, the proposed algorithm could resist the label noise. The theoretical results make sense, but there lack detailed proofs to make this paper self-contain.The paper studies the problem of robust machine learning, where the labels of the a fraction of samples are arbitrarily corrupted. The paper proposes an algorithm to tackle this problem and evaluates it on a standard datasets. The experiments suggest that the proposed algorithm is better than the baselines.	The papers studies machine learning tasks in the presence of adversarially corrupt data (during training). In particular, it is assumed that the labels of a small constant fraction of the datapoints are arbitrarily corrupted.  The paper proposes a natural method to solve this problem and evaluates it on various datasets. As pointed out by the reviewers, the theoretical contributions of this paper are subsumed by a number of prior works (which were not initially cited). The experimental results of the paper are interesting. However, the method proposed  and evaluated is not particularly novel. In my opinion, the problems studied in this submission are important (in particular, the memory/space consideration in the context of robustness). However, this work still needs work and is not ready for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Simulation Transformer (SiT) is a transformer based approach for particle-based fluid simulations. The SiT method is able to leverage the material specific properties learned in the "abstract tokens" to better simulate the properties of the solid interacting with the fluid. SiT models are much more light-weight (i.e., far fewer parameters)Simulation Transformer (SiT) uses the Tranformers' attention mechanism to attend to critical particle interactions. SiT is evaluated in four simulated environments: FluidFall, FluidShake, BoxBath, RiceGrip.This paper proposes to learn particle dynamics of physical systems by the means of a transformer. The paper investigates using a vanilla transformer as well as a more customized variant called 'Simulation Transformer' Both networks are validated based on variety of experiments of simulations.The proposed model seems to perform better than the baselines in most settings. The mechanism does not offer the main advantage of transformers which is not having to require to compute specialized edge level embeddings, which are expensive.	The submission received split reviews: two reviewers recommended weak accepts, and the other two weak rejects.  The AC went through the reviews, responses, and discussions carefully.  The AC agrees that this paper is well-written and has demonstrated the possibility of using transformers for particle-based physical simulation.  The AC also believes that the authors have addressed the concerns of reviewer dYcg, despite that the reviewer didn't engage in the discussions.  The contributions are however not most exciting, and none of the reviewers would like to champion the submission.  Further, the AC agrees with the knowledgeable and responsible reviewer ZsKn that the presentation and experiments can be better positioned to highlight the key contribution. As reviewer ZsKn has summarized, it's recommended that "the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non-simulation settings), and try to understand better that way how the new layer may help in a more apples-to-apples comparison."  The recommendation is reject, and the authors are encouraged to revise the paper for the next venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The idea of combining learning representations and ensemble clustering is interesting and possibly promising. I think this paper in its present form is not ready to be published though. The description of the algorithm is often too vague and fragmented in mini-sections.The paper wants to show that ensemble learning methods, in particular consensus clustering, can improve the clustering accuracy when combined with general representation learning/clustering blocks. The writing is in general clear and undestandable.The authors propose a learning-based approach for image clustering. The idea is reasonable and the method seems sound, however a similar approach has been proposed in Zhuang et al., ICCV'19 (Local Aggregation for Unsupervised Learning of Visual Embeddings)	This paper proposes a model for learning using ensemble clustering. The reviewers found the general idea promising. However, while promising, all reviewers noted that in its curent form the paper is not fit for publication. The reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work. Because of all these reasons, this paper does not meet the bar of acceptance. I recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes Squeezeformer, a novel hybrid attention-convolution architecture for ASR through a series of extensive architectural studies and improvements over the Conformer architecture. Note, Conformer has been the de-facto architecture for E2E speech processing tasks. The authors have open sourced the code and trained model checkpoints which should be supremely helpful to the community.The paper improves upon a Conformer model for speech recognition. The paper finds experimentally that the subsequent activations are are highly correlated in the higher confomer blocks. The experiments show that adding each change one-by-one gradually improves the performance.The paper considers the Conformer architecture popularly used in recent speech- input tasks. Under the CTC objective, they observe the following properties and motivate solutions to give Squeezeformer. They additively demonstrate each's improvements over Conformer-CTC.	The paper conducts thorough analysis of the Conformer architecture and brings insights and techniques from other fields to simplify and improve the model structure, which is also demonstrated to show nice gains. Though as pointed by reviewers the novelty is limited, the study is very useful to the field.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper gives an algorithm to sample a distribution given access to pairwise comparisons from it. The sample complexity of the resulting algorithm may depend on the structure of the distribution. The authors give another algorithm for sampling from a parametric Markov chain whose stationary distribution approximately matches the unknown one.This paper considers perfect sampling from a discrete set D given data in the form of pairwise comparisons. To develop their sampling scheme, the authors use Coupling From the Past (CFTP), which is a general technique for exact sampling from the stationary distribution of a Markov chain.The paper studies perfect sampling from pairwise comparisons. The algorithm doesn't need to know the distribution Q, but the complexity guarantees will depend on the structure of Q. The running time depends on the spectral gap of the Markov chain M, which depends on Q.The paper studies the problem of obtaining perfect samples from a discrete distribution with access to pairwise comparisons of elements. The major technical innovation is a new variant of the Coupling from the Past (CFTP) algorithm that allows the perfect sampling. The submission was reorganised from a longer paper to fit within NeurIPS's strict page limits.	The authors show how to perfectly sample a discrete distribution, given sample access to the Bradley-Terry-Luce model on subsets of size 2. While the learning problem has been previously studied extensively, this work initiates the study of sampling. Technically, the authors introduce re-weighting and rejection sampling ideas that speed up coupling from the past by utilizing an approximate learning algorithm; these techniques could be useful in other applications, as the authors hint in the rebuttal.  The reviewers agreed that the paper is technically quite strong and that it's quite well written. The authors responded to all remaining questions by the reviewers, clearing the path to the paper being accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper evaluates two moving average strategies for GAN optimization. Experiments confirmed high performance of averaging. The obvious weakness of the paper is technical novelty. It only mentions quite general tendency of averaging (seems not specific to GAN).This paper tries to adapt the concept of averaging to GAN training. In a simple min-max example the iterates obtained by gradient method do not converge to the equilibrium of the game but their average does. This work first provides intuitions on the potential benefits of exponential moving average (EMA) on a simple illustrative example.The submission analyzes parameter averaging in GAN training, positing that using the exponential moving average (EMA) leads to more well-behaved solutions than using moving averages (MA) or no averaging (None) The theoretical underpinnings in Section 3.1 are quite thin, and focus on describing a bilinear saddle problem.	This work analyses the use of parameter averaging in GANs. It can mainly be seen as an empirical study (while also a convergence analysis of EMA for a concrete example provides some minor theoretical result) but experimental results are very convincing and could promote using parameter averaging in the GAN community. Therefore, even if the technical novelty is limited, the insights brought by the paper are intesting.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces and describes Multi-LexSum: a benchmark for multi-document summarization and three levels of granularity. Summarizing legal texts is a non-trivial task which is a core task that many lawyers do. The experiments are well done and reasonable in light of the claims made by the paper.The authors present a new dataset of 9,280 expert-authored summaries from large-scale civil rights lawsuits. They experiment with state-of-the-art models for abstractive summarization (BART, Pegasus, LED, and PRIMERA) on their new dataset.Multi-LexSum is a law-specific domain multi-document text summarization dataset. The summaries in this dataset vary from long, short, and tiny and derived from multiple documents. The paper conducted a good series of experiments to support their claim.This paper introduces Multi-LexSum, a multi-doc summarization dataset based on civil rights lawsuits from U.S. federal courts. The results indicate that exiting popular summarization models cannot achieve satisfying performances.The authors introduce a new set of summaries of long legal documents concerning civil rights cases. This data is focused on a subject area with high potential for social impact. The existence of multiple levels of granularity is a very nice feature.	This is a very high-quality dataset, which is especially noteworthy since legal NLP benchmarks are very difficult to build. The authors approached every aspect of the process with extreme care. It will likely have an impact on law practice and start interesting discussions about the use of ML in these settings. The paper is also very well written and enjoyable to read.  Most reviewers are heavily in favor of acceptance. Reviewer Aeqk brought up some weaknesses, but at least from the AC's perspective, these seem to be answered well by the authors.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Theoretically, they show that RELU networks with a single hidden layer converge to the global minimum under simultaneous gradient descent. It is not clear to me why Figure 1 should concern "overparametrization" rather than model complexity.This paper studies the effect of model over-parametrization in GANs. It considers two types of training of the GAN model, one with the simultaneous gradient descent ascent and one where the discriminator is trained to optimality for every generator update.Theoretically, it shows that a GAN with over-parameterized 1-layer neural network generator and a linear discriminator can converge to global saddle point via stochastic optimisation. Similar results are obtained for nonlinear generators and discriminators under some conditions.In this paper, the authors proposed to analyze the over-parameterization in GANs optimized by the alternative gradient descent method. The paper does not provide any bound on the gap between training error and test error. The experimental results cannot fully validate the authors' claims.	### Paper summary  This paper investigates theoretically and empirically the effect of increasing the number of parameters ("overparameterization") in GAN training. By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically). This paper provides an explicit threshold for the width of a 1-layer ReLU network generator so that gradient-ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data). The authors also provides a more general theorem that generalizes this result to deeper networks.  ### Evaluation The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of "overparameterization". After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator  distribution (produced from a *fixed* set of latent variables z_i) and the empirical mean of the data. R3 argues that this is not sufficient to "understanding the training of GANs". At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)? While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance. And importantly, this paper is the first to study this question theoretically.  I also read the paper in more details. I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent-ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting. I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance. See below for additional comments to be taken in consideration for the camera ready version.  ### Some detailed comments - Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it. The fact that this is standard in some other papers is not a reason to not be clear about it. - Theorem 2.2: in the closed form of the objective when $d$ is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression. The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning). This should also a bit more clear in the introduction -- it seems to me that the main novelty of the work is to look at the gradient-descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to $d$). - p.6 equation (8): typo -- the  $-\mu d_t$ term is redundant and should be removed as already included from $\nabla_d h(d,\theta)$. - p.7 "numerical validations" paragraph: Please describe more clearly what is the meaning of "final MSE". Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (e.g. after a fixed number of iterations, or according to some stopping criterion?). Please clarify.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.There is relatively little work that focuses on the intersection between privacy, fairness, and accuracy. The experimental protocol developed on synthetic data and on CelebA and CIFAR10 visual datasets is convincing. Theorem 2 and its proof is included in Appendix A.Theoretical results corroborated by experimental results. Important topic and interesting research angle. Related work on state of the art regarding the trade-off between accuracy and fairness. Relevant discussion and comparison can strengthen the merit of this work.Research topic is interesting. Understanding the tradeoff between privacy, fairness, and accuracy is essential for the development of trustworthy algorithms. Supportive experiments look sound and serve as a good explanation for the theorems.The authors look at the problems of privacy, fairness, and accuracy from a new perspective. Findings are supported with strong theoretical proofs and empirical results. I think to get more conclusive findings, the authors need to experiment with other popular fairness notions.	Meta Review: The paper studies the intersection between fairness, privacy, and accuracy. Reviewers are overall positive about the novel insights that the paper provides. Minor concerns are well covered by the rebuttal.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes an approach to construct conformal prediction sets. The paper claims that such a method produces cautious classifiers that can produce "I don't know" answers in the face of uncertainty. The method is broadly applicable to any classifier.This paper applies Conformal Methods to multi-class classification. The authors seem to be the first to attempt multiclass classification. They effectively build an independent classifier for each class that estimates whether an example comes from that class within a certain confidence.Conformal prediction is similar to multi-label classification, but with a statistical sound way of thresholding each (class-specific) classifier. The concrete, deep implementation of the approach is rather straightforward and substandard for ICLR. Since a validation set is used to compute the quantiles, substantial 'power' is lost.	The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft-max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper focuses on offline learning for linear contextual bandits. It provides a novel family of pessimistic learning rules that generalizes over the Bellman--consistent pessimism and lower confidence bound strategies. The statistical guarantees established here are proven to be minimax optimal.This paper proposed a new confidence set estimation approach for linear contextual bandits based on the pessimistic principal. The optimality gap is derived using the dual norm techniques. The paper suggested that using l_p norm with p=infinity gives the best suboptimality guarantee.This paper proposes a family of offline pessimistic learning algorithms for linear contextual bandit based on confidence sets. Among them, the algorithm based on $\ell_\infty$ confidence set has the smallest suboptimality gap. It also proves lower bounds for classes of linear contextualBandit problems.The paper studies batch (offline) learning in linear contextual bandits. They analyze a class of pessimistic learning rules indexed by different l_p norms. They show that each learning rule under a norm is minimax optimal under the specific norm.	The reviewers are in agreement that this paper provides a minimax optimal solution to the problem of offline linear contextual bandits. This new family of learning rules beat state of the art approaches and provide a unified view on existing approaches, such as Lower Confidence Bound and Bellman-Consistent Pessimism. The theoretical results are backed by reasonable numerical simulations. Accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a method to denoise low-dose CT images. In contrast to previously proposed methods, no proprietary projection data is required. The method operates on the image domain as well as on the spatial frequency domain. The authors show that the combination of networks operating in the image and spatial frequency domains leads to better denoising results.The paper proposed to denoise low-dose CT in both image and spatial frequency domain with the combination of L1 and MS-SSIM loss. The proposed work only compared vertically with different compositon of the I and F Unet.Although the methodology is sound, I disagree with the authors in that "refining the spatial frequency of the CT imageimproves low-dose reconstructions when used in conjunction with an image-domain network" PSNR improvement over a simple image-based U-net is marginal (+ 0.3 dB) especially when considered the additional complexity.This paper tests the hypothesis that a dual-domain cascade of U-nets outperforms single-domain cascades. The results suggest that this is the case. The paper is straight forward, well-structured and the aims, methods and results and discussion are interesting, informative and clearly presented.	Interesting idea, well written paper. I suggest the authors to include and compare with relevant previous work as suggested by the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper targets to demonstrate social perception and human-AI collaboration in common household activities. It shows the development of a multi-agent virtual environment that is used to test an AI agent’s ability to reason.This work tackles problems related to human-AI collaboration. The main theme of the paper and research presented fits well ICLR. The authors do an excellent job presenting the challenge, the multi-agent virtual platform (open source planned for the future if I understood correctly?)This paper contributes a new challenge task (called Watch and Help) and a multi-agent virtual environment evaluation platform for goal recognition and collaborative planning. The paper also provides several benchmark agents and compares their collaborative performance with both the hierarchical planning Alice agent and a human-controlled Alice avatar.The paper was published in the journal Autonomous Agents and Multi-Agent Systems (ACMAS) The authors used a combination of existing techniques to solve the problem rather than coming up with new learning techniques from scratch. The work is significant to researchers interested in modelling social intelligence.	Summary: This paper provides an interesting and unique challenge problem on human-AI collaboration, with sample baselines. I think this is an extremely important topic and the community should embrace such challenge problems.  Discussion: Reviewers agreed this paper should be accepted, particularly after seeing that ICLR has accepted such challenge papers in the past.  Recommendation: I'd really like to see this get a spotlight as it would be great to highlight this innovative challenge to the community.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is clear and the main idea is rather interesting, but the presented experimental validations are arguably weak. It is not clear where the non-monotonic change in denoising as a function of network size comes from. The types of tasks and datasets on which the proposed architecture is being tested are rather small.I think overall I appreciate the idea behind the work. I think the work is quite novel, and it also connects to bodies of literature (hopfield networks -- attractors based and more mainstream GRU/LSTM nets) I would really love to see more analysis, going beyond measuring entropy.In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to “denoise” a recurrent neural network. The idea is that for every normal step of an RNN, one induces an additional "dimension" of recurrency in order to create attractor dynamics around that particular hidden state.	The paper is well written and develops a novel and original architecture and technique for RNNs to learn attractors for their hidden states (based on an auxiliary denoising training of an attractor network). All reviewers and AC found the idea very interesting and a promising direction of research for RNNs. However all also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope. Reviewers demand experimental comparisons with other (simpler) denoising / regularization techniques; more in depth experimental validation and analysis of the state-denoising behaviour; as well as experiments on larger datasets and more ambitious tasks.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The biggest strength of the paper is clearly the development of a large-scale polyp pathology dataset with object-level boundingbox, segmentation annotations, and pathology labels. The actual improvements of the proposed approach over other classification techniques is very small.The proposed method provides a joint polyp characterization and detection. Commonly, these two tasks are addressed separately. The evaluation did not show clear benefits of the proposed approach. In Table 2 and 4 there is not a clear tendency to better performance.Anchor-free approach with attention mechanism for polyp detection. Using attention mechanism, the architecture removes the setting of priors, that is, anchors. The architecture bases on the body of ResNet-50 and the head of FCOS.	The methodological contributions and technical novelties are relatively small, but the dataset is very useful. Therefore, if the authors want to claim that the dataset is a major contribution, the dataset should be publicly released; otherwise, this contribution cannot be realized.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present a novel 2-stage technique to improve the classification accuracy of feedforward ANNs. After the feedforward pass, a so-called introspective stage occurs, the goal of which is to ascertain why the particular class label was provided rather than a different label. This stage is modular and can be added onto networks under varying task conditions.In this paper, the authors propose a modification to standard neural networks used for object classification tasks to incorporate “introspective learning’ This consists on training a multi-layer perceptron (MLP) on the neural network introspective features. These are obtained by calculating the gradients over the last layer weights of the model on a loss function corresponding to posing an introspective question: why is the correct label A instead of B? The authors apply this procedure to several neural networksThe paper proposes an interesting and novel approach for improving robustness and calibration under distribution shift. Results on CIFAR10-Corrupted are very promising, and the authors further show the method is able to improve in active learning and OOD detection.The authors present a new inference pipeline for neural networks. They augment the features processed by $f$ with the loss gradients with respect to the weights of the last layer. Experiments show slight improvements in classification accuracy and cuts of calibration errors.	The reviewers all appreciated the novel concept behind the work. I agree with this, I think the principles behind the work are novel and interesting, and I would encourage the authors to improve the validation of this method and publish it in the future.  However, reviewers also raised a number of issues with the current paper: (1) the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablations/comparisons; (2) it's not clear if the improvements from the method are especially significant; (3) the writing could be improved (I do see that the authors made a significant number of changes and improved parts of the paper in response to reviewer concerns to a degree). Probably the writing issues could be fixed, but the skepticism about the experiment results seems harder to address, and while I recognize that the authors made an effort to point some existing ablations in the paper that do address parts of what the reviewers raised, I do think that in the balance the experimental results leave the validation of the work as somewhat borderline.  While less important for the decision, I found that the paper is somewhat overselling the contribution in the opening -- while the particular concept of using gradients as features in this way is interesting, similar ideas have been proposed in the past, and the paper would probably be better if it was more clearly positioned in the context of prior work rather than trying to present a new "framework" like this. It kind of feels like it's biting off too much in the opening, and then delivering a comparatively more modest (but novel and interesting!) technical component.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.RepB-SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy in the representation space. The experiments demonstrate the effectiveness of RepB-BDE over almost 10 baselines in the popular offline RL benchmark D4RL.The paper deals with batch RL (aka offline RL) It builds on "Representation Balancing MDP (RepBM)" and tries to improve the procedure by "stationary DIstribution Correction Estimation (DICE)Model learning is an important component for offline RL, which is usually done independently from policy evaluation / optimization. The authors propose a new model learning method that takes policy evaluation error into consideration. In terms of policy evaluation, the authors show empirical advantages over previous model-based offline OPE algorithms.	The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.  The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning.   There are concerns on writing style and comprehension.  - The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods.  - The presentation is very dense and quite hard to grasp, even with the Appendix. - The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper.  I would recommend including the response to R1 in the paper.  Other relevant and concurrent papers to potentially take note of: - Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization (https://openreview.net/forum?id=wiSgdeJ29ee)  - Robust Offline Reinforcement Learning from Low-Quality Data (https://openreview.net/forum?id=uOjm_xqKEoX)  Given the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re-writing to make the manuscript more accessible, which in turn to increase impact of this work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the relationship between disentanglement and multi-task learning. The number of learning tasks and also the relevance of different tasks could matter. The authors try to address one of my questions on the number of tasks, but the conclusion is inconclusive.The work is quite interesting, and I think fundamental analyses of multitask learning representations like this are important within multitasks learning research. The analysis goes through multiple different network architectures and tabulates different metrics, and is reasonably thorough. The visualizations are also clear and fairly convincing.This paper studies the connection between disentanglement and multi-task learning. They present several sources of evidence that multi- task learning makes features quite a bit more disentangled. They also show some additional findings, such as that the reverse is not clearly true. Overall the main claim of the paper is interesting and moderately well supported.The paper provides an empirical analysis of the effect of multi-task learning on the disentanglement of the learned latent representation. Authors report the experimental results on three synthetic datasets, comparing a hard parameter sharing strategy with a shared backbone and separate task-specific heads with the single-task case and randomly initialized networks.	This paper aims to look at the relationship between disentanglement and multi-task learning.  The authors claim to show that disentanglement emerges naturally from MTL.  The main discussion was whether the claim that disentanglement emerges naturally from MTL has been adequately demonstrated.  The main   issue is that MTL results in more extraction of information and that is hard to disentangle from the disentanglement metrics used.  Reviewers agreed the work was interesting but not as complete as would be desirable.  I also feel it is not ready for ICLR presentation, but   with further work could be a nice future contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work proposes a sparsity enforcing structure for GANs by splitting layers into the generator for the sparse vector and for the final image. The proposed method yielded improved performance over conventional methods in FID, IS for generators and PSNR for denoising with DIP.The paper proposes viewing CNN's through the lens of sparse coding. In practice it mostly boils down to encouraging sparsity in the last layer activations. The metric improvement is an interesting effect, but the promised understanding of the network internals is forgotten in practice.This paper proposes a CNN-based (specifically GAN-based) solution to tackle image synthesis using sparse coding concept. The proposed method utilises the generator from a generative adversarial network to synthesise a sparse representation, i.e. the sparse code.The paper describes image generation as a sparse coding reconstruction process. The method is relatively simple and it could possibly be widely adopted if the experiments in the paper generalize to a wider range of problems. The paper does not discuss any downsides of the technique which I find a bit unexpected.	This paper introduces sparse modeling-inspired regularizations to improve deep neural network-based image generators. Experimental results on both (low-resolution) image synthesis and deep image prior-based inverse problems are used to validate the proposed method. The majority of the reviewers were against the acceptance of the paper. As summarized by Reviewer tsoA: "There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes." The AC agrees with that summarization and recommends rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper considered the approximation property of GCNNs on the frame of the ridgelet theory. The main results showed the cc-universality ofGCNNs in a constructive and unified manner.This paper uses the ridgelet theory technique to show the universality of group convolutional neural networks for various groups and feature spaces. The paper provides a proper amount of details and explanations. However, I found the paper a bit hard to follow.The paper describes a mathematical analysis of group convolutional neural networks, showing that they are universal approximators. To show this, the authors derived the ridgelet transform for G-CNNs. This is a result in itself, which is further used in this paper to provide a constructive proof of universality.The authors prove the (cc-) universality property of group convolutional neural networks (GCNNs) with one hidden layer using the tools from ridglet theory. While the work seems rigorous, I think it misses some motivation and an analysis of the theoretical results.	The paper studies the approximation properties of group convolutional neural networks. It establishes the “cc-universality” of group CNNs, i.e. that such networks can approximate any continuous function over any compact set, using a new constructive approach which is based on a generalization of the ridgelet transform. The proof is constructive, in the sense that approximating networks are given in closed form by discretizing the transform. This approach may have applications beyond the scope of the paper — most immediately, to identifying classes of functions for which neural network approximations are accurate in a quantitative sense; this can be tied to the decay properties of the ridgelet transform. Reviewers found the paper to be clearly written and of high technical quality, albeit somewhat mathematically dense in its presentation. Universal approximation theorems provide an important piece of theoretical background, as well as a “sanity check” for new network architectures; having a unified, constructive approach to derive them could stimulate further work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposed a method to learn the geometric details of clothing mesh via perturbing texture coordinates. The paper first trains multiple networks (each network is for one specific camera view) to predict the texture coordinates in the corresponding camera view, then recover 3D shapes from multiple camera views.This paper presents a general approach to embed high frequency information into low-frequency data with a particular focus on improving the performance of virtual clothing. To address over-smoothing issues in the predicted meshes, authors proposed the texture sliding method. The texture sliding neural network (TSNN) is trained using the ground truth offset computed for each camera and pose.This work proposes an approach to encoding high frequency wrinkles into lower frequency texture coordinates, dubbed texture sliding. Once texture perturbations are recovered from at least two unique camera views, 3D geometry can then be reconstructed to recover high-frequency wrinkles.The paper proposes a method to correct high-frequencies details in the textures of animated clothes. The main idea is to train a network to learn the 2D offset in the UV space for a given pose and view. The paper shows results using a t-shirt, on interpolating to novel views and 3D reconstruction.	The 4 reviewers all had a consistent view of this paper:  concern that the scope of the work was overstated (paper claims, without evidence, to apply in more generality than the 1 example scenario shown); concern about the difficulty of implementing this approach (1 TSNN required for each rendered viewpoint); and lack of examples showing how the method performs under more challenging scenarios.  The AC encourages the authors to revise the work in response to the reviews.  That would involve additional experimentation and examples, and some attention to revising the manuscript.   After two of the reviewers complained of lack of clarity in the algorithm description, the authors replied, "We explain our algorithm in the paper; the reader can refer to our code for implementation details."  I hope the authors can be more responsive to the readers' concerns than that in their revisions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides a data set of SRH images from a number of types of brain tumors as well as control tissue. It also provides several benchmarks for accuracy of classification of these images. The community needs more medical imaging data sets, and this is one covers a large number of tumor types and patients.The authors introduce a public dataset of stimulated Raman histology images of brain tumors (OpenSRH) This dataset includes over 1,300 whole slide images from 307 patients. This is a rich dataset, which includes raw acquired data, patch-wise tumor classifications and processed patches.The SRH dataset is apparently unique and would allow real-time brain tumour segmentation and classification with significant consequences for patients. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives.The paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices.SRH reduces the time from sample extraction to usable information by about 30 minutes. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy.The paper introduces a new resource for brain optical imaging, namely OpenSRH, which is valuable for optimizing brain tumor surgery. The established benchmark motivates future work to address the domain gap on the existing pretraining data and aggregation methods to better model slide-level and patient-level prediction.	There are contrasting review rating ranging from 9 to 3. The reasonable concerns about this paper not been well validated for ML community. The proposed datasets has been benchmarked with some classical and some relevant DL methods. I have taken into account the expertise of reviewers and their concerns carefully. I do agree with some reviewers opinion about this dataset will be importance to medical CV community. I do agree with authors response "The open questions for the ML community that OpenSRH may foster innovative discoveries include the following: 1) domain adaptation between SRH images and other histology images such as H&E images in the large scale TCGA project; 2) using multiple instance learning (MIL) to avoid expensive dense patch annotations; 3) different aggregation methods for patch-based training, including clustering, attention, or MIL; 4) self supervised learning and comparing different augmentation strategies for SRH images; and 5) data efficient training of ViT architectures using SRH data. " Even though the authors have not verified these innovative applications on the openSRH dataset. I will go with acceptance opinion of few reviewers for this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a benchmark / dataset, HW-NAS-Bench, for evaluating various neural architecture search algorithms. The benchmark is based on extensive measurements on real hardware. An important goal with the proposal is to support neural architecture searches for non-hardware experts.The benchmark is based on an algorithm and search space. The set of analyzed hardware is limited. There are few mentions of the analyzed Deep Learning models. It is suggested to add more details regarding the use of other network types.The paper promises to release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet. The proposed dataset seems useful for research on hardware-aware NAS algorithms. The paper contains some interesting analyses.HW-NAS is an important area of research, in particular for bringing powerful DL models to edge devices and for reducing energy consumption. The proposed HW-NAS-Bench fills an important gap and can prove to be very useful for practitioners and HW- NAS researchers.	This paper presents a new NAS benchmarks for hardware-aware NAS. For each of the architectures in the search space of NAS-Bench-201, it measures hardware performance (energy cost and latency) for six different hardware devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware-aware measurements on as many as six (very different) devices.   The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime. All reviewers appreciated the paper and gave (clear) acceptance scores.   Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware-aware setting, and I expect this work to change this, and to open up the very important field of hardware-aware NAS to many more researchers. For this reason I recommend to accept this paper as a spotlight.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper analyzes the convergence behaviour of the general one-hidden-layer fully connected neural network. Under certain assumptions, the authors prove that the GMM nduced learning algorithm converges linearly to a critical point of the empirical risk function.This paper considers the problem of learning one-hidden-layer neural networks with Gaussian mixture input in the teacher-student setting. The main techniques used in this paper seem to be based on existing approaches in Fu et al, 2020 and Zhong etal, 2017. The presentation of the current paper is good. There are some concurrent works [1,2] also study the training and generalization of neural networks.In the paper, the authors provide theoretical analysis of learning one-hidden-layer neural networks when the input distribution follows a mixture of location-scale Gaussian distributions. I think the authors may consider adding a few more relevant references about theoretical aspect of Gaussian mixtures.The paper focuses on the behavior of gradient descent in one-hidden-layer neural networks (with fixed second layer weights) In this setup, the labels are generated by an unknown teacher network with the same architecture, and the student aims to recover the teacher weights. The paper provides a precise theorem to guarantee convergence of this algorithm, and studies how the learning process can depend on the parameters of Gaussians.	This paper gives a way to learn one-hidden-layer neural networks on when the input comes from Gaussian mixture model. The main algorithm uses [Janzamin et al. 2014] as an initialization and then performs gradient descent. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give a local convergence result when the samples come from a mixture of Gaussian. The paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors identify correctly weaknesses in their method, i.e. overfitting of the networks. The approach itself is also potentially interesting. The paper can be largely improved in terms of language and form, e.g. no brackets around Figure references.	The review highlights numerous problems with the current progress of the work. While MIDL encourages early submission of ongoing research as short paper, this work is not mature enough for publication. I therefore recommend rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed a simple linear programming model for learning logical rules for KG completion. The model selects candidate rules from KG with explicit constraints and then solves a linear programming problem. The authors conduct experiments on several public datasets and the model has better efficiency than baseline models.The paper presents a method to obtain weights for a knowledge graph scoring model for link prediction. There are numerous good algorithms for mining rules from knowledge graphs (e.g. AnyBURL and AMIE). Less research has focused on the problem of creating scoring functions based on an (implicit) list of rules.The paper proposes a simple method to generate logic rules, where rules are generated by a shortest-path heuristic and rule weights by solving a linear program. In the light of the current state-of-the-art, the paper falls short severely. It's also overly limited in the type of rules that can be generated and how they can be combined.This manuscript exploits the idea of linear programming into knowledge graph completion tasks. It then presents a model to prove the sufficiency of their idea. Some interesting experimental results are well demonstrated. However, it is still hard to judge whether the algorithm is an improvement on previous work.	The paper shows how to make use of a linear program for extracting logical rules for knowledge graph completion. Overall, the reviewers and I agree that this is an interesting and important direction for research. Moreover, the presented approach shows good performance with rather small sets of rules extracted. However, all reviewers point out that the related work is not well discussed. While the authors have improved the related work sections during the rolling discussion, overall the positioning of the new method has still to be improved, including a better empirical comparison across different datasets. Overall, we would like to encourage the authors to polish their line of research based on the feedback from the reviews.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work considers an important problem of generating adversarial examples to attack a black-box model. The paper proposes a new approach to consider an adversarial example as a result of a sequence of pixel changes from a benign instance. Therefore, the adversarial generation problem can be considered as a bandit problem.The paper proposes a new approach for generating black-box adversarial attacks based on slowly-varying contextual bandits. The proposed approach achieved query efficiency and success rate which are not too far away from the current state of the art.The effectiveness is demonstrated on various classifiers for ImageNet and compared against several baseline methods. The paper proposes to use time-varying contextual bandits in order to improve the query efficiency of score-based adversarial black-box attacks.This paper proposes a new black-box attack that assumes access to loss-oracle of the target model. The attack exploits temporal and spatial correlation of block-wise pixel perturbations in a Bayesian optimization framework to achieve query efficiency.	The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers.  At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function $\ell$ is minimized and why minimizing it makes sense (assuming, e.g., that $\ell$ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes $x_t$ when the procedure is started from $x$). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1)  Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where $B_p$ and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that $x_T$, which is the final outcome of the algorithm, remains in the $L_p$ ball). Another example is the $Discrete\ Approximate\ CorrAttack_{Flip}$ paragraph which requires that every coordinate of $x$ should be changed by  $\pm\epsilon$. It is also not clear what "dividing the image into several blocks" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously $x$ was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same $\epsilon$ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper.  The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors introduce DiCGAN, an algorithm to learn a generative model that comes up with samples whose likelihood is based on a real dataset but adjusted given user preferences. They train the critic to assign high values to samples with higher preference values and thus the generator tends to move its samples towards these points.The motivation of this study is to estimate the distribution of desired data from the entire data distribution. The proposed solution extends existing GAN solutions by introducing an additional pairwise loss on the discriminator. The idea is natural and neat, and it is also proved to be effective in the reported experiments.The paper proposes a differential critic that learns the preference direction from pairwise preferences. A corresponding loss function is added to the WGAN loss to ensure that the model learns samples that have high rank. Empirical results show that the method is able to reflect the ranking from the user.The paper addresses the problem of training a GAN to match the distribution of part of the dataset called the 'desired data distribution' instead of the whole dataset. The authors propose to use an additional set S of pairwise samples (X_1,X_2) called the set of preferences to guide the training of a WGAN. This set specifies that the user prefers sample X_1 over X_2.	The paper presents a method to regularize the discriminator in  GAN training with a ranking loss based on the user preference for a desired set within a larger dataset. The tradeoff between GAN loss and preference loss dependence on the distance of the set to the full dataset and the authors consider two regimes : "small and major correction". A major correction is needed when the targeted set is very different from the whole density, authors propose in this scenario to replace samples from the data by samples from the generator. The setting in the paper is interesting and can be useful in practice.   There was a lengthy discussions between the authors and the reviewers, the discussion pinpointed issues , some of them were addressed in the rebuttal . Some issues remain unanswered regarding the clarity and some claims in the paper.  The clarity of the paper needs further improvement and  1)  clarify section 3  the setup and the background section   2)  justify claims about the method, in  the strong correction scenario  when fresh generated samples are introduced how  is this an effective procedure? (conceptually / theoretically).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper aims to optimize a function in RKHS, subject to a certain soft-constraint. It argues that by carefully combining Gaussian processes, UBC, and primal-dual algorithms, a non-trivial algorithm can be designed.The authors study a stochastic bandit problem when the reward function and constraint function lie in a reproducing kernel Hilbert space (RKHS) with a bounded norm. The paper considers soft constraints that may be violated in any round as long as the cumulative violations are small.This paper provides a unified framework CKB for kernelized bandits optimization with unknown kernelized constraints based on primal-dual optimization. This framework can employ general exploration strategies (GP-UCB and GP-TS), and achieve sublinear cumulative regret with sub linear cumulative constraint violation. Experiments on synthetic and real-world data are conducted.The paper considers the problem of black box optimization under soft constraints. The objective of the learner is to maximize a function f subject to $g \leq 0$, where both $f$ and $g$ are elements in RKHS.	The paper provides new techniques (algorithmic as well as analytical) to solve black box optimization of smooth functions with constraints. The reviewers are largely in favor of the paper's contributions, and the author responses have helped to clarify several aspects of the presentation and connections to existing work. Therefore, I recommend that the paper be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposed a loss function that leads to cooperation between two individually optimized agents in a matrix game. The authors further proposed a method to generalize this loss to non-matrix games and showed its effectiveness. The writing is very clear and easy to follow.The paper focuses on the problem of multi-agent cooperation in social dilemmas. The authors use the bias toward status-quo in human psychology to motivate a new training method. Experiments show that SQL achieve better social welfare than LOLA and standard independent RL.Coordination is a problem in MARL. When multiple agents are learning at the same time, they can get stuck in poor equilibria. This paper presents a new method for improving coordination. The approach uses a status quo loss and a method for converting multi-step games.The paper studies the problem of learning cooperative behaviors in social dilemma problems for multi-agent deep RL with independent agents. The authors propose a simple loss that emphasizes the "status quo",  which increases the emphasis on the one-step reward. To extend this idea to a temporally-extended sequential setup, the authors introduce a clustering algorithm that reduces the game into a matrix game.	In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies differentially private algorithms in federated learning. It proposes to take advantage of the randomness in Relative Entropy Coding to achieve good privacy-utility trade offs. On four benchmark datasets (MNIST, FEMNIST, Shakespeare, Stack Overflow linear regression), under same privacy epsilon, the proposed method can save communication for ~(4000, 20, 800, 100) times.This paper introduces a compression and privatization technique to federated learning based on Relative Entropy Coding (REC) For each round, client $s$ aims to privatize its local model update $w_s$ by releasing $\Delta_s = w_s + N(0, \sigma I)$. However, instead of directly adding Gaussian noise, the client first picks $K$ random vectors from a prior distribution.The paper proposes a differentially private and communication-efficient method to aggregate the client updates in federated learning. The method is based on a recently proposed compression technique relative entropy coding. The authors further modify this technique to satisfy differential privacy guarantees and perform various experiments to back their claims.This paper proposes a compression scheme for federated learning built upon previous relative entropy encoding works. It then proves that with some small modification (clipping the model updates), the algorithm is inherently differentially private. Empirical evaluation shows the proposed method can achieve much more communication reduction at the cost of accuracy degradation.	This submission describes an approach to compressing the communication in federated learning. The key idea is using a set of random samples from a prior distribution and then performing importance weighed sampling. The work performs an analysis of the privacy guarantees of this process and experimental evaluation. The main issue with this work is the authors appear to be unaware that the basic problem they pose is solved in a more comprehensive and lossless way in a recent work https://arxiv.org/abs/2102.12099 (Feldman and Talwar, ICML 2021). That work shows that any differentially private randomizer can be compressed via a simpler algorithm that performs rejection sampling using a PRG. The algorithm does not loose privacy or utility (under standard cryptographic assumptions) while guaranteeing low communication. In contrast this work loses significantly in utility and provides opaque privacy guarantees. This submission analyzes  a randomized that adds Gaussian distribution and, in particular, the compression technique in (Feldman and Talwar) applies to it. The technique proposed in this work is very similar in spirit (with prior distribution corresponding to reference distribution in the earlier work. In light of the earlier work I do not think the contributions in this submission are sufficient for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper focusses on how to extract good policies from experts in imitation learning scenarios. It proposes a series of trick for policy extraction from Cross-Entropy-Method-based (CEM/iCEM) trajectory optimizers.This paper presents an approach to distill a model-based planning expert into a policy to enable real-time execution on robotic systems. The policy is learned via imitation learning of trajectories from the expert. This approach is tested on four continuous control tasks and achieves good performance compared to baselines.This paper combines GPS and DAgger to learn a policy network by imitating a model-based controller. Their approach uses both the iCEM controller and the learned policy with DAgger-like relabeling. The experiment results show promising results.This paper presents a method to combine zero order optimizer with imitation learning. Several components are essential for good performance, including policy guided initialization, DAgger and adaptive auxiliary loss. The right most figures in Figure 4 is really hard to unpack with so many information.	This paper proposes a method to solve high-dimensional, continuous robotic tasks offering a trajectory optimization and a distill policy. The paper is well-written and the work is promising. It is very relevant for the robotics and RL communities.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a gradient-based method for solving Offline MBO problems using infinite-width DNN models. The approach facilitates efficiently optimizing designs X via a proposed bidirectional objective. Results in the paper appear to show the effectiveness of this principle on a standard benchmarking framework. Offline model-based optimization could suffer from the distributional shift issue where the model bias could be exploited by the optimization process to output poor designs. Unlike previous works, this paper introduces a directional learning approach based on neural tangent kernel to facilitate the distribution matching. The proposed method achieved the best performance on six out of seven widely studied offline model- based optimization task.This paper proposes bidirectional learning for offline infinite-width model-based optimization (BDI) The authors claim that the backward mapping can distill more information into high-scoring designs. Experiments show that BDI achieves state-of-the-art performance in both continuous and discrete tasks.	This paper studies Offline Model-Based Optimization. This paper proposes a gradient-based method for solving Offline MBO problems using infinite-width Deep learning models. The key novelty of the paper is in proposed use of a distillation objective to constrain the optimized design-score pairs.  All three reviewers identify the novelty of the problem and the approach.  The paper also presents strong empirical evaluation on standard benchmarks.   The rebuttal discusses yielded constructive changes in the paper, and the authors are expected to account for the discussion and suggestions in the next iteration of the manuscipt.  The AC concurs with the reviews and the discussion thereafter.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Active learning is an integer optimization problem that minimises the distance Wasserstein distance between unlabelled pool of data. This is done in a feature space (in this case trained using self-supervised methods all the data) The method outperforms existing active learning methods for very small labelling budgets.This work tackles the problem of active learning. In every iteration the subset of the unlabeled data pool that needs to be labeled is selected. Generalized Benders Decomposition algorithm is used to solve this IP through relaxations.This paper proposes a (batch mode) active learning method that chooses a subset of data points to be labeled through approximating the whole dataset in terms of Wasserstein distance. The selection is formulated as a large-scale mixed integer programming. Experimental results show that the proposed method is better (or competitive) than baseline methods like k-center, k-medoids, and WAAL.This paper studies active learning from a mixed integer programming perspective. The active learning strategy is representation-based, and aims at selecting a core set that minimizes the Wasserstein distance. I think overall the approach makes sense, but I didn't find significantly novel contributions, either theoretically or empirically. As a result, I'll vote for a weak acceptance.	This is an interesting submission, which was overall well received by the reviewers. I would recommend the authors to discuss further the vast modern litterature on efficient computation of Wasserstein distances and their minimization (see, e.g. Peyré and Cuturi 2019, and references therein)
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces a new offline imitation learning algorithm, to be compared with BC. The main idea is to re-use the old learned policy as a reward function in a policy- gradient step. The empirical results show that it works very well on classic locomotion tasks with artificial noises.This paper proposes an IL method for learning from noisy demonstrations. It uses this bound to motivate the construction of an algorithm. The method achieves superior performance to standard BC and comparable performance to some prior IL works.Theorem 1 seems to be incorrect. The writing quality of the paper can be further improved, especially in Section 4.3. The objective of IL is to obtain policies that can generate or keep being on optimal state trajectories as the true expert does.	This paper focuses on the problem of performing imitation learning from trajectory-level data that includes optimal as well as suboptimal demonstrations.  The authors wish to avoid the requirement of a separate filtering process that would throw away the bad trajectories.  The authors propose a clever innovation that allows for leveraging the policy that is itself being learned to reweight the samples for a next round of weighted behavioral cloning.  The paper is also somewhat theoretically rigorous and provides insight into the problem.   The reviewers pointed out some initial issues related to clarity and the authors did a good job of addressing reviewer concerns.  Ultimately all reviewers agreed that the core innovation of the paper was interesting and empirically worked reasonably well.    One older line of work that I think is quite relevant, but which is not discussed, is the empirically observed "clean-up effect", described by Michie and colleagues in the 90s (e.g. "Learning to fly" Sammut et al 1992).  This clean-up effect is intuitive and reportedly achieved for free in settings where the learning objective is mode-seeking and the dataset is large, insofar as the mean value of the resulting policy *should* produce actions that corresponds to the average action produced by demonstrators in the same situation.  I think it would be worth discussing how the analysis of this paper relates to this empirical phenomenon. In particular, it would be worth clarifying in what regimes the suboptimality of training from a dataset with noisy examples arises and how likely this is to effect the mean value of the learned policy (for context, it is fairly common in practice to evaluate the student policy in BC settings by only using the mean action value; perhaps this point was present in the paper, and I missed it). From a certain perspective, the innovation of this paper is to accentuate the clean-up effect.  As noted by a reviewer, and subsequently incorporated into the paper, the actual algorithm has some similarities to versions of recent "offline RL" algorithms (though of course it does not leverage rewards).  In particular, the motif of performing a weighted regression could perhaps be a bit more thoroughly contextualized by connecting it to other weighting factors (e.g. see Critic Regularized Regression).  That said, I leave this entirely to the discretion of the authors.  The final scores were 8, 7, & 6.  I see this as a strong paper and will endorse it for a spotlight.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a simple solution to improve the existing self-supervised representation learning by adding an additional branch to predict the rotations. Experiments are conducted on CIFAR-10, ImageNet, and two PhC datasets and the results show that the model with the additional branch can outperform the corresponding baselines.This work presents a framework for learning representations with invariances (insensitivity) to some transformations and sensitivities to others. The advantage of the proposed method is demonstrated relative to SimCLR and SimSiam on Cifar-10, ImageNet, and a new scientific application domain: learning frequency responses of photonic crystals.The paper advocates Equivariance Self-Supervised Learning (E-SSL) as a more general framework than Invariance SSL. The proposed E-SSL framework boils down to adding an additional equivariance objective (mainly 4-fold rotations) to popular I-SSL methods. The empirical results show encouraging results in CIFAR-10 and ImageNet.This paper proposes a framework which generalizes self-supervised learning (SSL) to also learn equivariance behavior. The authors focused on four-fold rotations and showed that combined with existing SSL methods lead to improvement in classification performance on CIFAR10 and ImageNet.	This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i.e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4-way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation, obtaining a reasonable improvement, and makes a good case for using an equivariant seeking loss. The authors are strongly encouraged to release their code (including training details for reproducing ImageNet results) as the improvements they present are central to the acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a system for using natural language feedback generated by large language models(LLMs) to improve task planning in embodied environments. The system does not require training a specialized model for the task and uses pre-trained models prompted with information from the environment.This paper proposes using language as a form of embodied feedback for LLM-based robot planning. Three types of environment feedback, i.e. success detection, passive and active scene descriptions, are expressed in language. It is unclear which type of feedback should be active at each time step.This work extends prior algorithms for robotic planning with LLMs to incorporate feedback, which increases tasks success rate in presence of imperfect low-level control and a dynamic environment. The approach is simple and effective. The types of feedback considered are diverse.	This paper presents a closed loop planning system that leverages LLMs for providing candidate plans given the success of action execution and the world state. Central to the approach is querying the LLM given the instruction and text prompts capturing success of skill execution and a description of the scene.   This is a systems oriented paper demonstrating and evaluating the utility of a LLM for interleaved planning and execution. The paper is well organised and clearly written. The results highlight the utility of LLMs to enable task execution before an explicit learning of a task model (given primitive robot skills amenable to composition). Further, experiments demonstrate contextual plan adaptation and success goal reaching for simulated and real data sets.   The main limitation of the work was observed to be the specialised nature of feedback prompts necessary for successful task execution. A clarification was sought if the proposed system has a principled way to determining the necessary feedback and to what extent the feedback is to be adjusted for long-running plans. In response, the authors clarified that the feedback is not hand crafted. Further,  the authors articulated their contribution towards demonstrating  the ability of LLM-based planners to integrated multi-modal feedback without focusing on the specific problem of determine how much or how best such a feedback may be generated. In response to a reviewer’s suggestion, the authors provided a comparison of models with and without feedback with an increase in the task horizon.   Additionally, the reviewers inquired the degree to which plan recovery is possible in the proposed framework. In response, the authors drew attention to results where plan adaptation occurs when an object falls is no longer in the reachable workspace. The reviewers further requested for a comparison with a baseline that searches over a set of high-candidate plans. As suggested, the authors provided a comparison with a Next Best Decoding that uses the next most likely skill.   Overall, the inclusion of new experimental insights strengthens the paper. A clear articulation of the technical advance in relation to Zang et al. 2022 is also recommended in the main manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper introduces a new dataset of video quality metrics for many video compressions. The paper is well written and provides clarity of thought. The evaluation scheme used is thorough. The results section could have been written in a more lucid and better way.The paper introduces a dataset and benchmark for video and image quality of video encoders. The dataset consists of compressed videos and subjective scores, collected via crowd-sourced comparison. The benchmark consists of 2045 videos and 10,800 viewers.The authors construct a new benchmark of video quality metrics on a wide range of video compression methods. It considers more video encoding standards and collects subjective scores via crowd sourcing. The authors argue that we need new benchmarks because existing benchmarks are commonly investigated using older compression standards, such as AVC.This work is dedicated to video quality assessment via trainable algorithms, focusing on measuring video quality after compression. The existing benchmarks contain videos compressed using outdated standards, thus they might seem irrelevant for evaluating modern algorithms. This paper introduces a new video quality benchmark for measuring the quality of videos encoded via nine actual standards.There are many video and photo quality metrics. However, most of them focus on resolution and quality subsets, temporal pooling, and computational-complexity-evaluation. The authors present the largest compressed video dataset, which include new coding standards and subjective ratings.	The submission receives 5 reviews form 5 reviewers and all are positive about the proposed dataset presented in the submission. AC reads all reviews and discussions and agree with the reviewers. AC recommends to accept the submission as a spotlight.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper builds and extends the theory of infinite-width neural kernel computations. It explicitly computes the NNGP and NTK kernels for a wider range of activation functions. Most theorems proved in this paper are incremental, but still preserves enough significance.Neural Tangent Kenrel offers a compelling framework to (partially) understand some theoretical aspects of neural networks. In the infinite-width regime, it has been known that NTK with ReLU activation has a closed-form analytic formula which enables exact computation of the NTK in this limit. This paper proposes an approximate kernel computation method that leverages some tools from functional analysis.In this paper, the authors try to accelerate the computation of NTK with general activation functions. For the q-homogeneous dual kernel,  the authors extend the finite Hermite polynomials approximation technique. A method is proposed to compute the dual kernel of the derivative of activations without knowing the activations. The paper is well organized and well written.The paper is concluded with some experiments that show that the methods quickly (in terms of both wall-clock and number of series terms) and accurately approximate known NNGPs/NTKs. The authors also observe a 106 x speedup on a toy task compared with the kernel evaluation.	Most prior works on neural kernels have focused on using the ReLU activation. In this work, the authors provide new methods that can approximate multi-layered Neural Network Gaussian Process (NNGP) kernels and Neural Tangent Kernel (NTK) matrices for a wide range of activation functions. All the four reviewers recommended acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposed a transformer-based generative model, named anoformer, to detect anomaly for time series in an unsupervised manner. Anoformer combined the Transformer model and the GAN model by adopting transformer-blocks as the generator and discriminator module of GAN.The paper proposes a novel transformer-based GAN framework called AnoFormer for time series anomaly detection. The proposed model is enhanced by a two-step masking strategy consisting of 1) random masking and 2) entropy-based re-masking. The paper is easy to follow and well-organized. The performance gains are significant.This paper tackles the task of detecting and localizing anomalies in time series data. It follows a transformer approach where time series are automatically masked. The reconstruction error is used as an anomaly score. The method is evaluated on a few datasets and achieves better results than a set of deep learning approaches.	This paper proposes a transformer-based GAN method and a two-step masking mechanism for time series anomaly detection. The proposed method is demonstrated on a variety of datasets.   After rebuttals, both Reviewer 73J5 and Reviewer W6VQ remained negative. The main concern is the novelty and significance of the proposed method.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Existing methods do not utilize the multi-view geometry constraints in the learning. Using the pointclouds from SfM methods, we can generate implicit 3D representations. The paper heavily relies on existing structure-from-motion pipelines to compute the camera parameters.This paper introduces a new method for geometry-consistent neural implicit surfaces learning. The proposed method includes a theoretical analysis of the gap between volume rendering and point-based SDF modeling. The paper is well organized, and the evaluations are sufficient to support the proposed method.This paper presents neural implicit surfaces learning by enforcing explicit SDF constraints. It analyzed the gap between volume rendering integration and implicit SDF learning. Based on that, it proposes an explicit surface point SDF supervision. The results demonstrate high quality 3d scene geometry reconstruction.The authors of this manuscript propose Geo-NeuS, a novel neural implicit 3D reconstruction method. They combine a SDF-based neural implicit surface representation, that is usually only optimized with the reconstruction loss. The proposed method clearly improves quantitatively (Tab 1) as well as qualitatively (Fig 4) over the state-of-the-art.	This paper introduces new and useful losses, presents a good experimental setup, supply analysis on the bias, and is clearly written. I encourage the authors to discuss similarities and differences to NeuraWarp and pointcloud->SDF methods in their revision.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.SignCorpus is the largest ever collected sign language pretraining dataset. This is very useful for finetuning neural network models for multilingual sign identification and finger spelling. Overall the paper is very well structured, easy to follow and critical for sign language ML research in my opinion.This paper focuses on resource scarcity across sign languages. The authors release a multilingual sign language dataset SignCorpus. They also release a pre-trained sign language model Sign2Vec.The experimental results show realistic improvements on multiple benchmarks and competitive results on the others. The data gathering procedure is well described and justified, using techniques inspired of other tasks, datasets and models that have similar obstacles, such as low resource resources.The authors propose a new sign language dataset that contains sign language videos in different languages. The authors also propose a novel network,  Sign2Vec, which is a large-scale pretrained model for sign language tasks.This submission introduces a multilingual benchmark of signed languages. The benchmark includes both fingerspelling and sign recognition. The key weakness of the submission is the lack of human toplines for native signers for each of the 10 languages.The submission refers to OpenHands library, developed and presented at 60th Annual Meeting of the Association for Computational Linguistics. In this work, several sign multilingual datasets are introduced, described, and modeled. The sequences of pose keypoints for multilingual videos are the greatest contribution of this work.	This paper presents a significant collection of unlabelled signing data (4.6K hours) across 10 sign languages, which have been pre-processed and converted to pose-keypoints to remove identifiable information. The paper also presents Multi-ISLR a multilingual dataset with label-alignment extracted from 11 other labelled datasets for 7 sign languages. Furthermore, the paper also provides a multilingual model, Sign2Vec, pre-trained on the unlabelled data, and fine-tuned on the labelled data, which shows the SOTA results for known tasks. Lastly, the paper also introduces a dataset for fingerspelling across 7 languages. It's likely that this work will pave the way for new research on multilingual signed language tasks.  **pros:** * The paper provides a significant expansion for unlabelled and labelled signed data (previous work focused only on Indian Signed Languages) * The data gathering procedure is well documented and justified * The experimental results provide evidence of the value of the data.   **cons** * For the SignCorpus there is a lack of a human verification process, that assesses the quality of the pose keypoints; i.e. whether they can be used by native signers to extract the target gloss.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an approach to tackle the top-k elements in the ranking problem. This could be very helpful in domains like recommender systems, information retrieval, ... The document mainly focuses on the Borda count algorithm and its connection with other approaches.The paper is well presented, the notation is easy to follow and overall the model and the objectives are described clearly. A point that is left quite not tackled is any possible dependency of the method complexity on the size K of elements selected as the most highly ranked.The paper is well-written and I found it to be easy to follow. Additional experiments with various settings support the effectiveness of proposed method. The time complexity of proposed BC algorithm has been lowered compared with the other methods.Algorithm 1 can be applied to relatively large scale applications. Authors showed the conclusion mainly by a concentration bound with some further computations on the parameters. The authors studied the sample complexity of Algorithm 1 on more general cases.	Meta Review: This paper studies the problem of top-K recovery from a fixed set of queries. In each query, the user is presented m items and they select the item with the highest noisy utility. The authors propose a simple solution to this problem, take top-K most frequently chosen items, and prove that it is near optimal. The proposed solution is simple and valid only under strong assumptions, the utility of the query is additive in its items and the item utility noise is independent. Nevertheless, the problem is studied in depth, including a lower bound and comparison to model-based approaches to solving the problem. All reviewers liked the paper and I support acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes combining model-based RL with high-level skill learning and composition through hierarchical RL, into a single reinforcement learning framework. The approach consists of multiple learned components working together. I wonder how stable learning of such dependent separate components is, and how much tuning is required.The paper deals with learning reusable hierarchical skills in sequential decisions. The experiments are done in the locomotion domain and show better or comparables results to DREAM. Despite its computational complexity, the paper is original and sufficiently empirically motivated for acceptance.The authors propose an extension of the Dreamer method to incorporate high-level planning via the cross-entropy method and temporally extended skill policies. The experiments show that the method is at least as good, and sometimes better, than Dreamer when training from scratch, and universally (although marginally) better when adapting to new tasks.This paper presents a model-based RL approach that learns a high-level plan generator that produces a sequence of latent continuous variables as skills for a task. The approach is shown to be more sample efficient than a recent model-base RL approach (Dreamer)	This paper proposes a unified model-based framework for high-level skill learning and composition through hierarchical RL. The proposed approach combines high-level planning in a low dimensional space with low-level skill learning, where each low-level skill is a policy conditioned on the high-level task. The low-level policies are learned by using a mutual information objective. The proposed approach is evaluated on locomotion tasks, and is shown to be overall more data efficient than alternative baselines. The reviewers agree that this work is original and sufficiently empirically motivated for acceptance. Two reviewers were concerned by the experimental setup and the transfer setting that are somehow too simple, but the authors fixed these issues in the improved version based on the feedback.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a non-autoregressive (non AR) way to perform text to speech synthesis. It uses a VAE based setup adapted from the recent image paper NVAE to build two stacks of hierarchical VAE blocks. The key claims are that it results in improved speed, and reduced model footprint.This paper presents BVAE-TTS, which applies hierarchical VAEs to the problem of parallel TTS. The main components of the system are a dot product-based attention mechanism that is used during training to produce phoneme duration targets. The quality of the speech produced by the system is only evaluated on a single dataset.This paper combined fastspeech with a hierarchical VAE to achieve parallel and high quality text-to-mel syntheisis. The key strength of this paper is the architecture is new. My opinion is that it's a borderline for ICLR, since the importance of the proposed VAE was not well justified.BVAE-TTS is yet another non-autoregressive speech synthesis model. The paper is well written and results are strong, although I would prefer if the method itself were explained more clearly.	Non autoregressive modelling for text to speech (TTS) is an important and challenging problem. This paper proposes a deep VAE approach and show promising results. Both the reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper. This paper will not be the final VAE contribution to TTS but represents a significant enough contribution to the field to warrant publication. It is highly recommended that the authors take into account the reviewers' comments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose and measure the performance of two SOTA methods such as Grad-cam and IRNet for the generation of weak-labels. Recent literature indicates that the use of semi-supervised methods in segmentation is not the most appropriate strategy.Fusion of weakly supervised with a fully supervised approach to address the challenge posed by the availability of annotated data is very interesting. Simple idea opens up the possibility of deploying CNN-based semi-supervised approach for semantic segmentation of Chest X ray in hospitals.The paper tackles an interesting problem in how to train segmentation networks. It builds on top of recent advances in weakly supervised and semi-supervised learning. The paper is mainly an application paper with limited methodological novelty but seems to lack some experimental rigour.The results of this paper are promising as it clearly shows how global image labels can be leveraged in radiography. Main weakness of the paper relates to the lack of baselines. The idea of using grad-CAM to generate saliency maps as weak labels is not new as reported by the authors.	The motivation for using semi-supervised learning in medical imaging is clear in order to reduce the cost of acquiring dense expert annotations. This paper presents an interesting study on applying recent semi-supervised learning methods to chest x-ray segmentation. There are some conflicting reviews here but in general, the reviewers find the work interesting enough to be presented but with too limited technical novelty to justify an oral presentation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Authors propose a novel algorithm for multi-batch reinforcement learning with nearly minimax optimal regret and $O(H + \log\log K)$ batch complexity. Authors propose polynomial time implementation, and provide a lower bound on batch complexity to show that their algorithm is nearly optimal in this parameter too. Strengths: Nearly optimal regret bound in the first term, nearly optimal batch complexity, iterated logarithm of policy switches.This paper studies the episodic reinforcement learning (RL) problem modeled by finite-horizon Markov Decision Processes (MDPs) with constraints on the number of batches. The technical contribution includes a near-optimal design scheme to explore the unlearned states and a computationally efficient algorithm to explore certain directions.This paper studies multi-batch reinforcement learning in the finite-horizon MDPs. They propose an algorithm that achieves near-optimal regret bound $O(\sqrt{SAH^3K})$ in $K$ episodes. The proposed algorithm has a strong theoretical performance that nearly matching the lower bound.The paper studies a batched reinforcement learning (RL) problem where the interaction is divided into $M$ epochs (‘batches’). The agent chooses the batch lengths before the interaction starts and can only update its policy at the beginning of a new batch. I think that the framework is interesting and the results are nontrivial.	The paper studies the batch RL problem, in which the algorithm first decide a switching schedule and then switch the policies based on this schedule. The proposed approach achieves a good regret upper bound matching existing non-batch algorithms (although the lower-order terms are still large). The batch complexity on the other hand matches the lower bound (up to log factors).   The reviewers believe that the theoretical contributions are solid and qualified to be published in NeurIPS. The authors did a good a job in addressing the computation complexity in the rebuttal phase. The meta-reviewer suggests the authors to further clarify presentation issues. Also, it would be good to cite the recent RL theory papers in the tabular setting (including those with a generative model).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a tensor decomposition leveraging contrastive self-supervised learning based on the data augmentation for positive samples and the unbiased estimation of negative samples. The proposed method optimizes the objective by using an alternating least squares algorithm.The authors present  "augmented" tensor decomposition. The objective is designed for downstream tasks (by way of "alignment") in addition to data reconstruction. The quality of the decomposition is determined not only by reconstruction error, but by correct clustering on features.This paper propose a novel self-supervised framework for CP Tensor Decomposition. An iterative method is developed to enable  an alternative least squares fashion. The experiment results on various datasets show its efficiency.In the paper, the authors investigated a new variant of CP decomposition (CPD) for the classification task. Compared to the classic CPD, the new one imposes the self-supervised loss. Better embedding spaces, modeled by latent factors, can be learned by augmenting the training data.	The authors proposes augmented tensor decomposition (ATD) to adapt data reconstruction towards the downstream tasks, e.g., appropriate feature clustering.  It leverages data augmentation and self-supervised learning, with optimization accomplished akin to alternating least square (ALS).  Significantly superior performance is obtained in experiment, compared with existing CP methods and ALS.      All the reviewers, including myself, find the paper a solid contribution to the methodology and analysis. There were a few concerns and clarification requests, and the rebuttal did a good job addressing them. These additional results and insights can be included in the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a benchmark to measure the quality of the systems in the growing domain of code intelligence. The authors presented three Transformer-based systems (with BERT-style encoder, GPT-style decoder and encoder-decoder) to set up baselines.This paper proposes a comprehensive benchmark dataset for program understanding/generation. It consists of 10 tasks across 14 datasets, where 5 datasets are newly introduced. The collection of these tasks cover code-code, text- code, code-text and text-text settings.This paper presents a benchmark dataset focused on source code processing (understanding and generation) tasks. The paper describes each of the tasks and datasets, and also provides strong baseline models based on pretrained language models and the corresponding results.	The paper proposes a comprehensive benchmark for program understanding and generation consisting of 10 tasks across 14 datasets. All the reviewers agree that this dataset has potentially be used as a standard benchmark to measure progress in this domain. Therefore, I recommend acceptance of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper explores a very important question in dynamical system identification of how to make recurrent neural networks (RNNs) learn both long-term and short-term dependencies without the gradient vanishing or exploding limitation. They suggest using piece-wise linear RNNs with a novel regularization technique.This paper proposes a type of regularization for piecewise linear RNNs (PLRNNs) that encourages the network to learn line or plane attractors. The paper argues that this regularization alleviates the vanishing and exploding gradient problem.The paper proposes a novel regularization term to PLRNN. I think the idea is quite novel and interesting. But the experiment section is confusing by missing some explanations. And the presentation of the neuron model is not sufficient enough to prove that rPLRNN find interesting and interpretable dynamics. vanilla RNNs simplify the analysis as dynamical systems without those gates in LSTMs and GRUs while suffering exploding or vanishing gradients. This paper proposes a regularization scheme for training vanilla Relu RNN to tackle the exploding and vanishingGradients issue. The experiments show the competitive performance comparing to LSTM.	This paper describes a clever new class of piecewise-linear RNNs that contains a long-time scale memory subsystem. The reviewers found the paper interesting and valuable, and I agree. The four submitted reviews were unanimous in their vote to accept. The theoretical insights and empirical results are impactful and would be suitable for spotlight presentation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present a novel approach to discovering disentangled latent variables from observations and auxiliary data generated by a latent variable model. They present identifiability results for their approach (up to permutation), and present empirical comparisons with related work on synthetic data.This paper proposes to achieve disentanglement of latent factors by simultaneously learning them with the sparse causal graph that relates them. Building on recent results in the nonlinear ICA literature, permutation-identifiability is proven under a number of assumptions. The paper is written and structured clearly and easy to follow.The paper is very well written and easy to follow, despite several more complex statements. The proposed method is evaluated on two synthetic datasets. While the proposed method outperforms the shown baselines, it is not clear how robust the method is. The only major drawback of this paper is the limited experimental evaluation.	The reviewers unanimously acknowledge the clarity of the paper and the relevance of the theoretical contribution. One weakness pointed out by reviewers is the limited experimental evaluation. Overall, this constitutes a good theoretical contribution at the intersection of causal representation learning and nonlinear ICA, which we recommend for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies procedural image programs. Procedural image programs are image representation learning methods based on synthetically generated unnatural images (e.g., fractals and noise) The authors collected OpenGL fragment shader codes to generate diverse synthetic images and trained their image representation model on this dataset.Manuscript presents a method to train neural networks over synthetic data. The work collected a large collection of 21K OpenGL programs for rendering diverse set of synthetic images. Experiments show that the collected programs can be used for pretraining in both supervised and unsupervised fashion.This paper proposed a dataset for learning representations without access to real images. The dataset is composed of twenty-one thousand OpenGL programs. The synthetic data can help reduce the concerns about using real images in training such as human face.	This work presents a method for training neural nets on synthetic data. This data is collected from a collection of thousands of OpenGL programs that rendered images, which are then used for representation learning. The big advantage of this approach is that it avoids of a lot of the biases that are present in natural image datasets. The proposed method is competitive for supervised and unsupervised scenarios. I find the results, especially those with finetuning (as done during the rebuttal period) relatively compelling.   While I agree with reviewer vdrw that, on the whole, the major contribution (an image collection) is not very strong, I still think this kind of approach will be widely interesting to the NeurIPS community. Precisely because the work shows carefully (albeit empirically only) that procedurally generated datasets could be useful for representation learning, especially if you want to avoid the various pitfalls of natural image sets.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal here is multi-task learning and generalization. The expected one-step reward for any member of the task family can be written as $phi(s,a,s')^T w$. The authors propose universal successor features (USF) $\psi$s. Using USFs, the Q values can be interpolated across policies and tasks.The main problem - to me - with described approach is that the Q* value now lives in a much higher dimensional space. This paper proposes new ideas in the context of deep multi-task learning for RL. Ideas seem to me to be a rather small (epsiilon) improvement over the cited works.This paper considers the challenging problem of generalizing well to new RL tasks. The main approach is a combination of two recent approaches: Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI)	This paper addresses an importnant and more realistic setting of multi-task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi-task RL problem.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a semi-supervised approach to perform salient object detection in videos using a combination of pseudo-label prediction and adversarial noise reduction training. The authors show the result on one of the biggest VSOD dataset (DAVIS) as well as 4 other dataset.This paper proposes a pseudo label generation and processing algorithms. The proposed UGPLG generates temporally consistent pseudo labels. The adversarial learning strategy can suppress the noise. The experimental results show that the method can achieve better performance with fewer gts.The paper presents a method for semi-supervised video salient object detection. It adopts Cross-Frame Global Matching Module (CFGMM) to exploit temporal information. Adversarial learning is adopted to generate reliable pseudo-labels. It uses Uncertainty-Aware Dual Decoder Module (UADDM) to locate uncertain regions.	In this paper the authors propose an approach for semi-supervised salient object detection using a combination of pseudo-label prediction and adversarial training, showing improved results on a number of benchmarks. Some concerns about more detailed analysis of aspects of the approach were raised, but seemed to be mostly addressed in the authors’ response. Some reviewers also expressed concerns about there being sufficient technical contributions, but seemed satisfied enough with the analysis and strong positive results.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper describes dynamic residual adapters designed to adaptively account for latent domains. This framework injects adaptivity into networks, preventing them from overfitting to the largest domains in distributions. The approach closes a large amount of the performance gap to domain-supervised solutions.This paper studied how to learn a neural network with multiple domains without knowing the exact domain label (by merging all the domains as a large domain) Then they proposed dynamic residual adapters and weighted domain transfer to address this issue.The authors propose a method for latent domain learning. The method consists of two parts: dynamic residual adapter and weighted domain transfer. Empirical results show that the method brings improvement to minority domains.The paper claims to contribute a new method *dynamic residual adapters (DRA) coupled with *weighted domain transfer (WDT) to tackle *latent domain learning. The proposed method improves the model performance against small domain datasets.	While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors show a new image processing through DL aplication set-up. They use semi-automatically annotated images and simulations as ground truth. The results show that the network could be able to generalize and be more accurate than the annotations performed to train it.The work is well motivated and the paper is well written and easy to follow. The major drawback is the missing manual ground-truth here. The simulation of training data is not well explained. The improvements in speed seem rather moderate.The authors replaced the u-net architecture to replace reconstruction using  phase-shift minimum intensity projection. However, they did not compare their approach to a convention method that do reconstruction using CNN then do the binary segmentation for the reconstructed technique.This can find good use in lab settings where actual images are not needed. It is similar to the old ideas where people were trying to detect detection in compressed videos without decompressing videos. The technical novelty is limited as the authors used readily used tools from the literature to achieve their goal.	the reviewer's opinion was split on this paper, but after rebuttal no major concerns remained so I recommend acceptance. However, the authors should aim to incorporate the answers given to the reviewers concerns in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A new way to automatically construct and train variational families for models expressed as general probabilistic programs. Given a program $p$, it first generates many (prior) samples from $p$ to identify a collection of control flow paths that collectively have high mass under the prior. Then, a separate variational family is trained to target the restriction of the model to each control flow path. During training, control-flow paths with low ELBOs are periodically pruned.This paper proposes a novel way of constructing a variational guide by breaking down the problem into sub-programs. The paper demonstrates that this approach results in improvements in inference performance. The proposed method seems to be a nice and useful addition to probabilistic programming languages.Support Decomposition Variational Inference (SDVI) targets probabilistic programs with stochastic support. Main idea is to perform variational inferences separately on sub-programs with static support. The paper is clearly written and the structure is easy to follow.The authors address the problem of designing variational proposals for universal probabilistic programs with stochastic support. They demonstrate their method's performance on a toy program, an infinite mixture model, and learning Gaussian process kernels in a PCFG. Their results appear to clearly set a new state of the art in log-predictive density and ELBO.	The reviewers have reached consensus after processing the authors' feedback. They all agree that this manuscript presents an interesting approach to applying variational inference in a setting of probabilistic programming that is of interest to the community. The reviewers raise tangible points that the authors have incorporated into their revision. I recommend that the authors continue to polish their manuscript to clearly address these points in the final version of their manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper revisits a broad range of vision and language tasks (VQA, VLN, SNLI-VE, Image Captioning) and evaluates the would-be performance for previous state-of-the-art methods. It’s not surprising that the CLIP-based vision backbone - after being trained on 400M image-text pairs - can outperform ImageNet-trained baselines.CLIP's resnet-based visual encoders consistently outperformed their Imagenet pre-trained counterpart. The authors found that CLIP's ViT- based visual encoder performed far worse than the resnets.This paper performs empirical analyses of applying the CLIP to various vision-and-language tasks. It demonstrates the potentials of the model in generalizing to different downstream applications, and provides some suggestions for model deployment. Experimental results show that CLIP pretraining leads to competitive performance.This paper explores how features from CLIP affect the performance of vision-and-language models. The experiments suggest that the simple change to CLIP offers significant benefits over commonly used encoders such as BottomUp-TopDown.	Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks.  There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is difficult to parse. The significance is unclear, especially given what appears to be a significant gap between the theory and the experiments. The presentation of the parallelized algorithm has the flavor of being dishonest. The experiments are very far away from the theory.New method has interpretation of Frank-Wolfe algorithm. To decrease the variance of the dual gradients, one can use a parallel mini-batching, which provides the method with a provable acceleration given a set of parallel computational units.In this paper, the authors start with a general framework that can solve composite problem G(y)+H(y) where G can only be accessed via a stochastic gradient oracle. This is in turn applied to solving a constrained convex function, where the framework is evoked for its smoothed dual problem. The resulting Frank-Wolfe-type algorithm is shown to have a convergence rate in the duality gap of O(1/sqrt(eps))	The authors design an algorithm for composite stochastic optimization that leverages both smoothness and strong convexity with respect to the same (general) norm, using a stochastic counterpart to recent work by Diakonikolas and Guzman. They then show how to leverage this algorithm and randomized smoothing in order to create an algorithm for constrained smooth convex optimization based on exact gradient evaluations and linear optimization computations. Compared to Frank-Wolfe, the algorithm requires strictly less gradient evaluations and parallelizes the same amount of linear optimization computations.  The paper received generally favorable reviews, with the exception of reviewer 3QVT who did not engage in discussion and whose critique I found unclear. I agree with reviewer rQnJ’s assessment that even though “all the building block are quite known in optimization community (accelerated methods, duality, Bregman distances, smoothing, etc.), the whole approach fits perfectly together and provides the reader with a number of nice and useful observations.” Consequently, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper leverages the recent breakthrough of Masked AutoEncoders (MAE) for the domain of robotics. It shows that visual knowledge can be transferred from different domains such as ego4d into specific robotics domains thanks to MAE. The paper finds that MAE is superior to CLIP pretraining.This paper evaluates vision encoders pretrained using masked autoencoders on large scale datasets. The baselines compare to only ViT architectures. These are generally harder to train than simpler conv architectures like ResNet.This paper shows a way to use large scale visual self supervision of in-the-wild, massive datasets, and use it to improve behavior cloning on robots. Specifically they use the Masked Autoencoder (MAE) framework of He et al, on a massive dataset of {ImageNet, Ego4D, etc. image frames.This paper scales up vision models and data in order to train better vision backbones for visuomotor policies using behavior cloning. The authors conduct extensive experiments to show that scaling both the model and data substantially improves performancce on these tasks.	This paper uses a masked autoencoder as visual pretraining for robot manipulation tasks. The paper shows strong results and received positive initial reviews.  The additional experiments provided during the rebuttal strengthen the paper's contribution claim.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies the classification group robustness of large-scale image-text pre-trained models. The paper shows that neither a naive version of the zero-shot, nor linear probing on the target dataset, is the optimal choice.This paper studies the group robustness of foundation models. It shows that large scale pretrained foundation models can still have a bad robustness on some group shifts. Then the paper proposed a contrastive adapting method that take advantage of supervised contrastive learning.The problem of group robustness (in terms of worst-group performance) is particularly challenging when some groups are underrepresented in the training data. Classical approaches use under/over-sampling, reweighting, and two-phase training to deal with this issue. The authors propose to use contrastive adapters without retraining from scratch.	This paper received unanimous recommendations of acceptance from the reviewer. The authors did a good job addressing concerns from the reviewers, especially with the additional ablation studies to decouple the gains from other techniques such as SupCon. The AC agrees with the reviewers regarding the contribution of this paper and recommends acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the problem of identifying (discovering) synonymous entities. The paper proposes using the "contexts" of the entities as they occur in associated text corpora. The key novelties of the approach lie in the "matching" system used. Experiments are conducted on three different datasets to show the efficacy of the proposed approach.The paper presents a neural network model (SYNONYMNET) for automatically discovering synonymous entities from a large free-text corpus with minimal human annotation. The solution is fairly natural in the form of a siamese network, a class of neural network architectures that contain two or more identical subnetworks.It is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. The coverage of prior work ignores the relevant work of Gupta et al. 2017 EMNLP. The paper is full of English mistakes.	This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.  The main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross-attention mechanisms).  Adding those experiments would make the experimental setup stronger.  There is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed method is novel and well-motivated. The paper is written well. The motivation is clear. The reference is thorough and well described. Please provide more details about the proposed method. Maybe provide a visual result to demonstrate the performance.A combination of U-Net, memory network and Hough voting from three views sounds novel. Paper is well written and easy to follow. The problem aiming to solve is clinically relevant. Some details need to be clarified.The proposed use of a memory network in addition to U-Net is a novel idea. Results are encouraging and show the potential of the proposed method. The paper misses a lot of literature review for instance segmentation in medical imaging.The paper is well-written and easy to follow. The proposed method achieves state-of-the-art results. Experiments on a large-scale cross-sectional MR imaging study shows the effectiveness of the proposed method.	All reviewers found interest and merit in this paper, some negative points were satisfactorily addressed by authors. The paper should be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a Hermite variational auto-encoder which use Ornstein Uhlenbeck Semi-group to p(z_i|z_ i+1) which i denotes the latent layer number. It has clear theoretical inspiration and had solid analysis on variance reduction.This paper studies the training of deep hierarchical VAEs and focuses on the problem of posterior collapse. It is argued that reducing the variance of the gradient estimate may help to overcome this problem. The authors provide an analytical analysis of bias and variance. Last they train multiple VAEs models, measure the posterior collapse and observe a phase transition behaviour.Algorithm works by smoothing the Gaussian parameters w.r.t. the centered Gaussian rv. The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers. It is possible to construct models where the ELBO has a reasonable value but the smoothed objective behaves catastrophically.	This paper develops a smoothing procedure to avoid the problem of posterior collapse in VAEs. The method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers' concerns. However, there is one remaining issue that would require additional discussion. As identified by Reviewer 1, the analysis in Section 3 is only valid when the number of layers is 2. Above that value, "it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically". Thus, the scope of the analysis in Section 3 deserves further discussion. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work considers the graph task of learning node representations that are invariant to small edge perturbations. It achieves this through a data augmentation procedure that samples new “fake” edges and regularizes the GNN equivariant representations to be unable to predict these fake edges.The paper falls well within the scope of the conference. Its writing could be improved and would benefit from a thorough language revision. The state-of-the-art is appropriately identified and even though its novelty is somehow incremental, its value seems beyond doubt.The paper propose an unsupervised method for self-training of graph-neural-networks. The authors provide information-theoretic justification to their method. Their approach is based on maximizing the mutual information between a perturbed graph topology and its node representation.This paper develops a framework for unsupervised learning of graphs. The goal is to build graph representation using an encoder that is useful for downstream tasks such as graph classification. The decoder is tasked with minimizing the conditional entropy of the perturbation.	The paper is concerned with learning transformation equivariant node representation of graph data in an unsupervised setting. The paper extends prior work in this topic by focusing on equivariance under topology transformations (adding/removing edges) and considering an information theoretic perspective. Reviewers highlighted the promising ideas of the approach, its relevance for the ICLR community, and the promising experimental results (although improvements over prior work are not necessarily significant on all benchmarks).  However, reviewers raised concerns regarding the novelty of the method and the clarity of presentation with respect to key parts of the method. These aspects connect also to further concerns raised, e.g., related to mathematical correctness as well as the significance of the proposed loss function, the benefits of motivating it from MI, and the improvements over GraphTER. The rebuttal didn't fully clarify these points. While the paper is mostly solid, I agree with the reviewers' concerns and -- currently -- the paper doesn't clear the bar for acceptance; it would require another revision to improve upon these points. However, I'd encourage the authors to revise and resubmit their work with considering this feedback.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, their goal is to improve calibration and accuracy by augmenting a classification model with a GP. They base their model off RIO (ICLR 2020) which targets regression problems. They propose a model, RED, which instead tries to predict the residual between the predicted confidence score for the true class and 1.The method is straightforward to implement and performs well against the baselines considered on classification tasks for 125 UCI datasets. It is not demonstrated whether RED would outperform other confidence scoring and OOD detection methods. Unless there is a good justification for the limited set of baselines, I believe the paper's claims to generality are limited.This paper solves an interesting problem of predicting uncertainty in NN without re-raining/modifying the existing NN. It would be good to apply SVGP directly to some of these datasets and compare the results against NN+SVGP results.Red is a new methodology to produce reliable confidence scores to detect missclassification errors in neural networks. The idea is to combine kernels based on both input and output spaces to define a (sparse) GP that estimates the residual between the correctness of the original prediction and the maximum class probability.	In this paper, the authors use a GP classifier to detect if the output of a NN classifier has been decided correctly. The GP takes as input the original input vector x and the output of the NN, i.e. the calibrated posterior probabilities given by the NN. It uses that as an input vector for the GP classifier to decide if the sample was correctly decided. The output of the GP will serve as confidence in the output of the NN. The results are comparable/superior with the state-of-the-art and the authors have repeated the experiments with over 125 different datasets. The reviewers of this paper were all cautiously positive about the paper, but all of them pointed towards the reduced novelty of the paper. Also, none of the reviewers were willing to champion this paper as a must-have at ICLR 2021.   For my reading of the paper, I would tend to agree with the reviewers’ comments. Also, I find that using the same NN, rather shallow, with the same configuration for all the datasets seems rather limited. Given that this method is independent of the underlying classifier and that the databases used are low dimensions and a low number of training examples, I would have liked to see what a random forest or a GP can accomplish. Also, I would have used bigger NNs that can be trained to overfit the sigmoid outputs for classification of higher accuracy. I believe that having a diversity of underlying classifiers is more relevant than having 125 datasets. We need to find the best classifier or ensemble and then apply the different mechanisms for estimating if the output is the correct one. Otherwise, the proposed method might only be workable for this specific NN configuration. In the tables, it can be hinted that this might be happening, as about 80% of the cases MCP and RED are indistinguishable in the AUROC values.   Also, for all of these datasets a GP could be used as an underlying classifier, and given the premises of this paper, the authors could check how well calibrate a GP classifier is. Also, there has been considerable work on calibrating NNs when they are trained to overfit. Comparing with those methods should be straightforward, as they provide more information than just a confidence score. This is probably the most influential paper: https://arxiv.org/abs/1706.04599 (1000+ references), but there are some recent papers too.     Finally, if the goal is to use a GP to detect if the classification done by the NNs is accurate, using a GP might be an overkill, as the complexity of the GP, especially for large datasets might end up being larger than the underlying classifier.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a bootstrap framework to study the generalization problem of deep learning. It decomposes the traditional test error into an ‘Ideal World’ test error plus the gap between. Empirically, it demonstrates that such gap (soft-error) is small in supervised image classification.This paper defines a metric of generalization gap between the ideal world and real-world via 'bootstrap approximation', and seeks to use his gap to explain some phenomena. I have respect for this paper trying to define some statistical terms in the deep learning society, but I have a very strong concern about the novelty.The authors propose a bootstrap framework for understanding generalization in deep learning. The bootstrap error (in terms of soft error) is consistently low in image classification settings across a number of architectures. The authors then provide empirical evidence to demonstrate that same techniques perform well in both over and under-parameterized regimes.This paper studies generalization through a novel "bootstrap" framework relating the test loss from training on a finite fixed training set to the loss when training on fresh samples at each iteration. The empirical bootstrap phenomenon is very interesting and the experimental results are quite surprising.	The paper is proposing a new framework for understanding generalization in the deep learning. The main idea is considering the difference of stochastic optimization on a population risk and optimization on an empirical risk. The classical theory considers the difference of empirical risk and population risk. This basically translates the practical motivation from finding good function classes to finding good optimizers which can re-use the data effectively. Although the paper provides no theoretical result, it provides an interesting empirical study. The paper somewhat demonstrates that SGD on deep networks is somehow good at re-using the same data. I believe this angle is very novel and might hope to future theoretical discoveries. The paper is reviewed by four reviewers and two of them argue its acceptance and two of them argue rejection. After discussion, this status remained and I carefully read and reviewed the paper. Here are the major issues raised by the reviewers:  - R#1: The paper is missing a theoretical study. The implications on the practical deep learning is not clear. - R#2: Choice of the soft-error is particular to the task and how to go beyond soft-max is not clear. - R#3: Finds the paper not novel as well as trivial or hard to understand. - R#4: The choice of soft error is ad-hoc.  I believe the issues raised by R#3 are not justified. First of all, novelty is very clear and. appreciated by other reviewers. Moreover, the paper is rather easy to understand and the results are very farm from trivial. However, the other issues raised by other reviewers are valid. Specifically, soft-error seems to be a limitation of the study. However, the authors respond to this concern and reviewer increases their score. I believe the theory is lacking but the paper is simply showing this novel approach and its empirical validity. A theory to explain this phenomenon would be amazing but not necessary for publication. Similarly, without a theory it is hard to expect any practical implication. Overall, I believe the paper is an interesting and novel one which will likely to lead additional work in the area. Considering we are still far from a satisfying theory of generalization for deep learning and the role of the optimization is clear, this angle worth sharing with the community. Hence, I decide to accept. However, I have some concerns which should be addressed by the camera-ready.   - Claims should be revised and authors should make sure they have enough evidence for them. For example, authors provide no satisfying evidence for random labels or very limited evidence for pre-training. I strongly recommend authors to either remove some of these discussions or present in a fashion which is not a result but part of the discussion for future research. - A section about limitations should be added. Specifically, the soft-error choice should be discussed in this limitation section. - Discussion section should be extended with the pointers to the relevant work on bootstrap literature as well as suggestions to the theoreticians. Not providing any theoretical result is always fine but authors should understanding why is it hard to make theoretical statements and where to search them.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper first demonstrates that the model shifts.---the difference between the updated model and the model before updating---hinder the monotonic improvement of model-based RL. To tackle this problem, the paper proposes CMLO, which introduces an event-triggered mechanism.This paper studies how to ensure optimization monotonicity of learning an accurate dynamics model for MBRL. They derive a lower bound for the derived policy performance improvement that depends on the one-step dynamics prediction error of the current model.The paper focuses on the monotonic improvement for model-based reinforcement learning. The primary reason for the model bias is a mismatch between the samples in the model learning stage and the policy optimization stage. To tackle the same, they formulate a constrained bi-level optimization framework for the MBRL problem.The authors consider an important problem in RL, the inconsistency of policy updates under shifted models. The monotonic results are not novel in MBRL. The intuition behind the event-triggered equation 5.1 is also not very clear. What will the fraction of state coverage give?	This paper studies the relation between model shift and the performance of model-based reinforcement learning. The paper proposes a new algorithm that leads to empirical improvement over certain data sets. All the reviewers agree that the paper provides useful theoretical insights into model-based reinforcement learning, and the experiments are also consistent with the theory.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal generation strategy is strengthened by two heuristic ideas, namely adding hindsight and diversity constraints. At a high-level, the approach could be thought of as finding a set of options (along with finding option goals and initiation sets), and then searching for the kind of behavior that maximizes option value.Reinforcement-learning scheme intended to strike balance between exploration and exploitation. Paper proposes decomposing state space into regions and developing policies for "solving" each region. Paper needs a detailed editing pass. Comments (hopefully helpful!) follow.Clarity: The paper could benefit from a clear presentation in order to help properly understand the proposed methods and all its components. The paper designs an intrinsic reward to encourage exploration while avoiding local optima policies. Exploitation is leveraged through a region-based memory.The paper presents a new algorithm called Regioned Episodic Reinforcement Learning (RERL) It combines ideas from episodic memory, with automatic sub-goal creation or “goal-oriented’ RL. The method works by dividing the state space into regions, where a different goal identifies each region.	This paper introduces Regioned Episodic Reinforcement Learning (RERL), which partitions the state space by generating a diverse set of goals and then explores the state space by learning policies that reach those goals. This idea is a combination of episodic memory techniques and “goal-oriented” reinforcement learning.   After the authors’ responses and the discussion phase, all reviewers converged to recommending the rejection of this paper. The main concerns regarding this paper are:  * Presentation. The proposed approach is not that well justified, some of the claims in the paper are quite imprecise, and there’s relevant related work missing. * Evaluation. The evaluation sometimes feels rushed and is not held in a diverse enough set of tasks, not capturing important properties one would want to capture.  I recommend the authors to pay close attention to presentation, as well as the experiments and analysis in order to make the paper stronger.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.InterPOLE uses Bayesian techniques to estimate decision dynamics as well as decision boundaries. Results on simulated and real-world domains show that their method explains the decisions in behavior data. The application to healthcare is useful and interesting.The paper proposes an algorithm for learning policies and internal models ("decision dynamics") from demonstrations. Offline experiments on a healthcare dataset show that the method learns interpretable decision dynamics, recovers biased internal models, and accurately predicts actions relative to prior methods.In terms of alternative approaches to modeling agent/decision maker behavior, I can think of at least one other alternative and I wonder how your method can be compared to it. ‘Agent Markov Model’ introduced by Unhelkar and Shah (Learning Models of Sequential Decision-Making with Partial Specification of Agent Behavior AAAI 2019) is also modeling agent behavior using a state space model.	Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning  The topic is maximally timely and important: Understanding human decision-making behaviour based on observational data. Any tangible steps towards this challenging goal are bound to be significant, and those this paper makes.  A Bayesian policy-learning method is introduced for this task, and validated on both simulated data and user exeperiments in a real decision-making task. The novel contribution is on learning interpretable decision dynamics  The paper is written clearly enough..  The updated paper clarified most major concerns the reviewers had. In particular, they added a user study.  The biggest remaining weaknesses are that  - relationship to the AMM model did not become completely clear yet  - the real user study has been carried out with only a small set of users. But a large-cohort study would be too much work to ask for a paper which has also a strong methodological contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a novel solution to an interesting problem. When KBs are automatically expanded user feedback is crucial to identify incorrect and missing entity attributes and relations. The human computation aspects of the paper are lacking sufficient explanation in terms of implementation in real settings.This paper presents a hierarchical framework for integrating user feedback for KB construction under identity uncertainty. It is unclear about the algorithm implementation, such as what is the implementation of feedback mention. The example for constructing positve/negative feedback is too vague.This paper introduces a method to integrate user feedback into KBs in the presence of identity uncertainty. The proposed hierarchical model looks reasonable and effective. The paper makes an algorithmic contribution. The contribution is, however, limited from the perspective of human computation.	The paper presents an interesting methodology. The results are interesting, however the paper really misses out on an in-depth discussion and reflection of the pros and cons of this approach as well as on a proper related work comparison to similar approaches.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors analyze a self-supervised learning framework for downstream (supervised) few-shot classification. The results are strong, though are curiously relegated entirely to the Appendix. The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution.This paper performs theoretical analysis of the relationship between supervised learning (SL) and self-supervised learning (SSL) in the context of few-shot learning (FSL) It aims to quantify the gap in training loss between SL and contrastive SSL on FSL tasks.In conclusion, the proposed theory makes little sense and is also over-claimed. The whole study is neither theoretical nor logical. In general, the presentation of this paper is poor. One reason is using odd/strange terminologies and equation expressions. Quite a few equations are hard to read and understand.The paper proposes to theoretically analyze whether self-supervised learning can help FSL. The main concern is that Theorem 1 and 2 are quite loose. Is it work for any fk and fq? Can you provide more strict error bound?The paper establishes a relationship between self-supervised learning (SSL) and supervised few-shot learning (FSL) method. The whole analysis and proof are based upon the two main assumptions: mean classifier and balanced class training data. Theorem-1 shows that the supervised loss is upper bound by SSL loss by a linear relation.	This paper proposed to theoretically explain why a pre-trained embedding network with self-supervised training (SSL) can provide representation for downstream few-shot learning (FSL) tasks. The review process finds that the paper may over-claim the results and that the results seem unsatisfactory. Both Reviewer 4 and Reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. The paper needs a substantial revision to improve clarity and accessibility. As pointed out by Nikunj Saunshi’s public comment, this paper may benefit from discussing the differences from the previous works, including [1].    [1] Arora et al., A Theoretical Analysis of Contrastive Unsupervised Representation Learning, ICML 2019
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed approach of requesting expert feedback for the top ranked anomalies is straightforward and unsurprising, given past work on active learning. The experiments on synthetic data are also unsurprising. These are based on a questionable premise: the instances that are "hard" to classify are treated as anomalies.This is an interesting paper on a topic with real-world application: anomaly detection. The introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. The second, intuitive part of 2.1 is extremely helpful.Reviewer: Most claims of novelty can be clearly refuted. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. The math in the paper is mostly redundant.	Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The most significant concern raised is that there does not seem to be an adequate research contribution. Moreover, unsubstantiated claims of novelty do not adequately discuss or compare to past work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a novel combination of approaches to improve clustering using a supervised learning method. The experiments show favorable results on the toy dataset and are competitive with methods that use a prespecified clustering. It would help to have a real world task where the approach taken (learning the similarity metric followed by off the shelf clustering) is the best.The authors propose a metric learning and clustering method based on the idea of learning the metric from the context. They use the self-attention block module of the multi-head attention based transformer to embed the data and learn a kernel using the ground truth labels. They demonstrate their idea on a toy dataset and present results on the Omniglot dataset.The paper is well written and considers using the context in clustering which is an important problem. The proposed solution is not new. Clustering is a very old problem and there are known datasets from different domains to evaluate a new approach.This paper proposes a method for producing representations for clustering that take into account global trends in the dataset. They claim to achieve competitive clustering performance on omniglot, which they attribute to the use of contextualised embeddings.	This paper proposes to use context-based metric learning, where an attention/Transformer-based mechanism is used to incorporate neighborhood information for deep learning-based metric learning. This was initially demonstrated on two simpler datasets, although larger ones were added during the rebuttal. On the whole, reviewers appreciated the simplicity and intuition behind the idea, but the consensus among all of the reviewers found several aspects lacking, including: 1) clarity of the descriptions in the paper, 2) novelty compared to existing work, especially that of Set Transformer for clustering, 3) lack of convincing results compared to baselines, or at least analysis/justification for negative results. While the reviewers appreciated the authors' rebuttal and experiments, it did not address many of these concerns. The idea is interesting and seems to hold some promise, so the authors are encouraged to refine these aspects in order to fully explore this idea and submit to a future venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Confident Adaptive Language Modeling (CALM) to early-exit dynamically during the decoding process. The authors provide a theoretical guarantee that local confidence measures could lead to the confident generation of global outputs. CALM is practically useful to speed up the inference.This paper introduces the "Confident Adaptive Language Modeling" (CALM) framework. It contains very detailed study about the source of errors of doing early exiting during the decoding process of Transformer based language model. Several empirical results, on text summarization, machine translation and Squad demonstrated the usefulness of the proposed method.Transformer language models such as T5 are the most important tool in the current NLP world. This paper first analyzes these types of model with an oracle to show what the best case speedups could be. It then proposes a few new ways to do early exit, for the T5 model, evaluated on downstream tasks.Proposed framework, CALM, is to find a method that allows for exiting the forward passes as early as possible while guaranteeing performance bounds. The proposed method is shown to (with some caveats) result in potential speedup of up to 3x.	This paper studies the error of early exit in decoding Transformer Language models, and proposes a method CALM to calibrate and accelerate model inference. Experiments on a variety of tasks (summarization, MT, QA) show effectiveness of the proposed method.  All reviewers find the paper solid and the author feedback convincing.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a novel architecture and regularization technique for RNN. The hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata is much simpler, and (2) forces RNN to operate like an automata.This paper is based on the observation that LSTMs use the hidden state to memorize information and the cell state (memory) is not fully utilized. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks.The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids. It is not a surprise that a CF can be learned by an architecture very similar to DPDA.	the authors propose to incorporate an additional layer between the consecutive steps in LSTM by introducing a radial basis function layer (with dot product kernel and softmax) followed by a linear layer to make LSTM similar to or better at (by being more explicit) capturing DFA-like transition. the motivation is relatively straightforward, but it does not really resolve the issue of whether existing formulations of RNN's cannot capture such transition. since this was not shown theoretically nor intuitively, it is important for empirical evaluations to be thorough and clearly show that the proposed approach does indeed outperform the vanilla LSTM (with peepholes) when the capacity (e.g., the number of parameters) matches. unfortunately it has been the consensus among the reviewers that more thorough comparison on more conventional benchmarks are needed to convince them of the merit of the proposed approach.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this work, the authors use machine learning tools to enhance the domain decomposition solvers. It uses a graph convolutional network to learn the preconditioner. Specifically, the GNN takes the domain D and its decomposition as inputs, and outputs the boundary matrices L.The authors proposed to learn the parameters in Schwarz preconditioner that generalize it to unstructured grid. The main difference compared to the prior work is the proposed new loss function inspired by Gelfand's formula. Experiments on the 2D Helmholtz equation have been done to demonstrate the effectiveness of the approach.This paper presents a learning-based approach to find the optimal boundary values for domain-decomposed PDE solvers. The major contribution is the application of TAGConv to solving PDEs. The method looks solid, and is well-validated.	The paper proposes a scheme to learn preconditioners for domain decomposition solvers. Graph neural network is used to learn interface matrices, and the training dataset consist of many unstructured grids for which optimal parameters are learning.  The loss function is adapted from Daulbaev et.and is based on the Gelfand formula (with average replaced by maximum). Theoretical study is provided. The paper is well-presented, contains a useful and practical algorithm which potentially will be used by many researchers doing numerical simulations.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work provides an empirical evaluation of similarity metrics used in example-based explanations methods. The goal is to provide decision support examples in the training set for a black-box model's prediction. The authors introduce 2 new tasks for assessing the reliability of the different methods.The authors conduct an empirical evaluation on two image datasets, two text datasets and one tabular dataset. They conclude that many of the state-of-the-art metrics do not even satisfy the first requirement. More extensive experiments on tabular data would have been good.The presented study tackled the problem of evaluating different metrics for generating similarity-based explanations. The experiment results revealed that the cosine similarity of the gradients of the loss performed the best. Overall, the methods adopted by the study are technically sound and the paper is well written.This paper is on the evaluation of XAI for supervised models. The experiments show that some similarity measures are better than others. The authors find an explanation for those results, based on geometrical properties of the measured objects.	This paper performs an empirical comparison of similarity-based attribution methods, which aim to "explain" model predictions via training samples. To this end, the authors propose a handful of metrics intended to measure the acceptability of such methods. While one reviewer took issue with the proposed criteria, the general consensus amongst reviewers is that this provides at least a start for measuring and comparing instance-attribution methods.   In sum, this is a worthwhile contribution to the interpretability literature that provides measures for comparing and contrasting explanation-by-training-example methods.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper targets at stylizing a mesh with a text prompt. It represents appearance of a mesh as spatially varying BRDF, normal and lighting. For evaluation, the paper compares with the state-of-the-art text prompt stylization method Text2Mesh.This paper presents a 3D mesh stylization method that performs both appearance and geometric styles by optimizing with a differentiable rendering pipeline. Compared with existing methods that simply optimize per-vertex color and displacement, it adopts a more physics-aware rendering model.This paper focuses on the task of 3D mesh stylization according to the text prompt. The key idea is to jointly learn three disentangled components, ie, the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition.The paper describes a method for generating a reflectance map (and lighting environment) for a given 3D shape, based on a text prompt, using a CLIP-guided loss. The impact of the work is incremental, but the general direction of text-guided geometry/ texture generation is a significant one.	This paper presents a new CLIP-driven stylization method given an input mesh and text description. Compared to previous works Text2Mesh, the paper introduces a more expressive rendering model based on learnable SVBRDF and normal maps. Many reviewers found the paper easy to follow, the idea promising, and the results visually appealing. They also expressed their concerns regarding the similarity to Text2Mesh, the limitations of the normal maps approach (compared to changing geometry explicitly), and the relighting and material editing of the stylized object. The rebuttal has addressed most of the concerns. The AC agreed with most of the reviewers and recommended accepting the paper.   Please revise the papers according to the reviewer’s comments: (1) change the title according to Reviewer kpuS, (2) add relighting/material editing/view synthesis results, and (3) highlight the pros and cons of the proposed method w.r.t. Text2Mesh.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a new gradient-based feature attribute method Distilled Gradient Aggregation (DGA) that extends FullGrad (FG) On quantitative and qualitative evaluations the authors demonstrate that their method produces better explanations than previously proposed methods. The authors' proposed method is, to my knowledge, novel.Distilled Gradient Aggregation (DGA) approach combines the advantages of both global IG and local FG approaches via a new sequential feature distillation algorithm. The DGA approach effectively addresses the limitations of local approach (FullGrad) that uses a single anchor point to compute attribution.The paper identifies pitfalls in two attributions methods, then proposes a novel attribution method that purports to solve the problem. The paper proposes using FullGrad with a different post-processing function, and engaging in sequential feature distillation. They run comparative tests on other attribution methods, and run quantitate ROAR and Perturbation tests.This paper introduces a new explanation method called DGA for attributing a function’s output w.r.t its input features. DGA is motivated by problems of some related work, i.e. Integrated Gradient and FullGrad. The current way the method is presented to the audience is quite confusing.	This paper proposes a new gradient-based attribution method Distilled Gradient Aggregation (DGA), which combines the strengths of both local and global attribution methods. The reviewers and meta-reviewer found the method novel, and supported by promising results both qualitatively and quantitatively.   During the rebuttal phase, the authors made a _thorough_ effort in response to each reviewer's comments. As recognized by two reviewers (hnze, cvDy), the newly added results and discussions have significantly strengthened the contribution.   The AC recommends acceptance given the paper tackles a critical problem, and presented an effective and convincing method that advances the field of explainable AI.   Authors are strongly recommended to include these new results and writing changes suggested by the reviewers for the final version of the manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper explored the over-smoothing problem in BERT from the perspective of graph. Theoretical analysis is new and provides interesting findings. The proposed fusion methods are not that novel and not well theoretically and empirically proved.This paper discusses the oversmoothing in transformer models such as BERT. It proposes a hierarchical fusion method as a solution. Pros: The theoretical analysis on BERT is a good contribution. The analysis methods is not completely new but applicable to analysis on transformers.The token over-smoothing problem is increasingly severe in deeper layers of a Bert model. The author applied a novel approach to shed light on an insufficiently investigated problem. Overall, I vote for accepting this paper.This paper tries to understand the over-smoothing phenomenon of Transformer-based models such as BERT. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks.	This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing.  In this paper, they also provide theoretical proof why higher layers can lead to over-smoothing. Empirically , they investigate the effects of the magnitude of the two standard deviations between two consecutive layers on possible over-smoothing in diverse tasks.  In order to overcome over-smoothing, they propose a series of hierarchical fusion strategy that adaptively fuses presentation from different layers, including concatenation fusion, max fusion and self-gate fusion into post-normalization. These strategies reduce similarities between tokens and outperforms BERT baseline on a few datasets (GLUE, SWAG and SQuAD).  Overall I agree with reviewers that this is a good contribution.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose U-WILDS, which extends the WILDS benchmark to the unsupervised domain adaptation scenario. They benchmark a comprehensive set of algorithms which make use of unlabelled data, and find that most methods do not significantly outperform ERM. The paper is easy to read and easy to follow, and the experimental evaluations are robust.The authors introduce an extension to the new, but popular, data shift benchmark WILDS data sets called U-WILDS. The main finding is that most of the techniques performed poorly, except data augmentation in the image problems. The major practical concern I have about this endeavor is standardization across new and old instances.The paper proposed an extension to the popular WILDS benchmark dataset by augmenting the different domain data with additional unlabeled examples. The dataset consists of data of various modalities including images, graph and text from various domains. Although the dataset itself is very useful, the analysis and bench marking needs some improvement.The authors present U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. They provide a large quantity of unlabelled data complementing 8 of the existing multidomain labeled datasets. They present reasoned conclusions and open-source datasets and implementations.	This paper presents U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. The authors propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain-shift problems. The vision behind sounds quite ambitious and convincing to me, namely, the proposed U-WILDS benchmark would be a useful and well-motivated resource for the ML community, and their experiments were very comprehensive. Although they did not introduce any new methods in this paper, U-WILDS significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation.  The clarity, vision and significance are clearly above the bar of ICLR. While the reviewers had some concerns on the novelty, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to strongly accept this paper for publication! Please include the additional rebuttal discussion in the next version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a second-order algorithm for training neural networks, in the L2 regression setting. It provides a theoretical analysis of its complexity in the over-parametrized regime. It does not provide empirical validation of the method, or an implementation.This paper studies the training algorithms for multi-layer over-parameterized neural networks. It starts from gauss-newton-methods and incorporates tensor-based sketching techniques and preconditioning to improve the per-iteration computational complexity.This paper aims to propose a second-order algorithm for training of neural networks with very large widths. The main claim is that the computation cost of each iteration of the proposed methods is sub-quadratic in terms of the network width. Several claims about the literature are not accurate.This paper proved that the second-order method can minimize the training loss in linea rate on multi-layer over-parameterized neural networks. The ideas in this paper can inspire new algorithms that enjoy lower per-iteration costs. Some of the analysis techniques can be useful in future studies.	This paper studies the performance of second-order algorithms on training multi-layers over-parameterized neural networks. The authors propose an algorithm based on the Gram-Gauss-Newton method, tensor-based sketching techniques, and preconditioning to train such a network, whose runtime is subquadratic in the width of the neural network. While some reviewers provide some weak support, none of them are in strong support, even after the author's response. I think one of the reasons is the lack of empirical experiments. Since the main claim of this paper is an efficient second-order algorithm, some experiments are necessary to back up this claim. Unfortunately, the authors did not try to add such an experiment during the rebuttal. I would suggest the authors add such experiments in the revision.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes to reformulate the QA task in SQUAD as a retrieval task, i.e., using question as query and paragraphs as candidate results to be ranked. Authors makes some modifications to elmo model to create better word embedding for the ranking task.This paper tries to study retrieval methods for multi-paragraph / multi-document reading comprehension. The basic approach is to embed the question and the paragraph and train a system to put the correct paragraph close to the question. I had a very hard time following the details of the proposed approach.This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residualnetwork on top of the word embeding.	I have to agree with the reviewers here and unfortunately recommend a rejection.  The methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces MATE, a multi-agent learning environment that models an asymmetric game between two teams of players. The cameras are fixed in their location but can change direction and zoom, with the goal of observing the targets.This paper introduces a new dataset for multi-agent reinforcement learning in tracking tasks. The environment could serve as a basic simulator for real-world applications. It could provide a guideline for algorithm and application researchers in the field of tracking and cargoes.The authors propose a new OpenAI Gym environment for the partially observable two-team target coverage problem. The environment comes with wrappers for built-in rule-based strategies to test against, three types of reward signals, and various configuration options for the number of agents in each team.This is the first open-sourced and standardized multi-agent environment specifically for target coverage problem. META provides peer-to-peer communication options. The HiT-MAC method should be performed as well in the experiments.	This paper introduces an interesting cooperative-competitive MARL environment involving groups of camera and targets with the objective of transportation. The benchmark environment appears to be scalable with novel components including partial observability and general-sum rewards in an interesting environment. The authors provide extensive baselines for MARL algorithms as well. The majority of reviewers appraised the work positively, and the authors adequately addressed the concerns brought up in the review along with a revision incorporating some components of feedback. I think this is a good addition to the MARL literature, and the authors should be sure to incorporate all the remaining feedback in the final version of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new task of semantic action-conditioned video prediction. The purpose of this task is to generate future frames given objects and action pairs. In general, I think it is an interesting paper. But I still have some concerns about the significance of the new task.The paper proposes the video prediction model that can handle multiple objects. The proposed model is built from attention-based capsule network. It generates video in a hierarchical feature computation. The model learns to discern each concept without any need for labels such as bounding boxes.This paper presents an interesting task that has the potential of sparking research in this direction. In addition, the authors propose an interesting architecture that generates extremely good video given some simple instructions. There are a few questions I have listed above, but the paper to me seems interesting enough consider for acceptance.	This paper presents work on semantic action-conditioned video prediction.  The reviewers appreciated the interesting task and use of capsule networks to address it.  Concerns were raised over generalization ability of the proposed approach, points on clarity, scalability, and handling of uncertainty/diversity by the method.  After reading the authors' response, the reviewers engaged in discussion.  Over the course of this discussion, the reviewers converged on a reject rating, noting that the concerns raised above were not sufficiently addressed to warrant publication at this stage.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies why optimization-based adversarial attacks have close to zero target transferability in attack real-world ASR pipelines. The control experiments are carefully executed and provide clear takeaway insights. The paper is also well-written and nicely structured.This paper presents an evaluation and explanation of the limited adversarial transferability in the ASR system. It first lists 11 known factors, e.g., smoothness of gradients, and then proposes four potential factors that limit the transferability. The method of how the authors come up with the factors is not explained.The paper conducts a systematic study on the phenomenon that attacks targeting ASR systems often have low transferability. Results show that many existing designs for improving the robustness of ASR can also prevent transfer attacks. Based on the findings, the authors also discuss the takeaways and future directions for the ASR.It is known that targeted transferability is ineffective against typical ASRsystems, but it is not fully understand why. The authors design a simplified ASR system in order to show which components of such a system are affecting the targeted transferable issue. I believe the authors provide a useful beginning of understanding why ASR systems may be not susceptible to targetedtransferability.	This paper explores why adversarial examples do not transfer well in adversarial examples on automatic speech recognition systems. The authors propose a number of potential causes that are then quickly evaluated in turn.  This could be an excellent paper, but in its current form, it is borderline. The main problem with the paper is that it proposes a number of causes for the limited transferability, and then evaluates each of them with one quick experiment and just a paragraph of text. In particular, none of the results actually convince me that the claim is definitely correct, and many of the experimental setups are confusing or would have other explanations other than the one variable that is aiming to be controlled for.  That said, even with these weaknesses, this paper raises interesting and new questions with an approach I have not seen previosuly. So while I don't believe the paper has done much to actually demystify transferability, it does take steps towards performing scientific experiments to understand why it is so hard. And these experiments, while not perfect, can serve as the basis for future work to extend and understand which factors are most important.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper tackles the problem of neural networks learning ‘simple’ features which cannot generalize well. It proposes to alleviate this problem of shortcut learning by generating synthetic data (“roadblocks”) in a manner which confuses the network while being close to the original training data. A new model is trained on such examples, and it is hypothesized that this new model learns more robust features.A method for OOD generalization is proposed. To prevent a model from learning "shortcut features" (simple features which do not work in OOD setting~spurious correlations), an autoencoder is trained. The explorer model is then trained on the reconstructed image. Some of the shortcut features were hopefully removed by AE to fool the Blocked model.The authors propose an approach to address the problem of shortcut vulnerability of deep vision models in image classification tasks. For a trained, shortcut exploiting model (BM) the authors propose to train a model (AE) that learns to modify training samples in a minimal way. Using these manipulated samples, an explorer model is trained (EM) in a supervised manner to predict the original ground-truth labels. The intuition behind the approach is that AE learns to manipulate input samples specifically to remove the shortcut informationThe paper proposed a learning approach for robustness to shortcut learning. The approach was evaluated on ColorMNIST, CelebA and BAR benchmarks. The paper is missing citations of the relevant literature on shortcut learning, simplicity bias, group robustness, and spurious correlations. The results on CMNIST are not directly comparable to prior work due to a different model architecture.	The submission describes a new method to avoid the shortcut learning behaviour in DNNs. After the rebuttal and discussion, most of the reviewers are positive about this submission since the proposed method does not require prior knowledge about the dataset and the strong empirical results for the debasing task. On the negative results, the reviewers argue that the experimental evaluation is not thorough enough. Overall, AC recommends acceptance but asks the authors to perform more rigorous evaluation for the camera-ready version including the fair tuning of the hyperparameters.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a new penalization framework for the stochastic multi-armed bandit problem with fairness constraints. They formalize the penalized regret to evaluate the performance of bandit algorithms. Experiments on synthetic data show its superiority over other existing methods.The authors deal with the stochastic multi-armed bandit problem with constraints in the least number of arms pulls for fairness. They convert such a problem into the unconstrained but penalized version. They propose the hard-threshold UCB algorithm with the analyses of its gap-dependent and -independent regrets.This paper examines the problem of reward maximization in a Multi-Armed Bandit, with the additional fairness constraint that each arm should be pulled for at least a fraction of time. The authors propose and analyze an algorithm where the UCB index of an arm $i$   is increased by a specified amount (which is a parameter for the algorithm) $A_i$The paper studies one variant of fair multi-armed bandit (MAB) problem. For each arm $k$ played, there is a known fairness parameter $tau_k$ which gives the fraction of times that this arm should be played. The authors propose a hard-threshold UCB-based algorithm with regret analysis and empirical good performance.	The paper provides an algorithm for the stochastic multi armed bandit (MAB) problem in the regime with fairness constraints. It continues a line of work that in high level define fairness as a requirement to ensure a minimum amount of exploration for every arm. The main concern I found in the reviews regards the definition of fairness in this paper. Although it follows the same high level narrative of previous works its exact definition and difference from previous papers is not convincingly motivated, and seems to be tailored to the proposed algorithm rather to a real world fairness constraint. This issue could have been mitigated by a novel or generalizable technique, or insightful experiments, but this does not seem to be the case given the reviewers comment about the limited novelty and basic experiments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tries to demystify the power of embedding and embedding space. It tries to connect the structure of learned embeddings and knowledge generation process. The authors tested Gaussian model, Preferential Placement (PP) model, and Directional Preferential placement (DPP) model.In this paper,  the authors try to study if we can learn about the human's process of generating new ideas or concepts from embeddings.Strengths. The author's methodology of approaching this problem seems fine.Weaknesses. One concern I have is the embedding.This paper studies data generation processes with the help of embedding spaces. It is an interesting problem, and the idea is worth further exploration. Since the authors did not give a clear conclusion, the technical contribution of this submission seems to be weak.This paper proposes two measures of word/graph embedding to character the evolution of word and graph. Many existing graph generation models are surveyed. The proposed measures are calculated on many real-world datasets. This paper lacks some key elements, such as problem formulation and rationale discussion.	The authors of this work introduced new metrics for node embedding that can measure the evolution of the embeddings, and compare them with existing graph embedding approaches, and experimented on real datasets.  All reviewers agreed that the work addresses interesting problem and that the proposed measures are nove, but there are too many flaws in the initial version of the paper, and despite the thorough responses of the authors, it is believed that there are still too many open questions for this paper to be accepted this year ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method of compressing neural radience fields (NeRF's) by learning mappings from latent codes to model parameters such that both distortion/reconstruction quality and the rate get minimized. While maintaining the same level of quality, this method is able to compress NeRF models for more efficient sender-to-receiver transmission.Compressing 3D scene is an interesting problem to explore. The paper proposes to add entropy penalized reparametrization into Nerf. Experiments show some compressing rates improvement with the proposed baseline. However, the multiple scenes experiments are not performed well to demonstrate the effectiveness of the proposed method.This paper proposes to compress nerf models with entropy loss. Instead of directly training nerf model parameters, it trains a new function F. The authors show that the function F could largely compress the original Nerf models while keeping similar PSNR.I think the paper is well written, and explains the details of the method well. I addition I think choices made in the method are intelligent and well justified. My concerns with the paper lie in two area. The first is that I am not convinced that the problem this paper seeks to solve is so relevant to the ML research community that is justifies the lack of novelty.	Description: The paper presents a method for encoding a compressed version of an implicit 3D scene, from given images from arbitrary view points. This is achieved via a function, learning with a NeRF model, that maps spatial coordinates to a radiance vector field and is optimized for high compressibility and low reconstruction error. Results shows better compression, higher reconstruction quality and lower bitrates compared to other STOA.  Strengths: - Method for significantly compressing NerF models, which is very useful since such models are often trained for every new scene - Retain reconstruction quality after compression by an order of magnitude   Weaknesses: - The need for decompressing the model before rendering can be done means reduced rendering speed. This also requires longer training times. - Experiments against other scene compression + neural rendering technique will have further strengthened the papers’s claims  - The techniques used are well established, and thus there is not as much technical novelty.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors examine whether SGD with a large, constant step-size avoids local maximizers. They provide a quartic loss function under which SGD converges to the function's sharper minimizers. While on the mathematical axis I consider the paper to be a good fit for ICLR, the reported results do not suffice to support the paper's (overly ambitious) narrative claims.Stochastic gradient descent may easily have very poor behavior. It could converge to a maximum, or diverge even in convex settings if the learning rate is too high. The paper introduces some interesting examples accompanied with relevant commentary.The abnormal behavior of SGD is out of expectation from the first sight. At the same time these behaviors should be as expected because the experimental settings are specifically chosen and break usual assumptions. It shows that SGD can converge to local maxima with high probability for specific cases.Many theoretical works have studied SGD, but they commonly rely onrictive and unrealistic assumptions about the noise. In the most general nonconvex case, many counter-intuitive phenomena of SGD may arise. This paper is well written and has a good presentation.The paper provides several artificial examples on which SGD has an unintuitive behavior. This includes: (1) SGD converges to a local maximum; (2) If the learning rate is not fixed then SGD takes arbitrarily long time to escape saddle points. The experimental section is nice and provides empirical evidence for the validity of the theoretical results.	Overall, the paper provides interesting counter examples for the SGD with constant step-size (that relies on a relative noise model that diminishes at the critical points), which provide critical (counter) insights into what we consider as good convergence metrics, such as expected norm of the gradient.   The initial submission took a controversial position between the mathematical statements and the presentation of the statements on the behavior of the SGD method in non-convex optimization problems. While the mathematical is sufficient for acceptance at ICLR, the presentation was inferring conclusions that could have been misread by the community.  I am really happy to state that the review as well as the rebuttal processes helped improved the presentation of the results that I am excited to recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents an analysis of camera placement for vision-based manipulators. It compares the performance of a disembodied third person camera vs. place the camera on the robot's hand/gripper. The authors find that the hand camera improves generalization.The paper studies the effect of visual perspective by using the images coming from a camera installed on the hand of the robot. They show results on six different manipulation tasks adapted from Meta-World and their choice of perspective improves OOD generalisation. It seems to suggest that a zoomed-in view of the object by using a hand perspective almost always helps.This paper presents an interesting empirical evaluation of the role of visual perspective in learning and generalization in the context of physical manipulation. The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed. An evaluation comparing the presented results with the proprioception-only case could further improve the analysis.	All reviewers consistently agree on the high quality of the research presented in this paper, such that it the paper clearly is significantly above the acceptance threshold of ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper addresses the problem of predicting links and time-stamps in a temporal knowledge graph. The experiments conducted over GDELT and ICEWS14 datasets show that the proposed GHNN offers significantly better results than Know-Evolve. The paper is well written and the model seems sound.This paper introduces a new model based on the Hawkes process to model the temporal knowledge base. The experiments show improvement with the proposed model compared to baselines. The results from different models are very close to each other.This work proposes a graph Hawkes Neural Network for event and time prediction on temporal knowledge graphs. Overall, the paper is nicely written and easy to follow. The proposed method only considers limited information from historical event sequences.	This paper propose Graph Hawkes Neural Networks (GHNN) which are suited for performing inference in temporal knowledge graphs. By combining the continuous modeling provided by cLSTM with strong assumptions about how events can impact one another, GHNN shows promising results compared strong baselines on the GDELT and ICEWS14 dataset.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The research field of video TAP is under-studied. This paper introduces a companion benchmark it would benefit the community. The data collection method provided in this paper shows a standard pipeline for the following research.The paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video.TAP-Vid aims to expand keypoint tracking in video with a greater diversity of points and longer duration point tracks that account for occlusion. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets.This paper constructs a new dataset for arbitrary physical point tracking. The proposed task, i.e., tracking any points (TAP), can benefit the development of embodied agents. Although optical flow-based methods can perform well in short-term videos for arbitrary point tracking, the drift accumulation problem hinders their application.This paper formulates the problem of ‘tracking any point’ on video data. The formulation differs from related problems by aiming to track points starting from any pixel in any frame. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks.This paper introduces a new benchmark dataset for the task of tracking any point (TAP), i.e., tracking an arbitrary 2D point defined at the first frame over the entire video. The dataset is composed of a mixture of real and synthetic data. For real data, human manually annotates the ground truth for a number of 2D salient points, and for synthetic data, perfect ground truth labels are provided.	TAP-Vid presents a benchmark that will be highly useful for tracking research for tracking arbitrary physical points on surfaces over long video clips. The reviews are all positive, with one reviewer raising ethical aspects in preparing the benchmark.  I find the rebuttal sufficient and adequate and hence recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes CoLES that uses contrastive learning to learn representations of event sequence related to user behavior. The method trains in a self-supervised manner by randomly slicing event sequences to generate sub-sequences. The results show 1~2% improvements which do not match the statements.The paper targets the problem of event sequence prediction in a contrastive self-supervised learning framework. They train this contrastive learning method by generating positive and negative samples via data augmentation method proposed as random slicing, which creates overlapping sub-sequences.Self-supervised learning is a rapidly growing domain where the amount of labeled data is limited. Contrastive learning with data augmentation enables us to design a loss function which makes semantically similar objects closer to each other, while dissimilar ones further away. CoLES algorithm randomly selects subsequences from the full event sequence from each user.	This work is well written and accurately covers the context and recent related work. It's a good example of how to apply self-supervised training to the event sequence domain. However, the combination of a lack of technical originality (composing a set of previously explored ideas) and significant improvements in results (results with CoLES overlap in error bars with RTD results) limits the impact of this paper.  Pros: - Well written. - Extensive evaluation. - Well formulated problem.  Cons: - Lack of technical novelty. The method appears to be general to all sequences rather than specialized for event sequences so the motivation for this design is not crystal clear. - Minor improvement in results from using the method despite written claims that the method 'significantly outperforms'. - Limited analysis that shows the periodicity and repeatability in the data.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.New tool for inferring underlying properties of partially observed spiking neural networks. Based on mean field modeling on net effect of unobserved neurons; validated on within model class simulated data. Strengths: Interesting attempt at extracting more interpretable latent models.The authors propose a new type of latent space model for neural spike trains, based on a spiking neural network (SNN) They use mean field approximations to abstract parts of the SNN, resulting in the latent dynamics. After pre-defining a single or multiple neural clusters they can recover the connectivity value(s) from snippets of 10 sec activity of only some observed neurons.The authors address a very difficult problem, the problem of dealing with unobserved populations in spiking neural networks. The work builds on work on mesoscopic modeling of SNNs by Rene et al. and extends this to a latent variable model, trained by the Baum-Viterbi algorithm.The paper proposes a latent variable model for modeling observed activity in spiking neural networks while taking into account the activity of unobserved neurons. The proposed model, neurLVM, is fit via the approximate “hard EM” algorithm. The model is validated in two simulations, where it provides accurate recovery of net population activity.	Dear authors,   Congratulations on your paper being accepted! The reviewers unanimously recommended acceptance. The reviewers made a number of recommendations on how to improve the paper further, in particular with respect to clarity of writing and explaining the motivation behind different analyses. We strongly encourage you use this feedback to improve the paper— if needs be additional clarifications can be added in the supplement. In addition, it would indeed be highly useful to make your source code publicly available, as you indicated in your response.   Best, your AC
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides an interesting model that can be used for a meta-learning situation where both data distribution and the number of features vary across tasks. The proposed model transforms features to belong to the same distribution family, learn distribution embedding with them and process variable-length inputs by using DeepSets.This work proposes Distribution Embedding Network (DEN), a meta-learning model for classification. DEN is designed for the setting where data distribution and the number of features can vary across tasks. It classifies examples based on an embedding of data distribution.This paper tackles the problem of meta-learning, namely learning from labelled datasets across related tasks. The focus is made on varying distributions across data features as well as varying numbers of features. A novel method for task generation is leveraged using binary classifiers. The proposed network is applied to a classifier aggregation task.The authors proposed a method for meta-learning that produces a distributional embedding of the task, and then uses this information to perform few-shot classification. The motivation for this method is not very well explained. The experiments are hard to interpret.	This paper addresses a meta-learning method which works for cases where both the distribution and the number of features may vary across tasks. The method is referred to as 'distribution embedding network (DEN)' which consists of three building block. While the method seems to be interesting and contains some new ideas, all of reviewers agree that the description for each module in the model is not clear and the architecture design needs further analysis. In addition, experiments are not sufficient to justify the method. Without positive feedback from any of reviewers, I do not have choice but to suggest rejection.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is interesting and easy to read. My main concern is about the feasibility of using a neural networks to learn cumulative quantities. The problem of learning cumulative quantities in a neural net is that we need two types of samples.This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations. By using these networks within a CFR framework, the authors manage to avoid huge memory requirements.The authors propose "Double Neural CFR", which uses neural network function approximation in place of the tabular update in CFR. CFR is the leading method for finding equilibria in imperfect information games. It is typically employed with a tabular policy, limiting its applicability to large games.	The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach. The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples). The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy. The paper should better discuss this more, even if it is not possible to provide theory. The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.AdaSpeech is a paper on practical TTS custom voice adaptation. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. The strengths of this paper are entirely through the strong numerical results.This paper proposes AdaSpeech, a Transformer-based TTS architecture derived from FastSpeech. It focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder.AdaSpeech is a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. The model is based on the TTS model in FastSpeech 2, with several additional components.The authors propose an interesting text-to-speech adaptation method for high quality and efficient customization of new voice. The proposed method consists of two-stage modeling : multi-phonetic-level acoustic condition modeling and conditional layer normalization.	The paper is about adapting a voice generation model to new speakers with minimal amount of training data. The key insight in this paper is that the voice can be adapted using a small set of variables -- the bias and the variance associated with the layer that normalizes the mel-spectrogram associated with the decoder. Additionally, they characterize voice at the utterance level to capture stationary factors like background acoustic conditions and at the phoneme level to capture factors such as prosody, though there are no explicit constraints to force such representation.  The strength of the paper are: + Simplicity of the approach + Empirical evaluation that demonstrates its effectiveness  The weakness of the paper are: - analysis of what the crucial parameters of the model represent - lack of clarity that is obvious from several back-and-forths between the reviewers and the author.  A few examples include: - “There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me.” - “ it is only the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. (Both the normalization parameters and the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK.”
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors introduce a simple extension of PPO that uses a single expert demonstration and a modified sampling algorithm to show better performance than vanilla PPO in a difficult 3D setting. This is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios get the method off the ground.The paper proposes a modification of the PPO algorithm which can accommodate a single task demonstration with the goal of faster learning in sparse-reward tasks. The baselines are Behavioural Cloning and vanilla PPO. The proposed method outperforms those baselines.This paper looks at problems that have sparse rewards, are partially observable, and havevariable initial conditions. It proposes to use a PPO+D, an on-policy method (PPO) for a policy with memory (GRUs) The method is tested on four environment variations, as constructed in the Animal-AI Olympics challenge env.	There was quite a bit of internal discussion on this paper. To summarize: - The idea is very neat and interesting and likely to work - The paper is likely to inspire future work - There are still serious doubts  about the experimental evaluation that is not entirely up to par with current standards   - The reviewers were not convinced 100% by the arguments about the 'custom' environments   - The reviewers were not convinced 100% that the baselines were given their best shot  While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the model-based reinforcement learning. They propose a posterior sampling algorithm and provide a Bayesian regret guarantee. Under the assumption that the reward and transition functions are Gaussian processes with linear kernels, the regret bound is in the order of H^1.5 d sqrt{T}The paper proposes a model-based posterior sampling algorithms with regret guarantees when the model is assumed to be drawn from a distribution randomly. The definition of $BayesRegret$ seems incorrect as it takes $M^*$ as input argument. There is no study on fundamental limit.The paper proposes a method to balance exploration and exploitation in reinforcement learning problems. The transitions and rewards are assumed to be sampled from Gaussian processes, and provide a Bayesian regret bound. The method is claimed to be computationally tractable.This paper proposes a new model-based reinforcement learning algorithm named MPC-PSRL. Osband & Van Roy (2014) already provides the algorithm posterior sampling RL algorithm, named PSRL for continuous domain. It is not clear whether both GP modeling and exploration via posterior sampling have a significant impact on performance.	This paper presents a model-based posterior sampling algorithm in continuous state-action spaces theoretically and empirically. The work is interesting and the authors provide numerical evaluations of the proposed method. But the reviewers find the contribution of the work limited.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work proposed an approach that generates radiology reports from chest x-ray images by splitting pathology-related sentences and the others. The experimental setup should be more rigorous. Does the training/validation/test split guarantee no patient overlap across the subsets?The proposed method uses CNN for image classification on Chest X-rays and CNN-RNN structure is then applied to generate reports. Different strategy is applied for normal/abnormal cases. The result shows good improvement and many state of the art methods are compared.The authors present a system for generating radiology reports on chest x-rays and providing disease classification and a heatmap to indicate areas of abnormality. The methods to determine the classification and heatmap are not novel (DenseNet and GradCam), nor are the methods to generate the reports.In this short paper, the authors present a new fully integrated pipeline to diagnose common thoracic diseases. The paper reads well and the methodology is relatively clear. The results seem promising. The authors however did not made clear what motivated their technical choices compared with Li et. (2018) and Xue et al. ( 2018)	This paper presents a pipeline for automatic interpretation of  medical imaging and learning-based diagnosis. On the one hand, the paper tackles an important and challenging problem, but on the other hand, the validation is very limited and novelty over previous work is unclear. This is therefore a borderline paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This submission derives new generalization bounds for tuning the regularization parameters for ridge-regression, LASSO, and the elastic net methods. In particular, the authors study two settings: (i) tuning based on multiple draws of train/test splits from a fixed (but arbitrary) problem distribution and online learning.This paper studies the validation loss of the elastic net as a function of its two hyperparameters. The authors show that, given a set of validation losses and a distribution over this set, a possibly small number of draws from this distribution are needed to minimize the average validation loss. The paper could use some more discussion and careful definitions.The paper seems to be theoretically sound and follows a logical progression. The literature section on exact (homotopy) methods for the elastic net and the lasso is lacking. It is not clear that the bound on the number of problems needed for a certain accuracy would be a useful addition.	The reviewers agreed that this paper should be accepted -- it studies an interesting and in someways "overlooked" problem and seems like it could be a starting point for others to build on. The paper did have some weaknesses. For example, we feel that the lack of experimental results is a missed opportunity -- the reviewers feel the paper would be made stronger by including at least a simple example where the standard approach of grid search + CV fails. Without such an example, it is a bit hard to be convinced of the importance of the problem being studied.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work introduces a new family of Bregman divergences for density ratio estimation with flexible models. The work aims at solving the train-loss hacking problem. The paper is well written, although quite difficult to follow without some theoretical background.The paper addresses learning the ratio between two densities from their samples. It is unclear to me how strong the theoretical claims are, and the writing needs significantly more work. I am also concerned about the reasoning behind several claims and the potential gap between the derived theory and experiments.The paper addresses an issue that arises in a particular formulation of the density ratio estimation problem. When one tries to directly fit a density estimation ratio, while minimizing Bergman divergence, it may be that overfitting causes the minimization problem to diverge to minus-infinity.The paper studies density ratio estimation (DRE), addressing the 'train-loss hacking' problems which often arise. The authors propose a new risk estimator for DRE, providing a non-negative Bregman divergence estimator, with the non- negative correction.	This paper discusses the likelihood ratio estimation using the Bregman divergence.  The authors consider the 'train-loss hacking', which is an overfitting issue causing minus infinity for the divergence.   They introduce non-negative correction for the divergence under the assumption that we have knowledge on the upper bound of the density ratio.  Some theoretical results on the convergence rate are given.  The proposed method shows favorable performance on outlier detection and covariate shift adaptation.  The proposed non-negative modification of Bregman divergence is a reasonalbe solution to the important problem of density ratio estimation.  The experimental results as well as theoretical justfication make this work solid.  However, there are some concerns also.  The paper assumes knowledge on the upper bound of density ratio and uses a related parameter essentially in the method.  Assuming the upper bound  is a long standing problem in estimating density ratio, and it is in practice not necesssarily easy to obtain.  Also, there is a room on improvements on readability.   Although this paper has some signicance on the topic, it does not reach the acceptance threshold unfortunately because of the high competition in this years' ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper about cooperative language acquisition, the authors try to tackle the problem of trying to learn a new agent which can adapt to a given community of sender or listener using a data of their interactions. They propose using a probabistic framework to infer the latents of a community and then learning a new compatible agent.The paper perfectly match the EC workshop, and I am looking forward to a discussion with the authors. The models also highlight some limitation of classic optimization of the lewis game (i.e. the lack of feedback).	Reviewers agree that the authors propose an interesting new framework for learning ad-hoc communication. There are some recommendations of improved baselines but both reviewers seem excited by the idea and we look forward to having the authors discuss this at the workshop and get important feedback on their work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors evaluate the generalizability of 10 stereo matching algorithms and do a detailed analysis. They find that most of the algorithms have good generalizable on active stereo images. The paper is fairly well written and easy to follow.This paper proposes a study of domain transfer and generalization abilities of different state-of-the-art (SOTA) deep learning-based stereo architectures when presented with active, instead of passive, stereo images. The authors provide valuable insights on generalization efficacy of different architectures.The paper introduces an Active-Passive SimStereo dataset. This is a synthetic dataset created with physically-based rendering engine Blender, that features 528 realistic and non-realistic images. The reported metrics show that existing methods can generalize to active stereo if trained on passive stereo.For the first time an active passive stereo dataset has been created. The noise introduce in the paper does not affect negatively most models. It is the first rendered based dataset for stereo vision. The mathematical notation of the paper and naming convention has some weaknesses.	The final scores for this submission are borderline, but the reviewers seem to agree on the positives (the contributions of the dataset itself) while the authors have gone through great lengths to address criticisms in terms of number of algorithms compared, the size of the dataset, the ability of the dataset to generalize to the real world, and the writing quality. Reviewer MHDH was also very thorough to catch an unintended overlap of train and test data, which the authors corrected. Taken as a whole, I believe the authors have done a great job addresses the perceived weaknesses of the manuscript and dataset. I also believe it is not obvious that deep methods would generalize from passive to active stereo (a concern of reviewer MHDH), so I believe this is a worthwhile study.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The idea sounds promising but may not be the first work. The performance is enhanced by an ensemble of multiple tricks. The baseline is mainly consisting of GOAD, OC-SVM, etc. Some recent SOTA baselines are missed.This paper proposes an ensemble approach, called SRR (Self-supervise, Refine, Repeat), for robust unsupervised anomaly detection. The proposed approach trains an ensemble of K detectors together with a joint self-supervised feature extractor on K disjoint subsets of the data. Experiments on tabular and image datasets are presented which show that the proposed ensemble approach is more robust at high anomaly contamination ratios.The paper tackles an unsupervised anomaly detection problem where the training set contains an unknown portion of anomalies. The idea is to filter out potential anomaly samples (data refinement) by ensemble model. The effectiveness of the proposed framework is validated on top of contrastive learning-based models.SRR employs an ensemble of multiple OCCs to give the potential anomaly refined samples from training. SRR brings the advantages of making the anomaly decision boundaries more robust and giving better data representations. The paper is ready for acceptance.	The paper worked on fully unsupervised anomaly detection and proposed to use self-supervised representation learning to improve the performance of one-class classification. This is a borderline case close to acceptance but cannot make it. Specifically, it is useful, but its novelty is the main issue, since it is not surprising that self-supervised representation learning can improve one-class classification without representation learning (this part is still much of the taste of ICLR) and an ensemble of multiple models can improve upon a single model (which is just "bootstrap aggregating" or "bagging" used everyday in practice and known to machine learning and statistics societies a very long time ago). After seeing the rebuttal, the concerns were not really addressed well and the issues were only partially solved. Thus, the paper is not enough to guarantee an acceptance to ICLR unfortunately.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes algorithms that compute personalized PageRank (PPR) under edge differential privacy (DP) The paper studies an interesting problem, and is relatively well written. However, there are several major issues with the paper.This paper studies the Personalized PageRank (PPR) problem. PPR is a very commonly used metric, and differentially private algorithms are not known. While there is no private algorithm for PPR, randomized response seems like too weak a baseline.The authors propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. Pros: Paper is organized well. Paper topic is interesting. Cons: theoretical bounds are not studied (NP-Complete etc)This paper focuses on computing approximate Personalized PageRank (PPR) vectors with differential privacy (DP) in graph representation learning. First, the authors analyze the non-private version of the PPR push algorithm with sensitivity. Then, they develop a variant of the PushFlowCap algorithm with bounded sensitivity. Finally, based on InstantEmbedding, they provide a sensitivity-bounded version for graph embedding.	Paper studies computing PPR in differential privacy setting. Given the importance of PPR in real world applications, we recommend accepting the paper as it brings an important problem to the DP community. However, we encourage authors to incorporate the comments from the authors, make sure that all the details of the proofs are made available in the final version, and clarify any comments reviewers raised. In particular, we encourage the authors to adequately address the criticisms of @Reviewer MDNv in the true spirit of science.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The empirical evaluation is fairly weak. The bound on Cifar-10 looks fairly loose even though part of it is non-vacuous. While FGD has shown to have a similar learning curve as SGD, computing two gradients in a row with one of them to be a full batch gradient could be computationally expensive.This paper studies the generalization property of gradient-based optimization algorithms using PAC-Bayesian bound. I like the idea of the paper and its results, while there are some unclear parts. I will raise my score once the authors resolve the following issues/questions.Generalization bounds are an extremely central topic of ML. While floored gradient descent seems a bit artificial, the extension to stochastic gradient Langevin dynamics is very useful. The writing is unfortunately poor, many typos and formulations which are difficult to understand.The paper uses data-dependent PAC-Bayesian bounds to derive generalization errors for a variety of gradient-based learning algorithms. The methodology for deriving the bounds is quite interesting, and the bounds themselves give nice insights into the performance of various algorithms.Weaknesses include some typos and basic mistakes.	Authors study generalization properties of gradient-based optimization algorithms via a PAC-Bayesian approach. Based on a data-dependent prior, authors establish a generalization bound for FGD, FSGD, GLD, and SGLD. The authors also provide convincing empirical studies to demonstrate that their results are not vacuous.  - Authors should better motivate the use of their seemingly synthetic algorithms.  - There are many typos in this paper, other than the ones listed by the reviewers. Although the reviewers did not raise this issue, authors should make sure their paper is ready for camera ready if the paper is accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper is an extension of previous work SPARTA, with two improvements. One is to use a learned belief model to sample in a large state space. The other is to improve the efficiency by replacing full rollouts with partial rollouts that bootstrap from a value function.There are many typos in the equation or some other paragraphs, which i will list in the following. This paper is not well written. Does this method can solve different imperfect information games? I want to see at least one experiment evaluated on another different game.Belief learning is showed to both have a good generalization and reduce searching time significantly. Experiments are mainly done on the benchmark problem of two-player Hanabi self-play. Learning belief representation makes sense in this problem setting.The paper proposed a computationally efficient search procedure for partially observable environments. The key idea behind the proposed method, LBS, is to obtain Monte Carlo estimates of the expected return for every possible action in a given action-observation history of an agent.This work provides a novel extension to the state-of-art approach (SPARTA) to solving Dec-POMDPs, a co-operative variant of partially observable stochastic games. Its main idea relies on the segmentation of action-observation histories into more manageable public-private factors.	This paper has some interesting ideas and is an incremental improvement over previous work. However, it needs further revisions and polishing. The relation to prior work is a bit unclear. Since you mention POMDPs, what would be an equivalent version of your method in POMDPS? Why not compare your algorithm with a state-of-the-art method for small discrete problems? It is also a bit unclear why training a model to predict beliefs would be faster than just calculating them (after all the data must come from somewhere)..
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work proposed three different calibration methods, temprature-based, margin-based and angle-based temperature scaling to enlarge the margin between targeted logit and non-target logits. This work is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods.The authors propose a novel and effective method to improve the transferability of adversarial attacks. They increase the logit margins between targeted and non-targeted classes. Strengths: Comprehensive experiments are presented and the combining logit calibrations have significantly better performance than previous methods.This paper designs a new logit calibration method which is inspired by knowledge distillation. The method uses logit calibrations in the CE loss function so that it can improve the targeted adversarial attack. Except for the primary temperature-based method, this paper designs margin-based and angle-based methods to solve different surrogate models.The paper takes a closer look at the vanishing gradient issue in the CE loss function which is commonly used to learn transferable adversarial samples. It suggests that the logit margin between the targeted and non-targeted classes quickly gets saturated during the optimization process.	In this paper, the authors propose novel method to improve transferability of targeted adversarial attacks by enlarging the margin between targeted logit and non-target logits.  Experiments on ImageNet with different methods demonstrated the effectiveness of the method. However, as is pointed out by the reviewers that there exist high overlap between the paper and the existing works, which significantly hinders the novelties of the paper. The paper are expected to clarify the novelty and provide more comprehensive evaluations.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tackles generative modeling via finding the push forward functions T (equivalently, the velocity fields v) that iteratively moves particles from a reference distribution toward the target data distribution. The velocity fields are solved by minimizing the f-divergence between the particle density at iteration k and the targetData density, which is shown to be in the form of gradient of density ratio.The authors present the Monge-Ampere Eq. (2), which solves for the _optimal_ transport from the reference distribution to the data distribution. But the method is constructed by simulating the gradient flow of f-divergence. The authors have addressed the concerns to some extent in the updated paper. The current presentation of the motivation may confuse or even mislead the community.This paper considers generative learning by discretizing a Wasserstein gradient with Euler methods. The proposed method is obtained by minimizing the f-divergence between the initial distribution and the target distribution. However, some typos/approximative statements make the reading experience difficult.	The paper proposes a discretization of Wasserstein gradient flow with an euler scheme, and propose a way to estimate each step of the euler scheme using ratio estimators from samples regularized with gradient penalties. Statistical bounds are given to bound the estimated flows from the wasserstein flow.    Reviewers have raised concerns regarding the assumptions under which results present in the paper hold, this was clarified by the authors (goedesic lambda convexity, log sobolev constant for the target density . lipchitizity of velocity fields).  The paper needs a revision to incorporate that feedback and to be in shape for publication.   Other concern were on earlier claims in the paper regarding the monge ampere equation and approximation of the optimal mapping this was addressed by the rebuttal.   Other concerns were also on explaining the relation of the work to score based models and energy based models.   Overall the paper needs to state in a clearer way the assumptions for the theoretical results and to acknowledge the limitations of those assumptions in analyzing the euler scheme.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper discussed the convergence of the gradient flows of two-layer neural networks. The theoretical analysis framework seems to be novel and some sufficient condition is proposed to support the strong theoretical results. The corresponding non-convex max-margin problem with the following dual reformulation is clear (maybe mathematically correct)This work analyzes the training dynamics of two-layer ReLU networks applied to separable problem data. It is based on the equivalent convex formulation by Pilanci, Ergen 2020. The main result states that the gradient-flow training dynamics provably converges to a "maximum-margin classifier"The paper studies the sub gradient flows when training a two-layer ReLU neural network. The main contribution of the paper is to prove that the non-convex gradient flows for the max-margin problem converges to the KKT points of the convex max- margin problem. This is a significant result which may be of interest to parts of the community.	The papers makes progress on the important question of implicit bias in gradient based neural learning. Remarkably they derive reasonable conditions for global optimality.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal is to continuously learn a model with new few-shot tasks without forgetting the previous ones. The paper introduces a prompt tuning-based framework that augments the pre-trained language model (T5) with continuous prompts. Prompts are simultaneously optimized for task solving and data generation.Lifelong learning and few-shot learning have been considered different ML paradigms. The authors argue that with the emergence of modern large-scale pre-trained language models, AI models now have the capability to achieve these 2 at the same time. This work attempts to formally define the LFLL problem.The paper extended the use of prompt tuning to the scenario of life-long language learning. Using the T5 model, the paper demonstrated the effectiveness of the proposed methods using text classification, named entity recognition (NER), and summarization tasks. The paper has several strengths that make it a promising submission.This paper explores lifelong few-shot language learning. Their framework trains the model as a task solver and data generator. They use pseudo data for new domains, and additional prompt embeddings for new tasks.	This work defines the new problem of lifelong few-shot language learning where the goal is to continually learn new few-shot tasks and use those to benefit future tasks while not forgetting previous tasks. With larger models, this is an important goal due to the cost of updating and retraining these models. The work also shows superiority to existing approaches like EWC and MAS. After the author's rebuttal, the experimental section is also thorough with evaluation on a good range of tasks and approaches such as adapters showing good results. While this setting appears simpler than the full lifelong-learning setting and the approach combines existing ideas, this work's contribution to the definition and thinking about this problem is valuable. However, the authors should more clearly state the advantages of their approach vs standard prompt tuning (with an emphasis of benefiting future tasks) since two reviewers seem caught up on this point. The other two reviewers comments were addressed by the rebuttal as they stated in their comments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.GLUE is a benchmark consisting of multiple natural language understanding tasks. It functions via uploading to a website and receiving a score based onprivately held test set labels. tasks include acceptability judgement, sentiment prediction, semantic equivalencedetection.The paper proposes a new benchmark for natural language understanding: GLUE. Models will be evaluated based on a diverse set of existing language understanding tasks. The work also collects an expert evaluated diagnostic evaluation dataset for further examination for the models.This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform. GLUE aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community.	This paper provides an interesting benchmark for multitask learning in NLP. I wish the dataset included language generation tasks instead of just classification but it's still a step in the right direction.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The physical model is well explained. The design of the data-driven model is somewhat arbitrary. There is a lack of comparison with existing methods. The significance and motivation of the proposed method need to be better explained.The paper is entitled "Deep Learning for Model Correction in Cardiac Electrophysiological Imaging" However since all data is simulated, there is really no "imaging" involved. Authors should indicate the simulation nature of the paper in the title to set fair expectations.	This work is clear, very original, however it lacks a bit of significance and is more of a proof of concept. Here are the pros and cons I've identified.  Pros : - This work deals with physics-based deep learning which is a very recent subject and one of growing interest for the medical community. - Results on simulated data look promising. - The theoretical part and the implementation is clear, too bad the code is not shared.  Cons : - The responses to the reviewers are very dense, often repeat the same explanations from one reviewer to another and lack relevance. It is not clear what has been changed in the manuscript in particular. - The proof of concept should have been to support in the claims of the authors, as well as the use of simulated data in the title as suggested by the last reviewer - In order to make the work more robust and quantitative, different physical models could have been considered, different Resnet architectures and an application to real data.  This work is extremely interesting but still a little immature. So I suggest an acceptation as a poster.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a method to predict when and which two nodes in a graph community will be linked (i.e., when and what event will happen) The paper first leverages community detection algorithms to divide the network into subgroups and then performs event prediction within each group.The paper describes a model for predicting events in a dynamic graph. Unlike most previous work, the model predicts both the incident nodes and the time step of the event jointly, rather than only predicting one given the other. The model formulation and training is based on temporal point processes.This paper mainly studies the forecasting problem on continuous-time dynamic graphs. The authors propose a united model composed of graph neural networks and marked temporal point process. Experiments are conducted to show the improved performance.The paper looks at the task of event prediction within communities of Continuous Temporal Dynamic Graph (CTDG) It aims at jointly predicting the event time and the two nodes involved in the event with the CEP3 method. The event prediction is clearly introduced and well formalized. My major concerns are about the model presentation and the experimental setup.	We agree with the AC that this paper is ready for publication. We encourage authors to incorporate suggestions for clarity improvements, in particular those mentioned by reviewer `XRhC`.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a compression method for Transformer-based encoder-decoder or language models. The idea of the proposed method is interesting, but there are a few concerns in terms of the presentation. It is hard to judge whether this paper has enough contribution for publishing as the conference paper.The paper contains a lot of substance, but it is very dense and hard to follow. The core “dictionary” technique isn’t really explained at a high level before the paper plunges into the details. Potentially a great paper, but if so it deserves to be much better explained.This work proposes a modification of the original Transformer architecture by replacing attention layers and layers in its Feed-Forward Networks with learned shared dictionaries. The proposed model, called DictFormer, has a smaller number of parameters and uses a smaller amount of computational operations. When evaluated against these models on popular machine translation, summarization, and language modeling benchmarks, Dict former achieves comparable or better performance.The goal is to reduce redundant parameters in transformer models. The proposed transformer model outperforms related work on the machine translation and language modelling tasks. The authors proposed an efficient transformer layer based on a dictionary of shared parameters instead of standard self-attention.	DictFormer is a method to reduce the redundancy in transformers so they can deployed on edge devices. In the method, a shared dictionary across layers and unshared coefficients are used in place of weight multiplications. The author proposed a l1 relaxation to train the non-differentiable objective  to achieve both higher performance and lower parameter counts.  All reviewers ended up giving the paper a score of 6 after increasing their scores during discussions. While the results are strong (better performance at much lower parameter counts), the paper is not clearly written. Several reviewers noted that the paper is difficult to understand and has a few unresolved points. For example, the method also ended up performing better than the base transformer model that DictFormer is supposed to compress. There seems to be a lack of understanding about what part of the model delivered the improvements. One reviewer said that this is potentially a great paper that deserves to be better explained. The basic concept of sharing a dictionary across layers should be simple enough to explain well and deserve a better explanation than eq 5.   The authors promise to release the code, which would be necessary for a full dissemination of this work. I recommend accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Attack problem formulation and a novel attack technique (to the best of my knowledge, I didn't see other poisoning attacks for off-policy evaluation methods). Effective solving techniques via influence functions (especially the greedy method for dealing with sparse L0 constraints). Strong empirical results.The main issue with this framework is the motivation for data poisoning. We probably also need to see if we can identify the perturbation introduced by using anomaly detection techniques. If they can be easily identified, decision-makers can run these techniques to check if there are any anomalies.The bi-level optimization approach is principled, well-justified and is applicable to a number of methods for off-policy policy evaluation. The paper introduces an important problem, namely data poisoning of off-Policy evaluation. Page 3 has some typos.The paper is well-written and easy to follow. The formulation of the poisoning attack as a BLO is natural. The novelty is somewhat limited from the modeling side. The experiments are good and show the efficacy of the attacking.	Meta Review: This work proposes a data poisoning attack for off-policy evaluation (OPE) methods. It also shows that several popular OPE methods are not robust to data poisoning attacks. The reviewers were all positive on this work, recommending strong accept, accept, accept, and weak accept. They praised the generality of the proposed problem and framework.  One main criticism that arose in several of the reviews was a lack of a stronger baseline comparator. In their response, the authors pointed out the challenge of identifying a suitable baseline, given a lack of previous work in data poisoning for off-policy evaluation. In an attempt to provide a suitable baseline, they developed their own comparator, namely a variant of the Fast Gradient Sign Method. They then showed that their main proposed framework outperforms this comparator. In my view, this response adequately addressed this reviewer critique.  The reviewers also made a number of other relatively minor comments and suggestions, and the authors addressed these in their replies.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Paper is very similar (especially all maths) to paper from same 1st author in Brudfors-2019. But clear novelty claimed to now use the MRF-NN model with a UNet instead of a probabilistic segmentation model. Unfortunately, visual results are not very convincing.The network is memory and runtime efficient, thus appropriate for certain clinical applications. The paper is well structured with appropriate language, and prior work is addressed decently. With larger UNet architecture (j=4,5), the difference of performance between MRF-UNet and UNet gets less significant.The combination of CNN and MRF is motivated from a different angle than previous publications. The presentation is clear, accurate, and concise, without unnecessary sidetracks. The MRF, judged based on the presented results, both quantitatively and qualitatively provides improved segmentations.The paper is fairly well-organized, well-written, and includes appropriate details explaining the methods. Experiments included both in- and out-of-distribution testing to demonstrate generalizability. While for low j (number of parameters) it appears that the proposed MRF-Unet may really improve segmentation.	This paper proposes a MRF-UNet product to improve the generalization to unseen data. The reviewers pointed out the high similarity and overlap with IPMI Brudfors-2019. Moreover, they questioned  the results, which are lower than current state-of-the-art.  Consequently, the practical use of this method seems quite limited. However this paper is well-written and the tackle problem is very important.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors introduce concept of q-calculus into neural networks along with its advantages. They define a family of stochastic activation functions based on standard functions together with q-Calculus.This work proposes to replace the regular deterministic activation functions used in artificial neural nets with stochastic variants. In its current form, it's more like a sketchy note rather than a serious academic paper. I would encourage the authors to significantly enrich the content of this writing before considering resubmitting.The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks. The experiments appear to show better training at early epochs, but none of the models appear to have been trained to convergence.	This paper proposes a new type of activations function based on q-calculus. The reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. The motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for (noisy) activation functions. After reviews, the authors have failed to present any updates to their paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is well motivated and mostly clear and easy to follow. The method is well formulated and the experiments details are clear. The paper is clearly written and adds a contribution to the GNNs society.The paper proposed a Topological Graph Layer (TOGL) plugin for Graph Neural Networks to boost the ability of topological structure detection. The paper provides new insights into topological aware GNN and I summarize the pros as follows.The paper is well-motivated, well-written, and builds on a good theoretical framework. Authors propose a topology-aware layer that is compatible with GNNs and can encode connected components and cycles. There are three questions that I would like to ask the authors.This paper introduces TOGL, a new layer for Graph Neural Networks (GNN) It makes the GNN "aware" of topological information during this training phase. It differs from the closely related work *Graph Filtration Learning (GFL)	This paper presents a new graph neural network layer that is sensitive to topological structure in the graph. Reviewers all believe the work is technically sound, and the experiments (particularly after author revisions) show clear benefits in cases where topological structure is important. The main questions are about whether the experimental evaluation is sufficient. While there are always more experiments that could be run, I tend to agree with the authors that the chosen experiments support the key claims in the paper, so it seems ok. The other question about the experiments is if they sufficiently convince the reader that topological structure is useful in practice. This seems more mixed. The paper would certainly be improved if there was a motivating application where there was a clear win. For example, molecular structures are used as motivation in the intro, but the best performing method on proteins doesn’t use the topological layer. All-in-all, though, there does appear to be clear improvements on carefully constructed cases, and there appear to be some benefits in real-world datasets.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This work considers the graph deconvolution networks. It proposed a graph deconVolutional networks to reconstruct the graph signals. The experimental results are very limited. The baseline methods are not well established. There are far better results on these datasets such as GIN.The paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations. It incorporates a denoising component based on graph wavelet transforms. The paper applies the proposed approach to tasks of graph classification in Table 1 and social recommendation in Table 2.The main contribution of this paper is that the authors design a graph deconvolutional network that combines inverse filters in the spectral domain and de-noising layers in the wavelet domain. Further graph autoencoders are proposed based on the graph convolutional networks.The authors proposed graph deconvolution layers (GDNs) and employ GDNs to learn graph embedding in a encoder-decoder framework. The writing and organization of this paper is good. The proposed method can outperform baseline methods on graph classification and social recommendation.	The covered topic is timely and of potential impact for many application domains, such as drug design. The paper is well written and presentation is clear. The proposed approach seems to have some degree of originality. Experimental results seem to be generally good, and in the rebuttal the authors have provided further experimental support to their main claim.  There are however some issues that have not been solved by the author’s rebuttal. I think two of them are the most important and related:   i) significance of contribution: although the authors have tried in the rebuttal to explain how the proposed approach differs from related papers, it seems that there are still doubts about the amount of innovation introduced by the paper. This issue could have been mitigated by SOTA experimental results in presence of a proper model selection, that, however does not seem to be the case here (see next point);  ii) model selection: the authors did not clearly explain the model selection procedure in the rebuttal. This is an important issue since it is often easy to get good results by picking the best run a posteriori. Unfortunately in the literature there are highly cited papers where model selection is not performed in a proper way and reviewers very often reject papers just looking at numbers without looking at how the numbers were obtained. So, I believe it is important to accept only papers where model selection is properly done and properly explained, so to allow for reproducibility of experiments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper seems to introduce a very important model to evaluate intepretatbility of neural networks. The main idea is to pool together interpretations coming from different systems and then selecting the best interpretation by voting. A running example can be very relevant in this section.Concensus approach retrains established architectures on the target dataset and averages their generated explanations from out-of-the-box explainers, such as LIME or SmoothGrad. The approach is evaluated for image classification on ImageNet and CUB-200-2011.Consensus is an evaluation method to measure the interpretability of a given deep model without the need of a dataset with annotated ground-truth concepts. The proposed method works by leveraging a set of pre-trained deep models (committee), and a reference model explanation method.The authors propose a novel method, called Consensus, to study the interpretability of models, when ground truth of interpretations is not available. The proposed method consists of three stages: forming a committee of deep models, aggregating the results in “quasi ground-truth”, and ranking models based on the similarity with the quasi ground- truth.	The reviewers all found that the Consensus method introduced seemed sensible and applauded the authors on their extensive experiments.  However, clearly they struggled to understand the paper well and asked for a clearer and more formal definition of the methods introduced.  Unfortunately, the highest scoring review was also the shortest and also indicated issues with clarity.  It seems like the authors have gone a long way to improve the notation, organization and clarity of the paper, but ultimately the reviewers didn't think it was ready for acceptance.  Hopefully the feedback from the reviewers will help to improve the paper for a future submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents a new framework to unify two streams of few-shot learning methods. Extensive results on several benchmark like PascalVOC, COCO, CUB and mini-ImageNet validate the proposed approach. Although the experiments are widely validated on several benchmarks, the comparison of the experiments miss several State-of-the-art paper results.Paper addresses few-shot learning in a combination of episodic meta-learning-based and pre-train finetune-based approach. It proposes meta-dropout, that consists in using dropout on training the base classes during meta-training, but not when tuning on novel classes. The paper is hard to read and the novelty and impact of the proposed contributions are not clear.This paper aims to improve the generalization power of meta-learning. It claims to propose an effective strategy named meta-dropout. The proposed strategy aims to prevent neural units from co-adapting excessively in the meta-training stage.The paper claims to have three contributions. The authors propose a unified framework for meta-learning based algorithm and pretrain finetune based algorithm. Drop out is applied for few-shot learning and achieve better generalization ability. Experiments on two tasks validate the effectiveness of the proposed algorithm. The pros of this submission is to validate the algorithm on different few- shot tasks but the contribution is unclear.	This work proposes an approach to unify pre-training-based and meta-learning-based few-shot learning, inspired by dropout.  None of the reviewers support the acceptance of this work, despite the authors' detailed rebuttals, with the majority of reviewers confirming their preference for rejection following the author response.   I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a new model for unsupervised event detection. It uses the above criterion to control the update rate of the latent states in a hierarchy of RNN layers. The paper includes some interesting experiments to support the effectiveness of the proposed method. Variational Predictive Routing (VPR) models continuous data as a hierarchical renewal process. VPR is time agnostic and can model a wide variety of temporal structures. Compared to a baseline method (VTA), VPR detects changes in synthetic datasets with high accuracy.This paper introduces Variational Predictive Routing (VPR), a spatiotemporal deep generative model for hierarchical video representation learning. VPR automatically detects expected and unexpected changes at each level of the latent hierarchy, which it uses to “route” bottom-up and top-down signals.The paper introduces VPR, an unsupervised hierarchical latent variable model. The latent representation is organized into a temporal hierarchy, with deeper layers representing features with slower rates of change. VPR represents a significant advance over previous models like CW-VAE.	Thanks for your submission to ICLR.  This paper considers a variational inference hierarchical model called Variational Predictive Routing.  Prior to discussion, several reviewers were on the fence about the paper, most notably having concerns about some of the experimental results as well as various clarity issues throughout the paper.  However, the authors did a really nice job addressing many of these concerns.  Ultimately, several of the reviewers updated their scores, leading to a clear consensus view that this paper is ready for publication.  We really appreciate your effort in providing additional details and results.  Please do keep in mind the concerns of the reviewers when preparing a final version of the manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this study, the authors observe that score-based denoising diffusion models can actually be decomposed into two phases. They argue that by substituting the first half of the two phases with VAE, higher quality images can be generated. They then propose DAED (Denoising Auto-Encoder + Diffusion)The paper investigate the behaviour of diffusion-based deep generative models (DDGM) The authors combine DDGMs with denoising auto encoders to introduce DAED. They show that this model can improve the overall sample and reconstruction quality. The interpretation of DDGM as generator and denoiser seems novel.The paper makes the observation that the reverse diffusion step of DDGMs can be decomposed into two distinct parts: one corresponding to denoising and the other corresponding to generation. The authors extensively study different aspects of such a decomposition and motivated by it, propose a new setup called DAED combining DDGm with DAE. The proposed DAED seems to have performance on par with DDG Ms for most setups.In this work, the authors investigate the generative and denoising capabilities of the Diffusion-based Deep Generative Models (DDGMs) They claim that a DDGM is a combination of two parts. The first one generates noisy samples from the pure noise by inputting more signal from a learned data distribution. The second one removes the remaining noise from the signal.	The paper analyzes diffusion-based deep generative models (DDGMs). The paper postulates that a DDGMs can be divided into two parts, a denoiser and a generator.   After the rebuttal and discussion period, three out of four reviewers supported acceptance of the paper. The reviewers aLmZ, QKfo, and 4jY9 all find the interpretation of a diffusion-based deep generative model as a decomposition of a generator and denoising part interesting. Those reviewers also note that this interpretation is useful as parts of the diffusion steps can potentially be replaced with a VAE, which would make the synthesis more efficient.  The reviewers aLmZ and QKFo also note that one comparison is perhaps slightly unfair in that the number of parameters of the two models considered was different; the authors cleared this up with new simulations showing that matching the number of parameters (as should be done for a better comparison) does not substantially change the conclusions of the experiment.   Finally, reviewer s9uB finds the approach and observations not to be novel and states that similar observations have been made in a recent CVPR paper by Benny and Wolf. While both the paper by Benny and Wolf and the paper under review study denoising diffusion models, the approaches taken by the two papers are substantially different, and both provide value to the community.  I also find the interpretation of the DDGMs as a denoiser and generator to be interesting and useful and therefore recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots.The paper presents Symbolic Reward Machines (SRM) which helps in achieving interpretable and explainable reward functions. The paper also proposes a hierarchical Bayesian framework to concretize SRM using expert demonstrations. The author evaluates the proposed method on a Mini-grid environment with varying complexity.This paper deals with the problem of learning non-Markov task specifications (or rewards) encoded as a reward machine. The authors adopt a Bayesian deep learning approach to simultaneously learn a reward approximator, a distribution over the parameters of the reward machine, and the policy to optimize the most-likely reward instance. They demonstrate that the inferred RM outperforms standard reward definition using standard RL approaches.The paper presents a new IRL algorithm built on top of a reward machine (FSA-like) representation. The machine is lifted to relational predicates that may have free variables that need to be inferred. The IRL procedure takes in a set of expert trajectories that are used to ground (or “concretize”) these variables and an RL algorithm that can be used to learn policies.The paper presents a method for inverse reinforcement learning for learning the non-Markovian (trajectory-based) rewards for symbolic transitions of a pre-speciefied automaton. Symbolic Reward Machines (SRM) are introduced, which define symbolic states, transitions and rewards. The method uses adversarial imitation learning, where the rewards that are computed by the SRM are used by a discriminator to classify expert and agent-trajectories.	The paper proposes a framework of Symbolic Reward Machine (SRM), an extension of Reward Machine (RM), for specifying interpretable and explainable reward functions. Then, a Bayesian IRL method is proposed to concretize an SRM using expert demonstrations. Experimental results demonstrate the effectiveness of SRMs in terms of training efficiency and generalization. The reviewers acknowledged that the problem of inferring symbolic rewards is important and that the proposed SRM framework is an important step in this direction. However, the reviewers pointed out several weaknesses in the paper and shared concerns, including (a) limited comparison with alternate approaches to tackle the problem and positioning w.r.t. the existing literature; (b) the domains seem rather simple since they require only a few demonstrations (implying that the holes being inferred might be quite small); (c) the novelty and theory around the SRM representation is not fully clear. I want to thank the authors for their detailed responses. Based on the reviewers’ concerns and follow-up discussions, there was a consensus that the work is not ready for publication. The reviewers have provided detailed feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA-NAS), using importance sampling. GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks. The proposed method could be broadly applied to micro/macro, constrained/unconstrained search problems.The authors introduce a method for NAS that repeatedly trains a generator to sample candidate architectures. The method is evaluated on three NAS oracle benchmarks as well as constrained NAS settings. While there are some promising experimental results, I lean slightly against acceptance.A paper proposes a Neural Architecture Search algorithm based on adversarial learning. The generator constructs architectures auto-regressively, which receives feedback from a GNN discriminator. Reinforcement learning (PPO) is used for training, to solve non-differentiability.In this paper, the authors introduce a NAS technique with an adversarial component. The discriminator learns to tell the difference between a set of good networks and randomly generated ones. This is quite a nice idea. The details regarding training and the generator architecture are relegated to the appendix.	This paper proposes a method for neural architecture search (NAS) based on adversarial methods. It uses a discriminator trained to distinguish between random vs. good architectures, letting the discriminator's scores serve as a reward signal for an autoregressive generator. I agree with AR1: this is a nice and clever idea. Reviewers generally agreed that the method was interesting, e.g. it's quite flexible in that it's able to incorporate constraints, and that the evaluation is rather extensive and shows that the method performs well across the board. Many minor criticisms were raised and addressed well by the authors in their responses and manuscript updates.  The major criticism shared by most reviewers was the high methodological complexity of the proposed approach, and the proportionally small gains shown over much simpler baselines. This criticism remained despite the authors' responses. The method is indeed complex: the same method without any adversarial component already performs well, and many important details of the model are relegated to Appendix A.2. (I would recommend, for example, moving Fig. 2 to the main text if at all possible. Also, the Appendix can/should be included in the main PDF for ICLR, rather than in supplementary material, as AR1 mentions.) It was not clear to reviewers that the adversarial component of the approach has a significant benefit. The authors respond by pointing to Table 7 showing that the discriminator reduces the number of queries and points out that in reality these queries correspond to expensive evaluations. If this is a major selling point of the method (it sounds like it could be), it should be highlighted and analyzed far more -- at least moved to the main text rather than an Appendix -- ideally with a real-world evaluation showing a practical large improvement in overall wall-clock time, rather than a benchmark where these evaluations are free. Perhaps the exclusive reliance on these benchmarks, though undoubtedly useful for quick experimentation, in the end holds back the paper and prevents the method's benefits from becoming apparent to the readers.  As a minor point (also raised by AR1), the paper is formatted incorrectly for ICLR: the font color is off, and more importantly the PDF is unsearchable (text cannot be selected, ctrl-F does not work), which makes it very difficult to quickly reference and review. Please try not to stray from the conference-provided style file for future submissions.  I appreciate the cleverness of the method, the extent of the evaluation, and the thorough responses to the reviews. However, unfortunately with the current presentation, it is too difficult to discern the benefit of the proposed approach from the manuscript. The approach is nonetheless intuitively appealing and seems quite promising, and I hope the authors will take the reviewers' good feedback into account and resubmit the paper in the future.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The manuscript describes a decision-tree learning approach to clustering time series / plans. The splits at each node in the tree are done based on LTL formulae that separate the set of plans at that node into two subsets while optimizing an information-gain measure.The paper, in summary, proposes a decision-tree based method to cluster a set of plan traces in an unsupervised fashion and yield an LTL formula for each cluster. Having a method that generates LTL formulas that distinguish one plan from another would be beneficial.	The paper presents a decision-tree approach for clustering plan traces and delineate them using linear temporal logic.  Both reviewers agree that the paper is well-written, it tackles a very interesting problem, and the technical methodology is sound. However, the reviewers also indicate two major problems with the paper: 1) the connection to XAIP is unclear; and 2) the work appears to be preliminary and incremental. Generally, the paper’s topic concerns plan summarization, which falls within the umbrella of XAIP, and a preliminary/incremental work is within the scopes of a workshop program. As such, in light of keeping the workshop a fruitful venue for discussion and dissemination of information, I am recommending accepting the paper.  Nonetheless, we urge to authors to reflect on the reviewer’s comments and address them in their revised version of the paper. Specifically, we hope the authors make a clear connection of their method to XAIP. Additionally, as the proposed approach builds heavily on the BayesLTL framework, the authors should include an overview of that framework and also explain the novelty of their approach compared to BayesLTL.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present "an accelerated first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions" The analysis aims to establish 'an accelerated rate' up to multiplicative factors induced by the manifold's sectional curvature.The paper proposes accelerated first-order methods for optimizing smooth and geodesically convex functions on Hadamard manifolds. The literature review is comprehensive and the background section is well-written. The theoretical results are well-motivated and presented.This paper proposed an accelerated first-order optimization algorithm for smooth and strongly geodesically-convex functions. It is proven in this paper that this Riemannian algorithm enjoys the same convergence rate as the Euclidean AGD algorithm without convex constraints.	The paper deals with accelerated methods on Riemannian manifolds. A particular challenge that the paper tries to address, which the AC believes is important, is related to the bounding of the iterates. The paper starts with an explicit bounding constraint on the manifold (and relaxes to the ball constraint for certain manifolds) and shows that the proposed algorithm can respect that while achieving acceleration. The reviewers including this AC see the merits of the paper. However, the paper in its current form is far from complete. A particular concern is on the empirical performance of the algorithm, resolving which should strengthen the paper. I would encourage the authors to build on the discussions and polish the paper accordingly.  Even though the paper has positive scores, the paper in its current form is a borderline paper with significant scope for improvement. To this end, the AC cannot accept the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function.The paper proposes a neural network based architecture to solve the approximate set membership problem. This approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b) The difference in the current paper is that the proposed approach relies on "meta-learning"A bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. The paper proposes a learnable bloom filter architecture.	This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors's substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.CyclesGym is a new benchmark environment for reinforcement learning (RL) applications to agriculture and farming. The benchmark allows for different crops to be produced over a span of multiple years. For the agronomist, this environment could provide a more realistic simulation of the real world than current benchmarks.In this manuscript, Turchetta et al. propose a new environment for crop growth models (CGM) simulation. The simulator is a wrapper around the software Cycle, one of many CGM that can be used for simulation of crop growth and crop rotation.This paper introduces CyclesGym, an RL environment based on the multi-year, multi-crop crop grow model (CGM) Cycles for open field agriculture. The targeted problem of RL applications in crop management/agriculture is understudied and has great potential for societal impact.The proposed benchmark improves on previous RL environments for an important domain with widespread consequences. Cycles appears to be a reasonable choice of underlying model, it is under active development and supported by research, it would be helpful if some justification for the choice was given.A key challenge all of these agronomic simulator-RL approaches have is that the simulators are not reliable. Until that is addressed, it's unclear that these simulators have any real value. The authors address that there is a sim-to-real gap that they are not addressing.	This proposal introduces CylesGym, the first Reinforcement Learning (RL) benchmark targeted at long horizon decision making in agriculture. Crucially, while prior work addresses single-year decision making, CylesGym captures the long term effects that one year's crop has on future generations.   The benchmark is clearly highly relevant and opens up a new frontier for RL researchers, making it a valuable contribution to the field. Furthermore, the benchmark does a good job of highlighting interesting opportunities for RL method development, such as costly information gathering, and evaluating current algorithms compared to baselines.   There was an active discussion between the reviewers and authors of the benchmark which resolved the majority of issues raised in the initial reviews. As a result there is broad support across the reviewers for the paper. A lingering concern is the sim-to-real gap which is mentioned at a number of places in the paper but could be emphasised more.   Lastly, the paper is well written and the evaluation sound. I believe this benchmark will be welcome by the community but I recommend that the authors address the concerns regarding the writing raised by  Reviewer MEYG, in particular regarding the utility for the agriculture community. In general I do not believe that issues which can be addressed in writing should be a reason to reject but those concerns should be addressed for the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies the problem of answering "first-order" questions that correspond to a single fact in a knowledge graph. The key contribution is a way of generating synthetic questions for the relations in the unseen domain for data augmentation. The evaluation appears to be well designed and shows some interesting and solid results.This paper presents a simple approach for domain adaptation in Knowledge Graph Question Answering. The paper consider the setting where the knowledge graph used to back the QA system contains the necessary facts for a test-time domain. To bridge the gap, the paper proposed a simple procedure for constructing synthetic questions.The paper studies the problem of single-relation QA. It proposes a data generation technique for domain adaptation. The paper is narrow and focused in its contribution, but the problem is significant enough to merit it.	This paper studies the problem of simple question answering over new, unseen domains during test time. A domain adaption framework and a seq2seq question generation method have been proposed to tackle this problem and demonstrates significant improvements over the previous baselines.  All the reviewers agreed that this paper is well-written and the results are convincing, but the problem is relatively narrow with a focused contribution.  Several reviewers also questioned whether this paper contains enough technical contributions. Some other issues have been already addressed during the discussion phase (long tail relations, presentation issues, and adding more related work).  However, we recommend accepting the paper considering the simplicity and effectiveness of the approach. We think it would lead to more discussion/future work in this direction.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper describes experiments that inject linguistic information (for example dependency structures) into BERT, then measure improvements in correlation with FMRI measurements. Linguistic information is incorporated by biasing attention heads to line up with dependency (or other) structures.An interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain. The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure can improve brain decoding performance.This paper tests whether fine-tuning large pre-trained language models with structural information can increase the correlation between these representations and the representations of brain activity. The injection of thestructural information is done through fine- Tuning of the pre- trained model by "guided attention"BERT is fine-tuned with and without a soft structural attention constraint. The results show improvements in decoding relative to baseline models. The single biggest concern is the extraordinarily high word perplexity scores in Table 2.	This paper explores the effect on decoding accuracy (predicting hidden representations from fMRI datasets) from fine tuning models by injecting structural bias.  This paper specifically focuses the attention of BERT on syntactic features of the text, which (for one dataset) appears to improve the decoding performance.  The paper's motivation is strong, and complex concepts are communicated clearly.  The review period was very productive.  There were some questions about analyses, and the validity of the statistical tests, but through some very thorough back and forth with the reviewers, this seems to have been resolved.  There is a good amount of analysis done on the resulting language models to try and determine the impact of finetuning or attention on the models. However, the results on the fMRI two datasets appear to be very different, and it's unclear why (and isn't clearly related back to the extensive language model analyses).  We would have liked to have seen a more thorough analysis of the stark difference in performance, and some convincing explanations for the difference based on the analyses.     P.s. A minor point, but the Wehbe paper uses Chapter 9 of Harry potter, not chapter 2.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The setting is not clearly written and some key definitions are not introduced properly (see specific comments below). Given that it is hard to understand the main results. The authors consider planning for Markov Decision Process. Precisely they study the benefit of convex regularization in Monte-Carlo Tree Search.This paper generalizes and build on top of the MENTS/E2W, and shows that the entropy regularization can be replaced with any convex regularization. Empirical results on Atari games confirm the value of policy regularization in MCTS.The paper provides theoretical analysis of the regularized backup for MCTS. It also provides some empirical gains on certain toy domains and some atari games. The paper seems to miss some highly related literature, in particular.The paper proposes a general framework for regularized Monte Carlo tree search, thereby generalizing the maximum entropy Monte Carlo planning of Xiao et al. The main problem is Proposition 1 and the definition of the convex conjugate.	Most of the reviewers pointed out a lack of rigor of this submission, unclear contributions, not too convincing claims and empirical gains. I thank the authors for the effort put in revising the paper and responding to the reviewer concerns. However, the reviewers did not deem them convincing enough.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper elegantly formulates it as an extension of the CTC framework and applies it to two tasks (ASR and OCR) The method shows robust training behaviors compared with the original CTC training. This paper would have a broad impact on the machine learning community.This paper presents a modification of the CTC training loss to cope with incomplete transcripts in the training set. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. The problem addressed by this paper is interesting and relevant.The paper is very well written with clear explanation of proposed method, well covered experimental results. Proposed idea is mainly based on existing DTW algorithm (SPRING), however it is extended and applied to CTC demonstrating significantly improved results on incomplete labels.This paper proposes a method to utilize partially labeled data for the CTC loss. In order to handle untrascribed lables, it introduces a wild-card for the beginning of the sequence and uses unconstrained endpoints for the ending. The proposed algorithm was evaluated with simulated masked data and it was shown that the proposed algorithm can handle incomplete labels more effectively.	This paper proposes an extension of CTC by considering the wild-card to adjust the label missing issues during training. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. It is empirically proved to significantly improve performance over CTC even if up to 40-70% label sequence is missed (overall performance similar to the complete label case) across different tasks.   As agreed by the reviewers, the paper is well presented and the problem is interesting to a broad community. Dynamic time warping with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e.g. word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness. Also the use of simulated data weakens the paper a bit.  The decision is mainly based on the clear presentation and fair experimental justification.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper builds useful insights and novel methods for graph convolutional networks, based on the Lanczos algorithm. The main advantage of the proposed method is its ability to generalize well in the presence of a small amount of training data.This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian. It further proposes an extension with back propagation through the Lanczos algorithm, in order to train end to end models.The proposed method is novel and achieves good results on a set of experiments. The authors discuss related work in a thorough and meaningful manner. The almost 10 pages are perhaps a bit excessive considering there was an (informal) 8 page limit.	The reviewers unanimously agreed that the paper was a significant advance in the field of machine learning on graph-structured inputs. They commented particularly on the quality of the research idea, and its depth of development. The results shared by the researchers are compelling, and they also report optimal hyperparameters, a welcome practice when describing experiments and results.  A small drawback the reviewers highlighted is the breadth of the content in the paper, which gave the impression of a slight lack of focus. Overall, the paper is a clear advance, and I recommend it for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper illustrates an adaptive based data augmentation method named as AdaAug. It searches adaptive augmentation policies in a class-dependent and potentially instance-dependent manner to improve the generalisation capability of deep learning models. The paper is well-written and easy to follow.This paper introduces a data augmentation method AdaAug that learns adaptive augmentation policies in a class-dependent and potentially instance-dependent manner to improve the generalisation capability of deep learning models. It proposes an efficient exploition-exploration workflow to search for an augmentation policy that optimizes the generalization performance.This paper proposes a search algorithm for instance conditional data augmentation. The contributions are two-fold: 1) it could be the first trial to automatically learn class-dependent or instance-dependent augmentations; 2) an efficient workflow is designed.AdaAug is an Automated Data Augmentation (AutoDA) method to learn a class/instance-dependent augmentation policy efficiently. The key ideas of AdaAug are two-fold. It uses a hidden feature of the original input to adapt the augmentation for each instance.	Reviewers agreed that this work is well-motivated and presents a novel approach for data augmentation around the adaptive augmentation policies. There were some concerns around the lack of ablation studies and unclear performance improvements, which were addressed well by the authors’ responses. Thus, I recommend an acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper attempts to learn embeddings for objects based on their affordances i.e., verbs that could be applied to them to realise their meaning. Here each dimension corresponds to an affordance or an aspect of meaning shared by actions, thus allowing a correspondence between nouns (objects) and verbs (their affordances) based on co-occurrences in text corpora in which they exist.The authors develop object representations based on the concept of affordances. They show the proposed approach is able to predict human judgements of object affordances better than distributional methods and LSA. They further show that the novel representations correlate well to a set of interpretable representations that were obtained via human judgement of object similarity.The authors design a distributional word embedding method inspired by Gibsonian theories of perception. They use matrix factorization techniques to derive low-rank object representations in what they call an "affordance space" They argue that the learned representations are interpretable, and that this affordance space "underlies the mental representation of objects"This paper uses a factorisation of a verb-object co-occurrence count matrix to predict which verbs are applicable to which objects. The main problem I have with the paper as it stands is that it's not clear what the overall goal of the work is. A further minor problem is that there is a whole related sub-field of computational linguistics which has been investigating a similar problem.	This paper is a computational linguistic study of the semantics that can be inferred form text corpora given parsers (which are trained on human data) are used to infer the verbs and their objects in text. The reviewers agreed that the work was well executed, and that the experiments comparing the resulting representations to human data were solid. The method employed has little or no technical novelty (in my opinion, not necessarily a flaw), and it's not clear what tasks (beyond capturing human data) representations could be applied to (again, not a problem if the goal is to develop theories of cognition).   The first draft of the work missed important connections to the computational linguistics literature, where learning about 'affordances for verbs' (referred to as 'selectional preferences') has long been an important goal. The authors did a good job of setting out these connections in the revised manuscript, which the reviewers appreciated.   The work is well executed, and should be commended for relating ideas from different sub-fields in its motivation and framing. But my sincere view is that it does not meet the same standards of machine-learning or technical novelty met by other papers at this conference. It is unclear to me what the framing in terms of 'affordance' adds to a large body of literature studying the semantics of word embeddings, given various syntactically and semantically-informed innovations.  It feels to me like this work would have been an important contribution to the literature in 2013, but given the current state of the art in representation learning from text and jointly learning from text and other modalities, I would like to have seen some attempt to incorporate these techniques and bridge the gap between the notion of affordance in text/verbs (selectional preference) and Gibson's notion of object affordance (what you can do physically with an object) in experiments and modelling, not just in the discussion. Such a programme of research could yield fascinating insights into the nature of grounding, and the continuum from the concrete, which can be perceived and directly experienced, to the abstract, which must be learned from text. I encourage the authors to continue in this direction. An alternative is to consider submitting the current manuscript to venue where the primary focus is cognitive modelling, and accounting for human, behavioural data, and where there is less emphasis on the development of novel methods or models.  For these reasons, and considering the technical scope of related papers in the programme, I cannot fairly recommend acceptance in this case.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The method is novel, however I do have some concerns regarding the method to ensure the consistency between the MRI used as a style image and the generated cardiac shape. The observed gain in performance in the segmentation task seem to be quite large and probably significant, however comparison with alternative methods is missing.The authors combine a variational auto-encoder and a GAN to generate synthetic cine MR images. They test their method on two well-known datasets showing good results. Unfortunately there is not enough space to discuss details about the method and potential limitations.The method described is interesting and should prove useful in the absence of large annotated datasets. Given the limitations of the short paper I think that this is an interesting  and reasonably well described work. Specific comments to improve the paper are below.The authors designed a GAN based synthesised method for cardiac data segmentation. I have some concerns regarding the results of this work. For instance, the segmentation results are quite similar or even slightly worse than the data augmentation. The novelty of the study is quite limited.	This paper describes a way to use VAE to produce more MR images, which in turn contribute to segmentation. Review comments are generally at borderline. The novelty and evaluation of the paper might be limited though.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In the paper, the authors study stable architectures for RNNs. The theoretical results seem interesting, although not very surprising. The presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, the paper needs improvement to meet the acceptance threshold.The authors studied contraction properties of continuous-time recurrent neural networks. They showed that a network of provably stable RNNs (net of nets) can be trained to reach competitive performance on several benchmarks, including sequential CIFAR10.This paper is primarily a theoretical contribution to the construction of assemblies of recurrent neural networks. We know that combinations of learned modular components can be powerful and far more tractable than learning bespoke models from scratch. So far, we have no theoretical guarantees that these combinations will actually remain stable. This paper develops the theory behind provably-stable combinations of RNNs using weight constraints and feedback mechanisms.Theoretical results constitute an evolutionary step in the understanding the conditions of stability. The submission proposes new theorems showing the stability of a class of RNNs. These results are then (partly) used to construct provably stable RNN combinations. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices.	In the context of recurrent neural networks, the motivation of the paper is to explore the "space" between fully trained models and almost not trained models, e.g. echo state networks, using a formal approach. In fact, a modular approach has proven to be very successful in many practical applications, and in addition brain seems to adopt this strategy as well. The addressed theoretical issue is stability of the network (i.e., the network implements a contraction map.) Specifically, it is assumed that a network is composed of a set of subnetworks that meet by construction some stability condition, and the problem is to design a mixing weight matrix, interconnecting the latent spaces of the subnetworks, able to give stability guarantees during and after training. Some novel stability conditions are proposed as well as two different approaches to design a successful mixing weight matrix. The original submitted paper was not easy to read, and after revision major problems with presentation have been resolved, although the current version looks more like an ordered collection of results/statements than a smooth and integrated flow of discourse. The revision has also addressed some concerns by reviewers on the role of size and sparsity of the modules, as well as the sensitivity of the stabilization condition on the mixing weight matrix has been experimentally assessed, obtaining interesting results. Overall the paper reports interesting results, however the novelty of the contribution seems to be a bit weak, e.g. stability conditions on recurrent networks (although different from the reported ones) were already presented in literature. Also the idea of exploiting, in one of the proposed models,  the fact that the matrix exponential of a skew-symmetric matrix is orthogonal to maintain the convergence condition during training, is not novel. Moreover, the experimental assessment does not provide a direct comparison, under the same architectural/learning setting, of the novel stability results versus the ones already presented in literature. Empirical results are obtained on simple tasks (using datasets with sequences of identical length), and relatively small networks, which limits a bit the scope of the assessment, as well as it is not clear if the observed improvements (where obtained) are statistically significant (especially when compared with results obtained by networks with the same order of parameters.) The quality of the assessment would increase significantly by considering datasets with sequences of different lengths, and involving more challenging tasks that do require larger networks.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper tackles the trustworthiness of concept bottleneck models (CBM) by improving the accuracy-interpretability tradeoff using a concept-based architecture. The paper is well-organized and easy to follow. Some improvements do not seem to be significant.The paper extends the concept bottleneck method by generating two embedding vectors for each concept. One represents the embedding when the concept is active while the other representing when theconcept is inactive. These two embeding are linearly combined through a scoring function (similar to the gating mechanism) the produces a probability score of which embedding is to be used. This extension increase the model capacity to encode more information about concepts.Concept bottleneck models implicitly learn to explain the downstream tasks in addition to learning how to perform them. Authors propose Concept Embedding Models (CEMs), a novel family of concept bottleneck models that address this issue.This paper tries to solve a challenging research question: to design XAI models which are good at task performance and interpretability. Compared to CBM, CEM contains an embedding generator layer that considers two embedding representations (one for activate and one for inactivate) and then produces a representation for one concept. Results show that the model produces high task accuracy and interpretable at the same time compared to CBMs.	This paper proposes Concept Embedding Models, which learn interpretable high-dimensional concept representations to exploit the tradeoff between accuracy, interpretability, and interventions on concepts. Reviewers vote for accepting this paper. The authors are encouraged to further improve this work based on reviewers’ comments in the camera ready and put the new experiments and discussions during the author-reviewer discussion phrase into the final revision, in particular the following:  - Add statistical significance test of experimental results - Compare training costs and model sizes - Better justify the proposed CAS mechanism - Investigate the robustness of learned concepts - Address the fairness concerns raised by reviewers in comparison with baselines
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes the novel idea of using contingency awareness to aid exploration in sparse-reward reinforcement learning tasks. They obtain great results on hard exploration Atari games and a new SOTA on Montezuma’s Revenge (compared to methods which are also not using any external data) The choice of extra loss functions is not very well motivated.This paper introduces contingency-aware exploration by employing attentive dynamics model (ADM) ADM is learned in self supervised manner in an online fashion and only using pure observations as the agents policy is updated. This approach has clear advantages to earlier proposed count based techniques.The core idea is to identify controllable (learned) features of the state, which in an Atari game typically corresponds to the position of the player-controlled character / vehicle on the screen. Once this position is known, one can use existing count-based exploration mechanisms to encourage the agent to visit new positions.	The paper addresses the challenging and important problem of exploration in sparse-rewards settings. The authors propose a novel use of contingency awareness, i.e., the agent's understanding of the environment features that are under its direct control, in combination with a count-based approach to exploration. The model is trained using an inverse dynamics model and attention mechanism and is shown to be able to identify the controllable character. The resulting exploration approach achieves strong empirical results compared to alternative count-based exploration techniques. The reviewers note that the novel approach has potential for opening up potential fruitful directions for follow-up research. The obtained strong empirical results are another strong indication of the value of the proposed idea.   The reviewers mention several potential weaknesses. First, while the proposed idea is general, the specific implementation seems targetted specifically towards Atari games. While Atari is a popular benchmark domain, this raises questions as to whether insights can be more generally applied. Second, several questions were raised regarding the motivation for some of the presented modeling choices (e.g., loss terms) as well as their impact on the empirical results. Ablation studies were recommended as a step to resolving these questions Reviewer 3 questioned whether the learned state representation could be directly used as an additional input to the agent, and if it would improve performance. Finally, several related works were suggested that should be included in the discussion of related work.  The authors carefully addressed the issues raised by the reviewers, running additional comparisons and adding to the original empirical insights. Several issues of clarity were resolved in the paper and in the discussion. Reviewer 3 engaged with the authors and confirmed that they are satisfied with the resulting submission. The AC judges that the suggestions of reviewer 1 have been addressed to a satisfactory level. A remaining issue regarding results reporting was raised anonymously towards the end of the review period, and the AC encourages the authors to address this issue in their camera ready version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Surprise, which quantifies the degree of changes in agents' environment, has received significant attention in the case of SARL. This paper explores surprise minimization in MARL by utilizing the free energy across all agents. The formulation of the energy-based model is theoretically akin to the minimum conjugate entropy objective.Paper introduces a new algorithm within the paradigm of Centralised Training and Decentralised Execution for multi-agent games. EMIX improves upon QMIX by adding a global surprise minimisation objective to the centralised-value function training. Paper also demonstrates improved performance than baselines on most games within Starcraft II.This paper proposed incorporating surprise minimization in multi-agent reinforcement learning (MARL) through temporal energy models for capturing the uncertainty across agents. With practical function approximation, the proposed algorithm according to the theory called EMIX was shown achieving superior performance.	All reviewers appreciated the quality of the paper, its contributions, clarity and theoretical justification, and novelty. I agree with the reviewers in recommending acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides an analysis of variational inference algorithms based on Gaussian approximation of Variational distributions. The theoretical results rely on the assumption of log concave distribution, an assumption that is also made in the study of Langevin type algorithms. The work is well situated among other contributions in the field.This paper proposes a method to find the optimal approximation of the target distribution, which has the smallest KL divergence. For the family of Gaussian distribution, the convergence analysis of the continuous-time dynamics and the discrete-time stochastic gradient algorithms are established under the strong log-concavity assumption.This paper produces an algorithm for computing variational posteriors that are normals or mixtures of normals. This is achieved via Wasserstein gradient flows. Remarkably, the system of the Langevin-type stochastic process this gives rise to was proposed before.	This paper proposes a novel method for variational inference based on Wasserstein flows. The key contribution is perhaps the rigorous guarantees that are derived from an assumption of log-concavity. While the initial submission was unaware of some existing work on VI that derives guarantees from similar log concavity or smoothness assumptions, the proof strategy that is given uses novel technical methods, and thus is of interest in any case. Readers would benefit from a detailed discussion that can contextualize this work to previous work, which the authors have committed to doing.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes a GNN-based method for objective space decomposition (ODA) in IP problems. The method is strictly designed for the ODA approach to be solved, which allows it to gains the performance improvements, but does perhaps not leave much room for further generalization.Existing ODA has to solve large amounts of IPs and it incurs unnecessary and redundant computation. Study proposes Markov Decision Process (MDP) framework for modeling the reduction procedure and learning the optimum reduction policy.This work proposes a learning-based approach to enhance objective-space decomposition (ODA) for solving multi-objective integer programs (MOIP) The proposed GNN model takes the instance information (decision variables, constraints, and objectives) as input, and then adaptively discards local upper bounds (to update the search region) without calling an IP solver.	In this paper, the authors exploit imitation learning with a two-stage GNN to learn the reduction rule for ODA to accelerate the solver and reduce the unnecessary computation. The authors evaluate the performances of the proposed method and demonstrate the advantages.    In sum, this paper consider an interesting application of machine learning for optimization and provide a promising solution. All reviewers provide relatively positive feedback of this submission.   Please consider the reviewers' suggestions to improve the submission:  - Justify the MDP modeling with concrete definition of the state and action for the reduction rule for ODA.   - Specify the data set construction and justify the generalization ability in the imitation learning.  - Provide comprehensive comparison, especially with PMOCO.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors describe a new method using Gate of Mixture-of-Experts to implement the compositional disentangled reconstruction objective. They run experiments across dSprites and 3DShapes and look into reconstruction error and different disentanglement metrics.The paper proposes a new approach for learning disentangled variational autoencoders. Experiments demonstrate promising results. Overall, the idea is technically sound and the results look promising. I hope the readers could clarify these questions and I will adjust the score accordingly.The paper presents a VAE variant that disentangles the features of the inference network at every layer. The approach is implemented as "recursive disentanglement network" based on a switch network. The results in dSprites and 3DShapes dataset suggest this variant performs better than well-known VAE networks.This paper proposed a recursive disentanglement network (RecurD) for the learning of disentangled representations from information theoretic perspective. The experimental results show RecurD outperforms some existing baselines on two benchmark datasets.	This paper proposes an algorithm for achieving disentangled representations by encouraging low mutual information between features at each layer, rather than only at the encoder output, and proposes a neural architecture for learning. Empirically, the proposed method achieves good disentanglement metric and likelihood (reconstruction error) in comparison to prior methods. The reviewers think that the methodology is natural and novel to their knowledge, and are happy with the detailed execution. The authors are encouraged to improve the presentation of the paper, by providing rigorous formulation of the "Markov chains" to avoid confusions, justification of the independence assumptions behind them, and more in-depth discussions of the learning objectives.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method for mean-variance trade-off optimization in RL. The method, named EQUMRL, tries to optimize the Pareto efficiency by maximizing the quadratic utility function. The experimental results show that the algorithm has better performance compared to baselines.This paper learns MV-efficient policies that achieve Pareto efficiency in terms of MV trade-off. Authors propose an approach called expected quadratic utility maximization (EQUMRL) The proposed EQUMRL does not suffer from the computational difficulties. Extensive experimental results are provided to show the superior empirical performance.The paper proposed an alternative objective (expected quadratic utility) to the mean-variance objective in an episodic RL setting. The new objective does not involve the squared expectation term, thus resolving the double sampling issue. The paper was nicely written and provided a simple alternative.The paper is well written and easy to follow in general. The survey in the appendix is helpful for understanding concepts and motivations from a finance and economics perspective. The paper considers an important problem and its proposed algorithm is justified by extensive experiment results.In this paper, authors propose a reinforcement learning approach to solve mean-variance portfolio optimization problem. Authors use well known results from mathematical finance to improve state-of-the-art for portfolio optimization using reinforcement learning. The paper is well written and the flow is clear.	This is a borderline paper with some reviewers voted for acceptance and some think it is not still ready. What is clear is more efforts by the authors is needed to make the paper appealing to reviewers with different interests. Changes such as better writing, more in depth literature review, more convincing experiments can definitely improve the quality of the paper. I personally do not think regret analysis is needed for this work, but it was mentioned by a reviewer. I would suggest the authors to use the reviewers' comments, revise their work, and prepare it for future conferences.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this paper, the authors propose a novel segmentation scheme that combines the block motion vectors for feature warping, bi-directional propagation, and feature fusion. Experiments demonstrate its effectiveness compared with alternative methods. The authors are expected to conduct more comprehensive experiments.This paper presents a feature interpolation strategy for fast semantic segmentation in videos. They first compute features of keyframes, then interpolate intermediate frames based on block-motion vectors (BMV), and finally fuse the interpolated features. The experiments show that the model outperforms one recent, closely related work wrt inference time while preserving accuracy.This paper advances a method for accelerating semantic segmentation on video content at higher resolutions. Experimental results on CamVid and Cityscapes show that the proposed method gets competitive results while saving computational time. Authors compare against similar pipelines for static processing and show gains in terms of computation time.	Strengths: Paper uses an efficient inference procedure cutting inference time on intermediate frames by 53%, & yields better accuracy and IOU compared to the one recent closely related work.  The ablation study seems sufficient and well-designed. The paper presents two feature propagation strategies and three feature fusion methods. The experiments compare these different settings, and show that interpolation-BMV is indeed a better feature propagation.  Weaknesses: Reviewers believed the work to be of limited novelty. The algorithm is close to the optical-flow based models Shelhamer et al. (2016) and Zhu et al. (2017). Reviewer asserts that the main difference is that the optical-flow is replaced with BMV, which is a byproduct of modern cameras.  R3 felt that there was Insufficient experimental comparison with other baselines and that technical details were not clear enough.  Contention: Authors assert that Shelhamer et al. (2016) does not use optical flow, and instead simply copies features from frame to frame (and schedules this copying). Zhu et al. (2017) then proposes an improvement to this scheme, forward feature warping with optical flow. In general, both these techniques fail to achieve speedups beyond small multiples of the baseline (< 3x), without impacting accuracy.  Consensus: It was disappointing that some of the reviewers did not engage after the author review (perhaps initial impressions were just too low). However, after the author rebuttal R1 did respond and held to the position that the work should not be accepted, justified by the assertion that other modern architectures that are lighter weight and are able to produce fast predictions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision. coreset WDRO approach is shown to outperform some benchmark methods on well-known machine learning datasets. I do not believe there is an immediate risk of the negative social impact of this work. The numerical experiments look very limited compared to the theoretical richness of the paper. It would be great to see more modern computational experiments with more visualizations.The paper introduces a method for computing a coreset for Wasserstein distributionally robust optimization (WRDO) problems by looking at the dual WRDO problem. The sound theoretical analysis, clarity of writing, and significance of the contribution all sum up to a strong submission.The purpose of this paper is to optimize the WDRO problem by corset method. The paper constructs a unified framework to construct the ϵ-coreset. The authors adequately addressed the limitations and potential negative societal impact of their work.The paper proposes a method to reduce the complexity of Wasserstein distributionally robust optimization (WDRO) in machine learning via coresets. The paper appears to be the first to employ the coreset treatment for the worst-case formulation in WDRO problems.	From the reviewers' comments and my own reading of the paper, the idea of bringing the notion of coreset from computational geometry to bear on the WDRO problem is novel and has the potential of further development. The authors should carefully address the reviewers' comments in the revision.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient.Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding?This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. The paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models.This work proposes an alternative loss function to train models robust to adversarial attacks. Instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix.	This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme. The paper is well-written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The architecture of the tracker is standard siamese. The novelty is at a technical level, modules of the "cross-guided" type have been proposed. It does bring an improvement, but not to the state-of-the-art level.This paper presents a Siamese-based single object tracking method using attention mechanism both in channel-wise and spatial-wise for learning deep correlation between exemplar and candidate images. Extensive experiments on UAV123, VOT2018 and VOT2019 demonstrate the effectiveness of the proposed method.This paper proposes a siamese network for visual tracking, namely SiamCAN. It utilizes cross channel attention, spatial attention, and anchor-free regression head. The method achives state-of-the-art performance on four visual tracking benchmarks.The experimental results show that this method has excellent performance on several public benchmark datasets. The novelty of the paper is deficient, the proposed method such as cross-attention mechanism and anchor-free regression have been previously exploited in existing models. The claims, method, and empirical methodology are correct.	All three reviewers initially recommended reject.  The main concerns were: 1) weak technical contribution and insight [R1, R2, R3, R4]; 2) incremental novelty (another variation of SiamFC) [R1, R2, R3]; 3) unconvincing experiment results against missing SOTA [R1, R2, R3];  The author's response did not assuage these concerns.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper provides a reformulation of the fundamental operations in Euclidean space that are used in neural networks for the Poincaré ball model of hyperbolic space. The paper was well-written, and the authors performed regular sanity checks. I’m assigning a score of 8 (a very good conference paper), and think that the paper is more or less ready for publication as is.Hyperbolic Neural Networks++ extends the existing work of applying hyperbolic manifolds to neural networks. The evaluation include direct comparison to HNN, and clustering with Transformers and seq-2-seq modeling.This paper bolsters the library of hyperbolic neural network building blocks by defining a variety of layers in a natural way. Some details could be improved, for example, the BLEU score is difficult to interpret, the architecture and task details in Section 4.2 are sparse, and the first two experiments report only in very low dimensions. Overall, the model shows general improvement over the baseline hyperbolics neural network models.This paper introduces an improvement of the "Hyperbolic Neural Networks" (HNN) proposed by Ganea et al. The idea of the paper was interesting to me but some motivations or choices were unclear. After reading the rebuttal and other reviews, the authors have addressed most of my concerns. Therefore, I am ready to increase my score.	The paper introduces new methods and building blocks to improve hyperbolic neural networks, including a tighter parameterization of fully connected layers, convolution, and concatenate/split operations to define a version of hyperbolic multi-head attention. The paper is well written and relevant to the ICLR community. The proposed methods offer solid improvements over previous approaches in various aspects of constructing hyperbolic neural networks and also extends their applicability. As such, the paper provides valuable contributions to advance research in learning non-Euclidean representations and HNNs. All reviewers and the AC support acceptance for the paper's contributions. Please consider revising your paper to take feedback from reviewers after reubttal into account.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors find that the popular triplet loss will force all same-class instances to a single center in a noisy scenario. After some analyses, the authors propose a simple sampling strategy, EPS. The method achieves good visualization results on MNIST and gets promising performance on benchmarks.This paper proposes a simple positive sampling mechanism called EPS for metric learning. The method is simple -- each sample selects its nearest same-class counterpart as the positive element. The authors provide both theoretical motivations and empirical studies on the proposed method.Using the sampled easiest positive, nearly all current metric learning methods got improved performance. I don't think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. I would suggest using three classes to derive theorems, rather than using two.This paper proposes an easy positive sampling method for deep metric learning. It aims to reduce the class collapse problem which is found to harm the performance of existing DML methods. The authors only provide theoretical analysis on the binary case and claims it can be easily extended to the multi-label case.	This paper is truly borderline. On one hand, the theoretical contribution seems novel and interesting, however, there appears to be somewhat of a gap between theory and practice.   There is unfortunately another problem. According to the authors, the main contribution of this publication is arguably the introduction of the nearest neighbor as the positive example in the triplet loss. However, the authors seem to be unaware of the history of the triplet loss. It was originally introduced by Schultz & Joachims 2004 as a loss over all triplets.  Weinberger et al. 2005 changed it and use the nearest neighbor as "target neighbor", which is called "easy positives" here, as the objective of LMNN. In 2009 Chechik et al. subsequently relaxed this positive neighbor formulation to any similarly labeled sample (going back to the Schultz & Joachims formulation) but sampling triplets. The re-introduction of the nearest neighbor as "easy positive" was then covered by Xuan et al. 2020.    Unfortunately all of this diminishes the novelty significantly and it is clear that the paper in its current form does not have a strong enough contribution. I do encourage the authors to take a close look at the original LMNN publication and Xuan et al and write an improved re-submission for the next conference that maybe focuses more on the theoretical contribution.  Good luck,  AC
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Approximate value equivalence is concerned with the study of families of models that yield $k$-step Bellman updates that are approximately equivalent to the Bellman update yielded by the true model. The paper provides a number of results that relate different models in terms of the error they incur.This paper proposes approximate value equivalence (AVE) which extends the previously proposed VE formalism by replacing equialities with error tolerances. The paper supports these results by providing intuitions and discussions of their implications. The submission looks technically sound and the core ideas are well-explained.This paper extends the notion of "value equivalence" among MDPs w.r.t. sets of policies and value functions. It is then possible, for any two classes of models, to find an epsilon for which the models satisfy the definition. Bounds that relate performance of policies across such approximate value equivalent models are also obtained.	A key discussion point in the rebuttal phase was the practical use of the proposed bounds, which two of the three reviewers brought up. The authors in response added an additional section (Section 6) and experiment to address this concern. While some concerns regarding the practical use of these bounds remain, the authors have made a sufficiently convincing case in my view. Hence, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The work proposes a task that requires several aspects of compositional reasoning. The author(s) introduce a notion of a "compositionality gap" to quantify the difficultly of each generalization type.The following work presents a CLEVR-based compositionality benchmark. The task of the model is to verify logical statements about an image. The statements are generated based on a grammar over a controlled set of relations/attributes/terms.This work proposes the CURI dataset to measure productive concept learning under uncertainty. The dataset is designed using a concept space defined by a language. The authors also design several out-of-generalization data splits that test models' ood generalization performance.	This paper was reviewed by 3 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an way to incorporate task information into multi-task learning (MTL) The hope is that more explicit knowledge of task information will improve MTL. While the results in Table 2 and Table 3 look promising, the presentation would benefit from more analyses experiments.This paper proposes a new framework that computes the task-specific representations to modulate the model parameters during the multi-task learning (MTL) This framework uses a single model with shared representations for learning multiple tasks together. The paper argues that providing OHV text labels is not always possible for tasks involving non-text modalities such as speech and images.This paper proposes a multi-task Transformer that does speech and machine translation within the same framework. It proposes task adaptive parameters by modulating model parameters to account for differences in tasks. The results in the paper don't look terribly good either.This paper combines Feature-wise Linear Modulation (FiLM) with a single/multi-modal transformer for a joint multi-task neural network. The proposed model applies the proposed model to two tasks, which are single-Modal machine translation and multi- modal (speech/text) machine translation, speech translation, and speech recognition.	This paper proposes an approach for improving MultiTaskLearning by providing a way of incorporating task specific information.  Pros: 1) All reviewers agreed that the paper is clearly written 2) Interesting to see a single model for AST, STS (speech-to-speech translation) and MT   Cons: 1) The work is not adequately compared with related work (some important references are also missing) - The authors did perform some additional experiments with T5  and pointed out some drawbacks but this needs to be explored a bit more. 2) The answers about scalability are not very convincing and need more empirical results.   Overall, none of the reviewers were very positive about the paper and felt that while this is a good first attempt, more work is needed to make it suitable for acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides evidence showing that optimizing a parametric mix of regularizers when training a model provides better generalization than using handcrafted ones. The results show that the best regularizers mix is dataset dependent, and that regularization matters most when limited data is available.This work defines a search space over 13 regularisation techniques and employs one flavour of Bayesian Optimisation + Hyperband approach. It concludes by substantiating three claims with corresponding experiments. The paper is easy to follow with a thorough literature survey (to my knowledge) and contains relevant experiments.This paper provides an empirical study of combining different regularizers. Fourteen regularizers including batch norm, weight decay, etc. are considered. Using 40 tabular datasets, they show mixtures nearly always outperform tuning a single regularizer.The paper presents a study on regularization methods for the feedforward fully connected neural networks. The study is formulated as hyper-parameter optimization task, heavily using Auto-Pytorch library.	The paper in its most recent version claims that deep neural networks, when very carefully regularized, outperform methods such as Gradient Boosting Trees on tabular data. This is genuinely surprising to me (in a good way), and I suppose it is as well to the community.  The paper initially received negative reviews with two key remarks that "The results are somewhat expected." (R4, R3, R2). Indeed, the original version mainly stated that very careful regularization helps on tabular data.  Naturally, the reviewers (including myself) seen then as the second key weakness that "All experiments are run on tabular data." (R4, R3).  Based on the reviews, the Authors have clarified and changed their message. I think it is well summarized by R2 "The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks."  As R2 said and was reflected in comments by other reviewers, "[...] convinced by authors response on paper novelty, technical contribution and (after the re-focusing) potential usefulness to the community".  Given the new message of the paper, a key new question surfaces. Is this indeed the first convincing demonstration that deep learning can outperform more standard methods on tabular data? R2 pointed out TabNet (see also Google Cloud offering) that already in 2019 claimed "beating GB methods for the tabular data". There is also NeurIPS work "Regularization Learning Networks: Deep Learning for Tabular Datasets"; their abstract opens with "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs". The latter work did not claim to beat GBT. Regardless, the two works should be carefully discussed and compared empirically to in the new version of the work.   I am also not yet fully convinced by the added comparison to GDBT. Arguably, AutoML from the sklearn package is not the most popular way to use GDBT in practice. How would regularization cocktails compare to GDBT from XGBoost, optimized using either random search or bayesian optimization?  Based on the above, I have to recommend the rejection of the paper. The key reason is: *the new reframing of the paper is exciting but warrants a much more detailed and careful evaluation*.  I really appreciate the work the Authors have put in clarifying and changing the message of the paper. I understand this is disappointing that we won't be able to include the work in ICLR. Nevertheless, I hope that the Authors found the feedback useful, and wanted to thank the Authors for submitting the work for consideration in ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces an additional objective to GAN training called “difference departure from normality” that is meant to encourage correlation between real and generated samples projected into some subspace. This is intended to prevent instability during training. The technique is applied to audio spectrogram generation.The paper proposes a novel conditioning-based regularization of GAN training. While it has been known that tying up the generator and discriminator's latent variables can help stabilize gan training, the proposed method is based on a different theory. The experimental results also support the authors' argument about the improved stability.This paper propose a constraint in GAN training, to improve generated samples fidelity and stabilize training. The proposed conditioning is based on limiting the generator from departing normality function of real samples. It is claimed that this conditioning will not limit the exploration of all modes of real data distribution.This paper proposes a conditioning method to train the GANs. The conditioning trick is based on the DFN (departure from normality) metric computed in the spectrogram domain of Schur decomposition to ensure the correlation between real and generated samples.	The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis. All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results. Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain. After consolidating the reviews and rebuttal, the area chair finds the reviewer's argument convincing and would not recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper describes a framework for assessing compositionality of representations by comparing the learned outputs against those of the closest compositional approximation. The paper demonstrates the use of this framework to assess the role of compositionality in a hypothetical compression phase of representation learning.The authors propose a measure of compositionality in representations. Given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation of the input x.The paper tackles a very interesting problem about representations, especially of the connectionist kind. The framework assumes the presence of an oracle that can give us the true compositional structure. The experiments seem to be fairly similar kinds of composition with very few attributes (mostly bigrams)	This paper presents a method for measuring the degree to which some representation for a composed object effectively represents the pieces from which it is composed. All three authors found this to be an important topic for study, and found the paper to be a limited but original and important step toward studying this topic. However, two reviewers expressed serious concerns about clarity, and were not fully satisfied with the revisions made so far. I'm recommending acceptance, but I ask the authors to further revise the paper (especially the introduction) to make sure it includes a blunt and straightforward presentation of the problem under study and the way TRE addresses it.  I'm also somewhat concerned at R2's mention of a potential confound in one experiment. The paper has been updated with what appears to be a fix, though, and R2 has not yet responded, so I'm presuming that this issue has been resolved.  I also ask the authors to release code shortly upon de-anonymization, as promised.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces an MCMC algorithm for private parameters by augmenting the parameters space with the private date. They propose an MH  and Gibbs sampling type sampler that alternatively updates unobserved variables and parameters.This paper presents a general MCMC algorithm for approximating the joint posterior distribution of model parameters and true/denoised data, given a corrupted data set. The algorithm follows the standard approach for Bayesian inference under measurement error. The paper also states the $O(n)$ time complexity of this step and presents a derivation of MCMC's ergodicity in this context.In this paper, authors propose a method for noise-aware statistical inference under DP. The aim is to enable Bayesian inference that can capture the DP induced noise. Authors show that the proposed Metropolis within Gibbs sampler targets asymptotically the correct distribution.	This paper presents a general MCMC algorithm for approximating the joint posterior distribution of model parameters and private data, given output from a differentially private algorithm. The paper provides new tools for Bayesian inference under privacy constraints, which can be useful for the differential privacy community. There are a few suggestions from the reviewers/discussion. First, even though the authors claimed their results are fully general, they should consider toning it down or at least clarifying upfront their "Record Additivity" assumption, which seems non-trivial. As one of the reviewers remarked, one of the limitations of this approach is that it will not scale well with high-dimensional data. The AC suggests the authors add this limitation in the discussion of the paper.  Other comments: While not critical, the AC also has a question regarding the following part: - In line 105: the assumption is that the density of the mechanism's output distribution is known. This has nothing to do with what privacy variant you use to analyze the mechanism.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies how the preprocessed data can be reused as auxiliary tasks in primary multi-task learning (MTL) for the multimodal emotion detection task. Two different hierarchical-level models, FLAT-MTL hierarchical attention model and HAN-Rock model, are proposed.This paper addresses challenges faced in the multi-task learning (MTL) models used in analyzing multimodal conversational data. The main challenge paper is trying to solve is on how to select relevant auxiliary tasks that avoid negative transfer. The paper is well written and easy to follow.The paper addresses multi-task learning for multimodal emotion recognition on two existing datasets (IEMOCAP and SEMAINE) It is motivated by the analysis of video-conferencing videos which is indeed a crucial and topical issue. However, this motivation is a little bit heavy-handed.This paper tackles conversational analysis problem and more specifically the Primary Multi-Task Learning. The paper adopts an experiments-driven approach to test the three hypotheses, but the main issue is that this approach adopts a specific neural network method.	The initial reviews for this paper were very borderline. The authors provided detailed responses as well as a few additional results and observations. The authors' responses answered the reviewers' questions and addressed their main comments (including in the discussion of related works as well as with more in-depth analysis in a new Section 5.1). Unfortunately, the reviewers did not come to a consensus.  Overall, this paper extends some current methodology for emotional classification, is well-executed, and provides a reasonably thorough study. The results are somewhat in line with previous results from other fields (and notably NLP), but the authors demonstrate the efficacy of using primary multi-task learning for multimodal conversational analysis.   Unfortunately, this paper also has some flaws as highlighted by the initial reviews. As stated above, the authors did provide a strong rebuttal, but given the different comments raised by the reviewers that spanned many aspects of the paper including motivation, possibly limited contribution and novelty, missing related work, somewhat shallow analysis of the results, I find that another full round of reviewing would be useful to assess the paper.  As a result, this remains a very borderline paper, and given the strong competition at this year's conference, I cannot recommend acceptance at this stage.  I suggest that the authors incorporate some of the discussions from this forum (and especially with respect to related work, new findings, and clearly defining the motivation and contribution of this work) into the next version of their paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper fails to be clear with respect to the use of the tensor product. It is unclear how this model generalizes. This is important research that mixes pre-existing knowledge and corpus-derived knowledge.MorphTE is a new word embedding compression method with morphological augmentation. The authors conduct experiments on machine translation, question answering, and natural language inference tasks. The method is well motivated and performs well.This paper proposes to incorporate morphemes while compressing word embeddings and achieve lossless compression while decreasing the number of parameters. The authors compare their method to several compression techniques and show that even with a 40x compression ratio, they are on par with the baseline. On 20x they can outperform previous baselines.	The paper presents a method to compress word embeddings using morphologically-enhanced tensorized embeddings. The main idea is decomposing a word embedding into a tensor product of multiple small vectors. This idea has been explored in previous work [1], the main contribution of this is using *morphological segmentation*, aiming to capture morphological features.   The proposed method is compared to prior work on multiple tasks (machine translation, qeustion answering, etc), overall exhibiting strong performance in terms of compression rate & performance (though sometimes gains are pretty smaller compared to Word2ket (e.g., Table 2)).   During the discussion phase, the authors provided the results with random, non-morpholgically inspired segmenation, which was useful. I’d recommend putting them into the main paper. Experimenting on morphologically rich languages (e.g., Finnish) would be useful. As well as sensitive study towards hyper parameters (e.g., dimensionality of morpheme embeddings) which can easily impact the compression rate & performance.   Overall, the reviewers and AC is positive about the paper.   [1] word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement https://arxiv.org/abs/1911.04975
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The approach proposed is basically an auto-encoder, consisting of a 2-step encoder-decoder network. In the first step, the sentence is encoded into a vector which is in turn decoded to a (smooth) indicator vector to mask words in the sentence. The second step is when the masked sentence is decoded into the output.The paper presents some interesting ideas and is well written, but the content is not quite sufficient for publication. The task of telegraphic sentence compression, whose usefulness is not a priori obvious, is barely motivated. The authors are encouraged to apply there methods to full lengths documents, which would make for a more substantial contribution.The paper explores unsupervised deep learning model for extractive telegraphic summaries. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.	This paper presents methods for telegraphic summarization, a task that generates extremely short summaries.  There are concerns about the utility of the task in general, and also the novelty of the modeling framework.  There is overall consensus between reviewers regarding the paper's assessment the feedback is lukewarm.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper points out that model update frequency and update quantization level are not disjoint decisions in federated learning systems. It explains how to modify and combine prior work on each of the two decisions to develop a new adaptive aggregation method. Strengths: The paper considers a relevant problem.In this paper, authors propose the framework for federated learning that adaptively adjusts the frequency of the communication rounds and the quantization efficiency in a synergistic way. The idea of adaptation of quantization and frequency of communications is very nice; however the broadcasting procedure seems to be very time-consuming.The paper proposes combining two orthogonal algorithms -- the lazily aggregated gradient(LAQ)method and adaptive quantization (AdaQuantFL)-- to reduce communication complexity in federated learning. At the heart of the combining strategy is Eq. 7, which allocates the bits used in each iteration. The authors theoretically show that such a bit allocation strategy leads to reduced communication.	Two of the initial reviews of the paper were mildly positive (2 scores of 6), and one was very positive (score of 8). However, these reviews failed to notice some severe issues with the paper, which were detailed by the Area Chair in an Extra Review which was provided late. The severe issues include: clarity of exposition (undefined notation in many places) and theory (vacuous or meaningless theorems and assumptions). I apologize to the authors for not having had the chance to defend against this late review. However, the issues are indeed severe.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a method called optimizer grafting. It uses two optimizers in one training session. One is to decide the update direction of parameters, and the other is to decision the update stride of parameters.The authors report on a technique to address learning rate hyperparameter tuning for deep learning referred to as optimizer grafting. The paper proposes a meta-algorithm that blends the steps of two optimizers by combining the step magnitude of one (M) with the direction of the other (D)The authors propose learning rate grafting as a method to explore the power and dynamics of optimizers. The experiments performed within the paper are of high quality, satisfactorily demonstrating the main claims of the paper. My main qualm with the experiments is the lack of motivation for the specific experimental configurations.The authors investigate the entanglements between the optimizer and the learning rate schedule. They propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer.	The paper proposed Trained ML oracles to find the decent direction and step size in optimization. The process they call grafting. Reviewers raised several concerns about the reliability of ML oracles in general settings which is valid. The rebuttal could not convince the reviewers to change their opinion.  Ideally for an empirical only paper with heavy reliability on ML for critical decisions, to meet the high bar of ICLR there must be several experiments (5-10 datasets or more) on diverse datasets and settings. Also, there should be discussions on when and how the method fails and related discussions. In that sense the paper does not meet the bar for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The proposed method does answer to the main motivation, which is to reduce inference time. While the proposed approach is ten times faster than the "simplified conventional approach", the conventional approach only takes around two minutes to plan the thermal ablation procedure. Some aspects of the experiment are confusing.The authors report comparable performance to a conventional method while being much faster. The results are a bit too short, e.g., there is no visualization of resulting paths. It also appears to be a modified re-implementation without much detail.This is a well executed paper with an interesting application of reinforcement learning to liver tumor ablation planning. The approach is 10x faster than the state-of-the-art and yields significantly better results in terms of placement accuracy.	The work describes the use of RL for trajectory planning for minimally invasive liver tumor ablation. The main motivation is that RL can accelerate planning over conventional methods.  Strengths according to reviewers: * Interesting problem. * Relatively well written. * Good description of RL methodology.  * Interesting results (comparable performance to conventional method, faster inference) * Uses public database, which may facilitate reproducibility and comparisons in future work.   Weaknesses: * Limited novelty with respect to machine learning. Main contribution is rather the idea of using RL to improve the specific application. * Not extensive evaluation.  Initial reviewer concerns regarding weak motivation and clarity were addressed to a significant extent, improving the manuscript, which is reflected to the increased ratings after rebuttal.   After a very productive rebuttal and discussion period, accompanied with significant updates to the paper, there seems to be a concensus that the paper is of acceptable quality for a publication in MIDL.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors address the issue of building robust neural networks by training them using an ensemble based loss function. The main concern in robust ML is that many attacks are transferable between models, leading to blackbox attacks. The authors derive a new methodology to promote "orthogonality" between different ensembles.Distance Map Layers (DML) can be used as the one just before the final layer in a neural network for classification. DML is mainly used to improve the diversity of predictions from the ensemble members. The experiments are extensive and really promising.This paper aims to improve the robustness of ensembles of neural networks to adversarial attacks. If the models in the ensemble are susceptible to similar attacks, then the ensemble is also vulnerable. The authors propose appending distance map layers (DMLs) to the learned representations of the member models.This paper proposes to use the distance map layers for ensemble-based defense. By randomly choosing the centers to vary over classifiers, and imposing the I-covariance matrices to be dissimilar and possibly orthogonal, thedistance map layers can be used to reduce the rate of transferability.	The paper proposes a method to improve adversarial robustness by diversifying the ensemble.   Novelty: As pointed out by several reviewers, promoting diversity of ensembles has been done in the literature, but there's still a moderate novelty in proposing the DML layer.   Empirical validations: The original submission lacks many important comparisons (e.g., with [1]). Despite the authors implicitly compared their method with (Pang et al) via Auto-attack in the rebuttal, it will be better if the comparisons are conducted in a well-controlled way to confirm that the improved robustness comes from DML instead of other hyperparameter settings. Further, it is not clear whether the proposed method is robust to hyperparameters.   Based on these, we recommend rejection but encourage the authors to improve their paper based on the comments.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper addresses a crucial flaw in the proof of the soundness of the algorithm of the general identifiability problem of Lee et al. 2019. There were some small parts that were difficult to follow or where the readability could be improved (see detailed comments below).The theory of g-identifiability is a complex one. It is not clear if the new algorithm can be used to solve the problem of how to identify certain types of effects. There are many problems with the way the theory is written.The authors clearly illustrate how the previously proposed g-identifiability algorithm fails in a simple scenario. The positivity assumption could be explained in more practical terms, i.e. what does it mean when one has to actually estimate the causal quantity of interest.Papers in identification theory can be verbose and full of jargon, but this paper is well-written. The paper addresses the issue of positivity which has been less carefully considered in the graphical identification literature. This paper highlights an issue with a high-impact paper in the literature.	Meta Review: In this paper, the authors revisit recent work (Lee et al., 2019) on identification of interventional distributions from a set that may include the observed data distribution and/or a set of interventional distributions.  (Lee et al., 2019) proposed a sound and complete identification algorithm for that problem.  The authors point out a critical flaw in that work: the completeness proof relies in a crucial way on absence of positivity.  However, absence of positivity (even including relaxations of positivity proposed by Shpitser and Pearl) results in the proposed algorithm not being sound, as the authors convincingly illustrate with examples.  The authors provide a repair of the algorithm proposed by (Lee et al., 2019), with new soundness and completeness proofs.  The reviewers were unanimously positive about this paper, and their questions were answered by the authors to their satisfaction.  Some reviewers pointed out that in addition to repairing a critical flaw in a recent paper, this work will also encourage authors in the causal graphical modeling community to pay more careful attention to important issues related to positivity or "overlap" assumptions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper is well-written and clearly describes the problem and results. The results for PL functions and overparameterized quadratics are tight, which is excellent. I like the paper but have some questions/concerns, due to which I have currently weakly accepted the paper.This paper studies an important problem in the distributed optimization. To train an overparameterized model over a set of distributed nodes, what is the minimum number of bits required to reach zero loss? The paper gives lower bounds in two cases: PL and over parameterized case.This paper considers distributed (centralized and decentralized) smooth nonconvex minimization problems in the general case and in the special (quadratic) overparameterized case. The authors give lower bounds on the amount of information transferred in terms of dimension, number of agents, amount of data, and solution accuracy. Then the authors give an algorithm by which these lower bounds can be achieved.	This paper considers the following problem in distributed optimization: To train an overparameterized model over a set of distributed nodes, what is the minimum number of bits required to reach zero loss. The paper gives lower bounds on the bit complexity for two settings: non-convex functions satisfying a PL condition and overparameterized quadratics. The authors then give an algorithm that (1) for PL objectives, has optimal communication complexity (up to logarithmic terms in the dimension of the problem) and (2) for quadratic overparameterized objectives, attains optimal communication complexity (up to logarithmic terms) with high probability.  This paper generated significant discussion in the initial author-reviewer discussion period. Most of the reviewers were quite positive on the paper, and found the results to be interesting and relevant to the community, and found the paper to be well-written. They found the results, which provide near-optimal sample complexity for two settings (PL functions and overparameterized quadratics) to be technically strong, and were impressed by the tightness of the results. One reviewer took issue with certain limitations of the paper, including: - A limitation of the lower bounds in the paper is that they concern only deterministic methods. - The results are only tight with respect to the parameters $D$, $N$, and $\epsilon$, and are not necessarily tight with respect to $L$ and $\mu$. - Some related work can be discussed in more detail.  These limitations do not seem to take away from the novelty, and neither I nor the other reviewers were convinced by the other issues raised in the discussion. As a result, I believe the paper is worth accepting as a starting point for future research in this direction. Nonetheless, the authors are encouraged to expand the discussion around these issues and limitations in the final version of the paper, as well as expand the comparison to related work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper studies the fair ranking problem under a model where socially-salient (protected) attributes of items are randomly and independently perturbed. The paper proposes a solution for this problem that can be applied to multiple non-disjoint attributes and both proportional and equal representation classes of fairness constraints.Theorem 3.1, their main theoretical result, upper bounds the fairness violation and lower bounds the utility of the ranking solution found using these constraints. Then, they provide a feasible relaxation of these constraints such that existing methods for optimization can be applied. They demonstrate empirical results on two “real” and one synthetic dataset.This paper studies fair-ranking where, for each item, its group memberships are only known probabilistically. The authors introduce an approximation algorithm to solve the fair- ranking problem. In experiments, the proposed approach is the most fair of all baselines and achieves the best tradeoff between utility and fairness.This paper studies the fair ranking problem under a setting where groups $G_i$ are not deterministically known. The goal of fair ranking is to find a ranking of top-$n$ items such that the utility with respect to $W_{ij}$s is maximised.	This paper looks at the fair ranking problem, a known variant of ranking where group fairness constraints (typically hard, sometimes soft) are imposed on the traditional ranking objective, but where membership of each item to be ranked in a group (aka the sensitive attribute's value associated with that item) is unknown.  The paper provides strong theoretical results and, especially post-rebuttal, strong experimental backing of the setting at hand.  Some assumptions are relatively strong, as surfaced by reviewers (e.g., 3sFW), but by and large reviewers believed the work to be well motivated and complete, and I agree with that.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a 3-phase heuristic algorithm to learn a causal graph from interventional data using continuous optimization. The paper is hard to follow. The proposed method is not a systematic approach and accordingly it's hard to reason about its use.This paper aims to extend the continuous optimization approach to causal discovery to handle interventional data as well as observational data. It describes a method for learning the causal structure over a set of categorical variables and reports strong empirical performance.The paper provides a novel approach in the area of structure learning for causal bayesian networks. The approach suggested fits the network before interventions, simulates the intervention on the fitted network and then again assigns a likelihood score to the network parameters.The authors propose a method for structure learning from observational and interventional data. Data is discrete-valued, there are no hidden confounders, each intervention affects only one variable, but the location of it may be unknown. A three-phase score-based, iterative procedure is proposed.	In this paper, the authors study how to incorporate experimental data with interventions into existing pipelines for DAG learning. Mixing observational and experimental data is a well-studied problem, and it is well-known how to incorporate interventions into e.g. the likelihood function, along with theoretical guarantees and identifiability. Ultimately there was a general consensus amongst the reviewers that without additional theoretical results to advance the state of the art, the contribution of this work is limited.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper investigates important AI-safety questions of whether new vision transformer based models are capable of detecting near OOD samples. The conclusions of the paper seem very preliminary and are hard to judge without information about the respective models predictive performance.Transformers are quite new to the community, and although the authors do not present positive results on the ability of transformers to detect OOD, it could lead to fruitful discussions for further research on the subject. I would have liked to see one other method for OOD detection presented, e.g., ensembling several models.	It appears as if both reviewers found some positive aspects and adapting and evaluating known techniques for OOD detection with either transformers or standard CNNs. The results are considered to be not exactly exciting and more work would be required to come to a more insightful conclusion. In summary I believe the paper is not yet quite ready for publication and could be further improved for a future date.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper deal with the problem of objections (or object centric) learning by using differentiable iterative refinement to improve the stability and tractability during training. The paper evaluates the proposed extension on the CLEVR-Mirror, Shapestacks, and COCO-2017 datasets.This paper notes that a number of state-of-the-art object-centric learning models rely on an iterative refinement step that can be understood as a fixed point algorithm. It then uses implicit differentiation to avoid a costly and unstable unrollment to increase the performance of SLATE.The paper combines existing techniques from two groups of works. First, unsupervised object-centric learning with sets representation. Second, implicit differentiation techniques. The paper is very nicely written overall. The logic flow take us through the author's thinking and reasoning clearly.Object-centric models are known to be brittle in training and with regard to hyperparameters. In their experiments, the authors show that the training of SLATE becomes more stable when truncating the gradients after the last refinement step instead of propagating it through the unrolled refinement procedure.	The paper proposes to treat object-centric models with iterative refinement procedures as fixed point operations and optimize them using implicit differentiation.  Overall, the reviewers find that the contribution of the paper is somewhat novel, although similar ideas have been presented in prior work in different contexts (supervised settings). Only one reviewer was more negative before the rebuttal, eventually increasing their score after discussion with the authors.  I, therefore, recommend acceptance and encourage the authors to address the comments raised by the reviewers in the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents aims to present a method for decomposing a CAD sketch into a set of of _modular-concepts. The modular concepts are compostions of lower-level primitives connected via constraints. The whole pipeline is trained end-to-end under a (permutation-equivariant) reconstruction loss.This paper contributes an end-to-end multi- module architecture that learns design concepts from CAD sketches. The author(s) define a DSL to encode CAD sketches and construct implicit and explicit representations of deep-learning based methods to detect design concepts and generate sketch graphs.The goal is to discover sketch concepts (modular structures) from raw sketches in the format of sketch graphs. DSL is formulated to enable a concise way to represent sketch concepts. The proposed method also is capable of auto-completion for CAD modeling.This paper proposes a novel learning based approach to discover the modular concepts (i.e., modular structure) from raw CAD sketches. To tackle the problem, the authors first define a domain specific language (DSL) such that modular concepts can be represented in a network-friendly manner. The authors perform experiments on a large scale CAD sketch dataset and mainly demonstrates its applications for design intent interpretation.	As summarized by reviewer 5G2f, this paper proposes a novel learning-based approach to discover the modular concepts (i.e., modular structure) from raw CAD sketches. To tackle the problem, the authors first define a domain specific language (DSL) such that modular concepts can be represented in a network-friendly manner.  A Transformer-based detection module takes in a CAD sketch sequence and outputs a a set of latent embeddings, which are further decoded to parameterized modular concepts by a generation module. The whole model is trained in an end-to-end self-supervised manner, using reconstruction loss plus regularization terms.  The authors perform experiments on a large scale CAD sketch dataset and mainly demonstrate its applications for design intent interpretation (i.e., parse modular concepts from a raw CAD sketch) and auto-completion (i.e., complete a partial CAD sketch).  All reviewers recognize the novelty and contribution of this work, and the reviewer-author discussion was quite fruitful as many points, ranging from designer/user interaction, comparison to baseline methods, and issues with the library size are discussed and addressed. With such clear contribution and applicability to the CAD domain, I highly recommend the acceptance of this work.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper proposes an approach called VoiceFixer which is aimed at restoring degraded speech signals. It considers a variety of speech degradations - additive noise, reverberations, clipping and limited bandwidth. The paper describes a two stage approach in which the first stage aims to produce restored mel-spectrogram and then a vocoder is used to synthesize the speech.This paper proposes a general speech restoration (GSR) task that tries to remove multiple distortions in a single model. In VoiceFixer, the authors employ a ResNet for modeling the analysis stage and a TFGAN-based neural vocoder for synthesis stage.This paper introduces a unified view of several speech restoration problems including denoising, decliping, dereverberation and audio super-resolution. The experimental results show that the proposed VoiceFixer combination of the model and the analysis-synthesis procedure are capable of effectively removing the speech distortions. Overall, I believe that the paper has great potential if some concerns are addressed.The system is a two-stage system composed of an analysis module producing mel-band masks and a synthesis module using a vocoder. The paper proposes a single system to deal with the speech enhancement tasks of denoising, dereverb, bandwidth extension (BWE) and declipping.	PAPER: This paper addresses the problem of learning methods for general speech restoration which generalizes across at least 4 tasks (additive noise, room reverberation, low-resolution and clipping distortion).  The proposed approach is based on a two-stage process, which includes both analysis and synthesis stages.  DISCUSSION: The reviewers wrote very detailed reviews which ask some important questions and point to some potential issues. The authors responded to all reviews, but only addressed a subset of the issues and questions mentioned by the reviewers. Novelty and comparison with previous approaches was one of the issues mentioned by reviewers. SUMMARY: While reviewers are supportive of this line of research, reviewers were also concerned with the novelty of the proposed approach and details of the experiments. In its current form, the paper may not be ready for publication.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose a method to derive counterfactuals, saliency maps, and so called isofactuals using invertible neural networks. I found the structure of the paper confusing and lacking in clear elicitation of contributions. The objective of each evaluation data-set and study should also be clearly outlined.The paper has some interesting ideas, and the qualitative evaluation is helpful is conveying those ideas. The writing often lacks clarity and the usage of space can be more judicious. The results of the human subject study are not very convincing.This paper describes a computational method to construct ideal counterfactuals and isosurfaces via invertible CNNs. It uses it to reveal biases in three different datasets. The reviewer finds the manuscript hard to follow, especially Section II.The paper presents a promising idea to build interpretable models by combining discriminative and generative approach. The proposed model uses an invertible neural network to model the data distribution. It is not clear which attributes such as smiling; gender are important for the classifier's positive /negative attractive decision.The authors propose a technique based on an invertible network to provide counterfactuals relative to one class of interest. The authors propose an attribution map based on those counterfactUALs and evaluate them in a qualitative manner, based on their own observations on 3 datasets.	All the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. I believe that the authors have improved the presentation of the paper after rebuttal, however, I still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The risk measure maintains a number of properties that are theoretical, while also showing improved performance relative to other algorithms. As far as I understand, the epistemic uncertainty is forced and controlled (because of splittingp the data) – is this unrealistic, and also only a fraction of the epistemic uncertainty?This paper studied the risk-averse problem that is important in RL scenarios. The combination of the aleatory risk and the epistemic risk was novel. The proposed method is very complicated, especially for the usage of the FTRL part.This work fills a necessary gap in the risk based decision making literature. It also highlights the limits of current approaches. Please see major comments below for the rebuttal to the paper. The author of the paper should be asked to explain his reasons for using a distorted utility function.Experiments show that additive risk leads to worse risk-sensitive performance than composite risk in RL problems. How to confirm that it is a particular case of the composite risk when the epistemic risk measure is replaced with expectation as announced in the abstract?	Meta Review: The paper proposes a novel quantitative measure for risk in RL. The measure is sensitive to aleatoric and epistemic risk, and avoids some theoretical issues with previously proposed additive measures of the two types of risk. The paper also proposes a novel risk sensitive RL algorithm (based on ensemble bootstrapping and distributional RL). The method performs well empirically.  Pro: * All reviewers agree that the proposed measure and algorithm is novel, sound, and original.  * 3 out of 4 reviewers agree that the paper is well written.  * The topic of risk sensitive RL is timely and important - 3 out of 4 reviewers rate the potential impact as good (3). * Empirical results justify the theoretical claims and the method performs well compared to other distributional RL methods  Con: * In their initial assessment, reviewers point out minor technical flaws and some difficulties in understanding parts of the empirical evaluation. The authors have commented on the issues raised, but only 1 reviewer has responded to the authors' comments. * PeBL points out a potential shortcoming w.r.t. the training protocol (splitting the data). Authors acknowledge this and point to other papers in the literature using a similar protocol. * The method seems somewhat complicated and could perhaps be presented better - this is based on questions raised by reviewers, and these questions should be helpful for the authors to improve the camera-ready presentation.  **Justification of recommendation:** 3 of 4 reviewers are in favor of accepting the paper. The only (weakly) negative reviewer has raised some major issues that are answered in the appendix of the paper (but the reviewer did not find the appendix) and answered in other sections of the paper (as pointed out by the authors). To me, the authors have adequately addressed all major and most minor comments in their extensive response (including the major issues raised by the negative reviewer) - unfortunately only the most positive reviewer responded to the rebuttal. Taking all this information together I can still confidently recommend acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper deals with the goal recognition design problem. It extendspreviously used definitions to consider a partially-informed actor. The BFS can be enhanced through causal graph-pruning, which is shown to be safe.The paper lifts the goal recognition design framework to planning domains with incomplete information. It presents a challenging domain for existing combinations of heuristics and search algorithms. The paper would greatly benefit from restructuring the sections such that the k-planner translation is introduced after the definition of the GRD-APK problem.	Dear Authors, thank you very much for your submission. We are happy to inform you that we have decided to accept it and we look forward to your talk in the workshop. Please, go over the feedback in the reviews and correct or update your papers in time for the camera ready date (May 24). Best regards HSDIP organizers
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new conceptual framework to assess the safety of model via maximum deviation from a reference model over a pre-specified certification dataset. The paper then demonstrates the computation of this deviation for interpretable models like trees and generalized additive models.This paper defines a new "safety" measure for a ML model given a good reference model. The safety metric is the maximum of outputs deviation between the target model and the reference model for some input sets. Extra work is required to generalize this metric to all ML models.This paper proposes to use maximum deviation, defined by the difference between a reference model and a trained model over a superset of the training data. Such a measure can be efficiently computed for decision trees, generalized linear and additive models. However, the proposed approach is somewhat too restrictive to generalize.	The authors propose to inspect learned models based on their maximum deviation to a reference model. They evaluate the feasibility of computing this deviation for a number of widely used model classes, including generalised linear models and decision trees. The idea is illustrated in cases studies.   Reviewers all appreciated the novelty and importance of the proposed problem and the contributions made to examine the feasibility of solving it. Their main concerns were regarding the limited discussion on choosing the reference model and the certification set. The authors expanded greatly on this in their rebuttal and in a revision to the paper.   The technical novelty is rather lower but the paper should not have trouble finding an audience in the NeurIPS community due to the broad applicability of the problem under consideration the clarity of the manuscript. I think the added discussion asked for by reviewers is well within what could be expected to be added for a camera-ready version (and indeed, this is already in the Appendix of the revision).
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper presents purely experimental findings, but the experiments are sufficiently broad. Investigating potential theoretical models or more models and datasets could be fruitful directions for future work. The structure of the paper is well-organized, just some sentences are hard to parse.The paper explores a new question and gives some interesting conclusions. But my major concern is its empirical studies cannot support the findings well. There are few discussions to provide the readers with some insights. I hope the authors carefully consider how to enhance this paper.The hypothesis of converging feature spaces is interesting, but the conclusion from the current experiment results is overstretching. While the authors emphasize the convergence of subspaces, the P- vector defined in the paper is actually the top singular vector of the feature space. It seems the angle typically converges to 10 to 20 degrees.	This paper presents an intriguing empirical phenomenon in deep learning. They train a variety of architectures for different tasks using different datasets and study the relationship between the learned representations. In particular they collect the representations into a large matrix and take the top left singular vector and measure the cosine of the angle. They show that it is much smaller than one might expect, about 10 degrees or so, and has an approximate monotonicity property as the network is being trained although it does not seem to converge to zero. Moreover this measure also correlates with performance.   The reviewers had divided opinions on this paper. On the one hand, the range of experiments is impressive and truly demonstrates that this is a pervasive phenomenon. On the other hand, it is not so clear what it means. In particular, suppose we have a collection of graphs which have close to the same degree distributions. If we take the top left singular vectors of all the adjacency matrices, they would also have low angles between them. While this is a very different setting and there is no analogy between the experiments in this paper and this toy model, it does raise philosophical questions about whether the phenomenon is meaningful or is a byproduct of something else about the data. This may be a challenging question to answer, but one reviewer brought up a natural next step: One could measure the principal angle between the subspace of the top k left singular vectors across experiments for larger values of k. The authors do bring up the point that the spectrum decays very quickly, so it could be that beyond a certain point the singular vectors behave somewhat randomly.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.N/A for reproducibility reviewers N/a for reproducer reviewers. N/A  for researchers for repercibility and reputation reviewers for the first time.The paper proposed an algorithm to learn a non-uniform pruning policy for compressing the deep neural network and maintaining good performance under adversarial attacks. Pruning policy is learned by reinforcement learning where each state is the parameter features of each layer, i.e., convolutional kernel sizes.The paper claims that a *non-uniform compression* improves the final accuracy of the pruned models on the original data and adversarially perturbed data. Two types of pruning are compared, channel and weight pruning, for multiple architectures. The strategy found is implemented on two state-of-the-art robust pruning method, HYDRA and Robust-ADMM.This paper shows that employing non-uniform compression strategies allows conjoining model compactness and adversarial robustness. The empirical studies are thorough and convincing. This paper may bring insights and exhibit impact on the network pruning community. The equation between L157 and L158 is informal.	After the discussion phase, the author's resolved (nearly) all problems regarding reproducibility (although the score in the review was not updated). In addition, one reviewer also increased the score to 5 in their reply. In view of the strengths and importance of the topic, I recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The goal of this paper is to improve our understanding of reward-agnostic metrics drawn from the literature through comparison with human behaviour and task reward. This paper compares two intrinsic reward methods against three baselines on three Atari environments on five metrics, including task reward and a simple metric for human similarity. The authors report the correlation between the different metrics.The paper as it stands, provide useful, but expected insights. The difficulty I've with the paper is that it's not clear what exactly you're after here. The curiosity and exploration is an important topic for RL research and we need more in-depth analysis.This work studies four task-agnostic metrics for evaluating reinforcement learning agents: human similarity, curiosity, empowerment and information gain. Experiments were conducted with three selected RL algorithms (PPO, ICM and RND) on selected atari games. Results show that a combination of task reward and curiosity better explain human behavior.This paper proposes to study three types of intrinsic motivations: curiosity, empowerment and information gain. They propose to compute these measures on the lifetime experience of RL agents and to use them as behavioral metrics. To evaluate these metrics, they perform a correlation study.	The reviewers agree that the paper, in its current form, is not strong enough to allow for publication.  There are specific weaknesses that need to be tackled: a better correlation study; a clearer relationship to existing literature (and improvement on the novelty); clearer, more precise use of descriptions.  The authors are encouraged to continue with their work and submit a more mature manuscript.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.PROSPECT is a database for proteomics research based on a massive collection of LC-MS/MS datasets already available from the ProteomeTools project. The paper is well written, and the package adds some functionality to the original database. The primary weakness of the paper is the relatively limited original content.This paper presents a newly annotated and integrated protein MS dataset based on the ProteomeTools dataset. The ultimate goal here is to provide an efficient resource dataset for training new ML models. The number of benchmarks presented here are few to none.This study curates and annotates the ProteomeTools Dataset. It can be used to train models in predicting properties for mass spectrometry (MS/MS) using experimental data. The paper highlights an area of research that is not as commonly represented in CS/ML conferences.This work is relevant in the field of proteomics analysis with Mass Spectrometry. Current models to predict the retention time, spectra given a sequence, or both have been processing the data in many different ways. Annotating it supposes a great novelty because it could help homogenize the training strategies and comparison between models.This paper presents a new deep-learning ready dataset for looking at protein mass spec data. Mass spec is a central instrument for much work across drug discovery and chemistry/biochemistry at large. This dataset is curated and ready for benchmarking where they aggregate and align the data.This paper presents a new annotated dataset for predicting proteomic properties from sequence data. This task is useful for determining which amino acid molecules exist in a sample and to what abundance. The authors clearly define two relevant machine learning tasks that this dataset could be used for. The evaluation metrics are well suited for their specific tasks.	Reviews were split. Reviewers agreed on the importance of the domain but differed in how they valued the contribution over ProteomeTools, which the dataset builds upon. A primary concern was lack of baselines/benchmarking. The rebuttal argues that baselines would not add much value since strong methods have been published in prior work. However, my interpretation of the reviewers ask is not to invent new heuristic baselines but to evaluate the prior methods on the newly prepared dataset. The authors do add evaluation of one such model (Prosit) to the revision. Evaluating more models in the same way could strengthen the contribution, and could be part of the ongoing maintenance of the dataset / code.  Overall, however, the majority of reviewers felt the paper does provide significant value beyond ProteomeTools and other prior work, through the new annotations, tooling, and usage recommendations. The revision adds an appendix (G) to clarify these contributions, and it may be useful to move some of that text into the intro of the paper (as I think many readers will want to know what are the main contributions beyond the base dataset). I agree with the majority that there is significant value added and recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper studies three instances of the multi-objective RL problem, namely smooth concave scalarization, CMDPs, and max-min trade-off. The authors proposed a new class of algorithms for these problems which updates a regularized objective which constrains the KL between the update policy and some anchor policy. The resulting algorithm achieves global convergence with exact gradients.In this paper, the authors studied policy optimization in multi-objective Markov decision processes. Three specific optimization scenarios are considered: smooth concave scalarization, objective constraints, and max-min tradeoffs. Some empirical experiments corroborate theoretical results.Anchor-changing Regularized Natural Policy Gradient (ARNPG) is a novel framework for designing policy- gradient based methods for multi-objective MDPs. It builds on first-order methods for convex optimization problems. In contrast to prior work, it allows convergence without artificial or impractical assumptions.This paper proposed an Anchor-changing Regularized Natural Policy Gradient (ARNPG) framework. It can incorporate ideas from well-performing first-order methods into the design of policy optimization algorithms for multi-objective MDP problems. Theoretically, the designed algorithms based on the ARNPG framework achieve $O(\frac{1}{T})$ global convergence with exact gradients.	This paper studies multi-objective RL, and in particular the settings of smooth concave scalarization, constraints, and max-min trade-off. The authors propose a framework for these problems called “anchor changing regularized natural policy gradient” (ARNPG) and show $O(1/T)$ convergence with exact gradients. The results do not require some of the assumptions used in related works such as ergodicity. The authors evaluate their approach on tabular environments, Acrobot, and (post-rebuttal) Hopper. The paper is well-written, the results seem correct, and the work provides a nice extension of the recent results on PG convergence to the multi-objective case. This contribution warrants acceptance despite the somewhat limited empirical evaluation.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The idea of the paper is that given X metrics that are used to evaluate the models, discriminative reference samples must be those that cause the systems to score very differently among them. The authors show that their variance-aware subsets obtain higher correlations with respect to human judgements than the original datasets (WMT16 to WMT20).The paper introduces automatically generated, variance-aware test sets for machine translation. 70 test sets covering 35 translation directions based on the datasets from WMT16 to WMT20. The authors promise to release the code for automatic test set generation which will be a valuable asset for future research.The paper presents a technique to filter machine translation test sets in order to keep a set of instances that allow to better evaluate and compare systems. While the technique is simple, I think it is an interesting approach, and correlation with human evaluations shows that the resulting test sets meet their purpose.	The paper presents a new evaluation dataset for machine translation, focusing on sampling test examples that can effectively rank different systems. The evaluation set contains a wide range of language pairs (35 language pairs) and shows a higher correlation with human judgments than original, unfiltered datasets.   All reviewers agreed that the paper points out a new perspective on the MT task, and the same idea of filtering evaluation datasets to improve discrimination ability can be applied for other tasks. The paper also includes in-depth analysis, such as what kind of sentences are getting filtered, and how it connects to human paraphrasing. My initial concern about using same models to compute variance and then to evaluate output was addressed during the rebuttal period.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper presents a label-invariant augmentation for graph-structured data. It conducts the augmentation in the representation space and augments the most difficult sample. Experimental results show that the developed metho outperforms classical GNN-based methods and recent graph contrastive learning.The authors propose a graph contrastive learning approach in which the augmentations are done in feature space such that they do not change the predicted label. They present experiments on  8 different benchmarks, and show competitive results.This paper proposes a label-invariant augmentation technique for graph-structured data. Different from the conventional node/edge modification and subgraph extraction. The experimental results demonstrate the effectiveness of the proposed augmentation strategy.	The paper finally received three unanimous scores 6/6/7 and all the reviewers think the authors have addressed their initial concerns. The AC also think positive to this work and suggst to accept this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision. adaptive attacks lower robustness by 40% in average across 7 defenses. The paper suggests that non-adaptive attacks lead to an overstate on adversarial robustness, and thus the authors recommend using adaptive attacks as a gold-standard.This paper performs a thorough robustness analysis of robust graph neural network models that counteract adversarial attacks. Extensive experiments demonstrate that the robustness of existing defense methods is overestimated. The proposed method can also offer a black-box attack to assess a model’s robustness.This work has revealed a serious issue of adversarial defenses on GNN. Existing defense GNN models can be easily broken by applying customized adaptive attacks. Authors also propose a new metric for measuring the quality of attacks/defenses.	The recommendation is based on the reviewers' comments, the area chair's personal evaluation, and the post-rebuttal discussion.   This paper provides novel insights into the effectiveness of various defenses proposed for graph neural networks. All reviewers find the results convincing and valuable. The authors' rebuttal has successfully addressed the reviewers' concerns. Given the unilateral agreement, I am recommending acceptance
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper discovers an interesting behavior of model fine-tuning: the performance is worse compared to linear probing on OOD data. The strength of this paper is the extensive and detailed toy and benchmark experiments. The suggested solution by combining linear probing and fine- Tuning also has good performance.This paper studies the problem of how to fine-tune a pre-trained model and obtain better results for both ID and OOD. Two methods, fine-tuning and linear probing, are investigated and compared. A new two-step variant called LP-FT is derived.This paper explores how different strategies for fine-tuning affect in- and out-of-distribution performance. The paper theoretically analyzes the tradeoffs in a simplified scenario with two-layer networks. Their experiments on a number of datasets including CIFAR, WILDS-FMoW confirm the intuitions from their theory.This paper contrasts fine-tuning (i.e., modifying all network weights) and linear probing based on their relative ID/OOD performance. It is known that fine- Tuning (FT) outperforms linear probing (LP) ID. This paper presents that the reverse is true OOD (FT outperforms LP). This paper suggests that this occurs because fine- tuning distorts features in conjunction with the final linear layer.	The paper provides a solid and thorough analysis to the two basic methods of fine-tuning, linear probing (LP) and fine-tuning (FT). The authors provide an important and highly interesting observation about the performance of both in and out of domain (OOD) setting. They validate the known phenomena that FT outperforms LP in the in-domain (ID) setting, but demonstrate that when tested on OOD data, LP is in fact more performant and back this observation with a theoretical and empirical analysis. The remedy provided is also a known, yet slightly less popular technique of setting the final layer (LP) first, then finte-tuning (FT-LP). The authors provide thorough experiments showing that this technique enjoys the best of both worlds, meaning ID and OOD. I found it worth noting that during the rebuttal period the authors provided experiments on additional larger scale datasets and models and the results of the paper carried over to these new setting. The reviews agree that the analysis provided is both interesting and novel. Even though the paper does not provide a new technique, there is a consensus that the understanding it gives on known techniques is a welcome addition to ICLR.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors consider the problems of prediction with expert advice and online linear regression with bounded, differentiable and strongly convex losses. Using curvature of the strongly-convex loss functions and properly tuning the learning rate, they make appear a negative term in the respective regret bounds of both the algorithm. The authors call this term as the "variance" and present trade-offs between regret and variance.When variance is large, it is possible to achieve negative regret, i.e., outperform the best expert. The paper shows the applicability of this result in several different settings. The writing and clarity of the paper can be improved.This paper introduces a new proof technique applicable to many online learning problems. Authors show how this new analysis leads to an additional negative term in the regret bound. The paper is a bit hard to follow because of the amount of information that is provided in the text.	The paper introduces the valuable idea of exploiting strong convexity of losses in online learning, together with variance-based regret bounds for contemporary algorithms like Squint and Metagrad, to introduce negative terms in cumulative regret bounds and make the algorithms useful in many applications such as early stopping in online-to-batch conversion and other settings.   A dominant concern from the reviewers' side was about the amount of (technical) material packed into the paper, which was alleviated by the detailed author response. As a result, the reviewers largely agree that the paper deserves to be accepted -- an opinion which I share.   PS. I request the author(s) to please resolve the incomplete [TODO]s in the paper checklist appropriately for the final version.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.DeepSIM is capable of generating the plausible results by manipulating its contents in both a low and a high-level manner. The editing effects of edge maps are not distinct from those of segmentation maps. The authors need to clarify why the VGGNet-based perceptual loss encourages the model to maintain the fidelity.This paper provides an augmentation method to enable single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself. During manipulation, the generator allows for making general image changes.This work explores a way of conditioning the generation on a primitive, which can be user-specified. As a result, one can produce realistic modifications to a given image by modifying - or sketching - some primitive. It represents the state of the art for deep-learning based single-image manipulation.	The reviews are a bit mixed. While all the reviewers feel that the paper proposed an interesting mechanism to train conditional generators from a single image and demonstrated good image editing results in the experiments, there are also common concerns about the practicality of the proposed method for interactive image editing. All the reviewers asked for the computation time, and some expressed the concerns about technical contributions. While these concerns were (somewhat) addressed in the rebuttal, the AC feels that it’s a hard sell to bet on the dramatic increase of computational capacity to make the computing time from an hour to realtime. Concerns about novelty also remained. Given the drawbacks, the final decision was to not accept. However, this work is promising and can be made stronger for publication in a later venue.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes ILQR-VAE, a novel method that allows to simultaneously learn latent dynamics and infer unobserved control inputs. The method relies on IQLR solver and recent advances allowing for implicit differentiation to maximize an Evidence Lower Bound on log-likelihood of observation. Authors show comparisons to other models on toy datasets and benchmark datasets for neural data analysis methods.The paper proposes a control-based variational inference approach that learns latent neural dynamics in input-driven SSM. It utilizes iLQR in the recognition model that transforms it into an optimal-control problem. The recognition model is implicitly implied by the generative model.This paper presents a new approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and - importantly - external inputs. This approach is enabled by using the outcome of an optimization algorithm (iLQR) in the recognition model.	The paper introduces a novel control-based variational inference approach that learns latent dynamics in an *input-driven* state-space model. An optimal control solution (iLQR) is implicitly used as the recognition model which is fast and compact. Reviewers unanimously agree on the high quality writing and high significance of the work. This paper advances the horizon of nonlinear dynamical system models with unobserved input, an impactful contribution to the neuroscience and time series communities.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.A GOL embedding space represents the direction and distance between objects represent order and metric relations between their ranks. The algorithm performs best in 80% of benchmark datasets for facial age estimation, HCI classification, and aesthetic score regression.GOL is a method for rank estimation using the order constraint and the metric constraint. The order constraint enforces the feature vectors of instances to be arranged according to their ranks. The metric constraint makes the distance between instances reflect their rank.The paper introduces an algorithm for learning an embedding space using which the rank of an object can be estimated, e.g. age of a person based on face images. The paper is easy to read. However, parts of the introduction and related work section are repetitive.	This paper proposes a new approach named geometric order learning (GOL) for rank estimation. Reviewers found that the idea is novel and the paper is well written. The authors have also clearly addressed most questions from reviewers in their responses. Thus, I recommend the acceptance of this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper addresses how to learn policies for tasks in which constraints are specified in natural language. They propose a model that encodes the different types of natural language constraints into intermediate representations. Then, they use this as input along with the observation to produce an action at each time step for a safe trajectory.This paper presents a new test environment, Hazard World, for learning the safe reinforcement learning agents with given natural language constraints. In this problem, the goal of the agent is to find an optimal policy that maximizes the cumulative rewards while satisfying the constraints given in natural language.The proposed system is made of two parts: a constraint interpreter that is (mostly) trained in a supervised way with Amazon Mechanical Turk to translate natural language orders into grid-world ad-hoc constraints. And a policy that is trained through PCPO, a TRPO-like constraint-aware policy optimization algorithm.The paper proposed an algorithm to learn a policy when provided with natural language constraints. The paper defined a navigation task called Hazard World, in which an agent navigation on the map to collect items. The authors defined three types of constraints to restrict agents to visit certain states.	The goal of the paper is to learn policies that can solve a given task while adhering to certain constraints specified via natural language. The paper closely builds upon prior work on constrained RL and passes the representation of natural language constraints by pre-training an interpreter. Experiments are done in a new proposed 2D grid-world benchmark. Although reviewers liked the premise, the main issue raised is that the way natural language constraints are handled is no different from the way it is done in prior work on constrained RL. The authors provided the rebuttal and addressed some of the concerns regarding paper details. However, upon discussion post rebuttal, the reviewers and AC feel that the paper does not provide clear scientific insight because the natural language part is processed separately from the policy learning part. We also believe that the paper will immensely benefit with results in more complex environments beyond the 2D grid-world. Please refer to the reviews for final feedback and suggestions to strengthen the submission.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors note a distinction between kinds of sparse networks in the literature. "Weak tickets" require training to perform comparably to the original network, while "strong tickets" do not. The authors provide an interesting new avenue for evaluating pruning methods, with the use of ground truth sparse networks.This paper argues that one reason evaluating the strong Lottery Ticket Hypothesis is difficult is the lack of ground-truth tickets. They circumvent this by embedding a winning ticket in the weights at initialization and evaluating how well different methods can recover it. Interesting idea but there's a mismatch between the proposed theorem and the experiments.This paper provides a framework for benchmarking different pruning methods on their abilities to identify strong/weak lottery tickets. The proposed framework is driven by theoretical analysis. The paper is not well-written and the logical flow can be improved.The authors identify that contemporary methods for finding very sparse subnetworks in deep neural networks (DNNs) do not find very sparse (and good) solutions. They question if this is a fundamental limitation (i.e. these very sparse solutions don't exist), or if this are a limitation of current methods in finding such solutions. To answer this question, the authors propose to either "plant" known good very sparse subsets in DNNs, or to try and findThis paper develops a framework that allows planting and hiding winning tickets within a randomly initialized network. It can be further used to assess the state-of-the-art pruning before training methods. The motivation and the correctness of the lottery ticket planting are not clear to me.	This paper takes on (in my view) one of the most important questions in the lottery ticket literature today: how small are the smallest lottery tickets that exist in our neural networks? Many methods have been proposed for finding weak lottery tickets (those that require training to reach full accuracy) and strong lottery tickets (those that do not), but we have no idea how close they come to finding the smallest lottery tickets. Moreover, in many cases, we only know how to find lottery ticket subnetworks early in training rather than at initialization. Is this a fundamental limitation on the existence of lottery tickets, or is this simply a limitation of our methods for finding them? I am personally very involved in lottery ticket conversations in the literature, and I believe I can speak with some authority when I say that these are vital questions where any progress is important.  Moreover, these are exceedingly difficult research questions, and (again, in my view) the authors should be commended for taking them on. A naive approach to these questions would involve brute force search over all possible subnetworks, which is infeasible even on the smallest of toy examples, let alone the meaningful computer vision tasks where lottery ticket work typically focuses.  I am sharing all of this information to provide background for my confident recommendation to accept this paper over the many legitimate concerns expressed by reviewers and those that I saw when reading the paper in detail. Those include that: * This paper does not solve any of these research problems in their entirety. * It focuses on toy networks smaller than those traditionally studied in the lottery ticket literature, and it is well known that lottery ticket behavior changes in character at larger scales. * Planting good subnetworks may be an unrealistic proxy for the kinds of subnetworks that actually emerge naturally. * There may be multiple good subnetworks in a network, not just the one that was planted. * The graphs are a bit hard to read. * I find the mix of pruning methods studied, which were designed with very different goals (pruning after training, pruning before training, finding strong lottery tickets), a bit confusing.  **The bottom line:** With all of that said, in my view, the paper asks good questions and provides an initial foothold that other researchers will be able to build on as we seek more general answers. This is similar to the contributions made by Zhou et al., which started the conversation on strong lottery tickets, and potentially even Frankle & Carbin, which kicked off the lottery ticket discussion but got many things wrong. Both papers were good first attempts at solving big problems, and both were highly influential despite their flaws. Similarly, even if this submission isn't perfect in every way, this is among the most important kinds of contributions that a paper can make. For that reason, I strongly recommend acceptance under the belief that this paper will help to foster a valuable conversation in the literature.  P.S. I really, truly, strongly beg the authors to redo their graphs following the style of some of the more user-friendly lottery ticket or pruning papers they have cited (e.g., Frankle et al., 2021). The graphs in this paper were really hard to parse. Really really really hard to parse. They're too small, the y-axis is often squished, gridlines would be helpful, the lines are overlapping in ways that are difficult to distinguish because the colors blend, etc. etc. This is quite possibly the biggest impediment I see to this paper's ability to have broader influence.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper introduces a method for embedding the hard-constraints for boundary conditions (BCs) for physics-informed neural networks (PINNs) It introduces the "extra fields" to transform the original PDE into equivalent forms so that the BCs can become linear.Physic-informed neural networks (PINNs) are a new paradigm for injecting the knowledge of governing equations into neural networks. This paper tackles a critical problem of PINNs since training for boundary conditions frequently collides with training for governing equations.Proposed work developed a reformulation of PDE to enforce boundary conditions (Dirichlet, Neumann, and Robin BCs) as a hard constraint under the PINN framework. Thorough experiments have been done to demonstrate the proposed approach's effectiveness in terms of accuracy.The submission introduces a new technique to enforce Dirichlet, Neumann, and Robin boundary constraints into the PINNs model. The authors test on 3 PDEs: the heat equation in a battery pack, the Navier-Stokes equation over an airfoil, and a 10 dimensional heat equation.	The paper considers using neural networks to solve PDEs with complex geometry by incorporating hard constraint into the approximation function class. The method is interesting and useful for practical applications of neural-network based PDE solvers. The authors have adequately addressed the concerns by the referee. The meta-reviewer recommends acceptance of the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors take up an important problem in unsupervised deep reinforcement learning which is to learn perceptual reward functions for goal-conditioned policies without extrinsic rewards from the environment. The problem is important in order to push the field forward to learning representations of the environment without predicting value functions from scalar rewards.The paper proposes an unsupervised learning algorithm to learn a goal conditioned policy and the corresponding reward function. The paper is very well written and easy to understand. Original UVFA [1] paper should be cited while citing goal conditioned policies.In this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully unsupervised way. For doing so, they simultaneously learn a goal-conditioned policy and a goal achievement reward function. These two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space.	This paper introduces an unsupervised algorithm to learn a goal-conditioned policy and the reward function by formulating a mutual information maximization problem. The idea is interesting, but the experimental studies seem not rigorous enough. In the final version, I would like to see some more detailed analysis of the results obtained by the baselines (pixel approaches), as well as careful discussion on the relationship with other related work, such as Variational Intrinsic Control.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper suggests a new graph neural network architecture that can be seen as a gradient descent minimization of a learnable energy function. The authors use channel-mixing matrices to infuse high frequencies into the architecture dynamics. Three variants of the new architecture are presented.The work is well-written and clearly presented. It builds on similar ideas as outlined in several previous works. The experiments has primarily focussed on one aspect while studying on several datasets with varying hetero/homophily.In this paper, the evolution of the GNN is explained as learning attractive and repulsive forces in feature space by the positive and negative eigenvalues of a symmetric 'channel-mixing' matrix. The authors present structural constraints on common GNN architectures, allowing them to be interpreted as gradient flows.This paper introduces a new family of models, GRAFF, on graphs wherein graph features are transformed according to a dynamical system given by the negative gradient of an *energy functional*, which is parameterized and learned. The primary value added explored in this work is to the analysis (and empirics) of the ability to handle heterophilious graphs.	The authors present a graph neural network for heterophilic data using gradient flows. The proposed architecture is quite simple...large sections of the architecture are fully linear dynamical systems rather than neural networks, and still achieve roughly SotA results on standard graph learning benchmarks. There was a significant amount of disagreement between the reviewers. Some seemed to think the strength of mostly linear methods meant that the benchmarks were too easy, but these are standard graph neural network benchmarks. A simple model performing well is not a negative, and can often be useful for puncturing hype (e.g. https://arxiv.org/abs/2206.13211). Simple architectures can also be useful for providing analytic insights which might get obscured in more complex models. Some reviewers seemed concerned about the scaling of certain tools (e.g. graph Laplacian eigenvectors), but these tools are only used for analysis, not for training. Nevertheless, I feel that there were enough general concerns around the paper that I have a difficult time recommending acceptance. Even if the purpose of the paper is primarily to drive analytic insights rather than achieve SotA results on big benchmarks, I would recommend the authors to show how these analytic insights can be used to improve models on big datasets to strengthen the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The experiments demonstrate the claims set out by the authors. The work is clearly motivated: inverse problems are common in medical imaging pipelines. Improving performance on this task is of clinical significance. The paper is well written and easy to follow.The paper is well written and structured. It was easy and enjoyable to read. The experimental setup is well designed and the results interesting. The proposed approach seems to require more steps and calculations than previous works.The proposed algorithm addresses the important problem of over-fitting that arises from data driven approaches. The Bayesian aspect is emphasized well with good theoretical background explanations. The authors should add a pseudo code in the Appendix section to understand the overall flow of the posterior weight estimation to add clarity for interested readers.The paper is relatively clear on what it tries to achieve and via what method. The motivation of the chosen method is clear as well. The structure is ok albeit not great. The technique appears to work relatively well, although figs 2,3 are quite small.	This paper introduces a new Bayesian approach to deep image prior using mean-field variational inference, where the goal is to enable pixel-wise uncertainty quantification, and mitigate the need for early stopping regularisation.  The paper defines a prior distribution in a Bayesian setting to learn the parameters of the weight prior, targeting an improvement in the quality of the reconstructed images for inversion problems (i.e., denoising, super-resolution and inpainting).  All reviewers demonstrated full support for the publication of the paper and the questions were more about clarifications about claims, experiments, etc.  I also support the publication of this paper, and encourage the authors to address the main comments by the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors refer to the neural network approximating the spectral coefficients as a PINN however it seems that the residual of the convection-diffusion PDE is not considered in the loss function. I suggest further investigations and analysis on the accuracy/efficiency of the proposed technique.The authors propose to use NNs to approximate the spectral coefficients of a spectral expansion. To this end, they propose Spectral Physics Informed Neural Networks (PINNs) Although the authors didn't discuss the choice of the approximation in detail, the paper is well written.The paper presents an extension of PINN to stochastic PDEs which contain partially unknown parameters. The key idea is to use "polynomial chaos expansions" to approximate the solution. I found the paper well-organized and clearly conveyed what the authors try to express.	This paper considers extending Physics Informed Neural Networks (PINNs) to stochastic PDEs and thereby be able to quantify uncertainties in their predictions. The authors may consider this opportunity to extend the experimental section to address limitations of the work that has been flagged by reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors proposed a neural network approach for quantile regression on censored data. They also proposed a new algorithm to mitigate the computational challenge when combining neural networks and the existing sequential grid algorithm for Portnoys's estimator. Strengths: solid numerical evaluations and comparisons with existing approaches.The authors first build on the linear approach on Portnoy 2003 and extend it straightforwardly to neural networks. This approach requires the sequential optimization of a new NN for each quantile level to be predicted, which is computationally heavy. A new method is proposed for simultaneous quantile regression , which is interpreted as a form of expectation maximization.The paper considers the task of quantile regression in a survival analysis context, i.e. one where some examples are right-censored. As in almost all recent works, the encoder backbone is a neural network. The authors start by adapting the sequential grid (SG) algorithm to work with neural networks.The authors use neural networks to replace the linear models in censored quantile regression. They further improve the sequential grid algorithm with bootstrap weights. The authors explain the soundness of their method from an analogy of EM.	This paper studies the quantile regression of censored data. Neural network models are used as the statistical model. Numerical results show the proposed algorithm is computationally efficient and attains high prediction accuracy compared to existing methods. Since reviewers agree that this paper is well written and interesting, I recommend accepting the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Max-Simes-Fisher (MaSF) is designed for deep neural networks and combines sample features extracted from multiple layers in the network. The method returns a $p$-value for every test sample at a specified significance level. Compared with existing methods for OOD detection, the proposed method achieves similar or better True Positive Rates.This paper poses detecting out of distribution (OOD) samples as a hypothesis testing problem. Proposed framework achieves high efficiency while detecting OOD samples and can be deployed in real-time setups. The approach lacks novelty and experiments are limited to justify the efficacy of the proposed approach.This paper constructs OOD detection as a hypothesis testing problem. It extracts p-values from each layer and channel in CNN from the empirical cumulative distribution. Max, Simes, and Fisher test statistics are adopted as spatial, channel, and layer reduction methods (MaSF) Empirical results show that MaSF achieves comparable or better results than GRAM, ResFlow, and Mahalanobis.The paper proposes a new framework to detect out of distribution samples for deep neural networks without retraining or modifying the model. The method is based on statistical hypothesis testing which is applied to activations of all or multiple layers. As the framework is novel and results shows good performance both in accuracy and computational complexity, I feel that the study deserves to be published.	The paper considers the empirical distribution of layer/channel in CNN ,and proposes to use global null tests with Simes and Fisher statistics to aggregate the p-values. This method is competitive while computationally efficient. The underlying theoretical insights are discussed in detail.  The paper received mixed ratings, and the discussions weren't active. So, AC carefully read the paper and inspected all reviews. Reviewer a8KZ comments were factually inaccurate in listing references, and lack substantial feedback on the actual content of the paper. Hence, the review was down-weighted.   The other negative reviewer Ni17, as an OoD expert, unfortunately did not offer more feedback to author rebuttals. From what AC comprehends, the authors should have clarified their The theoretical guarantee and compared properly with Liu et. al. 2020 energy-score (ES).  Considering the above, AC feels that the study deserves to be published.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a new form of multi-head attention. It can reduce parameters and FLOPs of Transformer models without performance loss on En-De translation. For pre-trained language models, no large-scale pretraining is required to convert attention.This paper analyzes the multi-head attention in transformers and suggests to use collaboration instead of concatenation of multiple heads. Empirical results on WMT’16 English-German demonstrates that the proposed approach reduces the of parameters without sacrificing performance.This paper presents an interesting collaborative MHA to enable heads to share projections. It can be easily applied to most existing transformer-based models, including NMT and pre-training models. The paper is well-written and organized, the experiments are thorough.The paper investigates the over-parameterization of attention heads in Transformer’s multi-head attention. Authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. They propose a reparameterized approach allowing the parameters of queries and keys to be shared between heads. This is called “collaborative attention”	This paper proposes an interesting collaborative multi-head attention (MHA) method to enable heads to share projections, which can reduce parameters and FLOPs of transformer-based models without hurting performance on En-De translation tasks. For pre-trained language models, a tensor decomposition method is used to easily covert the original MHA to its collaborative version without retraining.   This paper receives 3 weak reject and 1 weak accept recommendations. On one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting. On the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the paper's main claim. From the current results, it is difficult to draw a conclusion that collaborative MHA is better.   Specifically, (i) From Table 2, it can be seen that the proposed method is not effective for pre-trained models, i.e., even if the model size is not reduced much, the performance can be dropped significantly. (ii) More experiments, such as QA, more translation/generated tasks will make this paper more convincing. (iii) More rigorous experiments are needed to justify the practical value of the proposed method. If the authors try to emphasize that they go beyond practical realm, then probably a careful re-positioning of the paper is needed, which may not be a trivial task.   The rebuttal unfortunately did not fully address the reviewers' main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.FedGLOMO is communication-efficient that integrates variance-reduction, both at the server and at local client updates. Experiments show the efficacy of the proposed method. Assumption 4 is not standard and the authors claimed that it holds for alpha = n.This paper studies nonconvex and smooth optimization in a federated learning setting. The authors propose an algorithm with both local variances reduced momentum and global variance reduced momentum. They prove that the proposed algorithm achieves the best iteration complexity.Algorithm can be applied to many ML problems. I'm not very familiar with federated learning and it is hard to judge for me how restrictive it is. I find the paper well written. There are places where the statement could be improved.	Meta Review: The authors have addressed the reviewers’ concerns. All the reviewers are not against the publication of the paper. Please add the discussion with the reviewer into the final version, especially the discussion on Assumption 4. The authors should also add the new empirical results too.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.In this work, the authors propose an approach for “truly” unsupervised image-to-image translation. The setting is new, it might not make sense in practice. The authors only compare the proposed method TUNIT with FUNIT in Tables 1, 3 and 4.This paper proposes an unsupervised I2I translation method TUNIT where there is no any supervision signal. The proposed method additionally adds a pseudo label prediction branch to separate domains based on maximizing the mutual information. Experimental results look pretty solid.The paper proposed a TUNIT framework with a guiding network, which could encode style codes for the generator and predict the domain labels for the discriminator. The description of "Style contrastive loss" (Eq. (4) is confusing. More explanations are expected here.This paper tackles the problem of unsupervised image to image translation without making any assumptions about the existence of input pairs or input sets. To solve this task, the authors utilize a guiding network that generates a pseudo label as well as a style code which are then provided to a generator network. The generator given a source image produces an output that follows the structure of the source yet preserves the style and "domain" details of the reference image.	This paper defines a truly unsupervised image translation scenario. Namely, there are no parallel images or domain labels. To achieve robust performance in this scenario, the authors use 1) clustering and 2) generator-discriminator structure to map images from different domains and generate images for target domains.    In all, all the reviewers agree that this definition of unsupervised image translation is interesting. However, there are also several concerns for the real-world practical application and empirical results.  Unlike unsupervised text translation whose target language is known, the truly unsupervised image translation is difficult to make sense without identifying what is the target domain. This limits the contribution of this paper to some specific tasks instead of more general tasks. For the empirical results, the selection of data and the hyperparameter K do not convince the reviewers.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors propose MetODS: Meta Optimized Dynamical Synapses. The algorithm learns by updating its weights through interaction with the environment. The paper is well motivated by biological and existing artificial neural methods. I haven't seen any discussion on potential negative societal impact.The authors have improved the paper and addressed some of my concerns. While I still find the assessment to be problematic in some areas, I think the present results, together with the interesting idea, make for a paper that could be a useful contribution to the field.This paper presents a new neural network that is able to modify its own weights, known as the self-modifying network. The proposed algorithm MetODS shows significantly better performance across different kinds of tasks, from maze navigation to motor control.	This is exciting work that demonstrates the ability of self-modifying networks to solve meta-reinforcement learning problems. The reviewers all agree that this is strong work, and the authors have convincingly addressed most of the concerns the reviewers brought up during the reviewing phase. There are a few lingering questions about the applicability of the baselines, but these are quite minor. The authors have further promised to add analytical comparisons and additional details /motivation on the Hebbian update. Given this, I view this paper quite positively and encourage the authors to integrate the additional experiments and details they mentioned in the feedback stage.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The authors present a GAN-based architecture, called GT-GAN, for generating synthetic samples for regular and irregular time series. It comprises of two main components: (1) Autoencoder(AE) - encoder (Neural CDE) encodes time series into a latent space and decoder (GRU-ODE) recovers a continuous path from which synthetic time series is sampled.The paper presents a time series GAN model that uses neural ODEs and continues time-flow processes. The authors evaluate their approach against standard baselines and datasets. In particular, their method outperforms baselines in the irregular data setting with missing observations.This paper proposes GT-GAN, a framework for synthesizing regular and irregular time series. It uses various techniques such as generative adversarial networks, auto-encoders, neural ordinary differential equations, and continuous time-flow processes. Experiments show that this method can outperform baselines.	This paper presents a new model for time series data that can handle data sampled at irregular time intervals.  The proposed model makes extensive use of continuous time processes in a GAN framework.  Experiment results show that the proposed model consistently outperforms existing approaches.  All reviewers lean on the accept side and I support that consensus.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.The paper addresses the problem of slow convergence in training of object detection transformer. The main contribution is to use trainable anchor boxes (4D) in a layer-by-layer manner. They also motivate their contribution by visualizing positional attentions and explaining the advantage of their proposed method.This paper introduces dynamic anchor boxes as a query formulation for detection transformer (DETR) The basic idea is to use explicit positional priors and scale priors from anchor boxes. Anchor boxes are also adjusted layer-by-layer to learn the optimal anchor setting. The proposed query formulation introduces marginal computational overhead.The slow convergence of DETR is likely caused by the multiple mode property of the queries. To solve the problem, explicit positional priors are used to improve query-to-feature similarity and accelerate model convergence. This paper proposes DAB-DETR, a DETR variant that incorporates dynamic anchor boxes for object queries.	Somewhat borderline paper given the scores, but leaning on the side of accepting mostly because the positive (and weak positive) reviews are a little more persuasive. The negative review is a bit of an outlier; the main issues raised in the negative review are that the novelty is on the lower side or otherwise that the work is incremental. These complaints are largely not shared by the other reviewers, and furthermore seem not like deal-breakers. Still a borderline paper, but fairly safe to accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an algorithm to alter the structure of a graph by adding/deleting edges. The main idea is to use the idea of meta-gradients from meta-learning to solve the bilevel optimization problem.This paper studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. The proposed using meta-learning to compute the second-order derivatives to get the meta-gradients seems reasonable. The experimental results on three graph datasets show that the proposed model could improve the misclassification rate.This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters and leveraged recent progress on meta-learning for optimizing the adversarial attacks. Experiments on a few data sets prove the effectiveness of the proposed approach.	The paper proposes an method for investigating robustness of graph neural nets for node classification problem; training-time attacks for perturbing graph structure are generated using  meta-learning approach. Reviewers agree that the contribution is novel and empirical results support the validity of the approach.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This is one of the best papers I have reviewed in a while. The paper is very well written and the model clearly outperforms its baselines. The only question I have is in terms of the medical data. The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients.The goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner. The model is constructed on the basis of self-organizing maps (SOM) and involves reconstruction error in the training.This work addresses the problem of learning latent embeddings of high-dimensional time series data. The study proposes to cluster the data in a latent space estimated through an auto-encoder. The clustering is obtained by leveraging on the idea of self-organising maps.	This paper combines probabilistic models, VAEs, and self-organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time-series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization.  The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper-parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results.  The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated.  Thus, the reviewers felt the paper should be accepted.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.SuNCTt is a new loss designed to speed up the convergence of semi-supervised training. The loss involves the computation of similarity between anchor and other images with the same class. It is claimed to be considered as the form of neighborhood component analysis.This paper proposes a new method applicable to a specific case in unsupervised learning: having access to a small amount of labels during the training phase where label signal is not used. The main contribution of the paper is the proposed SuNCEt loss, which is modified on top of the regular NCE loss in instance discrimination training.Supervised contrastive learning can speed up representation learning, which is well supported. Despite it maybe practical to speed up the experimental cycle, the technique contribution is rather limited. Despite the paper has some interesting results, it lacks of experiments to show its real contributions.This paper shows that the semi-supervised contrastive loss can achieve similar performance as self-super supervised loss (SimCLR) with less than half of the compute. The improvement over SimCLR does not seem to be significant under the same training epochs. Using a small amount of labeled data can reduce computation.	The paper proposes to speed-up self-supervised learning for semi-supervised learning by combining self-supervised pretraining and supervised fine-tuning into a single objective. The proposed supervised loss builds on Neighbourhood Components Analysis and soft nearest neighbor losses. Most reviewers are concerned about the novelty of the approach and the significance of empirical results. I agree with both concerns. I appreciate the comparison between $\log \sum \exp$ and $\sum \log \exp$, but it seems a simpler cross entropy loss also achieves a similar goal (potentially somewhat slower). I believe adding more experiments comparing different supervised loss functions across different architectures can help improve the paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper aims to perform meta-learning across tasks that have different input data types by learning separate task-specific encoders. It is not clear from the abstract / introduction what is meant by “schema.”The paper proposes a trainable way to re-order or recover the ordering of features from sets of examples. This could be used to build a common feature space (or embedding) for a neural net. The paper is overall really hard to follow, statements are often confusing or misleading.The paper is well written and easy to follow. The major challenge lies in the supervision needed to train the alignment matrix or function. The ground-truth feature alignment matrix is almost impractical to collect. The empirical results are also not convincing.The paper takes on an interesting facet of few-shot classification: An encoder model that aligns to different predictor schemas to a common representation. Comprehensive experiments have been done with quantitative results and analysis, to show the effectiveness of the proposed approach.This paper addresses the problem of meta-learning weight initialization across tasks with different types of input features. It proposes Chameleon model that learns to align input features from different tasks by learning a permutation matrix for each task.	After carefully going through the reviews and rebuttal, and looking at the content of the paper as well, I feel there are some issues with the current manuscript. As also pointed out by AnonReviewer5 and AnonReviewer2, the text lacks clarity. From specifically defining what a schema is, to being more explicit about the limitation of the work.  I understand that the authors are interested in a largely unexplored setting, and hence there might not be a lot of prior work to cement the evaluation protocol. Particularly because of this I think such papers need to be upfront and clear not only in what is the setting and what is the evaluation but also what are the limitations and open problems.   I do agree that there is value in this direction of research, and that the idea of re-ordering the features using attention (which I have to agree it is reminiscent of Bahdanau et al., ICLR 2015 -- though the semantics of it and its purpose makes it novel here) might be a way forward. But I do think for the paper to make an impact (and be ICLR ready) it needs more work both in the writing and maybe on the experimental side as well (consider some more complex task, or be more explicit on what is the common aspect between tasks in the distribution that can allow chameleon to work)
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper provides a data set of SRH images from a number of types of brain tumors as well as control tissue. It also provides several benchmarks for accuracy of classification of these images. The community needs more medical imaging data sets, and this is one covers a large number of tumor types and patients.The authors introduce a public dataset of stimulated Raman histology images of brain tumors (OpenSRH) This dataset includes over 1,300 whole slide images from 307 patients. This is a rich dataset, which includes raw acquired data, patch-wise tumor classifications and processed patches.The SRH dataset is apparently unique and would allow real-time brain tumour segmentation and classification with significant consequences for patients. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives.The paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices.SRH reduces the time from sample extraction to usable information by about 30 minutes. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy.The paper introduces a new resource for brain optical imaging, namely OpenSRH, which is valuable for optimizing brain tumor surgery. The established benchmark motivates future work to address the domain gap on the existing pretraining data and aggregation methods to better model slide-level and patient-level prediction.	There are contrasting review rating ranging from 9 to 3. The reasonable concerns about this paper not been well validated for ML community. The proposed datasets has been benchmarked with some classical and some relevant DL methods. I have taken into account the expertise of reviewers and their concerns carefully. I do agree with some reviewers opinion about this dataset will be importance to medical CV community. I do agree with authors response "The open questions for the ML community that OpenSRH may foster innovative discoveries include the following: 1) domain adaptation between SRH images and other histology images such as H&E images in the large scale TCGA project; 2) using multiple instance learning (MIL) to avoid expensive dense patch annotations; 3) different aggregation methods for patch-based training, including clustering, attention, or MIL; 4) self supervised learning and comparing different augmentation strategies for SRH images; and 5) data efficient training of ViT architectures using SRH data. " Even though the authors have not verified these innovative applications on the openSRH dataset. I will go with acceptance opinion of few reviewers for this paper.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes a pluggable method that learns a dynamic prediction error bound \epsilon for some error-bounded learned indexes. With the proposed method, the learned index can make a better trade-off between space and time. The paper is well-organized and the motivation is easy to follow.This paper focuses on learning index with dynamic prediction error. The major contribution of this paper is proposing an efficient, pluggable d learned index framework. This paper is trying to solve a problem with practical significance.The paper presents a method to learn a variable error bound to use when building piecewise linear segments for learned indices for databases. It is based on an estimation function that uses a sample to estimate the mean and the variance of the remaining data which will be covered by the next linear segment.	All of the reviewers recommended acceptance, but the support was lukewarm, with the maximum score being “Weak Accept”. There were concerns about the limited applicability of the proposed method and lack of clarity of some of the arguments in the paper. Although the reviewers appreciated the mathematical foundations and experimental results, the negatives outweighed the positives.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper combines a widely used variance reduction technique SVRG with the greedy-GQ. It provides a finite-time analysis of the proposed algorithm in the off-policy and Markovian sampling setting. At last, it verifies the theoretical claim by two toy examples.The paper introduces a variance-reduced version of the Greedy-GQ algorithm for off-policy control, based on SVRG. The nested loop structure seems at odds with the single Markovian path of experience. The paper seems fairly incremental in terms of the proposed method, applying an existing method for variance reduction to an existing algorithm.The paper is well written and clearly presented. The sample complexity of the proposed algorithm is lower than Greedy-GD by a factor of 1/eps. It would be nice if the cumulative reward versus running time can be shown, which demonstrates the performance in a clearer way.Greedy-GQ is an RL algorithm for a control problem that extends on GTD, which is a prediction algorithm. While Greedy- GQ asymptotically converges to a stationary point, it does so with high sample complexity. The authors reduce the variance of Greedy GQ by incorporating SVRG.This submission deals with the classical value-based Greedy-GQ algorithm for off-policy optimal control. It develops a two-timescale variance reduction scheme to reduce the stochastic variance. The main contribution is to show that VR-Greedy- GQ achieves an improved sample complexity.	**Overview** This paper provides a way to combine SVRG and greedy-GQ to improve the algorithm performance. In particular, the finite iteration complexity is improved from $\epsilon^{-3}$ to $\epsilon^{-2}$.  **Pros** The paper is well-written. Reviewers believe this is a solid theoretical work on advancing value-based algorithms for off-policy optimal control. It has sufficient theoretical advancement and experiments demonstrations of the methods.   **Cons** Some reviewers are concerned that SVRG is not SOTA. SVRG is not used in practice. The techniques appear to be similar to some existing works.   **Recommendation** The meta-reviewer believes that the paper has solid theoretical contributions. SVRG is a component in the new algorithm to improve the complexity. It does not need to be "useful" or "SOTA". The paper is also well-written. Hence the recommendation is accept.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.This paper proposes an algorithm to verify whether or not there exists an adversarial example in an Lp ball of size espilon around a given training sample. As opposed to previous similar algorithm (Geocert), it sacrifices completeness in order to gain efficiency. Experimental results are very convincing, showing strong improvements in term of runtimes of the method.The paper proposes a method to verify local robustness networks with piecewise-linear activation functions. Unlike other approximate verification techniques, the method is able to show the existence of adversarial examples (in some cases) The method works by exhaustively checking each of the activation regions that are fully or partially within the ε-ball.The paper proposes Fast Geometric Projections (FGP), a method to certify the robustness of neural networks with ReLU as activation. FGP can verify whether in an $\ell_p$-ball around an input $x$ adversarial examples exist or not.The proposed verification method is incomplete and returns, given an input either one of 2 certificates (robust or not_robust) or abstains from certification. The scalability to large networks seems to be an issue, although the proposed method significantly outperforms prior work. The authors propose a systematic search over the convex polyhedral regions on which the network is linear.	The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise-linear neural network is constant in an Lp ball around a given point. This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees. The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return "I don't know"). The experiments show good results in practice. The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result. Overall, this is an important contribution worth communicating to the ICLR community, so I'm happy to recommend acceptance.
Below are multiple reviews of a paper. You need to summarize these reviews with the paper's strength, weakness, and a suggested decision.Machine learning systems often do not share the same inductive biases as humans and may extrapolate in ways that do not match the analyst's expectations. Authors investigated two different types of such inductive bias: feature-level bias and exemplar-based and rule-based bias.The paper proposes measures of how two types of explanation methods -- rule based and exemplar based -- generalize and extrapolate to unseen data regions. The paper is ambitious but falls short in clearly explaining the framework proposed and substantiating the idea with strong experiments.This paper proposes two measures for evaluating the feature-level bias and the exemplar-vs-rule bias in learning systems. The inductive bias of a given learning system is measured by its extrapolation performance difference when trained on different training conditions. The authors first verified their framework on a synthetic dataset with 2D inputs. The results confirm that generalized linear model favors rule-based generalization.This paper studies the extrapolation of machine learning models to unseen regions. It specifically studies two types of biases: feature-level bias and exemplar-vs-rule bias. I found that this paper was quite difficult to read since the authors present discussions without clear definitions.	The paper proposes a novel protocol for examining the inductive biases in learning systems, by quantifying the exemplar-rule trade-off (as measured by the exemplar-vs-rule propensity (EVR) defined in Eq. (2)) while controlling for feature-level bias.   Reviewers mostly agree that the problem studied in this paper is practically relevant and that the two bias measures are potentially interesting and (jointly) more informative than existing measures such as spurious correlation. However, a shared concern among the reviewers (with confidences scores >=3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)], some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)], and results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real-world setting (Reviewer yoH5)]. Based on the above concerns, the reviewers were not convinced that this work is well supported in its current state to merit acceptance for publication.
