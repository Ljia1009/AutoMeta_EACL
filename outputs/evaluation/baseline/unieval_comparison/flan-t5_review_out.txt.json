[
    {
        "coherence": 0.738237062732258,
        "consistency": 0.891520169073004,
        "fluency": 0.5469405203560244,
        "relevance": 0.1895845914061405,
        "overall": 0.5915705858918567,
        "Generated": "1",
        "Gold": "This paper addresses a core issue of the popular conditional neural process: the predictions at each test point are conditionally independent given the conditioning set. This is an inappropriate modeling assumption for many real-world datasets. I was disappointed when I found that these weren't used in the paper I think the novelty of the proposed methods is not striking. The proposed method looks like a direct implementation of a Gaussian process with deep kernel learning in the meta learning setting"
    },
    {
        "coherence": 0.5819447739326667,
        "consistency": 0.8477525941675585,
        "fluency": 0.601706586000976,
        "relevance": 0.12881509068859243,
        "overall": 0.5400547611974486,
        "Generated": "2",
        "Gold": "This is a very good paper. It provides a good overview of the RL learning process. It also provides some useful suggestions on how to explore the subject. It does not provide a clear explanation of the different modes of exploration in RL. It is not clear how to choose between exploration and exploitation, and how long to stay in each mode of exploration. This paper is very good at exploring the subject, but it is not strong enough to show improvements in performance."
    },
    {
        "coherence": 0.4983565358089343,
        "consistency": 0.7929661064822594,
        "fluency": 0.4911684983857408,
        "relevance": 0.21851126889068753,
        "overall": 0.5002506023919054,
        "Generated": "3",
        "Gold": "IGSD is a self-distillation based graph augmentation mechanism to alleviate the drawbacks of existing unsupervised MI based models w.r.t. their high dependency towards negative sampling. However it would have been better if the system designs and significant difference of the proposed model from existing work are discussed. Graph-level representations with only labels have been explored by many works. However, it's not easy to annotate every graph. This paper applies the ideas from semi-supervised classification task to improve the representation quality learned by graph neural network."
    },
    {
        "coherence": 0.405762360055993,
        "consistency": 0.7801849598418509,
        "fluency": 0.4199853973001009,
        "relevance": 0.21775120028693248,
        "overall": 0.4559209793712193,
        "Generated": "4",
        "Gold": "This paper provides a compelling theoretical explanation for a large class of adversarial examples. While this is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships. This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is unlikely such analysis is significant."
    },
    {
        "coherence": 0.2923309258974805,
        "consistency": 0.5410653767834249,
        "fluency": 0.3469293517976111,
        "relevance": 0.08302895825606468,
        "overall": 0.3158386531836453,
        "Generated": "5",
        "Gold": "I think this paper is a solid contribution to the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games, which are one of the simplest settings of multi-agent RL. It tackles a hard problem and makes solid progress. I think the algorithm and analysis are both nice contributions and definitely intend to study the latter further as I think aspects of it may be useful in other settings. Overall the presentation, while dense, is clear. However, there are a few issues that could need additional discussion: 1) Why does the uniqueness of the Nash equilibria not matter to the results? Quite a bit of prior work has had caveats when they are not unique. The results seem to hold if the assumptions are true for at least one equilibrium, presumably because of the minimax properties in a zero -sum setting, but I’m not quite clear"
    },
    {
        "coherence": 0.19137378520138504,
        "consistency": 0.4758536427701419,
        "fluency": 0.3566773992005496,
        "relevance": 0.12311249486496559,
        "overall": 0.2867543305092605,
        "Generated": "6",
        "Gold": "This paper proposes a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment and feature consistency. The authors divided the target data into source-like and target-specific samples and treat them by different learning methods. The main weakness is the performance compared to previous work. It shows that the results of the proposed method are just marginally good compared with [2] and [3] in 2 out of 3 datasets (table 1 and table 3). The performance is good in table 2"
    },
    {
        "coherence": 0.4000815350379078,
        "consistency": 0.6597483361844051,
        "fluency": 0.37755125001002116,
        "relevance": 0.08721329337486737,
        "overall": 0.3811486036518003,
        "Generated": "7",
        "Gold": "I've raised the confidence score for my review, as I understand the technical details better now, hence the weak accept. The paper proposes a new variational bound for sequential latent variable models. It is based on a partial rejection control step and uses a dice enterprise for sampling the ancestor variables. Empirical experiments suggest that the method outperforms previous work --- The problem is interesting. The notation is a bit cumbersome at times, but it is the case for most papers on SMC."
    },
    {
        "coherence": 0.21860854609453126,
        "consistency": 0.5279958361831074,
        "fluency": 0.48932008754574774,
        "relevance": 0.10462227107493888,
        "overall": 0.33513668522458134,
        "Generated": "8",
        "Gold": "A new meta-learning algorithm to address two shortcomings of standard meta-optimization algorithms: curvature mismatch and limited evaluation (the meta-objective is evaluated only with-in a K-step horizon, ignoring future learning dynamics). The proposed algorithm achieves a new state-of-the-art for model-free agents on the Atari ALE benchmark and yielded gains in multi-task meta learning."
    },
    {
        "coherence": 0.2683050897213578,
        "consistency": 0.4204978970813536,
        "fluency": 0.4849557968899586,
        "relevance": 0.1540058179608437,
        "overall": 0.33194115041337846,
        "Generated": "9",
        "Gold": "I think the paper in its current form takes us closer to understanding what made the Complementary planners work well in IPC 2018. Furthermore, the experiment section is missing many essential comparisons, making it impossible to judge how the algorithms compare to previous work. - ed. Franco et al. (2017). Automatic pattern generation for PDB heuristics: a review and meta-analysis of a new approach to the problem."
    },
    {
        "coherence": 0.18821182254646476,
        "consistency": 0.471589724259171,
        "fluency": 0.37648540120294216,
        "relevance": 0.057053986508393724,
        "overall": 0.2733352336292429,
        "Generated": "10",
        "Gold": "I like the idea of the paper, and was impressed by the experimental results showing that the proposed method EDNIL realizes good worst-group performance in a variety of settings, while addressing the sensitivity to initialization that we see in EIIL. A novel supervised ML model capable of identifying and adapting to different environments hidden in the training data. The method, eDNIL, is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations and the label predictions, produced by a multi-headed neural network."
    },
    {
        "coherence": 0.1063542571692905,
        "consistency": 0.3584925015436447,
        "fluency": 0.4229363772093041,
        "relevance": 0.07382886395806834,
        "overall": 0.2404029999700769,
        "Generated": "11",
        "Gold": "Summary: This paper presents a set of baselines and evaluations to outline a fair comparison of congestion control algorithms. It is a well motivated paper that positions itself as an effort to consolidate the diverging field of congestion Control algorithms. Can the author give any discussion on how much sensing information mentioned in the paper can be accurately measured in the real-world traffic control scenarios. i.e., how realistic are the sensing assumptions?"
    },
    {
        "coherence": 0.0875256768060968,
        "consistency": 0.20243056151614508,
        "fluency": 0.39587979443556953,
        "relevance": 0.196964299934529,
        "overall": 0.22070008317308512,
        "Generated": "12",
        "Gold": "### Strengths * This paper introduces a lifelong RL problem in which new tasks can be possibly be expressed as a logical composition of previous ones. Accordingly, they introduce algorithm (SOGPOL) that can autonomously determine whether a new task can be immediately solved using its existing abilities, or if a task-specific skill should be learned. This paper builds upon previous work on the Boolean Task Algebra For Reinforcement Learning extending it to the discounted and stochastic tasks and to the lifetime RL setup. The authors show that techniques introduce in the previous work can perform close to optimal in a zero-shot transfer scenario. Next, they look into the number of skills needed in order to solve all the tasks and arrive at some lower bounds which are encouraging since they are close to log|G|, this is somewhat supported by empirical analysis later on. The author also introduce a method for using the"
    },
    {
        "coherence": 0.062168310020042916,
        "consistency": 0.19196165023715578,
        "fluency": 0.5409813215257674,
        "relevance": 0.10991874000693702,
        "overall": 0.22625750544747578,
        "Generated": "13",
        "Gold": "This paper extends Q-leaning algorithm to one-sided-feedback settings. The authors use Azuma-Hoeffding's inequality instead of Bernstein's one so that the dependence on H is very loose. This paper proposes two algorithms for the classic inventory control problem. It establishes cardinality-independent regret bonds for the two algorithms in terms of length of episode and horizon. Numerical results are presented to demonstrate the effectiveness of the algorithms"
    },
    {
        "coherence": 0.1338265837670234,
        "consistency": 0.2743298430648811,
        "fluency": 0.38039832940524176,
        "relevance": 0.053566829916623815,
        "overall": 0.21053039653844252,
        "Generated": "14",
        "Gold": "This paper provides a useful and practical NLP backdoor learning benchmark. It also open-sourced the toolkit openbackdoor for implementations. It is a good benchmark work for backdoor detection, especially in NLP. It provides ambiguous settings for different scenarios and incomplete evaluation metrics to improve the way defense and attack algorithms are benchmarked. The authors provide a critique of evaluation in prior work and present \"OpenBackdoor\" which comprises implementation of several attacks and tasks."
    },
    {
        "coherence": 0.10991825555828005,
        "consistency": 0.33737776347239357,
        "fluency": 0.37194065955563155,
        "relevance": 0.1005678681100669,
        "overall": 0.22995113667409303,
        "Generated": "15",
        "Gold": "Low-degree polynomials for high-dimensional statistical inference problems are a prominent model. Weaknesses (major) - I did not identify any major weaknesses. Very minor notes: - line 108 - notation $c$ not introduced yet, why not $x_r$ for realization of root variable? - lines 117 and 122 missing parenthesis -line 189 - a constant $c for $Nc$ is discussed, but that is different than the $c $ as the root variable realization, right? The problem of tree reconstruction (from leaves to root) through low degree polynomieals. This paper also provide a result related to RBF kernel ridge regression for predicting root coloration. The topic of this paper is completely out of my area, and any technical comments I make will probably be unfair to the authors."
    },
    {
        "coherence": 0.3693413625231913,
        "consistency": 0.7978926126718526,
        "fluency": 0.43362077508091795,
        "relevance": 0.24523855867206706,
        "overall": 0.46152332723700723,
        "Generated": "16",
        "Gold": "IMPLANT> CONS ==== I do not see how this algorithm is an MPC framework: where is the action/control law computed in lines 7-16 of Algorithm 1? What did the authors mean in line 11 when they wrote, \"First action a_0(i) pi_theta ... \". I am concerned that the reward function in equation (4) is just a restatement of the optimality principle and do not understand why this is being rebranded as a new algorithm. GAIL is a model-free imitation learning approach with a learned reward function. However, the authors do not fully understand the role of the discriminator in GAIL."
    },
    {
        "coherence": 0.0998075552821086,
        "consistency": 0.27941617468266744,
        "fluency": 0.3784657823618259,
        "relevance": 0.04364719029653363,
        "overall": 0.20033417565578387,
        "Generated": "17",
        "Gold": "A method for training differentially private data synthesizer models for classifier training and for data synthesiser training. However, further consideration on these issues will be interesting and needed. The authors' effort in conducting experiments on quite a few datasets, and their integrity in presenting all the results no matter positive or negative. The technical part, especially the experiments, might need some improvement. The paper presents a survey of the current differentiallyprivate generative models, in which case, the survey part is very short and not in-depth. Or is that the paper proposes QUAIL, which in many cases performs worse than other generating models based on conditional GAN."
    },
    {
        "coherence": 0.14596881272313553,
        "consistency": 0.3229190301117344,
        "fluency": 0.29426420619196875,
        "relevance": 0.06707884885376833,
        "overall": 0.20755772447015175,
        "Generated": "18",
        "Gold": "A comparison of non-recurrent models and deep recurrent neural networks for time series outlier detection on time series. The results are so shallow that it cannot provide any insight into the underlying reasons for the obtained results. The paper evaluates a set of methods for time-series anomaly detection, including RNNs among the considered techniques. Nothing is wrong with that, this is just far from complete. There are dozens of methods to consider."
    },
    {
        "coherence": 0.17805774421989976,
        "consistency": 0.26383671624277555,
        "fluency": 0.3457530558877978,
        "relevance": 0.07813564229274139,
        "overall": 0.21644578966080363,
        "Generated": "19",
        "Gold": "### Strengths - The paper is very well written and well organized. The mathematical notation is clearly described and easy to follow. The theoretical results are well presented, with clear assumptions and, at each step, the assumptions and statements are described in an intuitive way. The experiments are well described and effective at supporting the main theoretical claims. ### Weaknesses FedAvg learns better representations than Distributed SGD, with a largely theoretical contribution, and some well executed small-scale empirical results. This work is very interesting and quite timely. Despite minor errors (probably typos in the math), this work is well worth reading. Theoretical analysis of Fed Avg with at least 2 local updates via convergence towards the ground truth backbone parameter $B_star$. N/A"
    },
    {
        "coherence": 0.08208050061580206,
        "consistency": 0.23919504731619107,
        "fluency": 0.34852691224124616,
        "relevance": 0.08781898879140748,
        "overall": 0.1894053622411617,
        "Generated": "20",
        "Gold": "This is a well-written, well-thought-out work on a topic that has been debated for a long time, and a useful one at that. Cisse et.al's core idea of using orthogonal regulariser for improving the robustness of quantized neural networks and demonstrating how it improves the accuracy of original model. This paper shows how a simple Regularization Scheme may become highly effective when it is supported by a good understanding of the underlying process."
    },
    {
        "coherence": 0.16278879443841893,
        "consistency": 0.37934184333161025,
        "fluency": 0.4608279690330365,
        "relevance": 0.11640960036717016,
        "overall": 0.279842051792559,
        "Generated": "21",
        "Gold": "This paper presents an unsupervised learning model for disentangled latent representation learning. The paper is well motivated and the OAT technique is kind of novel and impressive, but the paper reads like a manuscript in a rush: we see many typos, unclear subscripts, and unclear equations. Weird Table 1 columns and not strong experimental metrics. It is not clear how the proposed formulation for the latent space can find the true number of factors, while $|z_1|$ is gradually increasing up to $K$ over iterations of training."
    },
    {
        "coherence": 0.0499309103131843,
        "consistency": 0.12535637337501623,
        "fluency": 0.4699218371412996,
        "relevance": 0.10678415632613472,
        "overall": 0.1879983192889087,
        "Generated": "22",
        "Gold": "This paper presents a new 2D animation dataset for pixel-wise and region-wise correspondence. This dataset is made by converting three high-quality 3D movies into 2D Animation, thus it is characterized by rich image composition and complex motion. The advantages of utilizing open 3D films make the components in the film (e.g., background compared to CreativeFlow+[18]) more complicated than the previous dataset. The authors argue about their design choices, they establish a benchmark and evaluating existing methods on this. Finally, they offer a great analysis that might be useful for future works."
    },
    {
        "coherence": 0.1073690284071755,
        "consistency": 0.33779718056273766,
        "fluency": 0.26958867080774956,
        "relevance": 0.11973726165689202,
        "overall": 0.2086230353586387,
        "Generated": "23",
        "Gold": "This paper provides a good contribution to the literature on combinatorial semi-bandits as it improves the dependence on batch size from linear to logarithmic in many practical applications. The authors significantly improved the O(K) factor that appears in the previous regret bounds, where K represents the batch size. This paper also proposes a non-triggering version of the smoothness condition and show that dependency on K can be completely removed for the non triggering version. Computationally efficient algorithms are proposed to address this problem."
    },
    {
        "coherence": 0.22536763773430063,
        "consistency": 0.3924257134807737,
        "fluency": 0.12276808904451421,
        "relevance": 0.05961291804645269,
        "overall": 0.2000435895765103,
        "Generated": "24",
        "Gold": "An ensemble-based semi-supervised learning method for novelty detection. The aim of their training scheme is to create an ensemble of models that has a high disagreement on the out-of-distribution samples in the unlabeled set. The final decision of whether an input is considered as out- or in-distancing is based on a hypothesis test using the average disagreement between the softmax outputs in the ensemble. This paper does not clearly identify the limitations of the proposed approach and scenarios in which the approach would not work."
    },
    {
        "coherence": 0.2816892555600138,
        "consistency": 0.44295148857896066,
        "fluency": 0.2627733525961826,
        "relevance": 0.1211843749665882,
        "overall": 0.2771496179254363,
        "Generated": "25",
        "Gold": "This paper provides empirical and theoretical results to reveal that the popular SimCLR method may incur Dimension collapse by showing the low-rank property of the covariance matrix of the embeddings. It then proposes a simple sub-vector based CL method called DirectCLR to solve this problem. Weakness: 1. The analysis in Sec.4 is insightful for some toy settings, but lacks some extensions to the real settings in Sec4. Also experiments are not adequent to validate the points."
    },
    {
        "coherence": 0.14956459558895244,
        "consistency": 0.1805189848000152,
        "fluency": 0.26189200138058044,
        "relevance": 0.20550293355417756,
        "overall": 0.1993696288309314,
        "Generated": "26",
        "Gold": "## Strengths **S1. Large-scale experiments** Through a large set of experiments, the authors concludesively demonstrate the importance and influence of hyperparameters on current deep learning-based outlier detection models. **S2. RobOD demonstrates a method for creating an \"ensemble\"-like autoencoder model\"** By enforcing weight-sharing and adding reconstruction losses at varying depths, RobOD only requires a single autoen coder model that can \"simulate\" multiple autoen encoders of varying deeps and widths."
    },
    {
        "coherence": 0.14198042426869964,
        "consistency": 0.2742818180989117,
        "fluency": 0.3741750136250363,
        "relevance": 0.21308485251465106,
        "overall": 0.2508805271268247,
        "Generated": "27",
        "Gold": "This paper provides the first last-iterate convergence rates of extragradient algorithm or the optimistic gradient algorithm to Nash equilibrium of constrained smooth monotone games in terms of the gap function and Lipschitzs and monotone variational inequality, which match the existing lower bounds $mathcalO(T-1/2)$. The authors propose a novel notion, namely the tangent residual, and use it to discover the potential function needed to prove the result. This paper complements existing results from the unconstrained to the constrained setting."
    },
    {
        "coherence": 0.1319040799852331,
        "consistency": 0.20502678932470683,
        "fluency": 0.3595369282406741,
        "relevance": 0.053370989683748815,
        "overall": 0.1874596968085907,
        "Generated": "28",
        "Gold": "A systematic study of reward hacking in the environments with misspecified rewards. The authors conduct a set of experiments with 4 environments, several types of reward misspecification in each of them and several agents of different expressivity (model capacity). They notice that often the agents that are more capable end up obtaining high proxy reward, but low true reward. Besides, often the transition to the low real reward happens very quickly and authors call this phenomenon “phase transition”. Finally, they propose a baseline for anomaly detection to identify this phase transition. Reward hacking increases with agent capabilities so that increasing capability lowers the true reward function. This results in several interesting hacking behaviors."
    },
    {
        "coherence": 0.20695265934698875,
        "consistency": 0.3941422061429014,
        "fluency": 0.4022811443606963,
        "relevance": 0.13489494243972808,
        "overall": 0.2845677380725786,
        "Generated": "29",
        "Gold": "A method for accelerating distributed sparse regression in high dimensions. It combines Nesterov’s proximal gradient with consensus dynamics and a gradient tracking mechanism to track locally the gradient of f. It shows it converges globally at a linear rate, achieving both optimal iteration complexity and communication complexity under the first-order oracle. It is based upon recent advances in high-dimensional statistics."
    },
    {
        "coherence": 0.17398264191361143,
        "consistency": 0.3209757540324336,
        "fluency": 0.27034532387265353,
        "relevance": 0.09679855690953115,
        "overall": 0.21552556918205745,
        "Generated": "30",
        "Gold": "Is vanilla conformal based on a collection of candidate prediction sets, parameterized by a single parameter, which is tuned/calibrated using a held-out set for performing set-valued predictions on test data? This paper generalized a data-splitting conformal prediction approach to the adversarial attack setting by combining conformal predictor with randomized smoothing, which yields a prediction set with finite sample coverage guarantee under an l2-norm bounded adversariaal noise. The novelty of the proposed methods was very well illustrated, and the theorems 1 and 2 provided convincing results of the effectiveness of the suggested method. The motivation of introducing randomized Smoothing into the score function was very good illustrated."
    },
    {
        "coherence": 0.3066228216747433,
        "consistency": 0.4001282777747594,
        "fluency": 0.3166866736967195,
        "relevance": 0.05628031902151005,
        "overall": 0.2699295230419331,
        "Generated": "31",
        "Gold": "This paper builds a representation learning model/schema for parametrizing room and scene acoustics via a encoder-decoder style deep network interpolation machine. This paper lacks clarity as well as many important details to make it self-contained and reproducible (see \"limitations\" for more details). A NeRF-like representation for binaural room impulse responses given room geometry and emitter/listening position."
    },
    {
        "coherence": 0.1018722440538023,
        "consistency": 0.16852569754227043,
        "fluency": 0.5424712887169288,
        "relevance": 0.06442513906578995,
        "overall": 0.2193235923446979,
        "Generated": "32",
        "Gold": "Pros: This paper reinterprets the theory of PPO-clip based on the hinge policy optimization. They prove the global convergence of the PPO by introducing some assumptions. Besides, they generalize the algorithm to a new family of policy-based algorithms by regarding the policy as a generalized classifier. They also propose a range of policy optimization algorithms depending on the details of the classification algorithm that is used, and empirically evaluate some of them."
    },
    {
        "coherence": 0.12223798342256578,
        "consistency": 0.35436853215812647,
        "fluency": 0.5276183816292189,
        "relevance": 0.06870798012081099,
        "overall": 0.2682332193326805,
        "Generated": "33",
        "Gold": "The paper addresses the problem of sample-efficient inference for symbolic physical rules. It proposes a generative model along with a symbolic regression framework, in which forces are produced from a probabilistic context free grammar that mimics simple Newtonian physics. The paper is clearly-written with clear explanation of the proposed EM-style method. However, one of the main claim that the method \"enjoys the sample efficiency of symbolic methods with the ac- curacy and generalization of data-driven learned approaches\" is not well-supported in the experiments. While in Sec. 4.1 it shows the proposed method is more data efficient than the neural baseline, it's not clear how the method generalized to complete different scenarios."
    },
    {
        "coherence": 0.08616046370349072,
        "consistency": 0.28017682067136696,
        "fluency": 0.4237193423578137,
        "relevance": 0.09632831822245434,
        "overall": 0.22159623623878144,
        "Generated": "34",
        "Gold": "This paper evaluates a gradient-based data subset selection method in hyperparameter optimization. The proposed method, AUTOMATA, consists of three components. The effectiveness of the proposed method is experimentally validated on datasets of text, computer vision, and tabular domain. The limitations and potential negative societal impact are addressed. The authors provided extensive empirical results on different HPO search algorithms and HPO schedule algorithms, and in combination with different core set selection methods, and [...] Overall, the paper is clear and well written. Some parts could be improved. First, if Automata is a framework, then more emphasis should be given to its components and to its use. Second, the meaning of \"retaining the ordering of hyper-parameters\" is not clearly explained anywhere in the paper nor when the authors mention that final accuracy is not their main concern (line 74). Third, it was not clear to me if variable 'k' was"
    },
    {
        "coherence": 0.09288487292143184,
        "consistency": 0.3026623790632038,
        "fluency": 0.4954146959032808,
        "relevance": 0.053187977708487016,
        "overall": 0.23603748139910088,
        "Generated": "35",
        "Gold": "This paper presents a method for improving tail-label performance. It is based on the finding that the distribution of the norms of the learnt weight vectors also follows a power-law as does the distribution among the samples among labels. The main contribution of the paper is proposing methods for re-ranking which encourages precedence of tail labels and a data augmentation mechanism. Both the proposed methods outperform a host of highly competitive baselines on a variety of datasets by significant margins."
    },
    {
        "coherence": 0.1556749292468779,
        "consistency": 0.2435101368597249,
        "fluency": 0.3739738254675524,
        "relevance": 0.046988414696111876,
        "overall": 0.20503682656756678,
        "Generated": "36",
        "Gold": "This paper provides some nice proofs, builds distillation-based algorithms, and demonstrated the performance to multi-class problems in experiments. The main weakness of this paper is that, while the underlying problem framework is intriguing, it does not appear to make a significant impact, much like the various flavors of domain adaptations. The authors present a simple distillation method for control of the churn in a constrained optimization primal setting. They also show that their method, under certain assumptions, is equivalent to a simpler solution for a problem of interest, especially in production settings."
    },
    {
        "coherence": 0.07733587026538319,
        "consistency": 0.16042088624906164,
        "fluency": 0.47572697364931005,
        "relevance": 0.07835564577366017,
        "overall": 0.19795984398435376,
        "Generated": "37",
        "Gold": "I like the idea of linking 2-layer NN with copositive programs, the writing is good, and the proofs seem to be correct. However, the study on the whiten case with a singleton ReLU sign pattern is really not useful. When all the sign patterns of the ReLU are pinned, the representation power of a two-layered network reduces to a linear model (in a polyhedron). This paper proposes a convex formulation for shallow neural networks with one hidden layer and vectorial outputs."
    },
    {
        "coherence": 0.07115162959779174,
        "consistency": 0.2862571588877141,
        "fluency": 0.4491601882803334,
        "relevance": 0.061758937009269994,
        "overall": 0.21708197844377733,
        "Generated": "38",
        "Gold": "This paper proposes a framework based on principle components analysis (PCA) to speed up the missing data imputation. It divides the feature sets into two partitions -- the fully observed one and the one that contains missing values. The authors further propose to apply PCA to the imputed data. The major weakness is that the methodological contribution is quite limited. It has been widely regarded as the go-to tool to handle large datasets. Weakness: The main assumption is that introducing random missing is not acceptable without sufficient evidence and justification. For instance, clinical data has missing that is not completely random. Please see the paper published in Nature Digital Medicine on this topic."
    },
    {
        "coherence": 0.09322477419661714,
        "consistency": 0.1960993839192184,
        "fluency": 0.38170996758924075,
        "relevance": 0.08081736872631111,
        "overall": 0.18796287360784683,
        "Generated": "39",
        "Gold": "I found the underlying theory to be very strong, although the present experiments are bit limited and not very robust. Furthermore, standard experimental strategies are not followed, as an example, for classification tasks, self-supervised models follow \"linear\" and \"fine-tuning\" strategies, which is not followed in this work. (2) I wonder if there is any way to estimate the amount of mutual information maximized via the proposed CorInfoMax approach."
    },
    {
        "coherence": 0.16046328293200623,
        "consistency": 0.38445935448445784,
        "fluency": 0.4188469434451028,
        "relevance": 0.07499740790755241,
        "overall": 0.25969174719227983,
        "Generated": "40",
        "Gold": "## Strengths - I believe the main strength of the proposed solution is the ability of the learned value function to generalize to different goals and different scenes, which is ensured by conditioning on a target goal and learned features from the 2D layout of the specific scene in question (e.g. section 3.4 and Fig. 3, c). It is nice to see that an MLP can indeed generalize successfully when different goals & scene layouts are fed in, as illustrated by the multiple human navigation experiment (shown in fig."
    },
    {
        "coherence": 0.2113794836249545,
        "consistency": 0.45554416025467404,
        "fluency": 0.4455709383603653,
        "relevance": 0.08723620942083371,
        "overall": 0.2999326979152069,
        "Generated": "41",
        "Gold": "I think additional work on section 2.5 through section 3 would be helpful to improve clarity. As one example, \"y\" is unnecessarily overloaded: y denotes a specific attribute, this paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. In order to disentangle the identity information from the attributes the paper proposed to have p(y) = p (y|z= E_phi(x)) , i.e. have z independent of y. A generative model to learn the representation which can separate the identity of an object from an attribute."
    },
    {
        "coherence": 0.05619098769673813,
        "consistency": 0.18050893582589472,
        "fluency": 0.544491053443481,
        "relevance": 0.036104280031415775,
        "overall": 0.2043238142493824,
        "Generated": "42",
        "Gold": "The authors present a method for learning a representation of disease-related image patterns from regional volume information generated from structural MRI images. It overcomes limitations in previously published semi-supervised clustering methods and showed great performance on semi-synthetic data sets. Weak Points 1. The authors point out the lack of a good method to predict disease progression in a continuum based on representations explicitly modeling specific disease effects. To tackle this problem, they proposed a novel method called Surreal-GAN, a tailored version of generative adversarial networks, to learn separate representations of neurological and neuropsychiatric diseases in an unsupervised manner. The method frames the diseased brains as multiple disease- related features at various severity imposed on normal brains, and by learning these features and severities, it can generate a representing with improved reliability and explanability. 2."
    },
    {
        "coherence": 0.08331847592054907,
        "consistency": 0.24240876342593073,
        "fluency": 0.5157499867378195,
        "relevance": 0.0595178653077279,
        "overall": 0.22524877284800682,
        "Generated": "43",
        "Gold": "This paper provides sufficient insights into the performance gap between traditional solutions for numerical computation with physics based models against data-driven methods. It is not clear how the benchmark results points to ML algorithm improvements. The authors provided a dataset of four physical systems (spring, wave, spring mesh, naiver-stokes) including data, data generation methods, codes, and documents. The data, source codes and documents are all well organized. For a paper of dataset and benchmark the contributions should be novel/originality data, comparison and analysis of state-of-the-art methods, and the impact on the future of related fields."
    },
    {
        "coherence": 0.05833881963142903,
        "consistency": 0.2787090905519979,
        "fluency": 0.25889571049468335,
        "relevance": 0.02250577607914879,
        "overall": 0.15461234918931477,
        "Generated": "44",
        "Gold": "Korean language legal NLP dataset, LBOX OPEN, and a Korean legal language model trained on LCUBE, a decoder only model based on a GPT-2 architecture. However, it turns out that the legal corpora only represent 5.5% of the whole pre-training corpus, whereas news and books represent 90% of the pre -training data. LBox-Open, they propose two classification tasks, two legal judgement prediction tasks, and one summarization task based upon part of the data in this dataset."
    },
    {
        "coherence": 0.043462416744219384,
        "consistency": 0.1716557259137085,
        "fluency": 0.2904170493453858,
        "relevance": 0.0518048959994719,
        "overall": 0.13933502200069642,
        "Generated": "45",
        "Gold": "This paper is the first work to design a benchmark for graph neural architecture search (GraphNAS) including data points from 26K GNN architectures on nine prevalent graph datasets. It proposes a new search space and conducts analysis and visualization on the searched architectures and their transferability. This paper presents a unified and fair benchmark for GNN NAS algorithms. It provides a novel, but not important, search space."
    },
    {
        "coherence": 0.048483021742955906,
        "consistency": 0.0984684380876226,
        "fluency": 0.388428190503257,
        "relevance": 0.03392673809603576,
        "overall": 0.1423265971074678,
        "Generated": "46",
        "Gold": "This paper applies deep reinforcement learning to address the problem of portfolio management where decisions need to be made over time. **WaveCorr** is a new type of the permutation invariant principle for a class of functions (e.g., neural network blocks), which is capable of capturing both temporal and cross-sectional correlation structure for the training data. I have following major comments on the paper: The numerical section is well structured and the training procedures are reported in detail. Is it possible to include some simple test in the appendix to test the performance in this setting?"
    },
    {
        "coherence": 0.18090051677193447,
        "consistency": 0.3537567575775485,
        "fluency": 0.37654658401827096,
        "relevance": 0.1149565128410523,
        "overall": 0.2565400928022016,
        "Generated": "47",
        "Gold": "I found the paper very interesting in that it analyzes the over-squashing problems through a new, geometric lens. The theoretical motivation for using curvature-based tools as opposed to random-walk-based rewiring is well-done. The empirical evidence is not strong. The main weakness of the paper is experiments. First of all, is it fully-supervised node classification? b) To my knowledge Cora is not a homophilic dataset [1]. It would be helpful if more datasets from hetherophilic graphs (Citeseer, Pubmed) are also considered in experiments."
    },
    {
        "coherence": 0.17029483096468198,
        "consistency": 0.3823272317313899,
        "fluency": 0.22499892188784634,
        "relevance": 0.18036851898327763,
        "overall": 0.23949737589179895,
        "Generated": "48",
        "Gold": "1. A systematic study to measure the impact of many key hyperparameters and training strategies from previous work, such as distillation, DA, etc. 2. A simplified model which consists of layer reduction (structured pruning) and a single stage of distillation. The experimental results show that this simplified model works well compared to the more complex baselines. 3. Line 51, accuracy “loss” 4. Line 73, length should be extent 5. Line 144, 1-norm argument should be w or w_i 6. Line 150, bucket-A 7. The positioning of Tables 1 & 2 should be in order. 8. Line 178, add “multi-stage knowledge distillation” after “claim”. 9. Line 184, x2.5 should be 2.5x10 10. Number of brackets in (1) doesn’t agree! 11. Line 196, Fig. 4 is referenced in text but it is missing! 12. Line"
    },
    {
        "coherence": 0.12886469008977366,
        "consistency": 0.23139911828585863,
        "fluency": 0.4381791269986617,
        "relevance": 0.08106937420953597,
        "overall": 0.2198780773959575,
        "Generated": "49",
        "Gold": "I think this paper is a bit skewed, since you just the usual BYOL method without modifying it. Disentanglement of contrastive methods with a relaxed version of the original disentangling property. Furthermore, the authors found that the selection of the normalization function affects the results. The presented metrics are incomplete. DCI is not provided on most of the synthetic experiments. Beyond dSprites (which is an easy dataset) only factorVAE is reported which gives a limited comparison between the methods."
    },
    {
        "coherence": 0.04352760219284808,
        "consistency": 0.11880715685264867,
        "fluency": 0.34654569166633886,
        "relevance": 0.08192049189140498,
        "overall": 0.14770023565081014,
        "Generated": "50",
        "Gold": "A new NAS benchmark based on the results of surrogate models prediction. The authors have made several significant improvements over the past submission, however, there are still some concerns for me to see this work get published. I'd suggest authors taking a look at the recent public benchmark results from here: https://bbochallenge.com/ (the black box optimization challenges held at NeurIPS 2021). And update the paper accordingly to track the recent advancement in the field, and it may mislead the future. Once the author have addressed my concerns 1, 2 I'm open to accept this work."
    },
    {
        "coherence": 0.09446167215784664,
        "consistency": 0.25432056967726546,
        "fluency": 0.28881968307703915,
        "relevance": 0.08371352146401494,
        "overall": 0.18032886159404152,
        "Generated": "51",
        "Gold": "Strengths: Logit Anchoring Loss is a novel anchoring loss method for fine-tuning backdoored models to introduce backdoors while maintaining consistency on clean data. The authors point out that \"the surge in the usage of the large-scale neural networks makes it hard to train backdoord models from scratch, since it requires a large training dataset.\" This paper proposes a logit anchoring lose to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee of AWP."
    },
    {
        "coherence": 0.1497081186664947,
        "consistency": 0.3240486239624158,
        "fluency": 0.4242881171918244,
        "relevance": 0.10217323447879598,
        "overall": 0.2500545235748827,
        "Generated": "52",
        "Gold": "A novel approach to a complex set-covering problem with peptide overlays and a machine learning tool that predicts the effectiveness of a vaccine designed by a team of scientists. The authors do not provide any context for how their formulation differs from the well-established approaches. In terms of biology, the formulation is not very meaningful. However, it is not clear to me how the proposed approach compares to existing approaches."
    },
    {
        "coherence": 0.09296714184449112,
        "consistency": 0.23950652870923908,
        "fluency": 0.47166603430398896,
        "relevance": 0.05057893350681801,
        "overall": 0.2136796595911343,
        "Generated": "53",
        "Gold": "This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g."
    },
    {
        "coherence": 0.10544015010529374,
        "consistency": 0.2558319073829386,
        "fluency": 0.2841380423937991,
        "relevance": 0.05478553111102839,
        "overall": 0.17504890774826493,
        "Generated": "54",
        "Gold": "This paper addresses the problem of dynamic inference where the forward graph will depend on the input data. It focuses on real speedups instead of theoretical one, this makes the work more valuable. There is a latency prediction model that can estimate the latency of different mask operations. Early-exit networks, especially in the context of image classification and object detection, and should be re-examined in the future."
    },
    {
        "coherence": 0.1260314277233979,
        "consistency": 0.2650220085708447,
        "fluency": 0.204756341735833,
        "relevance": 0.05318753563305228,
        "overall": 0.16224932841578196,
        "Generated": "55",
        "Gold": "A method for partially labeled image segmentation in a medical image based on a multi-U-Net approach. It is compared with a set of currently published methods in the field. However, the application to WSI images is novel AFAIK. The authors state that “Binary Dice loss and cross-entropy loss were used as the loss function”. Is it a combination of both?"
    },
    {
        "coherence": 0.1037245073838929,
        "consistency": 0.21077707585646765,
        "fluency": 0.3318270448725663,
        "relevance": 0.12282635545818411,
        "overall": 0.19228874589277772,
        "Generated": "56",
        "Gold": "A generalization of a mel filterbank, used commonly in machine audition. A detailed interpretation on the relationship between each component of hand-crafted audio front-ends (such as mel-spectrograms) and learnable counterparts. A new learnable representation for audio signal classification is proposed. It is a welcome and unmatched effort, to the best of my knowledge. The only tasks where a truly significant improvement is reported are language identification and emotion recognition, which are also the tasks where all the methods perform the poorest."
    },
    {
        "coherence": 0.14086142785941372,
        "consistency": 0.39106015084427076,
        "fluency": 0.16793136653106588,
        "relevance": 0.0573735666962889,
        "overall": 0.18930662798275982,
        "Generated": "57",
        "Gold": "I enjoyed reading this paper that addresses difficult and interesting statistical problems. Although the paper is very well written, I found it a bit unfortunate that part of the introductory material had to go to the Appendix, but I understand it was necessary for space constraints. As explained in the paper, the construction of the sequential test for multinomial point hypotheses follows standard steps that are well known in the community and the theoretical results of this part are straightforward consequences of known results. I am not sure the results are really novel and in any case it seems to be an application of the results in Test Martingales, Bayes Factors and p-Values by Shafer et al."
    },
    {
        "coherence": 0.31180712682368145,
        "consistency": 0.48033180463601766,
        "fluency": 0.1000241155574418,
        "relevance": 0.1287313879599487,
        "overall": 0.2552236087442724,
        "Generated": "58",
        "Gold": "This paper provides a new uncoupled, regret-based dynamics that achieves O(T-0.75) where T is the number of repetitions of the game. The authors propose an accelerated version via optimism of the algorithm by Farina et al. (2019a). They also provide preliminary experiments on simple two or three players games. This is a good and important theoretical contribution to the existing literature. A main conceptual contribution is to establish a connection between the optimistic regret minimization to $phi$-regret minimization problem. Numerical simulations are provided in support of the theoretical results"
    },
    {
        "coherence": 0.17440721975204634,
        "consistency": 0.32035770411866266,
        "fluency": 0.19134036519693187,
        "relevance": 0.1085064378289332,
        "overall": 0.19865293172414353,
        "Generated": "59",
        "Gold": "This paper is a review of the work proposed in Chafer et al CVPR 2021 and also incorporates ideas from another work Ahn & Ahn. This paper uses an already available implementation of a hyper-parameter tuning algorithm. The authors use the attention analysis of the vision transformer (ViT) based classification networks to replace CNN based methods in [1]. This paper does not focus on reproducibility of the original results/claims of [1] and [2]. The authors may further analyze why the pseudo label generated by ViT is not good enough for segmentation."
    },
    {
        "coherence": 0.07190817789333595,
        "consistency": 0.2772024696286184,
        "fluency": 0.32159733997608153,
        "relevance": 0.0729333428414391,
        "overall": 0.18591033258486872,
        "Generated": "60",
        "Gold": "## Pros: This paper is very well written and has a very relevant topic and proposed method for this workshop. The overall methods section in \"Technical Approach\" is quite minimal in details. e.g. particularly the description of the visual extractor is kept very short. Even if it is mainly based on previous work I believe it would be helpful to have some more information overall in this technical approach section. This leads to one of the more important contributions of the work, the single loss function (section 3.5). Maybe adding more explicit mathematical notations in Section 3 could help for the comprehensability here. ## Cons: It is very good technical quality, i.e., everything is formulated without technical flaws. The paper is novel in the sense that it solves visual planning problems using the NS-CL approach beyond VQA tasks."
    },
    {
        "coherence": 0.17131747365717442,
        "consistency": 0.38456501759477985,
        "fluency": 0.17641920483776843,
        "relevance": 0.12573974954464343,
        "overall": 0.2145103614085915,
        "Generated": "61",
        "Gold": "A method for efficiently training neural networks with parameters that are constrained to be unitary/orthogonal matrices. The main strategy is to obtain a low rank approximation of the gradient update and then use Riemannian gradient descent either via retraction or exact computation of the exponential map (method projUNN-T). Overall, the paper is clear and well written; the appendix provides a nice mathematical background/review, many interesting remarks/extensions/experiments/..."
    },
    {
        "coherence": 0.13848782849594404,
        "consistency": 0.29950201866162646,
        "fluency": 0.2124434476020013,
        "relevance": 0.11308588375729721,
        "overall": 0.19087979462921725,
        "Generated": "62",
        "Gold": "This paper provides new algorithmic and complexity results for the problem of computing envy-free solutions to the fair division problem. Here, n agents must each be matched to one of n room given their valuations for the rooms and split the total rent for the n rooms among them, and the agents have quasilinear utilities, i.e., their utility for receiving a room is its value minus their payment which is their share of the rent. This is not surprising. What would greatly strengthen the claim is comparing to some other heuristicically optimized robustness metric. While this is really not that necessary since lexislack is basically this (since it doesn't minimize expected envy which is NP-hard) but this claim could be written in a more even handed way and/or evaluated against other poly-time heuretics that get stacked on the maximin rule."
    },
    {
        "coherence": 0.13621233091059695,
        "consistency": 0.28704054936542445,
        "fluency": 0.16486236852219374,
        "relevance": 0.05496362519745894,
        "overall": 0.1607697184989185,
        "Generated": "63",
        "Gold": "This paper proposes a learning framework for compositional representations of goals for goal-conditioned RL, and proposes to obtain a coarse specification of goals using discretization. The goals reside in a low-dimensional representation space that is obtained from high-dimensional sensory data, Strengths 1. The paper shows favorable performance over several baselines on multiple environments. Weaknesses 2. The method assumes that the observations and goals come from the same space, restricting the method from being used when goals are specified in different modality. This assumption should be discussed in the limitations."
    },
    {
        "coherence": 0.27041211892248823,
        "consistency": 0.4433543351420404,
        "fluency": 0.3514517317102857,
        "relevance": 0.2918225353661595,
        "overall": 0.33926018028524346,
        "Generated": "64",
        "Gold": "The authors propose a relatively simple and easy to understand methodology for achieving aggressive binary and ternary quantization for recurrent layers in order to preserve accuracy. This may incur lower overhead during inference and training time compared to applying batch normalization to both input matrix vector and hidden matrix vector products (i.e., matrix vectors and inputs). A method for reducing memory requirements in RNN models via binary /"
    },
    {
        "coherence": 0.1501609571977254,
        "consistency": 0.31636871977735265,
        "fluency": 0.29340911727094854,
        "relevance": 0.043027934112044426,
        "overall": 0.20074168208951776,
        "Generated": "65",
        "Gold": "This paper uses three techniques to \"sparsify\" a dense network over training. This paper shows that these three techniques can achieve good accuracy at a high sparsity levels on ImageNet and CIFAR-10, when compared to other approaches. However, a comparison with more relevant and recent State of the Art prunning approaches is lacking. The authors proposed a training algorithm for sparse deep neural network. Their key idea is that their method allows the pruned weights to be re-activated during training. The organization and the English usage are not well. The paper is more like a draft and needs improvements for many aspects."
    },
    {
        "coherence": 0.14053931788423657,
        "consistency": 0.2751601780530362,
        "fluency": 0.15523681596277425,
        "relevance": 0.07763759116293685,
        "overall": 0.16214347576574598,
        "Generated": "66",
        "Gold": "A novel framework for structured prediction, backed theoretically by a 2 layer ReLU model, that allows the base predictor to be of low complexity that can potentially generalize with relatively fewer labelled data. The authors propose a more data-efficient way to train generative models with constraints on the output; specifically they evaluate on image generation and pseudocode-to-code (SPoC) tasks. They train two separate models, a “predictor” and a ”denoiser”, which they then compose: the output from the ..."
    },
    {
        "coherence": 0.4422333594894833,
        "consistency": 0.8408444799750564,
        "fluency": 0.17812612514773607,
        "relevance": 0.05289008799572519,
        "overall": 0.3785235131520002,
        "Generated": "67",
        "Gold": "Recommendation: 2: Serious ethical issues that need to be addressed in the final version Ethics Review: The authors introduce the TGEA 2.0 dataset which is a large-scale and curated dataset in Chinese along with two benchmarks for diagnosis and pathology mitigation to improve quality of generated texts from language models. The main issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data. A recommendation was formulated in this regard."
    },
    {
        "coherence": 0.11021363304766615,
        "consistency": 0.21439140111369412,
        "fluency": 0.1892782037136228,
        "relevance": 0.08135374958196068,
        "overall": 0.14880924686423597,
        "Generated": "68",
        "Gold": "Asynchronous stochastic gradient descent for Byzantine distributed learning in a parameter-server setting using asynchronous updates, and without the need for storing training instances on the master node; this is the main selling point of the algorithm, and deems the method sufficiently novel in my opinion. The motivation for avoiding storing instances is not well fleshed out, but I would suggest reworking the motivation exposition a little. Note that the bounded gradient assumption is very strong!"
    },
    {
        "coherence": 0.2953363981089662,
        "consistency": 0.5209076228560747,
        "fluency": 0.20744008876240033,
        "relevance": 0.09472544516830188,
        "overall": 0.27960238872393584,
        "Generated": "69",
        "Gold": "This paper proposes a Network-Wise Quantization approach based on the inter-layer dependency of quantization. This paper outperfome previous SOTA by a large margin. The authors clearly state the difference and demonstrate the effectiveness of proposed techniques, but previous methods play a more important role in the discrete optimization problem. Besides, the motivation and theoretical analysis of substituting RSeR with ASoftmax are absent. To tackle the challenges of solving a larger scale combinatorial optimization problem, and the issue of overfitting, the paper proposed two approaches."
    },
    {
        "coherence": 0.24371325281793033,
        "consistency": 0.38301252649349143,
        "fluency": 0.3273340281934221,
        "relevance": 0.030608680689308453,
        "overall": 0.24616712204853808,
        "Generated": "70",
        "Gold": "This paper is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Ziv & Tishby 2017 work and the earlier work on information bottleneck theory of learning (Shwartz et al 1999). This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE)"
    },
    {
        "coherence": 0.16943296170890107,
        "consistency": 0.3821517875732398,
        "fluency": 0.2593792915033215,
        "relevance": 0.05848044813821165,
        "overall": 0.2173611222309185,
        "Generated": "71",
        "Gold": "This paper proposes an approach based on label neighborhood graph that improves the query complexity compared with regular active learning from both theoretical and empirical perspectives. This paper is very interesting and the proposed approach is technically sound. The evaluation has a theoretical part and an experimental part though. A new graph-based active learning algorithm NbrGraphSGD(G) that uses this label comparison by building neighborhood graphs. They experiment in synthetic and real-dataset against a few benchmarks."
    },
    {
        "coherence": 0.14120568272521797,
        "consistency": 0.41684935205288187,
        "fluency": 0.47301812401617666,
        "relevance": 0.10509915313880754,
        "overall": 0.284043077983271,
        "Generated": "72",
        "Gold": "This paper presents an interesting conceptual advance connecting causality, disentangled representation learning, invariant representations and robust classification. The authors propose a Counterfactual Generative Network (CGN), which is basically \"modular\" generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism, i.e. independent factors in the structural causal model of the data. In the context of generating natural images like those comprising the ImageNet dataset, the CGN once trained can be used to generate high-quality counterfactual images with direct control over of factors of variation"
    },
    {
        "coherence": 0.2755311491380517,
        "consistency": 0.4994967609621574,
        "fluency": 0.2208206744859776,
        "relevance": 0.18092829352223028,
        "overall": 0.29419421952710423,
        "Generated": "73",
        "Gold": "1. Yes 2. Consistency training 3. Yes 4. More ablation studies on the hyperparameters 5. Yes 6. Yes 7. Yes 8. Yes 9. Yes 10. No 11. Yes 12. Yes 13. Yes 14. Yes 15. Yes 16. Yes 17. Yes 18. Yes 19. Yes 20. Knowledge Tracing improves the accuracy of a student's prediction. The authors observe that the predictive model should obey certain 3 common sense constraints. If a question is replaced in the student’s data by a very similar question, the prediction should not change much. If an additional correct question is added to the data, the odds of the student being correct on the next question should go up, and the odds should go down for questions being removed and/or added with incorrect responses. The author would have appreciated more information about the 'skill sets' associated with each question and how that impacts the replacement."
    },
    {
        "coherence": 0.20560067593645362,
        "consistency": 0.5419190284701517,
        "fluency": 0.22881253933499449,
        "relevance": 0.03834466745173898,
        "overall": 0.2536692277983347,
        "Generated": "74",
        "Gold": "This paper works on the initialization of neural network parameter based on theoretically inspired optimization algorithm. It is to substitute the previous network initialization algorithm, such as Kaiming's method, [52], etc. The proposed initialization method is closely related to [52] by introducing the cosine similarity of sample-wise gradient for optimizing the initial network parameter. It further approximate and minimize the bound with constraint on gradient magnitude."
    },
    {
        "coherence": 0.1524599721529344,
        "consistency": 0.2090079937978592,
        "fluency": 0.3840264242398233,
        "relevance": 0.07106192694333262,
        "overall": 0.20413907928348735,
        "Generated": "75",
        "Gold": "A new type of generative models with a new inference method of latent variables is proposed. The authors propose a model based on a gradient of the latent variable with respect to zero vector. However, the model assumption that the one step gradient from zero vector equals latent vector is quite limited and greatly constrains the model expressiveness. A justification that such assumption is reasonable is badly needed. The model formulation needs to be carefully checked. For example, Eqn 2 is not entirely correct to me. The second term should not be binary cross entropy as there is no categorical variable involved. Also, please avoid using abbreviations (LBCE, LCCE) at the first time to introduce them, which are confusing."
    },
    {
        "coherence": 0.11712388591803012,
        "consistency": 0.21184190511340822,
        "fluency": 0.23573374706812805,
        "relevance": 0.03468945948801872,
        "overall": 0.14984724939689628,
        "Generated": "76",
        "Gold": "This paper presents a benchmark suite of tasks for evaluating the performance of learned visual representations for use in echocardiography. This benchmark, the echocardiographic task adaptation benchmark (ETAB), provides a meta-dataset constructed from 3 existing echocardiogam research datasets. This paper provides an evaluation protocol that helps grade overall representations, via an average across tasks and task-categories (Eq 1). Strengths are: * Tackles an important problem for an exciting application area (echocardiography); could spur significant ML methods development that is *useful* for improving the efficiency and quality of patient care"
    },
    {
        "coherence": 0.2710334125543731,
        "consistency": 0.5577070343195787,
        "fluency": 0.2819585557574758,
        "relevance": 0.1379935853698414,
        "overall": 0.3121731470003173,
        "Generated": "77",
        "Gold": "The authors describe a new graph-based Twitter bot detection dataset, TwiBot-22, of human and bot Twitter users, with 1M users in total. The authors perform extensive evaluation using latest methods and re-implemented 35 baselines for evaluation and compared the performance of their dataset. A new, larger and more comprehensive dataset for twitter bot detection. It includes five times more users than the existing dataset and is a heterogeneous information network."
    },
    {
        "coherence": 0.21169245279220378,
        "consistency": 0.523576419414135,
        "fluency": 0.15687957664116237,
        "relevance": 0.06391357795477012,
        "overall": 0.23901550670056781,
        "Generated": "78",
        "Gold": "This paper presents a new image dataset presenting everyday household items from around the world with a variety of countries and incomes. This dataset contains a collection of images. Their income tag is estimated by the country income for this demographic group. The results of the experiment are done well and Figure 6 is a helpful visualization of the model performance to drive home the point that models do better on \"higher-income\" representations due to the existing bias of the pre-training dataset."
    },
    {
        "coherence": 0.3352408625152902,
        "consistency": 0.41579370225821444,
        "fluency": 0.26156609556161114,
        "relevance": 0.08611444249571949,
        "overall": 0.2746787757077088,
        "Generated": "79",
        "Gold": "A dynamic tensor product that can outperform repeatedly building the static data structure. This paper studies a lightweight computation of a special case of linear regression where one of the factor matrices temporally changes. The authors propose a tree-based data structure to maintain the sketch of the data so that we can keep up with the temporal changes. To solve this problem, the author shows how [AKK+20] tree data structure can be used in combination with known sketching techniques."
    },
    {
        "coherence": 0.19174551200266668,
        "consistency": 0.3649080257633306,
        "fluency": 0.40611616261816436,
        "relevance": 0.06267737233129843,
        "overall": 0.25636176817886497,
        "Generated": "80",
        "Gold": "### Originality and significance** This paper provides a novel view of how contrastive learning can be used by itself, and without any additional RL training on top, to learn goal conditioned policies from pre-collected data. This is achieved by using the similarity learned during contrastive training as a Q-function, that is then used to improve a policy in the policy gradient step. It is also shown, via a proof, that the contrastive objective estimates the Q-factor, and some additional convergence guarantees are provided. The authors well motivate their approach, compare it extensively with related work on learning goal-conditioned policies."
    },
    {
        "coherence": 0.1511097937495499,
        "consistency": 0.3998310351331824,
        "fluency": 0.20306754617867973,
        "relevance": 0.16634196908980795,
        "overall": 0.230087586037805,
        "Generated": "81",
        "Gold": "Pros - The paper explores an interesting semi-supervised learning setting in which the unlabeled data contain not only the seen class but also novel classes. The authors recommend to compare with more SOTA methods. Since SimCLR is more powerful than RotNet, it is strange to see RankStats get worse performance than the original paper. The number of novel classes has to be prefixed. It is not clear whether the design of the proposed method really leads to the improvement of the performance or not."
    },
    {
        "coherence": 0.16052251903744486,
        "consistency": 0.1972670471587314,
        "fluency": 0.2924264164918088,
        "relevance": 0.07654083842457911,
        "overall": 0.18168920527814103,
        "Generated": "82",
        "Gold": "A methodological framework to transform/approximate offline algorithms in an online setting. This paper claims to develop a general framework for \"approximating offline algorithms using online algorithms**. But the online algorithms in this work do not directly attempt to approximate the offline algorithms. The offline algorithms take time-series X=x_1, ..., x_T as input and produces outputs in the form of decision points A(X) = (x_i, a_i), ...(x_t-d+1, ...), X_t and decide whether or not to produce a decision point at time t. Instead the Online algorithms predict class labels which are a loss."
    },
    {
        "coherence": 0.14838272283271556,
        "consistency": 0.2681957606976117,
        "fluency": 0.2858376903191393,
        "relevance": 0.174687578097373,
        "overall": 0.21927593798670988,
        "Generated": "83",
        "Gold": "This paper is original and significant, and the ablations are well chosen and are crucial in showing the efficacy of the proposed approach. The only weakness is that I wish the user preference component had a little bit more discussion; it’s still not clear to me how this component interacts with the rest of the system (and especially the training/reward function), nor how the various “preference embeddings” are specified/chosen. The problem of incorporating expert knowledge and collaborating with humans is becoming increasingly important over the past few years, especially with more agents showing strong potential for indoor tasks. This makes the main topic of this paper important and meaningful."
    },
    {
        "coherence": 0.18814316660261163,
        "consistency": 0.35467109325985563,
        "fluency": 0.146214194499844,
        "relevance": 0.11882877192628483,
        "overall": 0.201964306572149,
        "Generated": "84",
        "Gold": "I enjoyed reading this paper. Its organizational structure was clear. The ordering of the arguments allowed me to easily follow along without having to go back and re-read previous sections. The number of typos was a problem, and if this paper is accepted, it needs thorough proofreading. I would have liked some intuition in the main paper regarding the origin of the \"carefully chosen\" subset sizes in Figures 1 and 2."
    },
    {
        "coherence": 0.05370608584411938,
        "consistency": 0.11319586526772345,
        "fluency": 0.327328725995568,
        "relevance": 0.04878437723913749,
        "overall": 0.1357537635866371,
        "Generated": "85",
        "Gold": "Summary: Strengths: The paper is well written and clean. Weaknesses: I have several concerns regarding this paper. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next)."
    },
    {
        "coherence": 0.12427392783179554,
        "consistency": 0.3063155285876874,
        "fluency": 0.14585321613251792,
        "relevance": 0.07046077547940569,
        "overall": 0.16172586200785163,
        "Generated": "86",
        "Gold": "This paper addresses the problem of global rigid registration of 3D point clouds using a trainable cascade of MLPs. Strengths: It seems they have got a big improvement over the DeepGMR and DCP method. Weakness: The writing is rather unclear and the paper is difficult to follow. First, how is the region or part information is predicted without any supervised signal? Second, what is the purpose of defining the reconstruction loss and how will it help for the registration or region segmentation?"
    },
    {
        "coherence": 0.20857190840136017,
        "consistency": 0.37946888854761146,
        "fluency": 0.14905127583856595,
        "relevance": 0.11155685589064539,
        "overall": 0.21216223216954572,
        "Generated": "87",
        "Gold": "Good empirical study of several reward learning measures. The point is interesting and timely. While there is a very rich literature in this space with major claims and cool demonstrations, it looks like the evaluation methodologies followed are often fundamentally flawed. The paper does a good job in showing the existence of the issue, which is perhaps tacitly known but, to my knowledge, had yet to be formally investigated. Both theoretical and empirical evidence / arguments are presented, which makes it clear the problem exists across multiple settings. The theoretical results are based on very simple constructed examples, and the numerical results are for relatively simple domains. #### Weaknesses - not focused on robotics. This seems like it may not be the best fit for CoRL."
    },
    {
        "coherence": 0.1567131465468329,
        "consistency": 0.3599322274849736,
        "fluency": 0.07858500984873043,
        "relevance": 0.0823745068253081,
        "overall": 0.16940122267646127,
        "Generated": "88",
        "Gold": "This paper presents a simple but strong baseline tracker, named SwinTrack, for single object tracking (SOT). It adopts the classic siamese structure, and uses Swin Transformer for both feature extraction and fusion of template and search region. A special motion token which encodes past bounding boxes is introduced to further boost the tracking performance. This paper addresses the problem of general object tracking, where the target object is given during test time. The authors propose a Siamee architecture for single objects tracking based on the Transformer model. The encoder is divided into the two feature transforms and a transformer encoding step where correlations of spatiotemporal features on the concatenated branches are made. The idea to use SwinTransformer instead of a CNN in the branches seems to be novel and also appears to be fruitful at least in terms of algorithmic efficiency and surpassing the 70% limit in LAS"
    },
    {
        "coherence": 0.33245279610011635,
        "consistency": 0.5140932557370474,
        "fluency": 0.1291755440789064,
        "relevance": 0.08577798184711731,
        "overall": 0.26537489444079687,
        "Generated": "89",
        "Gold": "This paper focuses on the effects of adversarial robustness training on neural networks. Weaknesses: This paper could at the very least do a better job situating itself within existing robustness research. The authors' claims should be sufficiently couched within the experimental settings studied: supervised image classification for ResNet-based models on CIFAR-10 and two subsets of ImageNet. I would expect a comparison of “different architectures” to encompass for example transformer-based architectures, MLP-architectures, etc."
    },
    {
        "coherence": 0.40248467722967124,
        "consistency": 0.443811874674234,
        "fluency": 0.2582160407958563,
        "relevance": 0.20803029805409104,
        "overall": 0.32813572268846314,
        "Generated": "90",
        "Gold": "This paper addresses the problem of curriculum learning and presents a new technique (Boosted Curriculum Reinforcement Learning) that provides a tighter bound on the approximation error of the action-value function than standard curriculum learning. The authors also conduct interesting empirical investigations on fleshing out the importance of boosting in all environments. The main drawback is the empirical evaluation. They propose a Curriculum Learning method to \"learn\" a residual for each task, instead of trying to learn from scratch each of them. As far as I know, this is the first paper that applies boosting to the RL curriculum learning"
    },
    {
        "coherence": 0.20467084762448487,
        "consistency": 0.3684380623179538,
        "fluency": 0.10681154212600376,
        "relevance": 0.06489174030643685,
        "overall": 0.18620304809371982,
        "Generated": "91",
        "Gold": "A review of a paper on a measure of 'sample likelihood'. I am concerned about the message that is conveyed in the paper. Particularly, the following two points make me be reluctant to recommend an acceptance: 1) There is no measure on the tightness of the lower bound. How can we tell if this bound isnt tight? All results are dependent on the bound being close to the true value. 2) The sample likelihoods are dependent upon a certain \"model\" In the introduction (Cuturi, 2013), the authors mentioned the negative Shannon entropy."
    },
    {
        "coherence": 0.17964288314632346,
        "consistency": 0.32987639706366656,
        "fluency": 0.2153391656963877,
        "relevance": 0.1507402664282915,
        "overall": 0.2188996780836673,
        "Generated": "92",
        "Gold": "The paper is clear and well-written. The results appear to give only modest gains at best (against comparable baselines to the best of my knowledge). For a number of the results the variance is large compared with the relative difference---it would helpful to also include tests of significance for these improvements. Weaknesses & Clarifications for the authors: The paper achieves a little improvement in performance in all three benchmarks and ablation experiments are helpful in understanding the results."
    },
    {
        "coherence": 0.15146014190439405,
        "consistency": 0.3533139098562826,
        "fluency": 0.16246206524511106,
        "relevance": 0.08532786343849887,
        "overall": 0.18814099511107166,
        "Generated": "93",
        "Gold": "A container-based distributed marl framework with inter-process communication and PER to select high-value samples, thereby improving training efficiency. Strengths: 1) The field of the paper is very important and interesting 2) The writing of this paper is clear, and the method is intuitive and effective Weaknesses: 1). The author raised the problem of demanding data transfer at the beginning, especially when the GPU and CPU copy each other. 2). For the Inter-process Communication problem, the author proposed multi-queue manager and buffer manager to optimize, so as to meet the author's proposal: \"A critical design consideration of a successful distributed reinforcement learning framework is the uninterrupted learning of learners \". However the author lacks a detailed discussion of this module. Judging from the current version, he only used \"signal\" to control read and write conflicts. In Subsection 2.3, the approximate calculation of $p(mathb_t"
    },
    {
        "coherence": 0.10764951140554421,
        "consistency": 0.2630298298677413,
        "fluency": 0.0738316985065013,
        "relevance": 0.06321547900185716,
        "overall": 0.126931629695411,
        "Generated": "94",
        "Gold": "I think this paper should be published at the ICLR, but I think it should be refactored to make it easier to publish at the current venue. I would like to see a more clear explanation of why these results are non-trivial. The authors should have a clearer presentation of their results. The problem itself, although it is proposed in the reinforcement learning setting, is of no fundamental difference with the classical mean shift in change-point detection literature."
    },
    {
        "coherence": 0.1536151738836097,
        "consistency": 0.2808869142118718,
        "fluency": 0.09941360997776785,
        "relevance": 0.06221666819660372,
        "overall": 0.14903309156746328,
        "Generated": "95",
        "Gold": "This paper proposes a hierarchical compression algorithm for feature data, called Stripe-wise Group Quantization (SGQ), which captures both channel and spatial-level similarity in pixels, and hierarchically encodes the features in these two levels to achieve a much higher compression ratio. This paper presents the theoretical analysis for the convergence order of SGQC. Strengths: This work addresses the communication bottleneck and builds a novel communication-efficient CL system; it is better to bring out a well-through analysis in the experiment section. There are not enough demonstrations regarding communication efficiency."
    },
    {
        "coherence": 0.04229261042082702,
        "consistency": 0.05916577697439877,
        "fluency": 0.17856215562808905,
        "relevance": 0.1051490506545729,
        "overall": 0.09629239841947194,
        "Generated": "96",
        "Gold": "A good package for data analysts, but lacks a literature review and comparison with any existing work on NLP tasks. ICLR is a good package, but it lacks the literature reviews and comparisons with existing and relevant work. It's not a perfect package for NLP, but I expect to see performance plots for different tasks (whether just for this package or with comparison to some baseline alternatives). It is an excellent package for a wide range of tasks."
    },
    {
        "coherence": 0.1334860122122539,
        "consistency": 0.26925064439301705,
        "fluency": 0.07221681304235224,
        "relevance": 0.10834847478573963,
        "overall": 0.1458254861083407,
        "Generated": "97",
        "Gold": "This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function. This is the first work to apply this type of visual explanation to RL. The paper introduces an attention mechanism into reinforcement learning agents to identify the attended visual regions for vision-based reinforcement learning tasks. Specifically, they applied a mask-attention mechanism for the policies and value prediction neural network columns of the A3C model based on the convolutional LSTM (Xingjian et al., 2015). The novelty of the article should be outlined more clearly and more experimental work should be provided to justify which type of attention is most effective."
    },
    {
        "coherence": 0.09426243841744064,
        "consistency": 0.17866628185069658,
        "fluency": 0.09902768457292797,
        "relevance": 0.05625957021086226,
        "overall": 0.10705399376298186,
        "Generated": "98",
        "Gold": "This paper proposes a new perspective on dealing with label noise, called Class2Simi, by transforming the training examples with noisy labels into pairs of examples with noise similarity labels and then learning a deep model with the noise labels. Furthermore, the paper proves that the noise rate for the noise labeled is lower than that of the noisy label label. Hence it can be useful for the ML communities along with this."
    },
    {
        "coherence": 0.0675509547867251,
        "consistency": 0.2347524164870132,
        "fluency": 0.2971902216978,
        "relevance": 0.048400619930474245,
        "overall": 0.16197355322550316,
        "Generated": "99",
        "Gold": "This paper combines the convergence analysis of averaged stochastic gradient descent on kernel methods with the connection of kernel method with neural network to derive an optimal convergence rate of NN in NTK regime while the proof technique is novel. This paper analyzes averaged SGD for overparameterized two-layer NNs for regression problems. Particularly, they show that the averaged sGD can achieve the minimax optimal convergence speed, with the global convergence guarantee. To achieve, they propose a new parameter which captures the complexities’’ of the target function and the RKHS associated with the NTK. I am tending to accept the paper."
    },
    {
        "coherence": 0.18614271514312833,
        "consistency": 0.1428345862839865,
        "fluency": 0.34349496837177534,
        "relevance": 0.04798689390246398,
        "overall": 0.18011479092533852,
        "Generated": "100",
        "Gold": "We propose a geometric deep learning framework for contact prediction for PPI. They use a k nearest neighbor representation for each protein and compute geometric-based attention scores to convolute the messages over this graph. This paper aims to improve the prediction performance of the protein-protein interaction, which involves predicting partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins."
    },
    {
        "coherence": 0.0834410941172555,
        "consistency": 0.16787798334652904,
        "fluency": 0.5058384844083711,
        "relevance": 0.09567113942145124,
        "overall": 0.21320717532340172,
        "Generated": "101",
        "Gold": "Pre-Trained Representations and Policies for RL from Pixels, a Novel Technique for Unsupervised Representation Learning (RL) Using Particle-Based Entropy Estimation The paper addresses a very relevant reward-free exploration objective as a preprocessing to RL. The method iterates between minimizing a contrastive loss and maximizing an intrinsic reward derived from a k-NN entropy estimation of the state distribution and empirically evaluates the method over a set of visual Mujoco tasks and Atari games."
    },
    {
        "coherence": 0.08693215812630091,
        "consistency": 0.16681757868848462,
        "fluency": 0.29471909370636207,
        "relevance": 0.08109536503024456,
        "overall": 0.15739104888784805,
        "Generated": "102",
        "Gold": "This paper is a good introduction to truncated Gaussian kernels. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. This paper could be used in the future by others looking for faster model inference and training. If a Manhattan distance d is used i.e. dx,dyk in equation (6), why is this a FullCRF? It seems like the new CRF is no longer fully connected one."
    },
    {
        "coherence": 0.06826152071587634,
        "consistency": 0.1605417229823342,
        "fluency": 0.189842286239905,
        "relevance": 0.1073031674744865,
        "overall": 0.13148717435315052,
        "Generated": "103",
        "Gold": "This paper provides a novel method for determining which gradient estimators are effective for training single-layer binarized networks and which are not. This paper does not analyze the most common and successful straight-through estimator, which is a shifted and scaled version of the clipped ReLU STE. This connection is not made in the paper. Learning One Convolutional Layer with Overlay Patch. (ICML 2018) pp. 117-117. [1]"
    },
    {
        "coherence": 0.106685822037255,
        "consistency": 0.18196857464878813,
        "fluency": 0.12935407460994994,
        "relevance": 0.09273315822248833,
        "overall": 0.12768540737962034,
        "Generated": "104",
        "Gold": "The paper provides an interesting perspective (role-diversity) to measure the difference between MARL tasks. The experimental results, measured by the three role diversity metrics, answered a common concern question that why we should care about analyzing Multiagent Systems from the perspective of roles, or the alternative role definitions they propose. I would recommend the authors re-frame the paper narrative and improve the writing quality; I’d made suggestions under “Summary of review”."
    },
    {
        "coherence": 0.25359200386631275,
        "consistency": 0.43092010864143987,
        "fluency": 0.1582944520941628,
        "relevance": 0.1440153885865162,
        "overall": 0.24670548829710792,
        "Generated": "105",
        "Gold": "Pros - a reasonable approach to polyphonic music generation: figuring out a way to split the parts, share parameters appropriately, measuring entropy per time, all make sense - the resulting outputs tend to have very short-term harmonic coherence (e.g. often a ‘standard chord’ with some resolving suspensions, etc) - I do not hear the rhythmic complexity that is described in the introduction; the work by Johnson (2015) (ref. provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and sounds quite good (better, in my opinion)"
    },
    {
        "coherence": 0.2214548826204964,
        "consistency": 0.3511567406275323,
        "fluency": 0.13232650850830294,
        "relevance": 0.0989321305612258,
        "overall": 0.20096756557938938,
        "Generated": "106",
        "Gold": "Weaknesses: 1. There are a lot of important baseline models and datasets missing, the comparison is not comprehensive. The authors aim to build a foundation model for image-language and video-language tasks, which is a popular research topic. It achieves comparable or SOTA performance across many benchmark datasets. It has a potential societal impact. 2. The authors' approach is based on a vision-language foundation model that supports both image- language and video language pretraining in a unified framework."
    },
    {
        "coherence": 0.23868182322531994,
        "consistency": 0.5374251133393751,
        "fluency": 0.12531813809560802,
        "relevance": 0.09214081625159802,
        "overall": 0.24839147272797527,
        "Generated": "107",
        "Gold": "This paper proposes a neural deformation method, which utilize the deformation priors in a large-scale dataset. This method first deforms the input model back to the canonical space, and then deformations it to the target that satisfies the constraints given by the user. The authors claim they divide the test set in 143 sequences for seen train identities, and 55 sequences in unseen ones."
    },
    {
        "coherence": 0.19895129517174243,
        "consistency": 0.38471165306574484,
        "fluency": 0.14997065101256205,
        "relevance": 0.12209192228400745,
        "overall": 0.2139313803835142,
        "Generated": "108",
        "Gold": "The state-of-the-art (SOTA) approach to continuous learning is a promising direction for continuous learning, but the use of SOTA in this paper is overly generous. I would recommend removing most of the references to SOTA from the paper. The authors propose memory replay with data compression, which is both an important yet neglected baseline. However, the authors provide a novel method based on determinantal point processes (DPPs) to determine the quality of the data compression."
    },
    {
        "coherence": 0.22113434423476516,
        "consistency": 0.5417304967988522,
        "fluency": 0.2031464340166618,
        "relevance": 0.06455793017866276,
        "overall": 0.2576423013072355,
        "Generated": "109",
        "Gold": "This paper introduces a self-supervised approach for the task of surround-view depth estimation and project features of different views into a volumetric feature spaces, where overlapped views can enhance each other and get better depth maps. The problem setting is novel. The proposed method achieves improved results on DDAD and nuScenes datasets. The authors propose a novel surround-visited depth estimation method based on a unified Volumetric feature representation encoded from surround view."
    },
    {
        "coherence": 0.3073417391948098,
        "consistency": 0.45378604116094673,
        "fluency": 0.27667313275784533,
        "relevance": 0.08313208136282636,
        "overall": 0.2802332486191071,
        "Generated": "110",
        "Gold": "This reviewer's view is that soundscape 2.0 isn't as strong as it could be, or should be, be. The authors introduce SoundSpaces 2.0, a simulation platform that supports audio simulation with a number of improvements over existing platforms. The author has validated the simulation accuracies with real-world recordings, introduced a dataset based on the environment, and conducted benchmarking on two tasks. The contributions are significant. This paper proposes a new platform for simulating acoustic effects. The major difference compared to the prior work is that this work can compute sound properties on the fly and thus avoid using precomputed data. Therefore, the new platform can support arbitrary geometry and continuous outputs. The researchers also perform two experiments to provide evidence for the usefulness of this work."
    },
    {
        "coherence": 0.1600496147389118,
        "consistency": 0.34681043837512143,
        "fluency": 0.11472152623665358,
        "relevance": 0.0743897952277374,
        "overall": 0.17399284364460604,
        "Generated": "111",
        "Gold": "This paper proposes a new variable selection method called FLOWSELECT to perform controlled feature selection that does not suffer the problems existed in Knockoffs-based methods, e.g., “Fan Y. et al. RANK: Large-Scale Inference With Graphical Nonlinear KnockOffs.”. However, the authors does not discuss this related literature. Weaknesses: 1. The method does not guarantee the FDR control due to the dependence between the p-values. 2. The motivation and comparison of the proposed method is unsuitable. 3. The presentation of this paper can be improved. ## Strengths"
    },
    {
        "coherence": 0.0917757589490042,
        "consistency": 0.2984330233848407,
        "fluency": 0.16058907277509488,
        "relevance": 0.02809573226179583,
        "overall": 0.1447233968426839,
        "Generated": "112",
        "Gold": "This paper proposes a novel way of performing data augmentation in image-based reinforcement learning by perturbing regions in the frequency domain of the observed images. Strengths: - The explanation and visualization of the Spectrum Random Masking method are excellent. Weaknesses - I would love to see a real-world experiment where data augmented plays a more important role. This paper proposed a new data enhancing technique that masks out components in the frequencies space: the Spectrum random masking (SRM). To adapt with SRM, the authors propose new techniques to stabilize the Q value after augmentation."
    },
    {
        "coherence": 0.12398661454848718,
        "consistency": 0.2273090475496352,
        "fluency": 0.16410896840520886,
        "relevance": 0.06097268559898142,
        "overall": 0.14409432902557817,
        "Generated": "113",
        "Gold": "This paper presents a topic model based on adversarial training. It adopts the framework of InfoGAN to generate the bag-of-words of a document and the latent codes in infoGAN correspond to latent topics in topic modelling. To make the model work better, several add-ons are also proposed, combining autoencoder, loss clipping, and a generative model to generate text sequences."
    },
    {
        "coherence": 0.2806485527826687,
        "consistency": 0.4313307016366122,
        "fluency": 0.10888608406097279,
        "relevance": 0.07278024930198947,
        "overall": 0.22341139694556078,
        "Generated": "114",
        "Gold": "A theoretical explanation of the link between natural image statistics, autoencoders, and perceptual distances for small image distortions. This paper presents a number of observations linking distances in natural images and autoencodings, as well as perceptural similarity for humans. It is a good introduction to the field of perception and machine/deep learning. However, I think the most interesting contribution is the training of auto-encoding with noise and a Perceptual Similarity Distance."
    },
    {
        "coherence": 0.13361333557034707,
        "consistency": 0.30409492860151466,
        "fluency": 0.2893036014718745,
        "relevance": 0.042659606897481876,
        "overall": 0.1924178681353045,
        "Generated": "115",
        "Gold": "This paper develops methods to construct predictive intervals on regression for cross-sectional time series data that come with valid coverage guarantees. The method developed in the paper focus on two types of validity: cross - sectional validity where the randomness Is the randomlyness over time series and is typically temporal dependent. While cross-segmental validity can be achieved following the standard conformal inference technique, achieving longitudinal validity is the harder one. The authors propose temporal quantile adjustment to achieve longitudinal validity via quantile budgeting (TQA-B) and error based adjustment (TAA-E). The authors give error bounds for coverage and also support their claim empirically. This paper addresses the task of predictive inference with panel data consisting of a time dimension and sample dimension."
    },
    {
        "coherence": 0.1212104197240957,
        "consistency": 0.31173539269722633,
        "fluency": 0.17571657626512488,
        "relevance": 0.08939022153310286,
        "overall": 0.17451315255488742,
        "Generated": "116",
        "Gold": "A new design of intrinsic reward, as a variant of the count-based reward, to tackle the sparse-reward tasks in reinforcement learning. It is relatively novel to learn VQ-VAE and then use the discrete hash code from the codebook for state visitation count. The experiment result is positive, especially on hard exploration tasks ObstructedMaze in MiniGrid. LECO approach may be a better fit for the task, but I think the authors could add more information, for example, describing under which conditions tasks may require intensive leveraging of inter-episode novelty."
    },
    {
        "coherence": 0.16078787471293307,
        "consistency": 0.2615325192275711,
        "fluency": 0.2303674460688221,
        "relevance": 0.09820533153402533,
        "overall": 0.1877232928858379,
        "Generated": "117",
        "Gold": "Pros: -- Clarity and Soundness: -- Most of the effort was invested onto constructing the graph CNN that is claimed tailored for non-Euclidean matrices. But this part looks a bit theoretically (or even intuitively) ungrounded --- particularly, it is not easy to follow the rationale behind sec. 2.4. It is also unclear how is this construction suitable for handling non-euclidatean cases. This part may need some more explanations. This paper aims to tackle the matrix completion problem by drawing connection from prior work in image completion domain. It seems to be a combination of prior work: Multi-graph convolution combined with Dirichlet energy on row and column graph laplacian where the input rating matrix is corrupted with noise. I would prefer to vote for rejecting and ask them for upgrading the writing overall."
    },
    {
        "coherence": 0.11406123615564215,
        "consistency": 0.3453953574149815,
        "fluency": 0.21538604385377627,
        "relevance": 0.08004071176506053,
        "overall": 0.1887208372973651,
        "Generated": "118",
        "Gold": "The TAP-Vid dataset is a benchmark for long-term arbitrary point tracking tasks. The proposed task, i.e., tracking any point (TAP), can benefit the development of embodied agents. A novel baseline method TAP -Net is proposed to deal with the problem and gets superior performance on the proposed TAP Vid series point tracking datasets in terms of occlusion accuracy, average thresholded position accuracy, and average Jaccard thresholds."
    },
    {
        "coherence": 0.18547139048432876,
        "consistency": 0.3685069232324953,
        "fluency": 0.14532649550437657,
        "relevance": 0.06518285774878863,
        "overall": 0.19112191674249732,
        "Generated": "119",
        "Gold": "This paper proposes Non-stationary Transformers as a generic framework to tackle over-stationarization problem. It includes Series Stationarization and De-stationarily Attention module. Its implementation is straightforward and easy to understand. The authors need clearly discuss their difference. Table 4 requires readers to re-read Table 1 \"ADF Test Statistics\" column. Authors can consider adding information in figure 4 to clarify that datasets are sorted according to ADF Test statistics so that readers are easier to follow. This paper focuses on forecasting and introduces a new method for scaling attention. The input raw series is chunk-normalized to yield the input of the backbone transformer model. The chunk-wise scaling parameters are processed through MLPs to scale the rows of the attention matrix. This procedure is shown to consistently improve performance of transformer-based forecasting models."
    },
    {
        "coherence": 0.14413270656631158,
        "consistency": 0.30047625557454705,
        "fluency": 0.32536562901852767,
        "relevance": 0.03772982462988429,
        "overall": 0.20192610394731764,
        "Generated": "120",
        "Gold": "I think it is a good paper, which contributes to the community. But I do have concerns regarding the empirical experiments, I think that the environments are rather toy problems, and since DeepTop incorporate neural-networks, its main advantage over tailored analytical methods is in complex environments. The authors propose an algorithm to compute an optimal threshold policy in MDPs and RMABs with state information composed of a scalar state and a vector state. This function is used to construct the threshold policy where the action (0 or 1) only depends on the comparison between the vector state and the produced skalar value. In order to learn the mapping used for threshold policy, the authors use an actor-critic algorithm, where the scaar mapping and the associated threshold policy are used as the actor function, and neural network is used as a action-value function (Q-function) as the critic function."
    },
    {
        "coherence": 0.10264709913637371,
        "consistency": 0.21621474943196484,
        "fluency": 0.1663240398663635,
        "relevance": 0.07579087772264512,
        "overall": 0.1402441915393368,
        "Generated": "121",
        "Gold": "This is a nice study that sheds some light on the capability of transformers model to predict values of a function given an input via sample prompting. All the model is doing is doing sequential modeling that given a series of inputs and the corresponding function values, the model predicts the function value of the previous input. So in a way this can also be seen as sequence modeling. 2-layer ReLU is more complex than linear but also not as interpretable Decoder-only transformer models are capable of in-context learning all of the simple functions classes tested in the paper to a surprising extend. The mean squared error values computed using the transformers often match the best-case known algorithms (such as min-norm linear regression and LASSO)"
    },
    {
        "coherence": 0.14830205050733752,
        "consistency": 0.2944121152511493,
        "fluency": 0.19075294556238928,
        "relevance": 0.061163337384466894,
        "overall": 0.17365761217633574,
        "Generated": "122",
        "Gold": "This paper studies ways of subsampling tall-and-dense p-norm regression involving structured Vandermonde matrices. It shows that in this setting with additional structure, sampling by Lewis weights produces poly(p) * d row sized samples for all values of p. The theoretical and practical results are quite surprising and interesting to those working on optimization/randomized numerical algorithms. The first step is to argue that the rank of the regression problem is much smaller than the bound of $dO (nlog2n)+dp/2+Cmathrmpoly(1/epsilon)$"
    },
    {
        "coherence": 0.10237931014468475,
        "consistency": 0.2741585837590734,
        "fluency": 0.18135382884077675,
        "relevance": 0.040241128794540726,
        "overall": 0.1495332128847689,
        "Generated": "123",
        "Gold": "summary: This paper is extensible because it doesn’t require predefined sampling and dataset distributions (Q and P). It seems that in this framework, all sampled data should be saved and used in the calculation of P, Q, and a_t. Besides, this paper didn’t compare the proposed method’s computation complexity with other AL methods. 3. The second to last paragraph in section 3.3 is confusing."
    },
    {
        "coherence": 0.1654100578496441,
        "consistency": 0.4405760668444014,
        "fluency": 0.055183541954178676,
        "relevance": 0.08761898694919658,
        "overall": 0.1871971633993552,
        "Generated": "124",
        "Gold": "This paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The basic idea is to define accelerated gradient flows on the space of probability distribution. The defined flows include a term depending on the current distribution of the system, which is difficult to compute in general, the authors introduce an interacting particle approximation. The experiments are a proof-of-concept on simple illustrative toy examples"
    },
    {
        "coherence": 0.14119496542318238,
        "consistency": 0.22199135272818882,
        "fluency": 0.12174471643710162,
        "relevance": 0.05053917417611571,
        "overall": 0.13386755219114713,
        "Generated": "125",
        "Gold": "A new method to tackle the problem of generalization to unseen dynamics for model-based RL. Previous methods learn to predict a vector $Z$ that characterizes a particular environment dynamics from past transitions. However, as the environment id or label is not available, this vector inevitably contains redundant information, which might hurt the generalization of the model. The paper therefore proposes an interventional approach to estimate the probability that two vectors $Hatz_i$ and $Hhatzz_j$ belong to the same environment, and then uses a relational head to force similarity between them. The notation in the last paragraph of page 2 is confusing - what's the difference between Z and hat Z? I think that's partly due to slightly confusing notation which isn't well defined, and also due to the complexity of the method."
    },
    {
        "coherence": 0.12535057640722724,
        "consistency": 0.22578271690057067,
        "fluency": 0.23244227203695123,
        "relevance": 0.10907204064296525,
        "overall": 0.17316190149692862,
        "Generated": "126",
        "Gold": "A new look at why neural networks generalize despite optimizing to zero training error, over-parameterization, etc. The primary strength of the paper is novelty in that it could be a contribution of explaining why neural network generalization despite violating traditional ML theory and rules of thumb. The experiment section does not explain this neither. The only thing related to images is CIFAR10 and ImageNet. Based on the naming, I would expect to see more connection to low-level vision function or filters. 3. The function for images would be different in different time step. This statement seems to be expected as the network parameter is keep updating via SGD. In particular, the function represented by neural networks is expected to change over time until the learning rate is sufficiently small. This is what would expect by \"training\" the network: we expect the weights and bias terms to change during training (hence activations would change) until the network is fully trained"
    },
    {
        "coherence": 0.1374476446316817,
        "consistency": 0.17813201485375724,
        "fluency": 0.38011341619573835,
        "relevance": 0.1837278930054545,
        "overall": 0.21985524217165792,
        "Generated": "127",
        "Gold": "This paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. It is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network as a parametric function in Depth * Width 2 number of parameters, and then using standard L-2 covering and Dudley Integral. They extend this technique for CNNs, Resnets, Hyper-spherical Networks, etc and provide specialized bounds for each case. In the end, the authors provide comparisons to the existing bounds. Although intended, the bound in Theorem-1 depends on the number of parameter and hold only if m > d * (p2) = number of parametric parameters"
    },
    {
        "coherence": 0.09465432457347872,
        "consistency": 0.15776506286846625,
        "fluency": 0.3962269785340113,
        "relevance": 0.0849500327234103,
        "overall": 0.18339909967484164,
        "Generated": "128",
        "Gold": "This paper introduces a new normalization method named pixel-wise tensor normalization which improves both accuracy and robustness of the model. However, the results show somewhat improvement, but not significant. Also, the paper does not provide enough theoretical backups for the claimed algorithm, and it prevents me from being completely convinced. The paper still needs further polishment and is not ready for publication at the moment. These two techniques are easy to understand and bring improved robustness compared with the baseline."
    },
    {
        "coherence": 0.18167761041340483,
        "consistency": 0.3229393755834859,
        "fluency": 0.24234227205066763,
        "relevance": 0.1713178075210979,
        "overall": 0.22956926639216407,
        "Generated": "129",
        "Gold": "A scalable way of approximating the eigenvectors of the Laplacian in RL by optimizing the graph drawing objective on limited sampled states and pairs of states. The authors empirically show the benefits of their method in both types of goal achieving task. One use-case of the learnt representation is for reward-shaping that is said to accelerate the training of standard goal-driven RL algorithms."
    },
    {
        "coherence": 0.06268412069258063,
        "consistency": 0.1351690509461975,
        "fluency": 0.26522957557978516,
        "relevance": 0.027541403711437738,
        "overall": 0.12265603773250026,
        "Generated": "130",
        "Gold": "Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing (Herzig and Berant, EMNLP 2018) This paper describes a two-stage encoder-decoder model for zero-shot semantic parsing, which is very related. (Concurrent) decoupling structure and lexicon for zero - shot semantic parsing - Herzig et al."
    },
    {
        "coherence": 0.2330025908586864,
        "consistency": 0.42524296439283976,
        "fluency": 0.1658136971223629,
        "relevance": 0.04831216705928567,
        "overall": 0.2180928548582937,
        "Generated": "131",
        "Gold": "SABA is a finite-sum stochastic bilevel optimization problem. It is based on SAGA, and provides theoretical guarantees for both SGD version and variance reduction version. This paper proposes a simple framework for solving bilevel optimality. It involves only 3 unbiased estimations in each iteration. The authors provide theoretical guarantees and experimental results showing their superior performance. A finite - sum st. qquad z = argmin_z' G(x,z')."
    },
    {
        "coherence": 0.3904493986038579,
        "consistency": 0.4068574478715971,
        "fluency": 0.31210817019478276,
        "relevance": 0.11040692355179083,
        "overall": 0.3049554850555072,
        "Generated": "132",
        "Gold": "A new learning paradigm for Bayesian neural networks: learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a \"functional space\". The idea of extending Riemannnian Langevin dynamics to functional spaces is elegant, however it is extremely hard to follow the proposed method as details are kept to a minimum. The finite approximation of the posterior distribution is a function of the parameters theta, however ta displays parameters lambda."
    },
    {
        "coherence": 0.09893255767670364,
        "consistency": 0.2917413892640686,
        "fluency": 0.10874773392068575,
        "relevance": 0.09968937073741407,
        "overall": 0.14977776289971803,
        "Generated": "133",
        "Gold": "This paper addresses the timely and important problem of how to train RL agents such that they solve desired tasks while not engaging in undesired behavior that is not explicitly specified via the reward function. The main idea of the paper is to co-train an RL policy on this side effect measure with the aim of minimizing side effects. This policy is used for regularizing the reward-optimizing agent during training, such that the trained agent learns to bias its actions towards avoiding side effects when the task allows for multiple viable actions. Additionally, the generalization of the side-effect-avoiding policy is tested."
    },
    {
        "coherence": 0.2566632183267104,
        "consistency": 0.4001535036433198,
        "fluency": 0.27625212203489563,
        "relevance": 0.07126324709591637,
        "overall": 0.2510830227752106,
        "Generated": "134",
        "Gold": "This paper aims to decompose a scene into objects and infer the representations of 3D occupancy, color, and pose for each object from a single image of the scene without supervision. This paper proposes an autoencoding solution by combining the Slot Attention encoder with the GIRAFFE decoder. Each object is represented as a Neural Radiance Field (NeRF) additionally parameterized by the latent variables inferred from the encoder."
    },
    {
        "coherence": 0.27677700908670144,
        "consistency": 0.3101152861482274,
        "fluency": 0.462515359186587,
        "relevance": 0.09689637927381327,
        "overall": 0.2865760084238323,
        "Generated": "135",
        "Gold": "A method for transfer in reinforcement learning based on estimating a small set of factors describing a system and modelling the agent as a dynamic bayesian network. The method is evaluated on variations of a cartpole and a pong domain (both from images) and evaluated against a set of recent, competitive baselines. The authors do a great job at presenting the problem and the current approaches to the problem, giving enough context to the reader. The results show that the AdaRL outperform other methods, but the error bounds are very large."
    },
    {
        "coherence": 0.2713368629695235,
        "consistency": 0.4259552478174254,
        "fluency": 0.2123725590298795,
        "relevance": 0.14856535727111655,
        "overall": 0.26455750677198625,
        "Generated": "136",
        "Gold": "No justification of standardization technique It is unclear exactly what contributions this paper is making. First, as the authors acknowledge, there have been several recent works studying robustness to natural vs. adversarial perturbations. For completeness, one the authors miss is the following: Taori et al., Measuring Robustness to Natural Distribution Shifts in Image Classification. On arXiv. Weaknesses: This paper studies a relevant problem. The authors answer this question in the negative, but this is naturally expected: the perturbations that the authors study yield images that are likely in the distribution of natural images."
    },
    {
        "coherence": 0.13452146914968094,
        "consistency": 0.26893461259574575,
        "fluency": 0.1637229748984753,
        "relevance": 0.058029122432120384,
        "overall": 0.1563020447690056,
        "Generated": "137",
        "Gold": "This paper provides a new definition of safe SSL: \"a SSL algorithm is safe if it has theoretical guarantees that are similar or stronger to the complete case baseline\". The \"safe\" is related to the performance, i.e., the performance is guaranteed not to be reduced compared with the baseline supervised learning method. The idea of the paper is clear and sound. The bound on the variance of the DeSSL risk estimate and the generalization bound are useful. They give us generalization insights about SSL, and the benefit from the correlation between $L$ and $H$ on average."
    },
    {
        "coherence": 0.19711941060879082,
        "consistency": 0.3539151290170112,
        "fluency": 0.11023615488288892,
        "relevance": 0.17509500126990232,
        "overall": 0.20909142394464833,
        "Generated": "138",
        "Gold": "This paper is a good introduction to the Monte Carlo method. I found the experimental section to be a bit weak in my opinion. I expected a separate section that wud focus on experiments, and a more detailed and clearer demonstration of the benefits of the approach. Although I am not an expert in CEU preference elicitation/learning, the authors have cited some work which could plausibly have been used as baselines, although I am a non-assistant evaluator of CEU preferences."
    },
    {
        "coherence": 0.07095694830597232,
        "consistency": 0.29440147622300683,
        "fluency": 0.16435258146764065,
        "relevance": 0.06380252157456061,
        "overall": 0.1483783818927951,
        "Generated": "139",
        "Gold": "A novel framework for Weakly-Supervised Whole-Slide Image Classification. It uses a self-supervised contrastive representation encoder trained with large-scale data. It implements three components to aggregate the obtained patch-level representations: deep attention-based class-specific MIL aggregation(CDA), a positive-negative-aware model that pushes away positive and negative patches given its relevance to the slide-level prediction (PNM) and a weakly-supervised cross-slide contrastive learning with both hard negative and negative sample mining."
    },
    {
        "coherence": 0.21127768587425463,
        "consistency": 0.3147319379609494,
        "fluency": 0.349557503002696,
        "relevance": 0.18571768602113248,
        "overall": 0.2653212032147581,
        "Generated": "140",
        "Gold": "## Weaknesses: - The phase diagram partitions the space into four possibilities based on the choice of regime for each layer. However, more or less all the experiments and results are in the setting when the second hidden layer is in the linear regime. As a result, most of their findings are qualitatively very similar to that observed in Luo et al., 2021's work. Given their richer characterization of the phase diagram as compared to prior work, the authors nevertheless do not discuss these potential implications: e.g., it would have been understood to compare what happens in the condensed regime for both hidden layers. Further, quantitatively exploring the advantages/disadvantages of being in these different regimes on generalization/optimization could have been worthwhile too."
    },
    {
        "coherence": 0.13302173188598837,
        "consistency": 0.36844210608885747,
        "fluency": 0.13586036921318012,
        "relevance": 0.07855587662567592,
        "overall": 0.17897002095342546,
        "Generated": "141",
        "Gold": "This paper addresses a key problem in machine learning and central to the representation learning theme of the ICL. The motivation of using nonlinear transformation sounds not convincing. It is indeed known that traditional SW approximation generally requires a large amount of linear transformations. This paper proposes a graph-based algorithm for minimizing the pathwise energy to achieve the manifold walking to improve the interpolation in the latent space."
    },
    {
        "coherence": 0.19781937385381052,
        "consistency": 0.278121103240435,
        "fluency": 0.2971926990469593,
        "relevance": 0.10833373188416946,
        "overall": 0.22036672700634358,
        "Generated": "142",
        "Gold": "This paper presents a genetic algorithm framework for molecular optimization. It produces valid molecules through the crossover and mutation operations along with appropriate fitness functions. The main claim of the paper (benefit of two-stage procedure) is not supported by empirical results. The authors define chemically-relevant strategies for crossover/mutation, and show good results on constrained optimization of penalized logp. I'd expect a more thorough experimental evaluation."
    },
    {
        "coherence": 0.21423189296479775,
        "consistency": 0.4626700296661088,
        "fluency": 0.16172180454186527,
        "relevance": 0.03919022336541445,
        "overall": 0.21945348763454656,
        "Generated": "143",
        "Gold": "This paper proposes an effective approach for MinHash by permutating data vectors. It first randomly shuffles the data to break structures exhibited in the original data and then performs permutation K-times to obtain K hash values. Besides, this paper shows the theoretical background of the proposed approach is well described in the paper. Furthermore, it experimentally confirms that the experimental results follow the results of the theoretical analysis. Moreover, I am concerned about the experiment since the paper compares the proposed Approach to only the original approach."
    },
    {
        "coherence": 0.07306322177017857,
        "consistency": 0.162674509506828,
        "fluency": 0.14181989238735526,
        "relevance": 0.0317269585688522,
        "overall": 0.10232114555830352,
        "Generated": "144",
        "Gold": "This paper is poorly written, and looks like it was not proof-read. Presentation of the problem at hand is presented over so many times that it becomes confusing. Authors ought to better define the image description space of the objects and the haptic space. More interesting would have been a good explanation of the different sensors used in the anthropomorphic hand. The authors should have better defined the images in Figure 3 - you say that the image is as in Figure 3, but are you really giving the image of the object AND hand, or just the object itself (if so, you need to change the explanation). The authors propose a task of classifying objects from tactile signals but the proposed method is simple non-end-to-end predictors trained by supervised learning. Also, the performance seems to require a significant improvement."
    },
    {
        "coherence": 0.3495559345018851,
        "consistency": 0.3621968012762827,
        "fluency": 0.26580123623325386,
        "relevance": 0.13096793824041658,
        "overall": 0.27713047756295955,
        "Generated": "145",
        "Gold": "This paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research: normalize+scale and attribute normalization. It also demonstrates that these tricks are not enough w.r.t. normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue. A theoretical justification for normalization in model training on how it affects model performance and training time."
    },
    {
        "coherence": 0.22734787476729637,
        "consistency": 0.4320801624669358,
        "fluency": 0.24405964184055667,
        "relevance": 0.0793907728672985,
        "overall": 0.2457196129855218,
        "Generated": "146",
        "Gold": "I think the paper should be clear about it. Empirically, the method is comparable to FAL CON, but only in high-recall zone when the number of threads are large. FALCON is better than HNSW, it's a good baseline for nearest neighbor search. The authors developed a practical LSF approach called Falconn++ that combines the LSF property with the LSH to achieve a lower query time complexity beyond the vanilla LSH schemes such as Falconn."
    },
    {
        "coherence": 0.18660123592017147,
        "consistency": 0.2270822536305915,
        "fluency": 0.3377335171847828,
        "relevance": 0.08204308992624379,
        "overall": 0.20836502416544736,
        "Generated": "147",
        "Gold": "A model-based algorithm for stochastic game learning, Nash-VI, which achieves a regret bound of $widetilde O(sqrtH3SABT)$ after $T$ steps and a PAC-type sample complexity of $widthy mathcal O(H3SAB/epsilon2)$ for finding an optimal policy. Review ------- The authors introduce new algorithms to solve two-player zero-sum Markov games, as well as one-player Markov game in the reward-free setting. The approach is model based, based on successive episods of planning and counting for updating the model estimate. It involves solving a matrix game at each iteration, looking for a notion of equilibria that is computable in polynomial time. An extension to multi-player games is proposed for both reward and reward free setting."
    },
    {
        "coherence": 0.15910032354145492,
        "consistency": 0.16852409279481187,
        "fluency": 0.21638620756906815,
        "relevance": 0.041167661793004844,
        "overall": 0.14629457142458493,
        "Generated": "148",
        "Gold": "The loss minimisation in second-order learner for epistemic uncertainty quantification does not incentivise the learner to represent the uncertainty in a faithful way. n/a This paper provides a sounding argument for major approaches that claim to offer reasonable Epistemic estimates. The theoretical results are an important (albeit limited in scope) contribution to the literature. The authors support their arguments with theoretical results showing that predictors learned with loss minimization do not lead to meaningful estimates of episteic uncertainty. This paper is well-organized and easy to follow with many examples as a training dataset."
    },
    {
        "coherence": 0.2984522974609248,
        "consistency": 0.3908732360248079,
        "fluency": 0.2836033269899907,
        "relevance": 0.0889586998045045,
        "overall": 0.26547189007005695,
        "Generated": "149",
        "Gold": "A novel problem of characterizing the tail behavior of the random regret for algorithms that are designed to solve the multi-armed bandit problem. In particular, they show that the distribution of regret has a heavy tail in the large T regime for the standard policies, such as Upper Confidence bound (UCB), Successive Elimination (SE), and Thompson Sampling (TS). Subsequently, they propose a simple policy (algorithm) that has light-tailed risk together with minimax optimality for 2 armed bandits."
    },
    {
        "coherence": 0.3358051695466011,
        "consistency": 0.5193216421549534,
        "fluency": 0.32299958460995687,
        "relevance": 0.026834477048751405,
        "overall": 0.3012402183400657,
        "Generated": "150",
        "Gold": "A new 2D driving simulator with a focus on multi-agent coordination under partial observability (hence the name ‘Nocturne’) is published in the journal Advanced Micro Devices. It is a 2D-driving simulator designed for partially observed MARL research. It first reconstructs maps and replay objects' trajectories contained in real-world datasets, such as the Waymo Motion dataset. After that, traffic vehicles are turned into controllable agents with partial Observability, and actuated to arrive goal region according to a learnable policy. Nocturna is the only available simulator that can compute an agent’s visible objects and step the agent dynamics at above 2000+ steps-per-second"
    },
    {
        "coherence": 0.2980712805479526,
        "consistency": 0.41053176061931107,
        "fluency": 0.19961592577632709,
        "relevance": 0.13256694329425306,
        "overall": 0.26019647755946096,
        "Generated": "151",
        "Gold": "Review: This is a very interesting paper. It focuses on the issue of potential harms that can arise from the usage of modern large scale language models. It restricts their attention to autoregressive models, which a re typically used for language generation tasks. This by itself is already a valuable contribution, as it presents an up-to-date of relevant work. The authors were so nice to even reference works that were not included in the 'survey', so it is valuable collection of references. Characterizations of different benchmarks mapping have been done is not clear. No quantified analysis has been done on any language models mentioned in the paper."
    },
    {
        "coherence": 0.339693734628108,
        "consistency": 0.5042658209343777,
        "fluency": 0.31512667978668535,
        "relevance": 0.11867397929320339,
        "overall": 0.31944005366059364,
        "Generated": "152",
        "Gold": "Strong points 1. This paper introduces a searching framework of neural architectures ranking the candidates with two different metrics: the spectrum of NTK and the number of linear regions in the input space. These two metrics do not require the training of neural networks lightening the computational burdensome allowing to stack an equivalent number of cells for both the search and evaluation phase. 2. While no training has a benefit in the computational budget, the performance of their methodology is incremental improvements (or par) among various existing NAS algorithms. This work bridges between the advances in deep learning theory and the practice of NAS."
    },
    {
        "coherence": 0.24079570105132977,
        "consistency": 0.3881644015904063,
        "fluency": 0.25347012526020873,
        "relevance": 0.032071323824466105,
        "overall": 0.22862538793160272,
        "Generated": "153",
        "Gold": "This paper presents a modification to existing information theoretic feature selection algorithms which adds a strong relevance term estimated using a k-nn MI estimator. MRwMR-BUR is a new criterion for MI based feature selection. However, I think the paper could be improved from the following two aspects: 1. The authors have introduced the background of MI, OR, UR, and II. 2. The authors are suggested to improve the organization and presentation of the paper. The current version is not easy to follow. However the perspectives and methods are not novel. And there is an technical flaw in the analysis. [2]"
    },
    {
        "coherence": 0.21938797733526672,
        "consistency": 0.5098737600112488,
        "fluency": 0.24159829036559913,
        "relevance": 0.06252972914527422,
        "overall": 0.2583474392143472,
        "Generated": "154",
        "Gold": "This paper analyzes the phenomenon of memorization by Transformer language models in the light of model size, catastrophic forgetting, and memory of unique tokens such as part of speech. Larger models also tends to forget less, and they seem to memorize unique parts of speech such as nouns and numbers during training, as a function of model and dataset sizes, learning rate, and task (i.e. causal vs masked language modeling)."
    },
    {
        "coherence": 0.14481578416205818,
        "consistency": 0.25384591931955247,
        "fluency": 0.2443955705842212,
        "relevance": 0.10427492171847264,
        "overall": 0.18683304894607614,
        "Generated": "155",
        "Gold": "The authors present a method to improve the performance of graph embeddings by using PSL to reason using ontology axioms to predict the types of entities. TypeE-X's performance on a large number of datasets. A new method for identifying noisy triples in knowledge graphs that combines ontological information, using Probabilistic Soft Logic, with an embeddING method, such as ConvE or ComplEx. Knowledge-base refinement using a combination of two models."
    },
    {
        "coherence": 0.3964798021429546,
        "consistency": 0.40120323206002506,
        "fluency": 0.19823216686147047,
        "relevance": 0.10284320120614565,
        "overall": 0.27468960056764896,
        "Generated": "156",
        "Gold": "I think this paper merits attention, even if 'numerical errors can mess up neural networks' is a well-known fact. However, I think that should be explicitly explored. This paper raises an interesting problem in complete neural network verifiers about potential numerical roundoff errors in the verification, and in such cases the provided guarantees may be invalid. To show such a phenomenon, the authors propose to construct \"adversarial neural networks\" that can cause the complete verifier to produce imprecise results in floating point arithmetics and can thereby fool the verifier. They also showed it is possible to insert a backdoor to the network such that the backdoor is missed by the Verifier while it can trigger some behavior desired by the attacker. Although this paper has also discussed a possible defense, the writing appears inconsistent and unclear."
    },
    {
        "coherence": 0.49094595882163905,
        "consistency": 0.6609500905719112,
        "fluency": 0.27529008031466506,
        "relevance": 0.14252492704280156,
        "overall": 0.39242776418775427,
        "Generated": "157",
        "Gold": "This paper proposes a deep state space model for videos. The dynamics are defined by linear Hamiltonian Dynamics, and the motion matrix is further assumed to be block diagonal in order to separate different categories of actions. Like previous works, a latent variable z is introduced for explaining content and kept fixed for all frames. Experiments are carried out on Sprites and MUG to demonstrate the effectiveness. The authors propose a method called Halo that allows to disentangle the content from the motion in image sequences, in the VAE framework."
    },
    {
        "coherence": 0.4891431649500584,
        "consistency": 0.5979519734968639,
        "fluency": 0.1638215652454474,
        "relevance": 0.08379757202650615,
        "overall": 0.3336785689297189,
        "Generated": "158",
        "Gold": "Weaknesses: This paper introduces a new non-autoregressive generative modelling method for structured data and demonstrated its effectiveness on symbolic polyphonic music and character level and word-level text modelling. This paper proposes SUNMASK, which models discrete sequences without ordering assumptions. The authors emphasized the fine-grained control during the iterative generation which is very helpful for the readers to understand and potentially reproduce. However, this paper has several major shortcomings. Quantitatively, I think Bach Mock grading function is biased towards its own task, and it is less generaliziliby than BLEU score on the LM task"
    },
    {
        "coherence": 0.49253106813329944,
        "consistency": 0.7000034213551452,
        "fluency": 0.23329403185525294,
        "relevance": 0.08247959631910624,
        "overall": 0.377077029415701,
        "Generated": "159",
        "Gold": "A neural-network architecture that uses self-attention as a mechanism for data association to construct a memory of objects in the environment and demonstrate its effectiveness in a set of illustrative problems. The paper describes an entity-monitoring problem where the goal is to identify the distinct objects see in an episode where the agent/model moves through the scene/observes partial state. The key idea of OBM-Net to have fixed set of slots which allow tracking multiple hypothesis over time and use an attention mechanism to update the slots with observations over time DAF method, the paper is a good starting point. However, there are several issues with the paper in its current state. One of them is the apparent disconnect between the data association based description and motivation in the beginning, and the clustering focused experimental validation"
    },
    {
        "coherence": 0.3433512373706261,
        "consistency": 0.4370853249428781,
        "fluency": 0.31381503518959764,
        "relevance": 0.07298719187771897,
        "overall": 0.29180969734520523,
        "Generated": "160",
        "Gold": "The authors propose a method for evaluating the P(y,lambda|x) as a probabilistic graphical model, but it is unclear what the probabilistic graph looks like. This paper is similar to ensemble learning in that it proposes a way to evaluate a probability graph. It is not clear what it looks like, but the authors suggest that it can be used to evaluate an ensemble learning model."
    },
    {
        "coherence": 0.06481599485923228,
        "consistency": 0.14858363601418367,
        "fluency": 0.07722574060470432,
        "relevance": 0.02607485381446312,
        "overall": 0.07917505632314585,
        "Generated": "161",
        "Gold": "This is an interesting paper that discusses the negative sample mining strategy in unsupervised representation learning. The authors discuss the theory and method to conditionally select the negative samples based on the dot product of representations in noise constructive estimation (NCE). Their theory shows that the NCE with negative examples sampling from a conditional distribution q is lower bounded with mutual information, and the object has higher bias and lower variance. Eq. (1) assumes to use $k$ negative samples ($i k / i / (i - i)) (i + i = i + 1)"
    },
    {
        "coherence": 0.11192757785228213,
        "consistency": 0.2513389508280952,
        "fluency": 0.15306207873299826,
        "relevance": 0.09508568493764959,
        "overall": 0.1528535730877563,
        "Generated": "162",
        "Gold": "ODConv is a dynamic convolutional operation that combines two prior ideas, i.e. filter recalibration with attention in SENet and additive kernels in CondConv/DyConv, and also generalizes to all remaining dimensions. The technical part is easy to follow, and the experimental part is comprehensive. The results about the accuracy improvements look promising. Can the authors show further improvement upon architectures incorporating a SE-like module?"
    },
    {
        "coherence": 0.20964228633157553,
        "consistency": 0.3689870121262039,
        "fluency": 0.11294016228477972,
        "relevance": 0.01805491367451692,
        "overall": 0.17740609360426904,
        "Generated": "163",
        "Gold": "Weakness: This paper proposed a neural decoding method to process non-stationary data for brain-computer interface, called “EvoEnsemble”. The main advantage of the method is that the model can be adaptive to the variation of neural signals through evolution in the Bayesian framework. The authors mentioned the limitation of computational efficiency, but did not evaluate it. The paper proposes a method for adaptively updating neural decoders in Brain Machine Interfaces (BMI). Evidently the method consists in maintaining a population of decoderos, and evolving them according to a genetic algorithm, with fitness determined by the likelihood of the decoderons given the data."
    },
    {
        "coherence": 0.3379805076062132,
        "consistency": 0.43087140318854006,
        "fluency": 0.24065203097610272,
        "relevance": 0.13690245838684326,
        "overall": 0.2866016000394248,
        "Generated": "164",
        "Gold": "This paper presents a Lyapunov based analysis of the loss function in neural network training and derives a priori upper bound on the settling time of the training, which somewhat complements existing studies. The current analysis in Section 2 pertains to a single data point? How would having more data points affect the analysis and the results? In that case, what would be the yast? Or will be functions of the input? It seems to be an artifact used solely for the theorem. The loss function is likely not on the same scale; it is probably better to plot a \"normalized\" version so that the comparison indeed makes sense. The authors have analyzed the robustness to perturbations in Section 2.4. Specifically Eqn 22."
    },
    {
        "coherence": 0.13574418154378254,
        "consistency": 0.22789394235168067,
        "fluency": 0.1693082679081708,
        "relevance": 0.10194220915474794,
        "overall": 0.1587221502395955,
        "Generated": "165",
        "Gold": "This paper presents CloudBandit to solve the multi-cloud selection-configuration problem to avoid vendor lock in given a task. The main novelty of the paper lies in creating the dataset for the above problem. However, the current quality of work is not sufficient keeping in mind the standards of the conference. Also there are many similar approaches. The authors have run experiments against only 2 baselines viz. SMAC and RB."
    },
    {
        "coherence": 0.1319641701475416,
        "consistency": 0.21089684707803488,
        "fluency": 0.10967596151817272,
        "relevance": 0.1282967499005478,
        "overall": 0.14520843216107426,
        "Generated": "166",
        "Gold": "A new, more complex, dataset for the use of disentanglement representation learning in real-world settings. This work is an informative observation for the community which can be analyzed and built upon in future work. The negative observation in sim-to-real transfer is net positive for this work, as it is informative to the community in specifying a problem which hasn't been addressed in previous work. Overall Pros: This work provides a novel, interesting, and informative work."
    },
    {
        "coherence": 0.23571312828452096,
        "consistency": 0.39578966134914173,
        "fluency": 0.4324870505265651,
        "relevance": 0.11265299082318629,
        "overall": 0.29416070774585357,
        "Generated": "167",
        "Gold": "A new multi-modal visuo-tactile dataset is proposed, which is much more diverse than previous efforts, with wearable tactile sensors and egocentric camera, human-centric data involving diverse objects and scenes are collected. Extensive experiments have been done, and the results show that this dataset is effective on different downstream tasks. This paper proposes an interesting dataset that associates sight with touch. Compared to prior datasets that also contain tactile data, the new dataset is collected in natural environments with diverse objects"
    },
    {
        "coherence": 0.18225286793142803,
        "consistency": 0.3567138911005241,
        "fluency": 0.2138293073915403,
        "relevance": 0.0945214006935562,
        "overall": 0.21182936677926215,
        "Generated": "168",
        "Gold": "This paper proposes a new way to select a kernel (covariance function) for Gaussian processes. The authors borrow ideas from NN architecture search to propose a so called symbolical-optimal-transport kernel over the architecture of the kernel. This SOT kernel is then used for Bayesian optimization for model selection. This is fairly standard. A comparison to a Hellinger kernel-kernel is given. The computational advantages of this paper are well-explained."
    },
    {
        "coherence": 0.13532293046211313,
        "consistency": 0.1583727038900157,
        "fluency": 0.2785824958561386,
        "relevance": 0.17372170941589254,
        "overall": 0.18649995990604,
        "Generated": "169",
        "Gold": "This is a nice paper and I have no much complain about it. Particular strengths are * Principled characterization of discrete CTMC via the generator (transition rate) matrix * A well-thought way to represent the exponentially large rate matrix that is both flexible and computationally cheap. This paper develops a continuous time framework for discrete data. This framework includes optimum analysis, training, sampling and error analysis, which are nice"
    },
    {
        "coherence": 0.07580177401287824,
        "consistency": 0.14440905888158587,
        "fluency": 0.42209722031153624,
        "relevance": 0.062208673022098024,
        "overall": 0.1761291815570246,
        "Generated": "170",
        "Gold": "A bit-flipping white-box attack on neural network classifiers: a binary programmig problem formulated as an additive tradeoff between an effectiveness term and a stealthness loss on a batch of training examples. This paper proposes an ADMM based optimization method to conduct adversarial weight attack, and achieves superior performance compared with previous state-of-the-art methods on undefended models, and consistently outperforms previous methods on defended models."
    },
    {
        "coherence": 0.0975866198924468,
        "consistency": 0.2558196576181294,
        "fluency": 0.10393692814617464,
        "relevance": 0.09004518287450404,
        "overall": 0.1368470971328137,
        "Generated": "171",
        "Gold": "This paper is a comprehensive review of methods for detecting out-of-distribution samples. It distinguishes between three different cases of OOD samples: (1) detecting irrelevant inputs: (2) detecting novel classes: (3) detecting domain shift: that is, detecting samples that belong to a known class but stem from a different generating distribution due to image corruptions (a la Hendrycks and Dietterich, 2019; Hsu et al., 2010). The results are presented in an impressive number of experiments."
    },
    {
        "coherence": 0.3773682885377982,
        "consistency": 0.5237989551276261,
        "fluency": 0.40345185898422065,
        "relevance": 0.16906307730299658,
        "overall": 0.3684205449881604,
        "Generated": "172",
        "Gold": "The Karush-Kuhn-Tucker point of the constrained minimization problem tildemathcalL = frac1nsumn_i=1ell(x_i + delta_i(W), y_i) This paper describes the bias of adversarial training toward specific minimum-norm solutions or KKT points of a particular optimization problem. Their results generalizes the work of Li et al 2020 by proving the directional alignment with the adversary max-margin solution for deep linear models for L2 perturbations (Theorem 2) as well as convergence in direction for homogenous networks for L2, FGM, FGSM, and L2, Linf PGD perturbations."
    },
    {
        "coherence": 0.17974528673708837,
        "consistency": 0.17509704340692026,
        "fluency": 0.14701576606530314,
        "relevance": 0.026681472459718962,
        "overall": 0.13213489216725768,
        "Generated": "173",
        "Gold": "A simple linear mixture of the outputs produced by each component and then by showing that if the output of the components are linearly independent then you can find essentially a better ensemble. This paper studies composite neural network performance from function composition perspective. In the theorems 1, 2 and 3, the authors essentially prove that as the basis functions (pre trained components) increases (satisfying LIC condition), there are more vectors/objects can be represented by the basis. To me, this is a very straight forward result."
    },
    {
        "coherence": 0.19216108007257315,
        "consistency": 0.38022296317705867,
        "fluency": 0.2989637900056545,
        "relevance": 0.04688575924096095,
        "overall": 0.22955839812406184,
        "Generated": "174",
        "Gold": "A Sparse Low-Dimensional Decision \"SLDD-Model\" for Image Classification, Using Posthoc Feature Alignment with Human Interpretable Concepts. The method is interesting, the results are strong, and I am interested to see the method applied more broadly. My primary concern lies in the clarity of presentation for the method, and the lack of open-sourced code. This paper proposes a model that, for increased interpretability, has a very sparse last layer (5 neurons) which should make it feasible for a person to visually analyze each feature and understand its meaning."
    },
    {
        "coherence": 0.21264933897693336,
        "consistency": 0.37805224672891313,
        "fluency": 0.5293015351033722,
        "relevance": 0.09632863975944983,
        "overall": 0.30408294014216714,
        "Generated": "175",
        "Gold": "This paper proposes a more general variant of the predictive coding algorithm for learning deep neural networks. It is comparable to backpropagation in the classification of MNSIT and CIFAR-10 and in training a VAE on MNIST data (in terms of reconstruction loss on the test set). The authors address the computational complexity and scalability of the proposed model. The authors extend this to non-Gaussian densities for the internal and output units (and non-Dirac approximate posteriors), instead using the more general KL divergence formulation. This paper is coherent with the PC theory, as it enables the definition of layer-wise energy functions that represent prediction errors."
    },
    {
        "coherence": 0.24924692262242268,
        "consistency": 0.36268182428874585,
        "fluency": 0.4576648843878136,
        "relevance": 0.10206712140188824,
        "overall": 0.2929151881752176,
        "Generated": "176",
        "Gold": "This paper proposes a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse. Authors propose to augment GAN-based image restoration with another task specific branch such as classification tasks for further improvement. However, the novelty is limited and not well explained. While the results are better, the idea seems straightforward and has limited novelty."
    },
    {
        "coherence": 0.1107386592747193,
        "consistency": 0.3167591518222163,
        "fluency": 0.21565747569435958,
        "relevance": 0.059353929962615,
        "overall": 0.17562730418847755,
        "Generated": "177",
        "Gold": "This paper aims to improve the performance of the Nondeterministic Stack RNN (NS-RNN) using unnormalized positive weights instead of probabilities for stack actions and allowing the model to directly observe the state. The authors also use the new NS-NRN for a language modelling task on the Penn Treebank by introducing a memory-limiting technique. A renormalised NDN architecture for non-deterministic stack RNNs. A memory-limited version of the RNN-controller for nondeterministic PDAs."
    },
    {
        "coherence": 0.1482278032283003,
        "consistency": 0.26885672458246584,
        "fluency": 0.14153970357971762,
        "relevance": 0.060732913808412815,
        "overall": 0.15483928629972413,
        "Generated": "178",
        "Gold": "This paper proposes a new model for switching linear dynamical systems. The standard model and the proposed model are presented. Together with the inference procedure associated to the new model. This is exactly the methodological contribution of the manuscript. Experiments on three different tasks are reported, and qualitative and quantitative results (comparing with different state-of-the-art methods). The authors propose a structured variational approximation, but the factorization assumptions are not clear from the notation in section 4."
    },
    {
        "coherence": 0.27355667415581386,
        "consistency": 0.4047006689419226,
        "fluency": 0.4251702684356279,
        "relevance": 0.12675347662149142,
        "overall": 0.30754527203871396,
        "Generated": "179",
        "Gold": "This paper proposes to solve high-dimensional nonlinear parabolic partial differential equations by using an Actor-Critic neural network architecture. The performance benefits are well articulated although a bit repetitive. The paper lacks background and literature review, with a corresponding lack of references. The acronyms DBSDE should be defined. Otherwise it is difficult to understand the comparisons. This paper does not seem to articulate the addressed research gap and the benefit of the proposed method remained unclear in the broader context of learning-based PDE solvers."
    },
    {
        "coherence": 0.10142740127789693,
        "consistency": 0.1671648672886201,
        "fluency": 0.3099697570782018,
        "relevance": 0.057337938041424286,
        "overall": 0.15897499092153577,
        "Generated": "180",
        "Gold": "The authors of the paper argue that linear interpolations in the latent space have drawbacks because in typical modern use cases (Gaussian / uniform prior Pz) the aforementioned linear Interpolations are not distributed according to Pz anymore, and thus are likely to be out of the domain where decoder was actually trained. The key point is well-articulated and relevant mathematical details are derived in detail along the way. I have some concerns about the idea itself (see below); yet, while I disagree with some of the presented view points, I don't think that diminishes the contribution. I believe that the norms of the interpolants should be presented as well, such that to show if the desired property is true. I think the motivation is not very clear and should be improved."
    },
    {
        "coherence": 0.1045241948923454,
        "consistency": 0.27890051644650876,
        "fluency": 0.04210530096005431,
        "relevance": 0.03493574816011666,
        "overall": 0.11511644011475627,
        "Generated": "181",
        "Gold": "A semantic-guided masking strategy for predicting semantic parts from the image. Weakness: The whole pipeline of the proposed method seems a little bit complicated. It first learns to predict part segmentations from a pre-trained iBOT, then retrains an MAE from the scratch. Semantic-guide MAE training is an interesting idea, but it does not convice me that the reason is just to have a potential visual analogue of words. [Weaknesses]"
    },
    {
        "coherence": 0.16778578055731386,
        "consistency": 0.2581931713220705,
        "fluency": 0.08565996998174044,
        "relevance": 0.09259178819227883,
        "overall": 0.15105767751335092,
        "Generated": "182",
        "Gold": "Pros: 1. This paper is intuitively reasonable 2. It is not clear whether this paper would be a good fit for this conference. 3. The main metrics, zone generalisation and zone residuals, are defined based on different models. 4. The authors used a cutoff based upon simulations 5. This point is pushed to future research. 6. This paper proposes a quantitative framework to expand the inferences that can be done in the study of the causal effects of task stimuli on brain signals."
    },
    {
        "coherence": 0.43945291172113016,
        "consistency": 0.48351160009252026,
        "fluency": 0.33170385048844697,
        "relevance": 0.14852819786684648,
        "overall": 0.35079914004223595,
        "Generated": "183",
        "Gold": "[pros] This paper is basically clearly written. [Originality] Providing justification to the proximal operator approach is naive. [Quality] See [Detailed comments] section below. [Clarity] Despite all the theoretical arguments given to justify the proposal, the resulting proposal may simply be viewed as a naviive application of the pos. I am not convinced that the proposed training method is better than the standard SGD."
    },
    {
        "coherence": 0.2714740778098747,
        "consistency": 0.3878229061514138,
        "fluency": 0.14759248080743761,
        "relevance": 0.16647655356143926,
        "overall": 0.24334150458254133,
        "Generated": "184",
        "Gold": "A method that generates a \"unlearnable\" dataset to adversarial training with multiple perturbations, but has the following issues: Unrealistic assumptions. The authors assume that a data protector knows the hyperparameters (such as the loss function (l_1, l_2, or n_inf) and perturbation budget (rho_a)) of adversariaal training to be used by a user, which is unlikely to hold in practice. Although the author used a large perturbation buget to evaluate REM, the budget may differ across different applications for many reasons (e.g., due to different degradation in clean performance introduced by adversarian training). Also, recent studies (yuan and wu, \"Neural Tangent Generalization Attacks,\" 2021) have demonstrated that adversary training with a single type of perturbation cannot provide well defense against other Unlear"
    },
    {
        "coherence": 0.04679548208991433,
        "consistency": 0.11167438610457812,
        "fluency": 0.17653174356497364,
        "relevance": 0.12583549975010333,
        "overall": 0.11520927787739235,
        "Generated": "185",
        "Gold": "I think this work addresses a very important question for the neuroimaging community, as results of deep neural network-based segmentation have not been as impressive as in other fields and the success of 2.5D approaches is indeed somewhat counterintuitive. I would encourage the authors to extend this work (e.g. in a journal article) and to further work out the details and the exact mechanisms leading to better or worse performance of 3D vs2.5D approaches in neuroimage segmentation."
    },
    {
        "coherence": 0.28661486335616165,
        "consistency": 0.3821883975429025,
        "fluency": 0.24371604190183588,
        "relevance": 0.1735767573814495,
        "overall": 0.2715240150455874,
        "Generated": "186",
        "Gold": "This paper provides a solid basis for further work on RL. This paper investigates the regret analysis of a particular form of reward shaping where the shaping function is an approximation of the optimal state value function. It is remarkable that the authors managed to get rid of the impact of the horizon H. The two uses of shaping information presented in this paper lead to an interesting algorithm. Empirical evaluation on three domains is also provided."
    },
    {
        "coherence": 0.2965626662273278,
        "consistency": 0.3820815253598696,
        "fluency": 0.1431270191333373,
        "relevance": 0.04522745226528011,
        "overall": 0.21674966574645374,
        "Generated": "187",
        "Gold": "A nonstandard notion of tolerant testing of distributions over 1,-1n under the assumption that one not only has sample access but also ability to 1) evaluate the probability mass at any point in the domain (\"DUAL access\"), and 2) the ability to sample from the distribution conditioned on any subset of size two (PCOND access). This paper focuses on distributions on n-dimensional hypercubes. It leverages several oracles called COND, PCOND, and DUAL which allow the sampler to access about the true distribution other than point-wise probabilities. The empirical results show the implementation's number of samples for a wide array of problems."
    },
    {
        "coherence": 0.23520134510819596,
        "consistency": 0.3090450447658084,
        "fluency": 0.13433261867252264,
        "relevance": 0.10687913646910957,
        "overall": 0.19636453625390915,
        "Generated": "188",
        "Gold": "This paper is original and novel, and the authors propose and demonstrate a set of applications, ranging from image processing to a simple convolutional neural network. This reviewer likes the general idea of this paper. While this demonstrates that these operations can be *approximated* on INRs, the quality of the solutions is not very satisfying. As the authors did not address negative societal impacts nor the limitations of their contribution, this paper introduces a novel signal processing framework named INSP-Net that directly operates on IRNs without explicit decoding."
    },
    {
        "coherence": 0.18438395962009124,
        "consistency": 0.2896119735553547,
        "fluency": 0.12116353529658729,
        "relevance": 0.044563974296518684,
        "overall": 0.15993086069213797,
        "Generated": "189",
        "Gold": "A novel VIO method which estimates 5 DoF poes (with the elevation fixed) from monocular camera and IMU. In particular the proposed method adopt Unscented Kalman Filter (UKF) which is natually differentiable for the state estimation. With the aid of filtered roll and pitch using UKF, the front-view images can be converted to bird eye view images which are then used for pose estimation with Differentiable Phase Correlation. This way it retains the interpretability of the pipeline and also leverage the power of data-driven approach to learn the otherwise hand-tuned covariance matrices."
    },
    {
        "coherence": 0.2349704167776716,
        "consistency": 0.30219379811692787,
        "fluency": 0.14358855499240492,
        "relevance": 0.06707366538520662,
        "overall": 0.18695660881805276,
        "Generated": "190",
        "Gold": "The generalization error is bounded by the norm of embedding vectors, rather than the dimensionality constraints, and experimental results are provided to support their analysis. However, to the extent of my understanding of the paper, I have a number of concerns: 1. In a generalization bound like in Theorem 1, it is very typical of the Generalization Error to include a term that represents the complexity of the hypothesis function class. In practice, the positive training pairs are not restricted to 1-hop neighbors, but could also be 2 or more hops, in that case, it won't easily overfit the training data."
    },
    {
        "coherence": 0.19311691214385052,
        "consistency": 0.30811006124249074,
        "fluency": 0.06265204334543864,
        "relevance": 0.03815748769012566,
        "overall": 0.15050912610547637,
        "Generated": "191",
        "Gold": "This paper studies a novel problem, regularized inverse reinforcement learning. The authors propose several techniques to solve the regularization in different aspects. The experiments are convincing, but some cons are needed to revise. Regularized IRL is a nice contribution to the field. The mathematical foundation is rigorous and the experimental results are promising. I'm leaning towards acceptance because I enjoyed the paper and appreciate the technical contribution but I think the paper would be substantially stronger if it made a better case for why regularizers other than the normal Shannon entropy are relevant for practical IRL problems (perhaps even with an example application demonstrating a clear benefit over AIRL as well as other imitation learning baselines)."
    },
    {
        "coherence": 0.1745313062489351,
        "consistency": 0.18860762919131774,
        "fluency": 0.07966397736380489,
        "relevance": 0.025855316931566303,
        "overall": 0.117164557433906,
        "Generated": "192",
        "Gold": "I have read the authors reviews as well as the response by the authors. Key to these derivations is a lemma characterizing the probabilities of sampling This paper shows the arbitrary convergence rate of the stochastic proximal point algorithm(SPPA), under the convex assumption of the objective function. The authors use a novel approach to analyze SPPA. Numerical results show the efficiency of SPPA on some realistic data-sets. Major comments: In the numerical experiments, the authors use the default settings of SGD, Adam, etc, but select the parameter $lambda$ for SPPA...."
    },
    {
        "coherence": 0.1142841802811013,
        "consistency": 0.2439754842634631,
        "fluency": 0.0985378146890283,
        "relevance": 0.07720164618865126,
        "overall": 0.13349978135556098,
        "Generated": "193",
        "Gold": "A well-written, well-researched, and well-informed paper on interclass correlations in hyaluronic and osmotic imaging. It is a good introduction to the subject of interclass relationships. The problem being solved is not particularly common or clinically relevant. - The paper claims a very general methodological development, which should incorporate in-depth analysis of the performance and hyperparameter choices. However, I am missing a discussion of the results. This paper addresses a common challenge faced when training models with labels which are coarse and could benefit from domain knowledge to more correctly define them."
    },
    {
        "coherence": 0.15804172649359569,
        "consistency": 0.2087000583724117,
        "fluency": 0.10069103069757744,
        "relevance": 0.1357938807490542,
        "overall": 0.15080667407815976,
        "Generated": "194",
        "Gold": "A spectral clustering pipeline that achieves higher accuracy than a number of sota methods which were thought to be miles away better in performance. This work demonstrates that with the right pre-processing steps in place (like the radial projection onto the unit sphere), spektral methods could be brought back to the comparison board. I feel that there is only a limited understanding about what are the properties of the datasets that lead to these decisions/performance differences. I.e., why on CORA does the \"Conic\" perform poorly without \"+CS\"? why on \"arXiv\" outperforms all other methods? Weaknesses: The main thrust of the paper is an important point for researchers in this area: simple, but well structured and theoretically motivated graph models can perform surprisingly well against larger, more complex models. However, the experiments lack variety in the applicative domain; for"
    },
    {
        "coherence": 0.14810045595077415,
        "consistency": 0.24614209161172565,
        "fluency": 0.10125543597617465,
        "relevance": 0.21195693476884103,
        "overall": 0.17686372957687885,
        "Generated": "195",
        "Gold": "This paper introduces a neural network framework modulating the excitation functions within the self and mutually exciting count processes. However, the authors missed an extremely important influential paper by Feng Chen and Peter Hall (2013): https://www.cambridge.org/core/journals/journal-of-applied-probability/article/inference-for-a-nonstationary-selfexciting-point-process-with-an-application-in-ultrahigh-frequency-financial-data-modeling/99E7C55841B746DBD6CE20D657B2CC9D"
    },
    {
        "coherence": 0.29100373776057314,
        "consistency": 0.420742612153068,
        "fluency": 0.2753854985581076,
        "relevance": 0.09680488853319551,
        "overall": 0.2709841842512361,
        "Generated": "196",
        "Gold": "I think that the idea is nice and was very impressed by the numerical results reported by the authors. However, it took me a while to just understand the problem and the setting considered by the author. I advise the authors to make a in-depth revision of their paper, introducing more carefully their method so it can be understand by a broader audience. The paper introduces a novel objective function by imposing geometric constraints on the logits of uncertain samples. To overcome the dilemma of selecting the ball radius, the authors propose a surrogate objective, namely Wasserstein Normalization. An SDE grad flow is proposed for solving the Wasserstein normalization. Despite an apparently rigorous mathematical framework, there are a lot of vague assertions (see minor remarks) and arbitrary choices (two examples: certain and uncertain distributions are looked upon in the pre-softmax logins, why ? Is the continuity equation of proposition 7 the"
    },
    {
        "coherence": 0.16970138752115074,
        "consistency": 0.17188638657105101,
        "fluency": 0.31122331486420973,
        "relevance": 0.0807087554749831,
        "overall": 0.18337996110784865,
        "Generated": "197",
        "Gold": "This paper proposes an approach to model social influence in a scenario-independent manner by instantiating the concept of intrinsic motivation and combining various human abilities as part of a reinforcement learning function in order to improve the agent's operation in social dilemma scenarios. It also proposes a new intrinsic reward for MARL, representing the causal influence of an agent’s action on another agent counter-factually. This is good, however, if we’re thinking about situations where agents aren’t trained together and have their own rewards (the authors’ example: “autonomous vehicles are likely to be produced by a wide variety of organizations and institutions with mixed motivations”) then won’t these agents be exploited by rational agents?"
    },
    {
        "coherence": 0.17240956924155074,
        "consistency": 0.25197609457549003,
        "fluency": 0.23750613528719164,
        "relevance": 0.08070754998666883,
        "overall": 0.18564983727272533,
        "Generated": "198",
        "Gold": "This paper developed methods for resampling from the hindsight experience replay buffer. It was applied to DDPG, and evaluated using a set of four robot control problems. Results show that HCG performed better than a few baseline methods, and its performance was claimed to be insensitive to the choice of hyper-parameters. The reviewer is not sure that this strategy makes sense when applied to off-policy RL methods (DDPG in this case)."
    },
    {
        "coherence": 0.29114495368582033,
        "consistency": 0.3788317610876854,
        "fluency": 0.19828846132840916,
        "relevance": 0.21531010014025773,
        "overall": 0.27089381906054316,
        "Generated": "199",
        "Gold": "This is an interesting idea that possibly deserves to be disseminated to stimulate further progress in the search for bio-plausible back-prop. They incorporate a cool idea to convert a differential to a contour integral in the complex plane and then interpret this integral as a time integral of an oscillating teaching signal. They also show that allowing for finite beta enables robustness to noise, unlike EP which is only vaild in the lim $beta to 0$. The authors introduce a new variant of equilibrium propagation that relies on complex analysis and holomorphic functions, and derive an online unbiased version of the corresponding learning rule. It scales to large machine learning tasks such as ImageNet."
    },
    {
        "coherence": 0.2933393918022073,
        "consistency": 0.35407383167501083,
        "fluency": 0.143868050978976,
        "relevance": 0.10042949680333726,
        "overall": 0.22292769281488284,
        "Generated": "200",
        "Gold": "I find this paper interesting and overall well written. The setting of the paper is rather strange. Unlike the traditional online learning setting, there is no immediate feedback to the allocator on the quality of the allocation at each round. Hence, I am not sure about the applicability of the results in any interesting domains. The proposed algorithm is rather simple and natural. It mostly relies on the fact that the players are homogeneous, ie have the same valuation distribution. As a main weakness, the proposed algorithm crucially relies upon [...]"
    },
    {
        "coherence": 0.1328715324794839,
        "consistency": 0.2970238232190046,
        "fluency": 0.08115323285291166,
        "relevance": 0.046676197286436974,
        "overall": 0.1394311964594593,
        "Generated": "201",
        "Gold": "This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. The idea of using teacher model’s classifier to directly reshape the student modeling’s feature representation is somewhat novel. It considers the situation of single teacher model and multiple teacher models."
    },
    {
        "coherence": 0.1025080434457235,
        "consistency": 0.11548850101782199,
        "fluency": 0.06413782303069109,
        "relevance": 0.09565100823235552,
        "overall": 0.09444634393164802,
        "Generated": "202",
        "Gold": "I would strongly recommend revising this part and making it easier to follow and more self-contained. I need to read the original paper in the reference (Rebuffi et al. 2020) to get a more comprehensive understanding of the approach. However, the mathematical description of the methods is difficult to follow. The author’s claim that “this work is the first that uses NormGrad in the medical imaging context” does not hold true. This is a well-written, well-researched, and well-informed review of normGrad for chest x-Rays."
    },
    {
        "coherence": 0.11087917085647289,
        "consistency": 0.2016872356616098,
        "fluency": 0.08600866443700077,
        "relevance": 0.09454386963279782,
        "overall": 0.12327973514697031,
        "Generated": "203",
        "Gold": "Pros: This paper proposes an upper bound of error rate on novel classes in few-shot learning. It derives that the upper bound is decided by feature separability and classifier discrepancy. It seems that only the first two terms are optimized, which makes the entire method not persuasive enough. A new way to train Few-Shot embeddings is proposed. The authors find that the bottleneck is not the separableness of the novel classes, but the dispersion between classifiers trained on large datasets."
    },
    {
        "coherence": 0.20604881637401656,
        "consistency": 0.32482214251547153,
        "fluency": 0.12723152797622672,
        "relevance": 0.09971381326944484,
        "overall": 0.18945407503378991,
        "Generated": "204",
        "Gold": "This paper proposes a trojan reverse engineering method by exploiting the feature space input constraint. This is inspired by the authors’ intuition that the trojan and benign features will not affect each other. The paper then validates this intuition by analyzing and visualizing the features space differences between trojan vs benign features. The key observation that ‘neuron activation values representing the Trojan behaviors are orthogonal to others’ sounds both interesting and reasonable."
    },
    {
        "coherence": 0.258029895271507,
        "consistency": 0.43438407053145117,
        "fluency": 0.05785281957762582,
        "relevance": 0.09270058543829313,
        "overall": 0.2107418427047193,
        "Generated": "205",
        "Gold": "### Review ### Summary I am currently leaning towards recommending reject for this paper. While the approach and algorithm are novel, they also appear to be highly complex, don't provide a noticeable or consistent improvement over much simpler benchmarks, and I fear that the improvements that _are_ seen are likely due to variance in the results or are hidden behind the additional machinery in the algorithm. This paper studies an important area in hierarchical RL, which improves data efficiency by incorporating abstractions. In this paper, the authors propose an efficient option learning algorithm, which utilizes a TD(0) type objective and restricts the learned policy being not too far away from the past policy. In terms of different abstractions, the paper studies action abstraction through a mixture policy, and temporal abstraction through explicitly limiting the maximum number of switches between options. The method is well-motivated in general, however, I feel the notation in Al"
    },
    {
        "coherence": 0.20793224948677422,
        "consistency": 0.34481276690480284,
        "fluency": 0.08344748565438873,
        "relevance": 0.0787132555583532,
        "overall": 0.17872643940107974,
        "Generated": "206",
        "Gold": "This paper studies the prediction problem on tabular datasets and proposed a differentiable multi-layer fern-based architecture. The selling point is that the approach is computationally efficient on CPUs and can learn sparse representation. The prediction performance is on-par with state-of-the-art methods. [1] differential-tree architecture (NODE) for tabular data that has lower theoretical computational complexity to be suited for lower-end CPU devices. They show that their performance is somewhat comparable to other commonly used methods like CatBoost, TabNet and NODE. [...]"
    },
    {
        "coherence": 0.17895408350484332,
        "consistency": 0.3723301926736777,
        "fluency": 0.07987865002545479,
        "relevance": 0.062308720479015735,
        "overall": 0.1733679116707479,
        "Generated": "207",
        "Gold": "This paper does not do a good job in explaining why some of the sampling methods work better than others and in some cases, the explanations are trivial. This paper investigates the effect of task diversity in the training process of meta-learning. The findings indicate that increasing task diversity during the meta-training process does not boost performance. They evaluate the performance on four few-shot image classification tasks. Though the authors investigate several samplers, they only conduct the experiments on N-way K-shot few - shot image classification task."
    },
    {
        "coherence": 0.31236024066129703,
        "consistency": 0.3272172198043295,
        "fluency": 0.09321734097412997,
        "relevance": 0.12042920458058212,
        "overall": 0.21330600150508466,
        "Generated": "208",
        "Gold": "This paper proposes to employ a CVAE structure to generate pseudo OOD samples by providing some synthetic conditional information. Besides, they design a two-stage framework to train an OOD detection model by leveraging the generated pseudo EOD data. They show that their approach outperforms other SOTA methods in the task of out-of-distribution detection. The hyper-parameter ablations are quite thorough - and seem to cover the hyperparameters that I find interesting. The choice of CVAE does not seem to have been seriously ablated. There are many CVAEs possible as well as GAN-based and more recent models. This comparison could have significantly helped the significance of this paper."
    },
    {
        "coherence": 0.2189751401521103,
        "consistency": 0.2193272525170757,
        "fluency": 0.18458054094136225,
        "relevance": 0.03781494585707525,
        "overall": 0.16517446986690587,
        "Generated": "209",
        "Gold": "This paper proposes a method BAM for Bayesian learning in non-stationary environments: basically, at each time step, each previous datum may or may not be incorporated into the new posterior, so that old data from different states can be ignored. The method does not rely on parametric assumptions. Experiments demonstrate it to work well in various scenarios. In this paper, the authors propose a new framework, Bayes Augmented with Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. This method is evaluated on various analytical experiments. The main contribution is BAM, an algorithm that constructs an informative posterior distribution for actual observations Based on stored prior data."
    },
    {
        "coherence": 0.15905816429925598,
        "consistency": 0.170788578399459,
        "fluency": 0.06018835115816132,
        "relevance": 0.10698471275277599,
        "overall": 0.12425495165241307,
        "Generated": "210",
        "Gold": "I recommend a clear rejection of this paper. I think the key selling point and hypothesis behind this paper is not well supported. Experimental settings are also weak and there are insufficient convincing experiments to feel that this architecture is doing something useful. Transformer and PA-Transformer has 0.01K FLOPS. Could you explain the difference between the two? According to Table 1, Transformer and PAT-transformer have 0.01k (if not identical) FLOPs"
    },
    {
        "coherence": 0.14908222644884989,
        "consistency": 0.3092015244662322,
        "fluency": 0.05085637162428369,
        "relevance": 0.06612524653792634,
        "overall": 0.14381634226932305,
        "Generated": "211",
        "Gold": "## Negative - This paper extends RML Mapper with access to Excel data. It represents a solid piece of Java engineering work, although the chosen semantic representation approaches are not optimal. RML Extensions for Excel Sheets - a paper between a system/demo paper and a short research paper. It does not give many insights except for introducing those extensions, nor helps the reader understand which technical hurdles were taken. KG creation tool, not a tool for creating KGs, but a way to create a KG in Excel. This is an interesting contribution for a workshop."
    },
    {
        "coherence": 0.23534863826238622,
        "consistency": 0.46858938543864725,
        "fluency": 0.06494779103778807,
        "relevance": 0.035513827676265035,
        "overall": 0.20109991060377164,
        "Generated": "212",
        "Gold": "This paper proposes a new family of monotone deep Boltzmann machines where the pairwise potentials satisfy a monotonicity condition, giving rise to efficient mean-field iteration with provable convergence guarantees. The convergence is obtained by drawing connections with monotone Deep equilibrium models. Small-scale experiments are done as proof of concept. This paper is very well-written and easy to read. However, the novelty aspect of the work to be a bit lacking:"
    },
    {
        "coherence": 0.17586457777504383,
        "consistency": 0.3093038901708494,
        "fluency": 0.09449956935218051,
        "relevance": 0.04322602248005076,
        "overall": 0.15572351494453116,
        "Generated": "213",
        "Gold": "This paper introduces First Hitting Diffusion Models (FHDM), which differs from diffusion models based on infinite time (e.g., Langevin dynamics) or fixed time diffusion processes (i.e., DDPM); hence it can be used for data from discrete and structured domain by utilizing the exit distribution for learning and inference. The authors also introduce a new (to the best of my knowledge) data generation methodology, suitable for arbitrary domains. Section 2 introduces the relevant background for first hitting diffusion processes, providing some simple examples like hitting of the sphere or the hyperbox."
    },
    {
        "coherence": 0.2038311749040883,
        "consistency": 0.22529145366872255,
        "fluency": 0.08863161660376438,
        "relevance": 0.04620617121055873,
        "overall": 0.14099010409678348,
        "Generated": "214",
        "Gold": "This paper proposes a reactive approach to reducing slip by modifying the trajectory rather than the gripping forces. The results show that a proactive approach leads to less overall slip, it would be interesting to discuss if there is ever a case where RSC would be preferable to PSC. Learning a predictive model for preventing slip detection and avoidance in manipulation tasks. They implement two solutions: one a purely reactive method which uses an LSTM to detect slip events triggering the controller to slow it's velocity. The second implementation is also an SLTM but takes in both past data from the tactile sensor and future planned actions from the controller."
    },
    {
        "coherence": 0.07864179260176632,
        "consistency": 0.21402941088638758,
        "fluency": 0.15751015366097376,
        "relevance": 0.021460440737774603,
        "overall": 0.11791044947172556,
        "Generated": "215",
        "Gold": "Is it possible to make the plots in Fig. 5 2D? No, it's a bit hard to parse. The authors propose AlignKGC, a system for jointly learning Align KGC models can be applied to a wide range of knowledge graphs. However, it remains unclear whether some of the improvements brought by different model components can be attributed to random variation vs actually from more seed entity/relation pairs. EA % is used when evaluating knowledge graph completion, and only 3 % of EA is used for evaluating relation embeddings"
    },
    {
        "coherence": 0.17765322189983848,
        "consistency": 0.27761543267383415,
        "fluency": 0.12379246361004935,
        "relevance": 0.057582706491704536,
        "overall": 0.15916095616885662,
        "Generated": "216",
        "Gold": "summary: --- **Summary of Contributions** This paper analyzes the model of Random features in GNNs as suggested by Sato et.al., in the paper called RNI. A result proving the universality of the RNI framework is introduced, a first of its kind in low tensor degree GNN's. To evaluate the expressiveness of RNI and other more expressive GNN 's, the authors design two datasets wich require 2-WL distinguishing power (which is higher than the ones MPNNs have) **Strengths** - Novelty - The universality result on RNI is novel and further extends the hints of improved expressiveness explored By Sato Et. al. - Dataset design - the design of new datasets for expressiveness discrimination are an important contribution to the community. **Weaknesses** The Random"
    },
    {
        "coherence": 0.30976634503483796,
        "consistency": 0.37554941217364185,
        "fluency": 0.10461074739528946,
        "relevance": 0.12052410293257329,
        "overall": 0.22761265188408564,
        "Generated": "217",
        "Gold": "This paper presents a nice step back from much of the single unit interpretability literature, highlighting that it is unclear what some of these results mean if the selective units are actually necessary for the task performance. The experimental results (other than the optimization concerns below) seem sound, particularly including the multiple replicates + error bars to verify the experimental findings. Concerns and Suggestions: Class selectivity to interpret CNNs is not an essential component of good performance, in the sense that networks can perform well without showing class selectivity. However, this fact does not demonstrate that class selectiveivity is unimportant to a network trained in the normal way without regularization. It is of course possible that networks trained with different losses could learn different strategies for solving the same task."
    },
    {
        "coherence": 0.22451988564552194,
        "consistency": 0.36686165263227477,
        "fluency": 0.2039803563100307,
        "relevance": 0.04934797075914836,
        "overall": 0.21117746633674392,
        "Generated": "218",
        "Gold": "This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. It is a shortcoming in existing batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. # Quality The authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(2T) memory complexity. Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a one larger inner product."
    },
    {
        "coherence": 0.23751804396040674,
        "consistency": 0.3398314879173957,
        "fluency": 0.1812280496821821,
        "relevance": 0.049909029047653236,
        "overall": 0.20212165265190946,
        "Generated": "219",
        "Gold": "This paper describes a few-shot learning approach that takes into account the semantic relatedness of the ground truth labels rather than just using them as binary labels. This way both the support and query samples would have semantic features and empirically it has shown to work on many few - shot benchmarks on computer vision datasets. The authors propose to build a model that predicts both the image classes and the text label (or its Glove embedding) to improve few- shot image classification. The experimental results look convincing. However, the proposed solution looks overly complicated. A simple baseline should be considered."
    },
    {
        "coherence": 0.07431651727019088,
        "consistency": 0.16803648941171323,
        "fluency": 0.32813080222748314,
        "relevance": 0.024685633503102027,
        "overall": 0.14879236060312234,
        "Generated": "220",
        "Gold": "XmixUp is a cross-domain mixup method for deep transfer learning, it is not the same as Seq-Train. It is based on a multi-task learning set up instead of a two phase approach. The main idea of this paper is to compute the one-to-one mapping from source dataset classes to the target dataset classes. I think this idea of having one to one mapping is flawed in principle. For example, let us consider that the source dataset consists of 100 classes which represent different dog breeds. The target dataset has 10 classes representing different cars."
    },
    {
        "coherence": 0.06674074199081047,
        "consistency": 0.12094787291454642,
        "fluency": 0.28238180632792587,
        "relevance": 0.06394850473556087,
        "overall": 0.1335047314922109,
        "Generated": "221",
        "Gold": "A novel connection between adversarial attack on graph neural networks in a restricted black-box setup via node feature perturbation, and the influence maximization problem under the linear threshold model on the same graph. The authors make simplifying assumptions about the GCN model (all ReLU paths are equally likely) and the distribution of the individual nodes' thresholds, theta_j. The proposed attack outperforms the centrality-based baselines and the baselines of [Ma et al. 2020]"
    },
    {
        "coherence": 0.155926526308803,
        "consistency": 0.3187338648727997,
        "fluency": 0.1082233596091179,
        "relevance": 0.149265296590434,
        "overall": 0.18303726184528865,
        "Generated": "222",
        "Gold": "Weaknesses No mention of QPLEX in this paper at all (except for a reference related to over-generalisaiton). At the very least it should be in the related works section. What is $ki(s)$? Did you mean to have the expectation be over $pii$ for the first line? Are you missing a summation over all possible joint actions in the 3rd line? Why expand the expectation going from line 2 to 3, when you just bring it back in line 5? How do you get to the last line from the penultimate one?"
    },
    {
        "coherence": 0.060069466226239564,
        "consistency": 0.0926194687091333,
        "fluency": 0.21980450846800503,
        "relevance": 0.051006329699957144,
        "overall": 0.10587494327583376,
        "Generated": "223",
        "Gold": "I really enjoyed reading this paper. It has a very clear direction from the beginning with good experiments to back it up. I believe long-tailed classification is an interesting problem with clear real-world applications, so studying it in-depth is necessary for the community. Overall, I don't see any major drawbacks or shortcomings as the experiments and ablations combined with the analysis are solid as well. This work shows the effectiveness of intra-class compactness and inter-class separation, as well as the influence of data mixup on long tailed representation learning"
    },
    {
        "coherence": 0.22357173792871765,
        "consistency": 0.36583261180267984,
        "fluency": 0.1031818333371518,
        "relevance": 0.1526728170301196,
        "overall": 0.2113147500246672,
        "Generated": "224",
        "Gold": "This paper provides an interesting explanation of the condensation mechanism of neural networks at the initial training stage. Condensation can be viewed as a feature learning process, where the input weights condense on isolated orientations during training. This mechanism may provide a plausible explanation for the performance of the wide network. In particular, the paper shows empirically that the maximal number of condensed orientations is twice the multiplicity of the activation function used. Moreover, using polynomial approximations, a paper provides theoretical support in two cases: when the active function is of multiplicity one and when the input is one-dimensional. not fully convincing and unclear experimental analysis - low readability due to several typos in the manuscript - unclear improvement wrt the work in Luo et al, 2021"
    },
    {
        "coherence": 0.11236170543200764,
        "consistency": 0.25426260867937706,
        "fluency": 0.06194939862039045,
        "relevance": 0.1936292963792667,
        "overall": 0.15555075227776047,
        "Generated": "225",
        "Gold": "This paper presents state of the art results in kNN (retrieval), image classification with linear probing and fine-tuning (ImageNet), image detection, instance segmentation (both on COCO), and semantic segmentation. This paper proposes a new strategy for self-supervised training of vision transformer models by combining the self-distillation-based method DINO with mask image modeling loss. The momentum teacher model in DINO is taken as an online tokenizer. A mask Image Modeling Loss that computes Self-Disstillation Loss between the reconstructed tokens and the output tokens of the momentum teacher is proposed. Extensive experimental results and visualization are given to show the effectiveness of the proposed self supervised training framework. The main difference between DINO and proposed iBoT is 1) the masked image as input for the student model; 2) the MIM loss for self"
    },
    {
        "coherence": 0.09307173337762079,
        "consistency": 0.25528171034199443,
        "fluency": 0.36102011101807696,
        "relevance": 0.18408267243548504,
        "overall": 0.2233640567932943,
        "Generated": "226",
        "Gold": "This paper is a generalization of Chen et al. (2021), where the sample size has been exponentially improved, while the time needed to find such a sample worsened. The time in the paper was shown to be exponential in the number of hidden images, due to a reduction to solving an instance of a regression problem where the signs of the entries are unknown or hidden. This paper summarizes the paper's reviews."
    },
    {
        "coherence": 0.12362316470048546,
        "consistency": 0.15948167858027318,
        "fluency": 0.48872442849061226,
        "relevance": 0.03532237975793369,
        "overall": 0.20178791288232614,
        "Generated": "227",
        "Gold": "I found (4) onward (through the end of pg. 5) very confusing and didn't really follow (4). I also found (5) onward very confusing, and wasn't actually following (4) - I'm not sure if it's a good idea or not, but I'll give it a try if I can find a way to get a sense of what's going on here."
    },
    {
        "coherence": 0.2776237941742698,
        "consistency": 0.4379736030872446,
        "fluency": 0.3823579383918707,
        "relevance": 0.07648804608102709,
        "overall": 0.29361084543360305,
        "Generated": "228",
        "Gold": "This paper introduces a method called MemUP to help recurrent neural networks learn long-term dependencies better. Specifically, the authors leverage a memory model to predict future outcomes with high uncertainty. By skipping all states with lower uncertainty, the training process takes less computational cost in backpropagation. The authors show experimental results on both supervised learning tasks and reinforcement learning tasks with improvement comparing with baselines. The methods are simple, and the intuition behind the methods all make sense to me. The paper is well-written, with details clearly formulated and illustrated."
    },
    {
        "coherence": 0.2500659699060399,
        "consistency": 0.3938530422436038,
        "fluency": 0.2911584673940491,
        "relevance": 0.10671499771409054,
        "overall": 0.2604481193144458,
        "Generated": "229",
        "Gold": "A new multi-modal visuo-tactile dataset is proposed, which is much more diverse than previous efforts, with wearable tactile sensors and egocentric camera, human-centric data involving diverse objects and scenes are collected. Extensive experiments have been done, and the results show that this dataset is effective on different downstream tasks. This paper proposes an interesting dataset that associates sight with touch. Compared to prior datasets that also contain tactile data, the new dataset is collected in natural environments with diverse objects"
    },
    {
        "coherence": 0.11977410722533484,
        "consistency": 0.18574145828682495,
        "fluency": 0.3168019479576048,
        "relevance": 0.05990310295473391,
        "overall": 0.17055515410612462,
        "Generated": "230",
        "Gold": "[2] [3] [4] [5] \"On The Measure of Intelligence.\" arXiv: preprint. [1] Chollet, François. \"On the Measure of Integration.\" [2]. [3]. [4]. [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [36] [37] [38] [39] [41] [43] [44] [46] [47] [51] [56] [61] [62] [65] [66] [67] [68] [69] [70"
    },
    {
        "coherence": 0.2280266145964227,
        "consistency": 0.36532509354654114,
        "fluency": 0.1178478138306183,
        "relevance": 0.0649602594842096,
        "overall": 0.1940399453644479,
        "Generated": "231",
        "Gold": "Recommender Transformer with a Pathway Attention Mechanism for Predicting User Action Sequences with High Accuracy and Efficiency Compared with Other Self-attention or Transformer based Sequential Recommendation Methods through Experiments. - (Originality) - It is interesting to introduce behavior pathways in long user sequences Besides, this paper utilizes a router to choose item patterns selectively. However, it has several weak points, especially in evaluating the proposed model."
    },
    {
        "coherence": 0.18837369425461226,
        "consistency": 0.2904023030178724,
        "fluency": 0.07826929296366082,
        "relevance": 0.04718683691687918,
        "overall": 0.15105803178825616,
        "Generated": "232",
        "Gold": "I think this is a nice idea and a good application of it. It is relevant for ICLR, and the idea is novel (specifically, as far as I am aware, no one has previously used results such as Theorem 1 and Feature Encoding Layers). The authors introduce a modification in the direction of gradient update of normal SGD that makes sure to minimize forgetting. Next the authors propose a virtual feature encoding layer (FEL) that applies task specific random permutation of features among different tasks without expanding the memory footprint. A continuous learning approach based on recursive gradient optimization. A projection matrix is computed incrementally and updated by integrating one of the aforementioned components. This paper provides a solid theoretical derivation and justification for it, which will be of interest to the continuous learning community."
    },
    {
        "coherence": 0.2523251038495721,
        "consistency": 0.48341045203169475,
        "fluency": 0.09603954769683236,
        "relevance": 0.056359847712519014,
        "overall": 0.22203373782265454,
        "Generated": "233",
        "Gold": "This paper addresses the out-of-distribution problem for node-level prediction on graphs from the invariance perspective. It presents a novel learning approach in which the whole graph is divided into n ego-graphs where n is the number of nodes. All the ego - graphs can be treated as a set of IIDs, the model can be trained to defend the adversarial attack from multiple environments."
    },
    {
        "coherence": 0.20937809294887605,
        "consistency": 0.4766902743042256,
        "fluency": 0.18211185059990773,
        "relevance": 0.05606505628403233,
        "overall": 0.23106131853426043,
        "Generated": "234",
        "Gold": "This paper presents a way to integrate a translation memory into a neural machine translation model. They introduce a novel method for finding matching sentences (which they do not properly describe) and they also have 4 methods from including this information into an NMT model, with the best approach using multiple sources combined with gated attention and a pointer network for copying rare words. How do they do this? At no point to they describe where they get the translation memory from and/or how big it is. This is a fundamental part of the model."
    },
    {
        "coherence": 0.23947300488437892,
        "consistency": 0.36170326779410145,
        "fluency": 0.2010545160842224,
        "relevance": 0.16416242547724352,
        "overall": 0.24159830355998657,
        "Generated": "235",
        "Gold": "This paper presents a new loss function for survival analysis based on proper scoring functions to less then penalty wrong predictions that are confident make under the log-loss. I would have liked to see an evaluation on more datasets, especially since the data in Appendix H indicate that the proposed approach is only marginally better than MLE-based model fitting. Furthermore, in addition to the metrics presented, conventional metrics such as the C-statistic would have been interesting. I further miss a discussion of alternative approaches to achieve well calibrated scores, especially posthoc calibration using the validation set as discussed in Guo et al, ICML 2017. Related work is incomplete, for example the use of tensor-trains in RNNs to model EHR data (Yang & Yang), Machine Learning for Healthcare Conference 2017 The Survival-AUPRC is not sufficiently motivated or an intuition of why it should be used/preferred, it seems"
    },
    {
        "coherence": 0.08597901832643044,
        "consistency": 0.16392240526835244,
        "fluency": 0.36918105091280745,
        "relevance": 0.15020206951694456,
        "overall": 0.19232113600613374,
        "Generated": "236",
        "Gold": "A review of two large-scale datasets to understand the human emotions after watching a video: Video Cognitive Empathy (VCE) and Video to Valence (V2V). The authors train machine learning models on the datasets and demonstrate that some models reach good performance, but there is room left for further improvement. The authors also provide a review of a large dataset of crowdworkers' emotional content and general emotional valence."
    },
    {
        "coherence": 0.0965522851562986,
        "consistency": 0.2418443190707488,
        "fluency": 0.2640949501834837,
        "relevance": 0.04909239932205056,
        "overall": 0.16289598843314543,
        "Generated": "237",
        "Gold": "A non-parametric form for the log-likelihood of the intensity function of a multi-dimensional Poisson process, involving multiple functions of groups of dimensions. For each element in the discretized state space, there is a parameter to be estimated. In turn, searching for the parameters amounts to solving a KL divergence minimization problem, which can be solved using convex optimization. This paper proposes a new model, called additive Poisson Process, for estimating the intensity of ..."
    },
    {
        "coherence": 0.14677846875888279,
        "consistency": 0.25414501423001074,
        "fluency": 0.31849279499113503,
        "relevance": 0.030310876516902452,
        "overall": 0.18743178862423276,
        "Generated": "238",
        "Gold": "1. A and B 2. to improve representation learning 3. no 4. to integrate the two steps in a real end-to-end fashion. 5. no 6. no 7. no 8. to provide more efficient features for clustering analysis. 9. no 10. no 11. no 12. to improve the spectral clustering connection. 13. no 14. to improve a deep-learning-based representation learning method. 15. no 16. no 17. no 18. yes 19. no 20. no 21. no 22. no 23. no 24. no 25. no 26. no 27. no 28. no 29. no 30. no 31. no 32. no 37. no 38. no 39. no 40. no 41. no"
    },
    {
        "coherence": 0.08196934135750146,
        "consistency": 0.11220998387615985,
        "fluency": 0.23474416839360904,
        "relevance": 0.047638403727691056,
        "overall": 0.11914047433874035,
        "Generated": "239",
        "Gold": "This paper proposes a new backdoor attack method on vision classifiers, where a warping-based trigger is adopted instead of patched triggers. To bypass backdoor detectors, the authors also propose a “noise” mode of training poisoned classifier. This paper provides substantial experimental evidence that the attack method is effective in terms of: 1. Demonstrating the attack on a variety of datasets (MNIST, CIFAR10, GTSRB) 2. Ablation studies of the proposition components (Table 1)"
    },
    {
        "coherence": 0.28339885547167776,
        "consistency": 0.2878914971270781,
        "fluency": 0.09275447979957398,
        "relevance": 0.1095597127232711,
        "overall": 0.19340113628040023,
        "Generated": "240",
        "Gold": "This paper completely solves the cascading-bandits problem for $Kleq L/4$. The key difference between the analysis in this work and previous work is the complete resolution of cascade bandits. The techniques/algorithms proposed in this paper are also interesting. Note that when the reward function is linear, the minimax regret is known. Cascading bandits have a non-linear regret function. Improved the existing upper and lower bounds on the regret in the tabular setting by developing a variance-aware UCB-based algorithm."
    },
    {
        "coherence": 0.12489613122200469,
        "consistency": 0.31931613362426464,
        "fluency": 0.04484249542457557,
        "relevance": 0.02651434804351174,
        "overall": 0.12889227707858916,
        "Generated": "241",
        "Gold": "The authors propose a unified sequence-to-sequence generation interface for 4 core vision tasks: object detection, instance segmentation, keypoint detection and image captioning. The authors explore different sampling strategies and mixing ratios for the tasks in the supplementary. [Weaknesses] - The proposed method is the first one that unified \"core\" computer vision approaches into a sequence. Multitask learning for Instance Segmentation and Keypoint detection is not helping compared to specialized single task models. The work misses ablation study on the benefit of a common loss function. This paper presents a single encoder-decoder architecture, with no task-specific heads. To differentiate among different tasks, descriptive task prompts are added as the starting token to generate the output sequence. Experimental results demonstrate a proof-of-concept that this is possible to tackle 4 tasks evaluated."
    },
    {
        "coherence": 0.06849110489392757,
        "consistency": 0.1277832031577425,
        "fluency": 0.12915888668229994,
        "relevance": 0.051043645733654705,
        "overall": 0.09411921011690619,
        "Generated": "242",
        "Gold": "This paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. However, unlike in dropout the masks are kept fixed throughout training/inference and applied to the bottom layers of the network. This paper provides a simple yet novel method to force CNNs from learning texture-based features. The authors have conducted experiments on synthetically altered images to show that the defective convolution indeed tends to learn less from texture while more putting more emphasis on edges."
    },
    {
        "coherence": 0.0667495911920391,
        "consistency": 0.16699088445465152,
        "fluency": 0.08428972298544089,
        "relevance": 0.04183798119385954,
        "overall": 0.08996704495649777,
        "Generated": "243",
        "Gold": "I am sorry to say that this paper does have merits and that the study definitely may be inspiring to colleagues working on diffusion models. Although the experiments are limited, they are sufficiently convincing to me. However, I must also say that all these remarkable developments come with an unacceptably low quality english usage. ## Weakness and potential improvement I found the intuition behind the proposed method interesting and easy to follow, but I still have some concerns. First, if I understand correctly, the structure of PDM is very similar to LSGM mentioned in the related work. I guess it can produce better distribution coverage than typical linear diffusions."
    },
    {
        "coherence": 0.09043339305034745,
        "consistency": 0.22885732542239928,
        "fluency": 0.15961973454518166,
        "relevance": 0.037406120529440784,
        "overall": 0.1290791433868423,
        "Generated": "244",
        "Gold": "This paper presents a method, based on Smith normal form, to construct Gaussian processes indexed by R such that they satisfy a specified differential equation. S1 - The paper is clearly written. W2 - While the proof of Lemma 1 is indeed trivial enough to defer to appendix, it may be useful to point out in the main text that (a) the proof is indeed given in appendice, and (b) the covariances are obtained by expanding f in a basis of the null-space of d with i.i.d standard normal coefficients."
    },
    {
        "coherence": 0.20997909014635646,
        "consistency": 0.27661977113288283,
        "fluency": 0.25072162142693716,
        "relevance": 0.05864838011636987,
        "overall": 0.19899221570563658,
        "Generated": "245",
        "Gold": "This paper describes a novel implementation of RED, regularization by denoising, which better leverages multicore architectures to achieve a significant speedup. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy. The crucial result is two sets of convergence guarantees showing that this delayed update will not cause too much error. The speedups achieved range from 6 to 8 on two tasks (compressive sensing and computed tomography reconstruction). The authors propose asynchronous parallel algorithms by combining the pre-trained deep denoisers. In particular, batch gradient and stochastic gradient are applied to the proposed algorithm framework by taking advantage of the coordinate separable structures of the problem. Convergence of the algorithms are guaranteed under the four specified assumptions. Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in"
    },
    {
        "coherence": 0.24354497061905836,
        "consistency": 0.374748945938028,
        "fluency": 0.2356487695701103,
        "relevance": 0.09870702901379727,
        "overall": 0.23816242878524846,
        "Generated": "246",
        "Gold": "This work aims to the heavy performance drop that takes place in the large batch training regime for object detection and segmentation. Specifically, the authors propose an algorithm named Adaptive Gradient Variance Modulator (AGVM) that is able to work with different deep learning backbones (e.g., CNNs and Transformers) and different optimization methods on MS COCO and ADE20K, which verify the superiority of the proposed method."
    },
    {
        "coherence": 0.11770349216681002,
        "consistency": 0.2640885928117853,
        "fluency": 0.19735393398022663,
        "relevance": 0.010689752537908291,
        "overall": 0.14745894287418257,
        "Generated": "247",
        "Gold": "I am overall in favour of acceptance at this point if the list of issues mentioned next are addressed for the camera ready: 1. The addition of past work was not well integrated. 2. The choice of visualizations in study 2 was based on their resemblance of scatterplots. This makes sense given the creation of the model, but it would be nice to have the I'd like to know exactly which forms were used. This complicates the still unsatisfactory experimental design description of Study 2 in the paper."
    },
    {
        "coherence": 0.1137722421835283,
        "consistency": 0.3536529771816484,
        "fluency": 0.27825028906449145,
        "relevance": 0.02773976009568276,
        "overall": 0.19335381713133773,
        "Generated": "248",
        "Gold": "Multi-category Multi-frame Multi-resolution Multi-modal Benchmark for Video Text Visual Question Answering (ViteVQA), which extends the previous text-based visual question answering task into the video domain. The task is novel, but there is still a large gap to human performance. It seems that the questions as demonstrated in Figure 2 is very simple with a very simple format,namely “waht is XXXx”, and the answers are also very simple ,such as “yes”, “no”, and so on. In such a situation, the vQA task becomes very simple, which only exploit the characteristics of the short question and answer."
    },
    {
        "coherence": 0.18775140531560996,
        "consistency": 0.31884036892800527,
        "fluency": 0.18771786612129315,
        "relevance": 0.08602161785808494,
        "overall": 0.19508281455574833,
        "Generated": "249",
        "Gold": "Language grounding does help overcome language drift when two agents creating their own protocol in order to communicate with each other. The experiments prove that both constraints help the most (say, BLUE score) w/o PG+LM+G results are quite nice with interesting increases in linguistic diversity. This leads me to wonder if this approach would show more gains with a human evaluation (especially PG-LM+GM) and how robust your results are to hyperparameter settings."
    },
    {
        "coherence": 0.2307483312809796,
        "consistency": 0.27070630075243185,
        "fluency": 0.26145283323867985,
        "relevance": 0.06605715866792845,
        "overall": 0.20724115598500495,
        "Generated": "250",
        "Gold": "This paper proposes a novel 2D-3D Weakly Supervised Semantic Segmentation framework that jointly targets both 2D and 3D domains of image and point clouds. It can be deployed to segment any of those modalities separately. It only requires image- and scan-level class labels during training but can produce pixel- or point-level segmentation, depending on the input modality. This paper provides a practical task formulation and a method to jointly train semantic segmentation on images and corresponding point clouds"
    },
    {
        "coherence": 0.14824570135288098,
        "consistency": 0.34288027425932416,
        "fluency": 0.06144497492263344,
        "relevance": 0.04314976534196763,
        "overall": 0.14893017896920158,
        "Generated": "251",
        "Gold": "This paper proposes a method to prune deep neural networks. The aim is to reduce the computational cost and inference time while maximally maintaining the classification performance. The motivation of this work is to consider second-order structured pruning (SOSP), which considers the correlation information among the structures and layers when conducting network pruning. This paper provides detailed discussion in Section 4, which is appreciated. Meanwhile, the summary of this discussion could put before Section 2 as Related Work."
    },
    {
        "coherence": 0.08745511660512584,
        "consistency": 0.18816776592459666,
        "fluency": 0.12121231628390251,
        "relevance": 0.06279374762281097,
        "overall": 0.114907236609109,
        "Generated": "252",
        "Gold": "I think this is an innovative method to use the full 4D CTA data to predict the infarct core segmentation on a follow-up image. Authors actually perform a prediction task: Is the model size largely increasing with the number of time steps? Also, it is known that the tail of the time-intensity curve is of marginal added value, so that marginal improvement might be expected for 32T. This is a very good paper, but the DSC values are very low, and the accuracy of the results is very low."
    },
    {
        "coherence": 0.2387977317764377,
        "consistency": 0.2996199244593567,
        "fluency": 0.06616473050560082,
        "relevance": 0.06305933958827266,
        "overall": 0.16691043158241697,
        "Generated": "253",
        "Gold": "This paper studies boosting in RL, i.e., how to convert weak learners into effective policies. The authors provide an algorithm that improves the accuracy of the weak learners iteratively, and the sample complexity and running time do not explicitly depend on the number of states. In order to overcome the non-convexity of the value function (with respect to the policy space), the authors use a variant of the Frank-Wolfe method together with recent advances in gradient boosting. This paper attempts to deploy the boosting technique in the supervised learning on policy learning in the R setting. The author suggests a protocol to do policy aggregation from weak policies through the quantity $textitthe extension operator$"
    },
    {
        "coherence": 0.12675675065513267,
        "consistency": 0.13134742020468307,
        "fluency": 0.11935645761709865,
        "relevance": 0.03743288773251787,
        "overall": 0.10372337905235807,
        "Generated": "254",
        "Gold": "FasterRisk algorithm that automatically generates a pool of “risk scores” (sparse linear models with integer coefficients) in a diverse range of settings. The main improvement seems to be speed (which wasn’t very well quantified with respect to baselines), since performance-wise it was similar to a previous approach – and I wonder how useful this speed would actually be in this high-stakes offline setting. The authors identified a few major challenges in current methods, and described how these were addressed by the three components in their proposed algorithm."
    },
    {
        "coherence": 0.10498455319621695,
        "consistency": 0.1376014053652149,
        "fluency": 0.213053367267741,
        "relevance": 0.10829173000668947,
        "overall": 0.14098276395896558,
        "Generated": "255",
        "Gold": "DWMH and PVWMH can be separated and merged into a single model based on connected component analysis. However, the difference between the two models is not significant, and the author should evaluate the significance of the difference in the different models. Considering deep and periventricular WMH separately is a nice approach. It is known in the WMH field that segmenting the (often smaller and more rare) deep WMH is problematic. The authors mention the different rules that are used in clinic and which one they chose to implement (ventricle connectivity). The deep/periventricular classification is not fully automatic, but can be expanded"
    },
    {
        "coherence": 0.0632206629329696,
        "consistency": 0.13090311022992607,
        "fluency": 0.09031998435240911,
        "relevance": 0.06139757453008669,
        "overall": 0.08646033301134787,
        "Generated": "256",
        "Gold": "DCoM-Multi is a semantic type detection model that takes column values as text in addition to auxiliary features extracted by the exiting semantic TypeDetection method Sherlock. This paper proposes two versions: (1) Single, which takes a single sequence, and (2) Multi, which take multiple sequences using permutation. Experimental results on the VizNet corpus show that DCom models with DistilBERT/ELECTRA outperform Sherlock and other non-Deep Learning models."
    },
    {
        "coherence": 0.22599723147785802,
        "consistency": 0.24403133880272074,
        "fluency": 0.2126850131957607,
        "relevance": 0.09543785036341838,
        "overall": 0.19453785845993946,
        "Generated": "257",
        "Gold": "This paper proposes a method to incorporate Successor Features (SFs) in domains with continuous state and action spaces. It proposes an actor-critic architecture (a variation of the Soft Actor-Critic method) that learns disentangled representations for the environment dynamics and the tasks. The network architecture guarantees such disentanglement with two independent modules: one for the representation of the dynamics $phi$ (fed by the current state, action, and next state) and another for the task representation $boldsymbolw ($fed by task-specific information, such as the goal)"
    },
    {
        "coherence": 0.3158942928445557,
        "consistency": 0.3315055299773887,
        "fluency": 0.28696248592414497,
        "relevance": 0.15277462281684567,
        "overall": 0.2717842328907338,
        "Generated": "258",
        "Gold": "Convergence analysis of proximal gradient descent ascent (GDA) method when applied to nonconvex strongly concave functions. The authors provide a novel Lyapunov function and analyzed the convergence using KL local geometry. This is the first variable convergence result in nonconvéx minmax optimization and can potentially used to study other minmax algorithms. This paper analyzes a proximal gradient descent-ascent method for nonconverx minimax problem under specific assumptions. It is a good idea, but it is not the best for the saddle point objective."
    },
    {
        "coherence": 0.2391533788213809,
        "consistency": 0.26556592863077744,
        "fluency": 0.400341628674397,
        "relevance": 0.04064743496266459,
        "overall": 0.23642709277230498,
        "Generated": "259",
        "Gold": "This paper utilizes ground depth as geometry cue to improve monocular 3D object detection. The ground depth is estimated by camera pose and vanishing point. And the ground depth information is further integrated into the network by the transformer. The results are comprehensive and the results are strong. It outperforms previous sota by large margin (Tab.1). This paper claims to be top on KITTI for monocular 3-D image detection, which would be an impressive achievement."
    },
    {
        "coherence": 0.2234897311628707,
        "consistency": 0.17343906014151012,
        "fluency": 0.10289872985403058,
        "relevance": 0.16529664018585594,
        "overall": 0.16628104033606683,
        "Generated": "260",
        "Gold": "Weaknesses: The abstract asserts that you show that your model can benefit downstream tasks such as review comprehension, but you do not directly show this. If you make this claim, you should provide an experiment showing improvement on this task by incorporating your graph. The authors propose a method to automatically build a Knowledge Base of opinions and implications between them. The KB is realized as a directed graph where nodes correspond to opinions in a canonical form (modifier, aspect), and edges indicate implications. It is built by factorizing a matrix of item-opinion frequencies, and finding the top k neighbors of an opinion."
    },
    {
        "coherence": 0.1377268005853625,
        "consistency": 0.23219955506124207,
        "fluency": 0.22169330962386016,
        "relevance": 0.033457694226580216,
        "overall": 0.15626933987426123,
        "Generated": "261",
        "Gold": "I am not convinced by the experimental results. 4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting. A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting. 4c) Attention is not so much convincing for the need of attending multiple adjacent items."
    },
    {
        "coherence": 0.07443440348217031,
        "consistency": 0.1580643541347556,
        "fluency": 0.16366957635299118,
        "relevance": 0.05256684937764504,
        "overall": 0.11218379583689055,
        "Generated": "262",
        "Gold": "I think this paper is a thorough and well-written experimental paper, something which is really important but is also clearly underappreciated in the machine learning community. On the other hand, it was not really obvious to me why some of the objectives tested here are interesting. The trends in Fig. 2 show that these alternatives underperform an LM objective, which suggests that the NLP community can keep using that objective without worry—and everything else in the figure seems as we would expect. To me, the most interesting experiment is the final one in Section 6. This experiment seems like it could be the germ for a far more interesting paper getting at how these pre-training methods help with downstream tasks. The experiments are well motivated and well described. Regarding Table 1: which one of the \"LM forward\" models was used in the subsequent experiments?"
    },
    {
        "coherence": 0.26096109025235337,
        "consistency": 0.32857680038741793,
        "fluency": 0.23907502697221578,
        "relevance": 0.11547315663794464,
        "overall": 0.2360215185624829,
        "Generated": "263",
        "Gold": "Local convergence of gradient descent dynamics in GANs under strong technical assumptions is stable for t-GDA with a large enough timescale separation if and only if it is a local Stackelberg equilibria. This result is quite intuitive since a similar result has been proved by Jin et al. 2020. (the timescale had to go to infinity). Though, proving such a result for a *finite* time scale separation."
    },
    {
        "coherence": 0.3346493480049845,
        "consistency": 0.35865881368810143,
        "fluency": 0.15237500062958312,
        "relevance": 0.1107377020818825,
        "overall": 0.23910521610113789,
        "Generated": "264",
        "Gold": "This paper provides a new view point of RNN architectures and relates RNNs with ODEs and proposes QUNN, a more stable and has less complexity overhead in terms of input dimension while comparing with LSTM or GRU. This paper does not present or support its claims in a clear fashion, making it difficult to judge the merits of its claims. Given the nuance required for its claims, the authors should add an in-depth \"background\" section, so the reader becomes more familiar with the terms that will be introduced later"
    },
    {
        "coherence": 0.047237761127321046,
        "consistency": 0.0874797171943526,
        "fluency": 0.16799382869248208,
        "relevance": 0.05186068316817018,
        "overall": 0.08864299754558147,
        "Generated": "265",
        "Gold": "A method for pre-training a deep learning model to apply to a small dataset of histopathology slides using a single layer perceptron. However, there are a number of problems with this paper. 1. The number of slides per each task AND the number of patches extracted. 2. The size of the training/validation/test sets is missing, in particular: a total of 4 slides per test set. I believe the proposed distance can be useful in more difficult tasks and rare cancer datasets. I also think there is to be more discussion of this result."
    },
    {
        "coherence": 0.2246282978366778,
        "consistency": 0.3586546810476769,
        "fluency": 0.41758351045841935,
        "relevance": 0.05913443548772314,
        "overall": 0.26500023120762434,
        "Generated": "266",
        "Gold": "This paper provides a uniform generalization bound for over-parameterized neural networks by assuming that the target function resides in the reproducing kernel Hilbert spaces generated by Neural Tangent (NT) kernels associated with general ReLU activation functions. The first theorem is about the equivalence of the RKHSs of the neural tangent and Matern kernels over the hyperspherical domain. The results generalise existing results for the particular case $s=1$, i.e. the case of ReLU active functions [1,2]. The idea of the proof is to rely on an elegant use of Stein's Lemma as well as the recurrent relation between the NTK Lemmas 1 and 1 is a nice observation."
    },
    {
        "coherence": 0.18583892978490463,
        "consistency": 0.22858077314965983,
        "fluency": 0.35954141201957895,
        "relevance": 0.025985003421419746,
        "overall": 0.1999865295938908,
        "Generated": "267",
        "Gold": "#### Strength: This paper extends the Strong Lottery Ticket Hypothesis to binary networks and proposed a new method to obtain compact binary networks with better robustness.#### Weakness: The proposed methods search for sparse binary subnetworks within randomly-initialized networks. It's unclear whether such a scheme is better than training the parameters directly. ### Strength: A method for learning robust binary neural networks from random initialization."
    },
    {
        "coherence": 0.2631696310155997,
        "consistency": 0.30875849210602324,
        "fluency": 0.48731302780799196,
        "relevance": 0.06599524760979947,
        "overall": 0.2813090996348536,
        "Generated": "268",
        "Gold": "A method to avoid overfitting while finetuning the large pretrained models for downstream tasks on small scale datasets. It has been shown that many SOTA models usually overfit w.r.t. spurious correlations in the data and as a result fail miserably when tested for generalization on the out-of-domain data. A novel information bottlenecking technique for GLUE tasks that can be applied to out of domain data. It is a very sophisticated way of avoiding overfiting, especially when the data size is limited."
    },
    {
        "coherence": 0.4057723761363771,
        "consistency": 0.3743223659140317,
        "fluency": 0.3709225547100348,
        "relevance": 0.07358001349472966,
        "overall": 0.3061493275637933,
        "Generated": "269",
        "Gold": "[1] [2] [3] [4] [5] [6] This paper has provided an excellent motivation of their work in Sections 1 & 2 with references being made to human behaviors and heuristics, though the authors can choose a more realistic running example that is less extreme than making orthogonal decisions/actions. The authors have proposed to learn planners from demonstrations in multiple tasks via value iteration networks to learn the reward functions."
    },
    {
        "coherence": 0.31160059983555666,
        "consistency": 0.5415757848283653,
        "fluency": 0.16127605957468774,
        "relevance": 0.25016876004625205,
        "overall": 0.3161553010712155,
        "Generated": "270",
        "Gold": "This paper proposes a chronology-based benchmark to enable a better estimate of the model’s performance in an environment close to the real world. The authors highlight the non-stationary nature of the data using per feature analysis, t-SNE plots and an Optimal Transport approach. A novel protocal splitting the data called AnoShift which splits the data in IID, NEAR and FAR testing splits."
    },
    {
        "coherence": 0.1531580814225763,
        "consistency": 0.15036248075954078,
        "fluency": 0.2814834224116987,
        "relevance": 0.05151722169464454,
        "overall": 0.15913030157211508,
        "Generated": "271",
        "Gold": "This paper proposes a simple and effective adversarial learning method DIAL for robust representation learning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method, including some interesting ones, e.g., robustness against unseen perturbations, transfer learning (I like them). However, the novelty of this paper is insufficient, and using the domain adaptation principle and Learning Invariant Representation has been widely studied. Therefore, I vote for rejection."
    },
    {
        "coherence": 0.14766740687911786,
        "consistency": 0.2935643430242312,
        "fluency": 0.1864918092880844,
        "relevance": 0.042284736591097036,
        "overall": 0.1675020739456326,
        "Generated": "272",
        "Gold": "WPipe, a technique for reducing the memory overheads of weight versioning and improving freshness of weight updates in pipeline parallelism through a novel scheduling of the pipeline stages. The weakness include: 1. It tackles an important problem of how to fit rapidly growing model sizes into slower growing device memory, and this paper provides some improvements in that space. 2. It proposes an interesting approach to address key issues surrounding memory efficiency and weight update sematics. 3. The writing could be greatly improved as I found multiple portions difficult to follow. Although I am not hands-on with pipeline parallelistm, I feel I have pretty solid understanding at the high-level, but yet this draft was less accessible than expected. Below are some specific examples of confusing portions and questions for the authors: a) The idea (stated in 3.2) of splitting the model partitions to double the number of partitions and obtain two groups (G0 & The authors"
    },
    {
        "coherence": 0.10756667617942238,
        "consistency": 0.15198685436872514,
        "fluency": 0.14808990985181616,
        "relevance": 0.07677345274510092,
        "overall": 0.12110422328626616,
        "Generated": "273",
        "Gold": "This paper introduces four EEG signal transformations to model the real-world variability observable during deployment and then proposes a multi-pronged approach to evaluate the robustness of healthcare ML models. Four scalp EEG data shifts that would do a good job of capturing some real world variation of such datasets and then they propose using latent space integrity methods and predictive uncertainty as two ways of evaluating the behaviour of a model under varying types and intensities of the four proposed dataset shifts - remarkable effort!"
    },
    {
        "coherence": 0.28186226152397437,
        "consistency": 0.24945114684220948,
        "fluency": 0.30195861981818256,
        "relevance": 0.3401068108566142,
        "overall": 0.2933447097602452,
        "Generated": "274",
        "Gold": "A systematic and deep theoretical study of the problem of Byzantine-robustness in the heterogeneous setup, i.e., when workers have non-identical datasets and, as s result, their local loss functions are not identical. The authors propose and analyze an algorithmic tool called *bucketing* and show that it makes some known aggregators such as Krum, coordinate-wise median, and geometric median to be *agnostic* robust aggregatores. Under bounded heterogeity assumption, the authors derive that Robust Client Momentum method from (Karimireddy et al., 2021) converges for non-convex problems to a neighborhood of a stationary point. Next, it is shown that the size of the neighborhood can be made arbitrarily small for over-parameterized problems. Numerical experiments"
    },
    {
        "coherence": 0.28038267529971017,
        "consistency": 0.39559163267225683,
        "fluency": 0.26005388521084527,
        "relevance": 0.10526338573105876,
        "overall": 0.26032289472846776,
        "Generated": "275",
        "Gold": "Pros: Paper focuses on addressing important problem and draws interesting insights. Paper proposed a new Alter-ResNet architecture that is both more accurate and robust. Cons: Paper is generally hard to read and understand. Imperial results are reported in a variety of different forms and entirety of observations is hard to grasp. I read the paper carefully twice and I am still not 100% sure I understand ALL the intricacies of every experiment and how those unequivocally lead to insights drawn. Paper is just very dense. I believe the observations and insights are correct, the issue is manly expositions which could benefit from substantial improvements."
    },
    {
        "coherence": 0.2747139088468577,
        "consistency": 0.31996452702662426,
        "fluency": 0.42196691865385166,
        "relevance": 0.07066854388214706,
        "overall": 0.2718284746023702,
        "Generated": "276",
        "Gold": "I like the insight that method can use the model's predictions and gradients and thus we should be aware of model gradient errors. However in practice, we do not know the gradients on the trajectories that we attain and equation (12) requires computing the n-nearest points of $x$ in the replay buffer to estimate the model gradient. This seems potentially intractable in practice for large replay buffers, and seems like it will not always coincide with the true model gradient"
    },
    {
        "coherence": 0.06895589683525571,
        "consistency": 0.09286654436391463,
        "fluency": 0.4031591208845574,
        "relevance": 0.04801388879234001,
        "overall": 0.15324886271901694,
        "Generated": "277",
        "Gold": "A novel bone suppression method that improves lung nodule detection. International journal of computer assisted radiology and surgery, 11(4): 641–655, 2016. [1] --> Jens von Berg, Stewart Young, Heike Carolus, Robin Wolz, Axel Saalbach, Alberto Hidalgo, Ana Gim enez, and Tom as Franquet. [2]"
    },
    {
        "coherence": 0.21152005113765365,
        "consistency": 0.2337750044138497,
        "fluency": 0.46489753750560475,
        "relevance": 0.2344537925822865,
        "overall": 0.2861615964098487,
        "Generated": "278",
        "Gold": "This paper addresses a very relevant problem in histopathology image analysis. The authors have performed a relatively extensive set of experiments. The proposed methodology (combination of classical data augmentation and GANs) is very straightforward and logical. The presentation of the protocols the authors propose should have been clearer and better explained. It would have been quite helpful to itemize, compare and contrast the various approaches and most importantly explain what is the intuition and theoretical background of your choices."
    },
    {
        "coherence": 0.2608970167958294,
        "consistency": 0.26390321304972253,
        "fluency": 0.4063761335431803,
        "relevance": 0.06311989146932688,
        "overall": 0.24857406371451476,
        "Generated": "279",
        "Gold": "This paper focuses on the impact of training data source/distribution on certain downstream tasks. This paper matches the quality of a typical publication at NeurIPS conference. I strongly recommend acceptance. Below I justify my rating by discussing the main strengths of the paper along each of the suggested axes: **Originality:** Large pre-trained models like CLIP and ALIGN are becoming increasingly popular in the vision community. While most works make progress by scaling up data and models (e.g. SLIP, DeCLIP, FILIP), this paper is focusing on the effect of learning data source / distribution on certain downstream tasks. To the best of my knowledge, this exploration using large pre- trained models at this level of detail is novel."
    },
    {
        "coherence": 0.3052369656275747,
        "consistency": 0.38261801363160397,
        "fluency": 0.151232858708243,
        "relevance": 0.08429056833215809,
        "overall": 0.23084460157489492,
        "Generated": "280",
        "Gold": "TERM is a simple modification of the ERM paradigm. It consists in applying an exponential smoothing to the loss function. The authors provide several interpretations and connexions with (robustness/fairness/quantile regression etc...) literature and show how the term can be adapted to such problem. The current manuscript does have some novel interpretations such as the trade-off between avg-min losses. However, I am not an expert in this field, thus not sure how much the new analysis will contribute to the community. Strong points: the article is entirely reproducible as the author furnishes a well documented code."
    },
    {
        "coherence": 0.17490759460640193,
        "consistency": 0.22299576026604156,
        "fluency": 0.16604486538853072,
        "relevance": 0.06436880283799946,
        "overall": 0.15707925577474344,
        "Generated": "281",
        "Gold": "This paper provides a solution to the multi-fidelity variant of the best-arm identification problem. It builds on the ideas of previous algorithms that do successive elimination of arms until only one is left. A unique aspect is their joint consideration of the costs and accuracies of each fidelity level. They demonstrate their method empirically on a couple toy/synthetic problems. Multi-fidelity best arm identification is a very natural formalism with useful applications. While the paper falls short of solving this problem completely, I think the authors have made meaningful advances. Hence, I believe it would be a nice addition to the proceedings."
    },
    {
        "coherence": 0.16220775935466905,
        "consistency": 0.5342499524643155,
        "fluency": 0.22370874544641878,
        "relevance": 0.08053456087689637,
        "overall": 0.25017525453557493,
        "Generated": "282",
        "Gold": "This paper proposes a method for estimating new detectors of visual attribute conjunctions on images by unmixing its parameters and recombining them. The proposed method can be used to solve Zero-Shot Classification problems when only few attribute annotations are available in the learning dataset by augmenting the annotations with new synthesized attribute detectors. This paper presents a novel research problem about novel attribute classification and attribute labeling by decomposing the seen attributes into basic attributes, and then reassembling these basic attributes into new ones."
    },
    {
        "coherence": 0.3265341709899433,
        "consistency": 0.2840735761961154,
        "fluency": 0.29110660010365985,
        "relevance": 0.22685579115615606,
        "overall": 0.28214253461146865,
        "Generated": "283",
        "Gold": "A novel approach to training seq2seq models that addresses exposure bias and the MLE objective, without pretraining with MLE. The authors propose an approach based on edit distances and the implicit use of given label sequences during training. The aim is to generate a label sequence with respect to the current parameter vector of a conditional probabilistic model and then find the best possible completions for any prefix of that model-based label sequence given a prefix sequence generated using the current model and the exponential family model."
    },
    {
        "coherence": 0.15571562392065838,
        "consistency": 0.2871117959202529,
        "fluency": 0.2929432442190007,
        "relevance": 0.09039576474246977,
        "overall": 0.20654160720059545,
        "Generated": "284",
        "Gold": "This paper provides a good description of the state of the art and comprehensive experimental results. The methological contribution is mild, essentially changing a buiding block in a state-of-the-art neural architecture. DNN based speech enhancement method to speech analysis. I think it’s better to mention E. Hakan’s method here. I can’t access to the URL. Please check it. This paper uses a complex-valued network to learn the modified complex ratio mask with a weighted SDR loss for the speech enhancement task. A human listening test using MOS or preference score should be conducted."
    },
    {
        "coherence": 0.2785967132170419,
        "consistency": 0.25810233553430584,
        "fluency": 0.2727186841041617,
        "relevance": 0.06567619635330903,
        "overall": 0.2187734823022046,
        "Generated": "285",
        "Gold": "I think this is a very interesting idea and the paper is generally well written. I find some of the description of the models, methods and training is lacking detail. For example, their should be more detail on how REINFORCE was implemented; e.g. was a baseline used? That would make their improvements clear. Moreover, for TriviaQA their results and the cited baselines seem to all perform well below to current top models for the task (cf. https://competitions.codalab.org/ competitions/17208#results)"
    },
    {
        "coherence": 0.2907262858874,
        "consistency": 0.24798480033677686,
        "fluency": 0.2412437448508307,
        "relevance": 0.0728581600452894,
        "overall": 0.21320324778007424,
        "Generated": "286",
        "Gold": "A theoretical and practical paper on the relation between convolutional neural networks and Gaussian processes. The main contribution of the paper as presented by the authors is 2fold: 1. Some theoretical justifications about the correspondence between GPs and convolutionals networks with infinitely many channels are provided 2. The formulas for GP kernel computation for the considered network are provided and some experiments are conducted (on MNIST). I recommend turning the argument in section 2.2 into a formal, self-contained theorem that states a result on A_L, defined in eq. The authors themselves comment that computing kernel matrix takes more time than inverting it, just like in (5) and (6). This would make the precise claim easier to understand."
    },
    {
        "coherence": 0.19639185614382673,
        "consistency": 0.25499833042274933,
        "fluency": 0.28714929743554346,
        "relevance": 0.10768382964916678,
        "overall": 0.21155582841282158,
        "Generated": "287",
        "Gold": "Weaknesses: 1. The idea of \"fully parallelize SGD and forward/backward propagations to hide 100% of gradient communication\" is actually not new. [1] proposed using SGD with 1 step of staleness to overlap communication and computation. However [2] is not cited or discussed in this paper. 2. Both uses such a combination. [3] proposes the combination of [3]."
    },
    {
        "coherence": 0.19737085650646777,
        "consistency": 0.12659154727923072,
        "fluency": 0.23109295069682367,
        "relevance": 0.25427545930408396,
        "overall": 0.20233270344665155,
        "Generated": "288",
        "Gold": "This paper is a review of the state of the art on SDGD-like algorithms for smooth, nonconvex optimization. It focuses on a gap in the literature that was established for a restricted class of gossip matrices and associated graphs. The authors provide a solution method that achieve lower complexity bounds. This is an update on the current literature on non-conveX stochastic decentralized optimization."
    },
    {
        "coherence": 0.43712374223105455,
        "consistency": 0.4553385295320199,
        "fluency": 0.2698028545838237,
        "relevance": 0.10328752854572346,
        "overall": 0.3163881637231554,
        "Generated": "289",
        "Gold": "Pros: A self-supervised technique for controlling the productions of a Transformer-based pretrained generator. It consists in augmenting the architecture of the pretrained model with a special \"content-conditioner\" (CoCon) block which is able to exploit a contextual condition. At test time, this contextual condition is provided as context, and the parameters of the CoCon component learn how to approximately recover the missing portion based on this context (the portion itself) and on the prefix text preceding the removal. This paper proposed a way to control the content output a DNN-based language model (GPT-2 in the experiment, but not limited to it)"
    },
    {
        "coherence": 0.2273439416377007,
        "consistency": 0.3328719212508279,
        "fluency": 0.17295569551225842,
        "relevance": 0.14614539475007693,
        "overall": 0.21982923828771597,
        "Generated": "290",
        "Gold": "This paper presents a simple framework for continual semantic segmentation by addressing the overcompression issue in model learning. However the contributions are insignificant and the experiments are insufficient. The authors introduce two simple improvements for Class-Incremental Semantic Segmentation (CISS) problem. The first improvement is widening the network layers before the final classification layer to improve feature quality. The second improvement is adding a dropout layer after the encoder during the offline phase of the model training."
    },
    {
        "coherence": 0.14218393270565458,
        "consistency": 0.28378288082538355,
        "fluency": 0.1556293324944069,
        "relevance": 0.06871754416545355,
        "overall": 0.16257842254772464,
        "Generated": "291",
        "Gold": "Multi-Granularity Approaches for Source Code Summarization by leveraging Multi-Grained Features for Multi-Scale Contextual Descriptions by Using Transformer Architectures for Multi - Scale Content Summarizing by utilizing Multi-scale Features to Summarize Source Code by using a Pyramid-shaped input representation based on a pyramid attention mechanism for efficient feature aggregation and distribution across different hierarchies. Weaknesses:"
    },
    {
        "coherence": 0.13954013393251635,
        "consistency": 0.21846979245866907,
        "fluency": 0.33466857670604516,
        "relevance": 0.08478352071298192,
        "overall": 0.19436550595255314,
        "Generated": "292",
        "Gold": "Pros: This paper incorporates a concept called hypergraph network into reinforcement learning. The idea of hypergraph is to extend edge to hyperedge where a set of vertices can be considered at the same time. This seems natural for scenarios like continuous action control with multi-dimension action space. This paper focuses on learning action representations for problems involving high-dimensional action spaces. The aim is to build a flexible and general methodology for learning representations of multidimensional actions that can be combined with existing architectures (which mostly focus on learning state representations). The empirical results clearly illustrate the benefits of the method, however, the action dimensionality is small enough to include all possible hypergraphs."
    },
    {
        "coherence": 0.23042806200533145,
        "consistency": 0.2173177744962059,
        "fluency": 0.1458280684984872,
        "relevance": 0.0838419480067038,
        "overall": 0.16935396325168206,
        "Generated": "293",
        "Gold": "A task-agnostic U-net to model the retinal encoding process and characterize the representation of \"time in the natural scene\" in a low-dimensional compressed latent space. Moreover, they analyze the latent representations and find that static features and dynamic features are encoded synergistically. To the best of my knowledge, the finding that this finding helps to elucidate how the retina processes visual stimuli, this is a significant contribution [...]"
    },
    {
        "coherence": 0.3064423752688161,
        "consistency": 0.3272502078486809,
        "fluency": 0.18773046933656706,
        "relevance": 0.09148108881242273,
        "overall": 0.22822603531662172,
        "Generated": "294",
        "Gold": "This paper presents a framework for single-view, multi-object 6-D pose estimation. It is based on an object detector backbone, which receives a single RGB image as input, and a modified version of a Deformable DETR Transformer, which is provided with multi-scale features extracted by the object detector, as well as the center coordinates and extent of the bound. The authors evaluate the model on the YCB-V dataset."
    },
    {
        "coherence": 0.15622875872528644,
        "consistency": 0.2027665053646752,
        "fluency": 0.238027035144257,
        "relevance": 0.1733492854680281,
        "overall": 0.1925928961755617,
        "Generated": "295",
        "Gold": "This paper is an interesting contribution to network Granger causal modelling. The baseline approaches and evaluation metrics that are compared seem appropriate. The evaluations are extensive and provide some insight into the hyperparameter h. What I missed in the discussion was especially a discussion on the ablation experiments (the results of Figure 5). Is there an explanation for the fact that good choices for h are between 5 and 10? Is this a good paper?"
    },
    {
        "coherence": 0.5039054717375266,
        "consistency": 0.5362594066192796,
        "fluency": 0.3988547492310426,
        "relevance": 0.03345591753939419,
        "overall": 0.3681188862818108,
        "Generated": "296",
        "Gold": "A new loss for learning a selector given a predictor $f, denoted W(g,f,theta)$, that basically uses $mathbf It is a well-written paper with clear problem formulation, definitions & assumptions, and a practical iterative algorithm to approximate the estimators based on MWU, and tests on semi-synthetic experiments"
    },
    {
        "coherence": 0.33317644834439547,
        "consistency": 0.3542686441616769,
        "fluency": 0.22669275047830148,
        "relevance": 0.1607912875263271,
        "overall": 0.26873228262767523,
        "Generated": "297",
        "Gold": "Pros: - This paper introduces spread divergences. The authors discuss conditions under which the data generating process can be identified by minimizing spread divergence and apply spread divergerces to the examples of PCA, ICA, and noiseless VAE. - The authors introduce spread divergeces. This paper proposes a method for minimizing the spread of a data source, and a new approach for identifying the data source."
    },
    {
        "coherence": 0.22840505197722527,
        "consistency": 0.26192417936439666,
        "fluency": 0.35272628956019153,
        "relevance": 0.12457873955042331,
        "overall": 0.2419085651130592,
        "Generated": "298",
        "Gold": "This paper proposes a language-agnostic representation of source code based on the AST and proposes that the model learns to better represent code, without language specific features. A concrete example would help. ['PPR is a well-studied proximity measure * which* has'] ['Set'] - a syntax and grammar issue that should be addressed in the next section. This paper proposed to not only take positional information of each token, but to add additional structural information about distances of tokens in the abstract syntax tree. This information gives an edge of this model on several code summarization datasets. The simplicity of the proposed model (assuming it is released by the authors) puts the work in the state-of-the-art category in machine learning for code. While it looks standard, the idea seems to deliver well in the evaluation results. This is also the first work that shows cross-language improvements for"
    },
    {
        "coherence": 0.20092553766805357,
        "consistency": 0.4311848816360562,
        "fluency": 0.4400630840554686,
        "relevance": 0.12858137936057826,
        "overall": 0.3001887206800392,
        "Generated": "299",
        "Gold": "This paper presents an empirical study of language pre-training (GPT-2), image pre- training (iGPT), and training from scratch for offline RL. It feels that it often stops too early and presents a hypothesis without trying to explore it further. It also hypothesizes with empirical proofs that the image-pre-trained model fails probably because of large gradients, and that language pre training probably gives the Transformer context-like information that can be taken advantage of during down-stream tasks. This paper is a very interesting and thought-provoking analysis of representation dynamics between pretraining and finetuning."
    },
    {
        "coherence": 0.22105059365274865,
        "consistency": 0.2628565226119726,
        "fluency": 0.20493925023751638,
        "relevance": 0.2636149677157857,
        "overall": 0.23811533355450581,
        "Generated": "300",
        "Gold": "This paper studies the generalization properties of high-dimensional generalized linear models. Assuming that the inputs are Gaussian and the underlying data distribution follows a multi-index model, the authors obtained upper bounds for the population Moreau envelope risk in terms of the training loss and the Rademacher complexity of the function class in a variety of overparameterized settings, they show connections to existing results in the literature such as the optimistic rates for linear regression and the benign overfitting."
    },
    {
        "coherence": 0.2409282624970793,
        "consistency": 0.4189992391597097,
        "fluency": 0.09321930315083293,
        "relevance": 0.18345777709059044,
        "overall": 0.2341511454745531,
        "Generated": "301",
        "Gold": "I'm not convinced of the desirability of minimising the amount of paired data, or of the assumption that active querying for matched data is cheap (this is mentioned more in \"significance\", below). The authors propose the Importance Weighting with Rejection (IWRE) to solve the dynamics mismatch with importance sampling and support mismatch via rejection learning. They conduct experiments on Mujoco and Atari games and their method gets the best imitation performance."
    },
    {
        "coherence": 0.05318073184770891,
        "consistency": 0.14575297246098237,
        "fluency": 0.12449841578790692,
        "relevance": 0.06083838694900097,
        "overall": 0.0960676267613998,
        "Generated": "302",
        "Gold": "INTELLIGENT MATRIX EXPONENTIATION IN A NEURAL NETWORK - A VERY IMPORTANT PRECEDED EVALUATION A novel machine learning architecture that is in the form matrix exponential. It can effectively model the interaction of feature components and learn multivariate polynomial functions and periodic functions. Its universal approximation capability is explained and proved in the Appendix. Other properties related to periodicity, connection to Lie groups, the interpretation from the perspective of dynamical systems, and the robustness bounds are discussed."
    },
    {
        "coherence": 0.08726398488348093,
        "consistency": 0.22319122229642138,
        "fluency": 0.08072371985660108,
        "relevance": 0.04509435425969637,
        "overall": 0.10906832032404995,
        "Generated": "303",
        "Gold": "A system to help medical residents and their reviewers assess their learning using an information visualization dashboard, designed for and with them in a participatory process, deployed in their setting, and evaluated with them through a longitudinal study. A design study of a visualization system for tracking and reviewing resident physicians’ performance in  a medical training program. The authors have done well explaining the context and the relevant terminology. The design process and several design decisions are well explained and flow well in to the final design of the system. Finally, building such a system, completing the data pipeline, and maintaining it for over four months is considerable effort."
    },
    {
        "coherence": 0.16612337310147954,
        "consistency": 0.21365309065559174,
        "fluency": 0.1552107160356705,
        "relevance": 0.04591256188063508,
        "overall": 0.14522493541834422,
        "Generated": "304",
        "Gold": "This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning. The results show a significant gain in performance as well as the capability of the model to generalize composition program to unseen data effectively. Variational neural module networks (V-NMN) is a probabilistic model for Visual Question answering. It is not clear how these challenges are solved in section 2.1. I find the technical part is hard to follow. To optimize the objective function, it involves many challenges as well."
    },
    {
        "coherence": 0.23715879247452262,
        "consistency": 0.3302963693825252,
        "fluency": 0.07732481207975374,
        "relevance": 0.1919131723343622,
        "overall": 0.20917328656779094,
        "Generated": "305",
        "Gold": "This paper provides an empirical study of binary weight networks (BWNs), where they find that the commonly adopted scaling factor is not critical. There exists a subnetwork that stabiles early in training, and the proposed quantization algorithm is interesting, which has a potential of squeezing more redundancy out of standard BWNs. The authors propose a new quantization method that increases the compression of binary networks. They also analyze the optimization process and the structure of optimized convolutional networks to note that i) there are clusters of weights whose weight remain almost unchanged during the optimization stage. The paper would have much more impact if, after showing that BN can absorb the redundant factors, all real-valued parameters were dropped."
    },
    {
        "coherence": 0.2229604121857697,
        "consistency": 0.3783195470019781,
        "fluency": 0.11261984656572895,
        "relevance": 0.037504997680250826,
        "overall": 0.1878512008584319,
        "Generated": "306",
        "Gold": "This paper is a very thorough study of the ability of adversarial training to memorize random datasets. The authors do a great job of providing a significant amount of evidence for their claims. The theory 1 does not say anything about how TRADES can overcome the gradient instability of PGD-AT. This weakens the theoretical basis for robust overfitting, which is based on the fact that trades have a greater gradient instability than standard training."
    },
    {
        "coherence": 0.19621552507844536,
        "consistency": 0.22353716239751828,
        "fluency": 0.11260272011548048,
        "relevance": 0.0653366081651502,
        "overall": 0.14942300393914856,
        "Generated": "307",
        "Gold": "FlexConv is a novel parameterization method of CNN kernels by multiplying continuous kernels and with sigma-learnable Gaussian masks so that the size of the kernels can be learned during training. State-of-the-art performance on both sequential and image datasets demonstrates the effectiveness of the proposed method. An effective regularization is proposed to address the aliasing issue and generalize to higher resolution cases which are never seen during training"
    },
    {
        "coherence": 0.18296374159130935,
        "consistency": 0.2779145710806235,
        "fluency": 0.08475577638350645,
        "relevance": 0.09047335172912814,
        "overall": 0.15902686019614187,
        "Generated": "308",
        "Gold": "This is a good paper, but I would like to see it applied more broadly to other robotic tasks, and a comparison to Florensa et al. and perhaps the addition of a video which shows the learned behaviors. Overall, the writing is good... but I did find the main statement of the algorithm confusing! I understand one of the main reasons this is complicated is because the authors have tried to make this \"general\" or to be used with DQN/PPO/A3C... but if you present a clear implementation for one of them ("
    },
    {
        "coherence": 0.1314992325200322,
        "consistency": 0.27375864381878556,
        "fluency": 0.11309599983644313,
        "relevance": 0.04251359329913535,
        "overall": 0.14021686736859906,
        "Generated": "309",
        "Gold": "This paper improves existing DRL-based CO methods in two terms. Firstly, the authors leverage a continuous probabilistic space for the solutions, leading to a REINFORCEMENT-based method which is more efficient than previous Q-learning or Actor-Critic methods. Besides, a MAML-based meta-learning framework is proposed. This paper studies a differential meta solver for CO problems and its performance is demonstrated by using TSP instances (size 500, 1000, and 10000)"
    },
    {
        "coherence": 0.2059618746243247,
        "consistency": 0.2115429545910697,
        "fluency": 0.13107500950482776,
        "relevance": 0.16045513069900588,
        "overall": 0.177258742354807,
        "Generated": "310",
        "Gold": "This paper addresses the important problem of generating realistic phantoms. It introduces a “new” similarity measure by combining two established similarity measures. The authors used some vskip or any other format/spacing altering commands (see the spacing between abstract and introduction or any new section) ... without those commands the paper limit of 3 pages is not met . Without those command's the paper limits are not met"
    },
    {
        "coherence": 0.08257224509004343,
        "consistency": 0.20065433180248007,
        "fluency": 0.0385442815778502,
        "relevance": 0.04698666018398658,
        "overall": 0.09218937966359006,
        "Generated": "311",
        "Gold": "## Strengths (Reasons to accept) - This paper is generally well-formed. - It proposes a new protocol that can be utilized for estimating the extent to which each latent variable is mapped to a specific syntactic role. ## Weaknesses - I'm not sure this work is the first that attempts to combine Transformers into the VAE framework. For instance, Li et al. (https://arxiv.org/pdf/2004.04092.pdf) already proposed a model consisting of pre-trained Transformer encoders and decoders. The paper studies a research question that is at the heart of representation learning, which could potentially be very impactful. The model is evaluated on disentanglement of semantic roles and shows its ability to identify semantic roles"
    },
    {
        "coherence": 0.21826080941436468,
        "consistency": 0.2772385876726091,
        "fluency": 0.08488874522380764,
        "relevance": 0.08243457194172255,
        "overall": 0.16570567856312599,
        "Generated": "312",
        "Gold": "This paper addresses the problem of inconsistent predictions of large language models on fact-seeking prompts when these prompts are perturbed or rephrased. It then proposes a few different methods for addressing this inconsistency that operate on the same portion of the language model, namely, between the input token embeddings and the first hidden layer of the model. The paper evaluates the performance of the variants using a pooled collection of fact - seeking prompts (e.g., LAMA, LPAQA and ParaSel). The paper presents an approach for extracting factual information from LLMs, where authors discuss a user oriented setting where they take in varied natural language prompts and return the objects with information needs."
    },
    {
        "coherence": 0.27915582739235895,
        "consistency": 0.29622057186632095,
        "fluency": 0.07267628213247498,
        "relevance": 0.08616091760681467,
        "overall": 0.1835533997494924,
        "Generated": "313",
        "Gold": "This paper presents interesting insights, but I am not sure how much these findings can be generalized particularly in offline RL for locomotion tasks, where typical benchmarks consider all 4 types of data collection strategies (random, medium, medium-replay, medium expert) -- this paper considers only the latter two variants for their analysis. This paper studies the importance of the design decisions for supervised learning type reinforcement learning algorithms. Through extensive experiments, find that careful design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary."
    },
    {
        "coherence": 0.15567133588736268,
        "consistency": 0.20414017430956052,
        "fluency": 0.10604719816210315,
        "relevance": 0.04458832457361168,
        "overall": 0.1276117582331595,
        "Generated": "314",
        "Gold": "It's a very good choice. Learning two-player, zero-sum Markov games under a kernel mixture MG model where the transition kernel is assumed to be an element in an RKHS. The authors propose KernelCCE-VTR to learn the equilibrium of the game. This is achieved by first estimating the state value functions using kernel ridge regression. By relating the regression problem to a linear bandit setting, the authors can derive tractable confidence bounds used to construct optimistic/pessimistic estimates of the state-action value functions."
    },
    {
        "coherence": 0.15568028229731573,
        "consistency": 0.18931063396154302,
        "fluency": 0.13820702462507728,
        "relevance": 0.1283017575154754,
        "overall": 0.15287492459985286,
        "Generated": "315",
        "Gold": "A review of the literature on vanilla transformer and its implementation in a wide range of applications. A structural inductive bias helps to separate the information in hidden states of the Transformer (instead of using a single monolithic hidden representation) on modelling data with clear non-uniform structure. The authors propose an independent mechanism that divides hidden representations and parameters into multiple independent mechanisms. An experimental setup has various shortcomings, but the authors' approach to the problem of making vanilla Transformer better by handling different positions is interesting."
    },
    {
        "coherence": 0.24175365989067865,
        "consistency": 0.35628205845007904,
        "fluency": 0.13569031634322393,
        "relevance": 0.0600537126567514,
        "overall": 0.19844493683518324,
        "Generated": "316",
        "Gold": "This paper presents a method for iterative small molecule generation based on an autoencoder framework with graph neural networks. The authors explicitly acknowledge this on multiple occasions and point out why the differences matter. To strengthen this point, I would welcome if the authors could elaborate a bit more on the quantitative comparison in in Figure 2. The performance of their presented method increases with the size of the vocabulary. The MoLeR model is able to generate molecules similar to the training molecule distribution, from scratch or from a given scaffold. It shows better results than LSTM, JTVAE, CDDD-MCTS, etc."
    },
    {
        "coherence": 0.16256733425056746,
        "consistency": 0.24209601286807358,
        "fluency": 0.06644344439756582,
        "relevance": 0.06498748589616588,
        "overall": 0.13402356935309317,
        "Generated": "317",
        "Gold": "This paper focuses on the zero-mean Gaussian process with sample data generating process and the underlying function with a special case of the compositional functions, it would be better to extend it to general mean function. Theoretical work, it provides a novel lower bound for the GP posterior contraction rate when the true function has an additive structure. It is difficult to verify the proof methods in this journal due to heavy peer review loads this year."
    },
    {
        "coherence": 0.33618830523707066,
        "consistency": 0.3398651584665188,
        "fluency": 0.16434458320523768,
        "relevance": 0.07716924234744453,
        "overall": 0.22939182231406793,
        "Generated": "318",
        "Gold": "A variant of the indivisible item allocation problem. The authors study MMS and EF1 allocations, and the design of an approximation algorithm for optimizing the social welfare. A new variant of fair resource allocation problem: an agent utility is given by the maximum matching on the induced subgraph of his partition. They consider two fairness measures on the utilities received by each agent: max min share (MMS) - where the minimum utility of agents is maximized, envy free upto one item (EF1) - when the no agent envies others' allocation after removing up to one item."
    },
    {
        "coherence": 0.11941619057255425,
        "consistency": 0.2328110498434306,
        "fluency": 0.13593234798319875,
        "relevance": 0.08107166742487464,
        "overall": 0.14230781395601455,
        "Generated": "319",
        "Gold": "The authors propose a novel loss function for semi-supervised learning. It combines the cross-entropy loss, the within-cluster scatter as known from k-means, the distance between centroids and the margin between classes. The idea of the paper is to penalize maximize distances between pairs of samples from different clusters as determined by the classifier output as a proxy for unlabeled samples. The experiment section of this paper does not lend insight into what type of problems that are challenging for previous methods are addressed by the proposed approach."
    },
    {
        "coherence": 0.4256709721649623,
        "consistency": 0.47377919137093566,
        "fluency": 0.23383034564708174,
        "relevance": 0.17028087308760717,
        "overall": 0.32589034556764673,
        "Generated": "320",
        "Gold": "This paper focuses on the problem of identifying bad training data when the underlying cause is unknown in advance. Empirical results and comparison with baselines look interesting but it is my view that the paper still needs more work in particular around the experimental evaluation. The theory for worst case performances of valuation based approaches is motivating but it doesn't necessarily imply the method will fail as it's only a worst case analysis. The other drawback is that the DataSifter algorithm is highly empirical with no theoretical support. The authors criticize linear heuristic methods showing that worst case domination ratio of the methods are small."
    },
    {
        "coherence": 0.11354652110038023,
        "consistency": 0.19419515834027998,
        "fluency": 0.17566584702790883,
        "relevance": 0.0415811647803622,
        "overall": 0.1312471728122328,
        "Generated": "321",
        "Gold": "1. This paper presents a model for predicting the is-ancestor relationships of institutions based on their string names. To this end, it uses Set-Transformers to model the token overlap between the institution names. This use is nice but also not highly original. The experimental evaluation is on a single dataset only. Table 3: These examples show that string-based models are not sufficient for the task. It would be helpful to do an ablation study on your model, using different scoring functions as well."
    },
    {
        "coherence": 0.077643316665879,
        "consistency": 0.13575944288283961,
        "fluency": 0.3090674039773602,
        "relevance": 0.0259758434711664,
        "overall": 0.13711150174931133,
        "Generated": "322",
        "Gold": "This paper proposes Contextual HyperNetworks (CHNs) as an auxiliary model to generate parameters from existing data, and observations and other metadata associated with new feature to address cold start problem of new feature. Besides, it doesn’t need either re-train or fine-tune at prediction time. The motivation is clear and reasonable, and the proposed method with hyper-network is pretty interesting and attractive. However, experimental section has large improvement room. The paper proposed a few-shot meta-learning method for recommender system that uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features."
    },
    {
        "coherence": 0.07562959731159735,
        "consistency": 0.16211316158305544,
        "fluency": 0.1616444252190834,
        "relevance": 0.17982103492001378,
        "overall": 0.1448020547584375,
        "Generated": "323",
        "Gold": "This paper is a solid baseline for linear gradient descent. It does not provide unbiased comparisons with competitors, since all results are subject to extensive problem-specific hyperparameter tuning. The authors propose an automated tuning approach to the problem of global non-convex optimization with access only to function valuations. They propose an approach to automatically control the hyper-parameters of Directional Gaussian Smoothing (DGS) a recently proposed solution for the problem."
    },
    {
        "coherence": 0.20822284878686426,
        "consistency": 0.38453003759904253,
        "fluency": 0.32565402322449544,
        "relevance": 0.10374950724611592,
        "overall": 0.25553910421412956,
        "Generated": "324",
        "Gold": "This paper presents an efficient method for dataset distillation -- the goal is to extract/synthesize a small set of images that represent a dataset, so that deep learning models trained on the small set can achieve high (as high as possible) classification accuracy. The core contribution of this ppaer is to simplify and accelerate the conventional two-stage optimization procedure with TBPTT, where the gradients need to be propagated through a long chain. Instead, a kernel is used"
    },
    {
        "coherence": 0.29254618054157483,
        "consistency": 0.2316214163611016,
        "fluency": 0.3822900731508511,
        "relevance": 0.1267368062287815,
        "overall": 0.2582986190705773,
        "Generated": "325",
        "Gold": "Strengths: This paper proposes a framework to perform logical reasoning to answer questions. The authors describe a fact unit as a triple of (argument, predicate, argument). The authors also perform a detailed analysis that sheds more light on the internal workings of the model. ### Strengths - (From the previous review) All reviewers noted that the main strength of the work to be its good empirical results on the 3 benchmarks considered - this strength is corroborated with the additional results of competitive baselines and $FocalReasoner_DeBERTa$ This paper presents a novel graph reasoning model based on supergraphs constructed via fact triplets."
    },
    {
        "coherence": 0.0476365021791688,
        "consistency": 0.06157083660171235,
        "fluency": 0.31400554330333197,
        "relevance": 0.04984927853194972,
        "overall": 0.11826554015404071,
        "Generated": "326",
        "Gold": "This paper combines elements of two existing reinforcement learning approaches, namely, Deep Q-learning Networks (DQN) with Prioritized Experience Replay (PER) and Deep Deterministic Policy Gradient (DDPG). The paper contains almost no novelty as it crudely puts together three existing algorithms without showing sufficient motivation. Furthermore, as shown in the experimental sections, I don't see a evidence that the proposed algorithm consistently outperforms the baseline."
    },
    {
        "coherence": 0.18296728297139286,
        "consistency": 0.3854730445007978,
        "fluency": 0.3046071665896714,
        "relevance": 0.08477196568098126,
        "overall": 0.2394548649357108,
        "Generated": "327",
        "Gold": "This paper proposes a task-free continual learning algorithm in the context of online memory selection. The authors propose the notion of surprise and learnability seem well-suited to the current problem at hand. The computation of the Bayesian posteriors is also natural and the Gaussian assumption, together with conjugacy, results in efficient updates, which is nice. The InfoRS algorithm also seems natural for sampling among selective points with high information. Weaknesses: A new (information-theoretic) criterion to pick informative points and void outliers"
    },
    {
        "coherence": 0.23523835130514054,
        "consistency": 0.5406764456413347,
        "fluency": 0.10793952879385328,
        "relevance": 0.049954787975335584,
        "overall": 0.23345227842891603,
        "Generated": "328",
        "Gold": "A new image enhancement for low light images based on geometric and illumination consistency and a contextual loss criterion. This paper is a step in the field, and therefore not significant enough to permit acceptance in this venue. Image Enhancement using Unpaired-GAN based Translation Network (cycleGAN): A Novel Approach to Image Preservation. IEEE Transactions on Image Processing, 30:2340–2349, 2021. Weaknesses: Their model outperforms previous models for the benchmarks on the LOL and BDD datasets. It also improves object detection performance somewhat, but it was not compared with previous works. They need to address this issue and justify the use of cycle-consistency denied by recent GAN-based works."
    },
    {
        "coherence": 0.3098193061738798,
        "consistency": 0.41123008884635637,
        "fluency": 0.17375694361059077,
        "relevance": 0.050411171366981845,
        "overall": 0.23630437749945218,
        "Generated": "329",
        "Gold": "This paper proposes an alternative deep learning model for use in combinatorial optimization. The attention model is inspired by the Transformer architecture of Vaswani et al. (2017). Given a distribution over problem instances (e.g. TSP), the REINFORCE update is used to train the attention model. Experimentally, four different routing problems are considered. The authors show that the proposed method often outperforms some other learning-based methods and is competitive with existing (non-learned) heuristics. Next, I will touch on some strengths and weaknesses which I hope the authors can address/take into account."
    },
    {
        "coherence": 0.20442005613779698,
        "consistency": 0.421578033840186,
        "fluency": 0.06882537870730451,
        "relevance": 0.1638823944566659,
        "overall": 0.21467646578548835,
        "Generated": "330",
        "Gold": "A new method for counterfactual inference on time series data in healthcare, augmented by auxiliary confounding variables to reduce bias, is proposed by the authors. It is widely acknowledged that identifiability plays a pivotal role. In particular, in the methods based on neural networks, it is extremely difficult to address the identification issue. In light of this, instead it might be a better idea to provide some contrafactual bounds for the quantities of interest."
    },
    {
        "coherence": 0.24786699341121457,
        "consistency": 0.2672552464965389,
        "fluency": 0.10136531407433205,
        "relevance": 0.04596858852550031,
        "overall": 0.16561403562689647,
        "Generated": "331",
        "Gold": "Cons: This paper does not provide a solid overview of existing results on corrupted data and robust mean estimation. The theoretical results in this paper are incomplete and the experimental evaluation is insufficient. Theorems 1 and 2 have no formal proof, and there is no comparison to other state-of-the-art algorithms. Collaborative learning methods seem to have no solid theoretical understanding and it is unclear why the proposed algorithm build on it. The paper cites highly relevant papers, overclaims its results, and the way it is presented is misleading and confusing."
    },
    {
        "coherence": 0.21276426498398043,
        "consistency": 0.28358151127831716,
        "fluency": 0.3163445880013535,
        "relevance": 0.0842643862224483,
        "overall": 0.22423868762152488,
        "Generated": "332",
        "Gold": "The Simulation Transformer (SiT) is a novel transformer-based architecture for predicting particle dynamics that outperforms previous graph-convolutional and Transformer architectures in simulated environments. The aim of the paper is to provide an overview of the various aspects of the proposed paper and its contributions. Transformer is an extremely specific way to encode material type information as abstract tokens. The paper compares it to several non-transfomer based baselines, as well as a pure transformer."
    },
    {
        "coherence": 0.07401139112945211,
        "consistency": 0.19220134061251604,
        "fluency": 0.1522496546908842,
        "relevance": 0.06197559253351888,
        "overall": 0.1201094947415928,
        "Generated": "333",
        "Gold": "This paper is not ready to be published yet. I think the authors should consider comparing their method with the state of the art on consensus clustering which is not deep learning based. Iterates between clustering the images in the feature space of the network and updating the network weights to respect the clusters. In an experimental evaluation on a set of standard image clustering benchmarks they outperform previous work in most scenarios."
    },
    {
        "coherence": 0.10124966791734988,
        "consistency": 0.19891921962766967,
        "fluency": 0.10288276197163111,
        "relevance": 0.012021609235926347,
        "overall": 0.10376831468814425,
        "Generated": "334",
        "Gold": "Squeezeformer is a novel hybrid attention-convolution architecture for ASR through a series of extensive architectural studies and improvements over the Conformer architecture. Note, Conformer has been the de-facto architecture for speech processing tasks and SQUEZEFORMER simplifies the shows impressive improvements on the Conformers-CTC architecture. The authors also have open sourced the code and trained model checkpoints which should be supremely helpful to the community."
    },
    {
        "coherence": 0.14407230801959364,
        "consistency": 0.22027605270706746,
        "fluency": 0.25395640620958476,
        "relevance": 0.04332127274116625,
        "overall": 0.16540650991935302,
        "Generated": "335",
        "Gold": "This paper gives an algorithm to sample a distribution given access to pairwise comparisons from it. The technical contribution is to design a Markov chain whose stationary distribution matches the unknown one and then use the coupling from the past algorithm to obtain exact samples. The goal is to generate a sample from D given only the results of the pairwise Comparisons. The authors modify the CFTP strategy. Specifically, the transitions are modified so that the corresponding Markov chains have a uniform distribution. Then, rejection sampling is used to re-weight the sample in order to return to the target distribution over D. This paper explores an algorithmic problem on distributions when access model is not the standard independent samples."
    },
    {
        "coherence": 0.17859282639526042,
        "consistency": 0.2582079180690609,
        "fluency": 0.3229663623701008,
        "relevance": 0.07457318023678675,
        "overall": 0.2085850717678022,
        "Generated": "336",
        "Gold": "This paper tries to adapt the concept of averaging, well known is the game literature, to GAN training. In a simple min-max example the iterates obtained by gradient method do not converge to the equilibrium of the game but their average does. Overall originality is minor; projected significance is minor to medium. This paper evaluates two moving average strategies for GAN optimization. The basic idea seems to be reasonable. Moving average-based strategy would stabilize optimization process."
    },
    {
        "coherence": 0.1268792440609132,
        "consistency": 0.1718504670618782,
        "fluency": 0.21507290415728564,
        "relevance": 0.04070637482380811,
        "overall": 0.13862724752597128,
        "Generated": "337",
        "Gold": "I think the paper has two strengths. First, from the perspective of the legal NLP community, the dataset that the authors introduce is incredibly valuable for several reasons: 1. Legal NLP benchmarks are *extremely* difficult to build, given how expensive legal annotations are. This is a very high quality dataset, on US case law (also rare!). 2. Summarizing legal texts is not a non-trivial task which is core task that many lawyers do. Law students are required to summarize cases every day in class. Judges summarize cases in legal opinions. Lawyers summarize cases when constructing arguments in court and in legal briefs. Thus, this task could enable tools which have a meaningful impact. The authors present a new dataset of 9,280 expert-authored summaries from large-scale civil rights lawsuits that include three types of summary (tiny, short, and long). They experiment with state-of"
    },
    {
        "coherence": 0.21665741489513907,
        "consistency": 0.32921500757049704,
        "fluency": 0.24731338282509932,
        "relevance": 0.06666418309672363,
        "overall": 0.21496249709686477,
        "Generated": "338",
        "Gold": "## Pros - The paper is easy to read. ## Cons / Questions - I'm not sure if the term over-parameterization is well-defined in the paper. For training neural classifiers, the word \"over\" is clear because of the transition point of the double descending curve (for performance) within the regime of parameter space the paper studies. However, the experimental results cannot fully validate the authors’ claims."
    },
    {
        "coherence": 0.1915684971315718,
        "consistency": 0.1809581241683607,
        "fluency": 0.205915173565512,
        "relevance": 0.10051154532647527,
        "overall": 0.16973833504797997,
        "Generated": "339",
        "Gold": "Minor: This is a well-researched work on the long-tailed nature of deep learning. The authors are encouraged to simplify some notations. The organizations also can be reconsidered. Yes, the authors plan to release their code and data in the near future. Some important findings in the paper are based on the results of one single dataset. Some of the findings in this paper are supported with strong theoretical proof and empirical results."
    },
    {
        "coherence": 0.23840444894401702,
        "consistency": 0.38826313302012655,
        "fluency": 0.05459602872535905,
        "relevance": 0.04349541817859082,
        "overall": 0.18118975721702338,
        "Generated": "340",
        "Gold": "B] This paper provides multiple qualitative evaluation results. While it gets the point across, I still would have liked to see a quantitative evaluation. C] Another analysis that could have been done is to apply this approach and use several different pre-trained networks as feature extractor and check whether there is a decrease in false positives across all the networks, that would suggest that the method can truly make deep learning cautious across a wide variety of networks. This paper applies Conformal methods to multi-class classification. They effectively build an independent classifier for each class that estimates whether an example comes from that class within a certain confidence which is set before training time. In this way, they create meaning NULL predictions for an unidentified example, instead of the usual low-probability of an erroneous class."
    },
    {
        "coherence": 0.22643233064998286,
        "consistency": 0.24002277580834822,
        "fluency": 0.11707132053314562,
        "relevance": 0.03748919637616422,
        "overall": 0.15525390584191023,
        "Generated": "341",
        "Gold": "### Strengths This paper proposes a family of offline pessimistic learning algorithms for linear contextual bandit based on $ell_p (pgeq 1)$ confidence sets (called $widehatpi_p$). Among them, it also proves lower bounds for classes of linear context bandit problems indexed by $qin[1, infty]$. Furthermore, to support its claim, this paper also proposed a novel lower bound and complexity measure for linear context barit problems. ### Weaknesses"
    },
    {
        "coherence": 0.07414496042686762,
        "consistency": 0.18107982113074167,
        "fluency": 0.22536401255068375,
        "relevance": 0.02660635001333268,
        "overall": 0.12679878603040642,
        "Generated": "342",
        "Gold": "I am sorry to recommend rejection of the authors' proposal. This paper tests the hypothesis that a dual-domain cascade of U-nets outperforms single domain cascades. The results are straight forward, well-structured and the aims, methods and results and discussion are interesting, informative and clearly presented. Recommend rejection of a paper's proposal. A dual U-Net in both frequency and image domain to denoise low-dose CT images."
    },
    {
        "coherence": 0.16523497266700976,
        "consistency": 0.21152853622101475,
        "fluency": 0.12827077120196193,
        "relevance": 0.046665078556060634,
        "overall": 0.13792483966151176,
        "Generated": "343",
        "Gold": "This paper introduces an AI challenge where an AI agent needs to collaborate with a human-like agent to enable it to achieve the goal quicker. The main theme of the paper and research presented fits well ICLR. From my point of view the authors do an excellent job presenting the challenge, the multi-agent virtual platform (open source planned for the future) and all their functionalities (create agents that emulate human behaviours, the interface that supports evaluation, etc.) They also present a benchmark"
    },
    {
        "coherence": 0.14771858768584187,
        "consistency": 0.3166284361459403,
        "fluency": 0.19600350592974589,
        "relevance": 0.03523119642955485,
        "overall": 0.17389543154777073,
        "Generated": "344",
        "Gold": "I think overall I appreciate the idea behind the work. I think the work is quite novel, and it also connects to bodies of literature (hopfield networks -- attractors based and more mainstream GRU/LSTM nets). Here are some notes that I have: Denoising a recurrent neural net should be compared to other regularization techniques. Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising techniques."
    },
    {
        "coherence": 0.18830206208735353,
        "consistency": 0.20994048786166425,
        "fluency": 0.24091324323934474,
        "relevance": 0.07713623129422124,
        "overall": 0.17907300612064592,
        "Generated": "345",
        "Gold": "A joint polyp characterization and detection method based on a PCOS network with a classification subnet that merged global and local features. Comparison of the performance of the proposed method in a large dataset. Predicted attention heatmaps look not so accurate, but they can be useful for pathological classification. The architecture relies on the body of ResNet-50 and the head of FCOS. This paper aims to improve the classification accuracy of polyp pathology."
    },
    {
        "coherence": 0.2199236174286068,
        "consistency": 0.32174986633991187,
        "fluency": 0.23947549045195823,
        "relevance": 0.11594673827629003,
        "overall": 0.22427392812419172,
        "Generated": "346",
        "Gold": "The authors present a novel 2-stage technique to improve the classification accuracy of feedforward ANNs. In particular, after the feedforward pass, a so-called introspective stage occurs, the goal of which is to ascertain why the particular class label was provided rather than a different label. They then demonstrate how introspection can improve the generalizability of classifications to distributions that are shifted from the original training sets."
    },
    {
        "coherence": 0.23361785126893092,
        "consistency": 0.3362679998972384,
        "fluency": 0.2671906681697176,
        "relevance": 0.06989908416605815,
        "overall": 0.22674390087548627,
        "Generated": "347",
        "Gold": "A model-based approach with representation balancing (RepB-SDE) to cope with the distribution shift of offline reinforcement learning. The authors propose a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy in the representation space. The experiment part can be improved, e.g., including the state-of-the-art offline model-free baseline (Kumar et al., 2020; CQL)."
    },
    {
        "coherence": 0.20321793534829666,
        "consistency": 0.2621679333655924,
        "fluency": 0.3137012471425895,
        "relevance": 0.026027261455041296,
        "overall": 0.20127859432787998,
        "Generated": "348",
        "Gold": "I would have liked some more realistic, large-scale, multitask learning Multi-Hand Multi-Task Learning has a negative effect on the disentanglement quality of the learned latent representation. 3. The most important issue with this paper is the experimental setup. It is absolutely clear that the amount of information about the generative factors encoded in the latent embedding will correlate with all of the disengtanglement metrics used in this paper, regardless of whether the embeddding is actually disentangled"
    },
    {
        "coherence": 0.2492038245153606,
        "consistency": 0.32990775281828805,
        "fluency": 0.1829467569106071,
        "relevance": 0.05991586840048181,
        "overall": 0.20549355066118438,
        "Generated": "349",
        "Gold": "This paper proposes a CNN-based (specifically GAN-based) solution to tackle image synthesis using sparse coding concept. The proposed method utilises the generator from a generative adversarial network to decompose the original ML-CSC into two consecutive yet independent processes. It claims that the two functions are easier to learn. It also proves and adds image regularisation such as DIP into the framework."
    },
    {
        "coherence": 0.17015520507552376,
        "consistency": 0.1905237051023292,
        "fluency": 0.36991462638380973,
        "relevance": 0.10479642105683694,
        "overall": 0.20884748940462491,
        "Generated": "350",
        "Gold": "This paper describes a mathematical analysis of group convolutional neural networks, showing that they are universal approximators. To show this, the authors derived the ridgelet transform. This is a result in itself, which is further used in this paper to provide a constructive proof of universality. I see the beauty/elegance of the paper, but that is on math merits, not on ML principles per se. I believe the paper could be a great contribution to NeurIPs if it were to be made more accessible to a broader audience (like me, I like to learn from these papers, but this one was actually far out my comfort zone)."
    },
    {
        "coherence": 0.06396331385434639,
        "consistency": 0.146119082771703,
        "fluency": 0.11250109235938967,
        "relevance": 0.019257209024079538,
        "overall": 0.08546017450237964,
        "Generated": "351",
        "Gold": "I would suggest the authors turn down the claims in the paper. The experiments on 3D reconstruction is not well enough. The paper only presented one example of the final result (Fig.12/13), it would be better to: a) show more qualitative results, b) show quantitative comparisons with more baselines, c) the quantitative analysis on the geometric details, how good the method can capture the details comparing with the baselines. This paper presents a general approach to embed high frequency information into low-frequency data with a particular focus on improving the performance of virtual clothing. To address over-smoothing issues in the predicted meshes, authors proposed the texture sliding method that changes texture coordinates on each camera through the deep networks. The texture sliding neural network (TSNN) is trained using the ground truth offset computed for each camera and pose in Figure 4. Positive: The approach can be used to improve the quality of different architectures"
    },
    {
        "coherence": 0.09654379502883213,
        "consistency": 0.21025342106154118,
        "fluency": 0.0848209472055104,
        "relevance": 0.037406944766800385,
        "overall": 0.10725627701567103,
        "Generated": "352",
        "Gold": "This paper provides a public dataset of stimulated Raman histology and SRH imaging of brain tumors, and a benchmark for two computer vision tasks: multiclass histologic brain tumor classification and patch-based contrastive representation learning. The authors provide strong motivation for their work, and the data and code are all available (to reviewers at the moment, and to the public in the near future), and the appendix provides meticulous documentation of the data."
    },
    {
        "coherence": 0.07418193390989332,
        "consistency": 0.25519269598178146,
        "fluency": 0.11712966993909188,
        "relevance": 0.058079531375414065,
        "overall": 0.1261459578015452,
        "Generated": "353",
        "Gold": "A benchmark for hardware-aware neural architecture search (HW-NAS) and a benchmark for benchmarking hardware performance metrics such as energy costs and latency for six hardware architectures in this search space. The authors' datasets, which contain inference time and power usage measurements for several different hardware devices, should make it easier for researchers to experiment with NAS algorithms for finding better accuracy/inference time tradeoffs on a specific hardware device. However, the submission convincingly argues that many existing NAS benchmarks focus only on accuracy, or only provide very limited data about inference times."
    },
    {
        "coherence": 0.2750360934944445,
        "consistency": 0.3428274425610097,
        "fluency": 0.15416880649688217,
        "relevance": 0.11714552422335517,
        "overall": 0.2222944666939229,
        "Generated": "354",
        "Gold": "Learning one-hidden-layer neural networks with Gaussian mixture input in the teacher-student setting. This paper provides a precise theorem to guarantee the convergence behaviour of this paper. However, I still believe that the paper oversells its results and analysis, although in a much less strong manner (e.g. in the last sentence of the abstract) and that many concerns remain. For example, my concern on the tensor initialization has been partially addressed by the authors which showed that it performed as well as a random initialization close to the solution. I think the results in the paper are interesting and should be discussed in the introduction section."
    },
    {
        "coherence": 0.10409043709936472,
        "consistency": 0.23275371334461248,
        "fluency": 0.22731373579141256,
        "relevance": 0.0786872708425671,
        "overall": 0.16071128926948922,
        "Generated": "355",
        "Gold": "This paper is a short conference paper on deep neural networks. The authors identify correctly weaknesses in their method, i.e. overfitting of the networks. This paper can be largely improved in terms of language and form, e.g. no brackets around Figure references, Table 4 referenced, whereas only Table 1 is available. The Emergence of Train, Validation and Test Data is unfortunately unclear, 300 frames for training seems rather low in the context of deep neural Networks. The feature extractor selection is unclear, GoogLeNet and ResNet-18 seem to be outdated choices in 2021 and no details are given about the exact implementation of the feature extractionor."
    },
    {
        "coherence": 0.13772722890968764,
        "consistency": 0.25958616998163225,
        "fluency": 0.14567733500752786,
        "relevance": 0.06446866502513605,
        "overall": 0.15186484973099595,
        "Generated": "356",
        "Gold": "This paper presents a method to obtain weights for a knowledge graph scoring model for link prediction. The core idea is to formulate a linear program whose solution corresponds to a scoring method. Instead of incorporating all rules (there are typically many!) into the LP formulation, the authors propose a column generation approach. There isn't much pruning at all: only 50 candidate rules are generated per relation in the first place in the experimental study."
    },
    {
        "coherence": 0.17710504152430315,
        "consistency": 0.42051350017385575,
        "fluency": 0.15330443355291418,
        "relevance": 0.09844069151174553,
        "overall": 0.21234091669070465,
        "Generated": "357",
        "Gold": "This paper proposes a new black-box adversarial attack that assumes access to loss-oracle of the target model. It also provides a novel formulation of how the action space of perturbation should be searched by optimizing a surrogate reward function over a low-dimensional space comprising image block location and a PCA-based feature. This paper is clearly written. It proposes to use time-varying contextual bandits in order to improve the query efficiency and success rate which are not too far away from the current state of the art."
    },
    {
        "coherence": 0.265201626402543,
        "consistency": 0.27338494490367854,
        "fluency": 0.2338706840151341,
        "relevance": 0.05267655780336506,
        "overall": 0.2062834532811802,
        "Generated": "358",
        "Gold": "The idea is nice and reasonably novel in my opinion, but the paper has quite a few problems. The first problem is that the writing of the paper is awful. Already in the abstract there are several syntactical and semantical errors. It is actually fairly hard to read this paper, likely the idea is simple and one can understand it from the equations, but everything that's written pretty much makes the paper harder to read. The authors also write too strong claims for a scientific paper: many times they write that DiCGAN learns the user desired data distribution with finite data is not a possible not an interesting goal, the users should use \"approximates\" instead of \"learns\" and describe *how* it approximates it and what is lost. Finally, while the experiments are interesting, they're all on MNIST or aligned celeb-A in 64x64, and the samples are terrible."
    },
    {
        "coherence": 0.20747052466305407,
        "consistency": 0.30742427370406195,
        "fluency": 0.2932951767402816,
        "relevance": 0.0627264988505028,
        "overall": 0.21772911848947513,
        "Generated": "359",
        "Gold": "This paper aims to optimize a function in RKHS, subject to a certain soft-constraint (it can be violated in a sublinear manner). Then it argues that by carefully combining Gaussian processes, UBC, and primal-dual algorithms, a non-trivial algorithm can be designed. I feel the way different techniques are integrated is quite reasonable, and this overall sounds like a nice result. However, I have a few questions. 1. Is there any nice way to interpret the soft constraint formulation? 2. How does this work compare to Srinivas, Krause, Kakade, and Seeger 12? 3. Does submodular techniques seem to be needed to deal with those \"information gain\" things?"
    },
    {
        "coherence": 0.018736984260130634,
        "consistency": 0.07385422027915742,
        "fluency": 0.08895627022087237,
        "relevance": 0.030076166530125476,
        "overall": 0.05290591032257148,
        "Generated": "360",
        "Gold": "**Summary**: This paper studies the problem of learning cooperative behaviors in social dilemma problems for multi-agent deep RL with independent agents. The authors propose a simple loss that emphasizes the \"status quo\", which increases the emphasis on the one-step reward, to force agents to avoid short-term exploitation. To extend this idea to a temporally-extended sequential setup, the authors introduce a clustering algorithm that reduces the game into a matrix game with \"cooperate\" and \"defect\" options. This paper does not current convince me that the proposed solution SQLoss actually solves the cooperation problem (discussion below)."
    },
    {
        "coherence": 0.1299888363246885,
        "consistency": 0.17321461120695134,
        "fluency": 0.17149262426172357,
        "relevance": 0.09557833550099123,
        "overall": 0.14256860182358866,
        "Generated": "361",
        "Gold": "This paper proposes a differentially private and communication-efficient method to aggregate the client updates in federated learning. The method is based on a recently proposed compression technique relative entropy coding. The authors further modify this technique to satisfy differential privacy guarantees and perform various experiments to back their claims. The proposed method can achieve much more communication reduction at the cost of accuracy degradation. Empirical evaluation shows that the proposed method is inherently differentiallyprivate."
    },
    {
        "coherence": 0.30090104986431543,
        "consistency": 0.31736912676469387,
        "fluency": 0.3062838372272532,
        "relevance": 0.23826682114960243,
        "overall": 0.2907052087514662,
        "Generated": "362",
        "Gold": "Strengths: This paper presents an approach to distilling model-based planner trajectories into a policy to enable real-time execution on robotic systems. The paper discusses the merits of each approach and proposes Adaptive Policy Extraction (APEX), an approach that learns the policy from the expert using DAGGER and GPS. An auxiliary loss inspired GPS, which encourages the consistency between the expert controller and the learned policy, together with an adaptive weighting method, is added to reduce the multi-modal issue."
    },
    {
        "coherence": 0.1922929389885586,
        "consistency": 0.2792247339575426,
        "fluency": 0.17259852040116497,
        "relevance": 0.028043589516798008,
        "overall": 0.16803994571601602,
        "Generated": "363",
        "Gold": "A gradient-based method for Offline Model-Based Optimization, which involves processing a static dataset of designs X and function evaluations Y in order to solve the optimization problem X* = argmax_X f(X) for an unknown function f. A proposed bidirectional objective that encourages design-score pairs to be informative about the characteristics of the true objective function (the backward mapping). ### Strengths 1. This paper introduces the idea of neural tangent kernels into offline model-based optimization to address the out-of-distribution problem. Moreover, the authors propose a novel backward modeling, significantly improving the performance ..."
    },
    {
        "coherence": 0.1114105452893284,
        "consistency": 0.15083199691478802,
        "fluency": 0.34011449835903407,
        "relevance": 0.055926207796466294,
        "overall": 0.16457081208990418,
        "Generated": "364",
        "Gold": "This paper addresses active learning from a slightly different approach, with theoretical guarantees of integer optimisation problem (although not the performance in terms of model accuracy itself). This paper tackles the problem of active learning. In every iteration the subset of the unlabeled data pool that needs to be labeled is selected by posing it as an Integer Programming (IP) problem that minimizes the discrete Wasserstein distance. Generalized Benders Decomposition algorithm is used to solve this IP through relaxations. This paper proposes a (batch mode) active learning method"
    },
    {
        "coherence": 0.05244898010806554,
        "consistency": 0.1135608625782854,
        "fluency": 0.06815776658341614,
        "relevance": 0.016954081833554176,
        "overall": 0.06278042277583032,
        "Generated": "365",
        "Gold": "Summary: Theorem 1 seems to be incorrect. Assuming $T$ is large enough and $epsilon$ is small enough so that $sum_i=0T-1(1 - epsilion)i > 1$, and $p_Pi$ is a Dirac distribution at $pi_e$, then Theom 1 says $J(pi[e], R) > J_pi­e(p_e, R)$."
    },
    {
        "coherence": 0.10291578866789731,
        "consistency": 0.10458889110798153,
        "fluency": 0.4685423442182467,
        "relevance": 0.06086016260748963,
        "overall": 0.18422679665040378,
        "Generated": "366",
        "Gold": "### Strengths: - This paper presents a simple solution to improve the existing self-supervised representation learning by adding an additional branch to predict the rotations. Experiments are conducted on CIFAR-10, ImageNet, and two PhC datasets and the results show that the model with the additional branch can outperform the corresponding baselines. The story and the title are somewhat overclaiming. The authors bring the definition of equivariance in the introduction; however, can the authors be more specific what is the $T'_g$ in their framework? I do not see any post transformation on top of the model output on the newly added branch but just labels. ImageNet distribution, then eqn. (2) is a loss."
    },
    {
        "coherence": 0.26592508111703544,
        "consistency": 0.3074034813152186,
        "fluency": 0.3636414734457889,
        "relevance": 0.04795851918658539,
        "overall": 0.24623213876615707,
        "Generated": "367",
        "Gold": "This paper proposes using language as a form of embodied feedback for LLM-based robot planning. Three types of feedback, i.e. success detection, passive and active scene descriptions, are expressed in language to show how they improve the success rate for planning in simulated and real-world domains. This work extends prior algorithms for robotic planning with LLMs to incorporate feedback, which increases tasks success rate in presence of imperfect low-level control and a dynamic environment."
    },
    {
        "coherence": 0.10299651728694292,
        "consistency": 0.1642238047906397,
        "fluency": 0.47159137238708004,
        "relevance": 0.23068778136301685,
        "overall": 0.24237486895691987,
        "Generated": "368",
        "Gold": "This paper introduces a new dataset and benchmark for video compression, produced by new compression standards. The authors also provide subjective scores and results for various evaluated metrics. 3) The authors claim the video quality for FullHD resolution is more difficult to distinguish visually during subjective tests. Could you please provide references for the studies proving such metrics are inadequate? The authors present the largest compressed video dataset, which include new coding standards and subjective ratings."
    },
    {
        "coherence": 0.17688029865717791,
        "consistency": 0.1943303925315066,
        "fluency": 0.36015513621686074,
        "relevance": 0.09499656242427042,
        "overall": 0.20659059745745392,
        "Generated": "369",
        "Gold": "This paper builds and extends the theory of infinite-width neural kernel computations. It proposes an explicit expression of the dual kernel for polynomial activation. It uses random sketching techniques to accelerate NTK approximation, which is an extention to previous work which was only applicable for ReLU activation . This paper proposed an approximate kernel computation method that leverages some tools from functional analysis (Hermite decomposition) which serves well the purpose of the paper. The theoretical results are sound and the numerical results show some superiority compared to traditional methods used for NTK computation. A method is proposed to compute the double kernel of the derivative of activations without knowing the activations."
    },
    {
        "coherence": 0.22601272543629874,
        "consistency": 0.526906931769515,
        "fluency": 0.10530403231770599,
        "relevance": 0.12974984615347476,
        "overall": 0.2469933839192486,
        "Generated": "370",
        "Gold": "Weaknesses: Lack of novelty and related work. The related work section should be more specific (e.g. $epsilon$ in Line 185, how it is randomly sampled, with uniform random or normal random? And what is $P_X'$ in line 186? This paper tackles the task of detecting and localizing anomalies in time series data. It follows a transformer approach where time series are automatically masked, then the transformer attempt to reconstruct the a..."
    },
    {
        "coherence": 0.2150519285993215,
        "consistency": 0.40614332090793526,
        "fluency": 0.31175387260030507,
        "relevance": 0.08915635336014203,
        "overall": 0.255526368866926,
        "Generated": "371",
        "Gold": "A theoretical analysis of the gap between volume rendering and point-based SDF modeling and a solution by leveraging sparse points from SfM and utilizing patchmatch to provide geometry consistent supervision. This method is evaluated on DTU and BlendedMVS and achieves improvements compared with existing methods. We propose an explicit surface point SDF supervision where the surface points are estimated with SFM and ii) photometric consistency from classic multi-view stereo. The authors of this manuscript propose Geo-NeuS, a novel neural implicit 3D reconstruction method."
    },
    {
        "coherence": 0.09616780087251495,
        "consistency": 0.14840298513334357,
        "fluency": 0.47745881992784595,
        "relevance": 0.07350777990389967,
        "overall": 0.19888434645940103,
        "Generated": "372",
        "Gold": "This paper provides three sign language datasets which by itself is already useful (sign languages are largely under-resourced/under-studied). The authors also provide in-depth analysis of the extraction process, data attributes and a few baseline accuracy numbers using a graph-convolution network. Overall the paper is very well structured, easy to follow and critical for sign language ML research in my opinion. (1) SignCorpus is the largest ever collected sign language pretraining dataset. As the authors show this is very useful for finetuning neural network models for multilingual sign identification and finger spelling. I wonder if this can be used to speed up the data collection process itself (some kind of life-long learning paradigm)."
    },
    {
        "coherence": 0.09699379809622631,
        "consistency": 0.1719928604920853,
        "fluency": 0.1821546580979601,
        "relevance": 0.03295599748525553,
        "overall": 0.12102432854288181,
        "Generated": "373",
        "Gold": "I found the paper to be well-written and I found it to be easy to follow. The claims and the propositions in this paper are reasonable, additional experiments with various settings support the effectiveness of proposed method. 2. The time complexity of proposed BC algorithm has been lowered compared with the other methods while preserving the accuracy in top-k recovery accuracy. However, there is only Figure 3 demonstrating the improving speed in one dataset, which I found limiting. Adding a few more experiments showing the training time comparison will make the time complexity improvement more convinincing."
    },
    {
        "coherence": 0.22459247062363477,
        "consistency": 0.3521210277018869,
        "fluency": 0.27171565629652944,
        "relevance": 0.17805305430230048,
        "overall": 0.2566205522310879,
        "Generated": "374",
        "Gold": "This paper addresses the problem of learning reusable hierarchical skills in sequential decisions. It proposes a partially amortizing model with a latent forward dynamic and task reward function composing these modules in the latent skill space. The authors propose an extension of the Dreamer method (Hafner et al., 2019) to incorporate high-level planning via the cross-entropy method and temporally extended skill policies."
    },
    {
        "coherence": 0.08122842057972755,
        "consistency": 0.11346015679565524,
        "fluency": 0.11177565846433125,
        "relevance": 0.08621077463968893,
        "overall": 0.09816875261985075,
        "Generated": "375",
        "Gold": "BVAE-TTS is a non-autoregressive speech synthesis model (outputting spectrograms), with two key advantages over the aforementioned models: (a) no autoregressive teacher model is required, as in FastSpeech, which simplifies training, and (b) fewer parameters are needed. The key strength of this paper is the architecture is new. Can the author explain the difference? Also need to cite that paper because it published in June and very related."
    },
    {
        "coherence": 0.13725944722978672,
        "consistency": 0.35287287284661983,
        "fluency": 0.3793316136778653,
        "relevance": 0.0782117143426164,
        "overall": 0.23691891202422205,
        "Generated": "376",
        "Gold": "This paper is a promising paper that shows how global image labels can be leveraged in radiography to help train more accurate segmentation networks when manual ground-truth data is expensive to acquire. However, the novelty of this paper is in the application towards chest radiography, not its method. The paper is promising, but it lacks the experimental rigour that this paper has been able to achieve. This paper shows how Global Image Labels Can Be Utilized in Radiography to Help Train More Accurate Segmentation Networks."
    },
    {
        "coherence": 0.11418029060333408,
        "consistency": 0.16519665871520042,
        "fluency": 0.3557932096260717,
        "relevance": 0.1497639591572048,
        "overall": 0.19623352952545275,
        "Generated": "377",
        "Gold": "A novel algorithm for multi-batch reinforcement learning with nearly minimax optimal regret and $O(H + loglog K)$ batch complexity. Additionally, authors propose polynomial time implementation, and provide a lower bound on batch complexity of order $Theta(H/log_A(K) + log log K). This paper studies the episodic reinforcement learning problem modeled by finite-horizon Markov Decision Processes (MDPs) with constraints on the number of batches. The technical contribution includes a near-optimal design scheme to explore the unlearned states and a computationally efficient algorithm to explore certain directions with an approximated transition model. However, there are a few aspects that affect the quality of this paper."
    },
    {
        "coherence": 0.20551548466923508,
        "consistency": 0.3932000291715051,
        "fluency": 0.4184758481484431,
        "relevance": 0.08517335684064381,
        "overall": 0.2755911797074568,
        "Generated": "378",
        "Gold": "A tensor decomposition leveraging contrastive self-supervised learning based on data augmentation for positive samples and the unbiased estimation of negative samples, which can align with downstream classification tasks. (clarity) The authors' approach is well motivated and the novelty of the authors' contributions is made immediately clear in Section 1. It's a good paper, but it's not a very well-motivated one. The experiments are extensive and promising, and the results on various datasets show its efficiency."
    },
    {
        "coherence": 0.05659528929110479,
        "consistency": 0.05277371057200577,
        "fluency": 0.3140842906579138,
        "relevance": 0.055769105518745955,
        "overall": 0.11980559900994259,
        "Generated": "379",
        "Gold": "This paper presents a benchmark dataset focused on source code processing (understanding and generation) tasks. It describes each of the tasks and datasets, and provides strong baseline models based on pretrained language models and the corresponding results. This is a relevant contribution that will likely have a strong impact and be used by the community. This paper proposes a comprehensive benchmark dataset for program understanding/generation, consisting of 10 tasks across 14 datasets where 5 datasets are newly introduced."
    },
    {
        "coherence": 0.32267687809458206,
        "consistency": 0.4291352032846246,
        "fluency": 0.17928476961166498,
        "relevance": 0.09196273554950202,
        "overall": 0.2557648966350934,
        "Generated": "380",
        "Gold": "This paper explores a very important question in dynamical system identification of how to make recurrent neural networks learn both long-term and short-term dependencies without the gradient vanishing or exploding limitation. They suggest using piece-wise linear RNNs with a novel regularization technique. I found the paper interesting. My main concern has to do do with the motivation of learning line or plane attractors. For example, it could be that simply adding l2 regularization on all of the weights would also lead to better performance (rather than the specific regularization proposed), or perhaps the better performance arises from some other mechanism (undiscovered)"
    },
    {
        "coherence": 0.3481502935429609,
        "consistency": 0.3424242747561698,
        "fluency": 0.1993766707853487,
        "relevance": 0.07363946435567359,
        "overall": 0.24089767586003824,
        "Generated": "381",
        "Gold": "## Minor points - I think it would be a good idea to mention the dependency of the method on the observed auxiliary variable (e.g. time) is a necessary condition for the application of this approach in the abstract of the paper. I would be curious to know how the authors expect these assumptions to restrict the applicability of their method to different scenarios. #### Minor comments * Page 5, last paragraph: typo 'which we which we' * Appendix page 43, 4th line in B.1: double punctuation 'with negative slope of 0.2. .' * Page 7, paragraph _Sparsity_: It is stated that the minimal graph is found that 'properly fit[s] the data'. Is it that the data is fit up to a small error $epsilon$ which is supposed to be lower than the regularization?"
    },
    {
        "coherence": 0.24425682448910715,
        "consistency": 0.2506666470362449,
        "fluency": 0.40141692641684,
        "relevance": 0.07568287350165072,
        "overall": 0.2430058178609607,
        "Generated": "382",
        "Gold": "Weaknesses: This paper only shows the performance of their approach on the ImageNet dataset. More experimental results are required. A method to train neural networks over synthetic data. The work collected a large collection of 21K OpenGL programs for easily generating diverse set of synthetic images. These data samples can help pretraining while avoiding the unfair biases and other contaminations that arise from natural datasets. The collected programs can be used for pretraining in both supervised and unsupervised fashion. This paper proposed a dataset for learning representations without access to real images, the synthetic data can help reduce the concerns about using real images in training"
    },
    {
        "coherence": 0.2581577756488907,
        "consistency": 0.1902948734737855,
        "fluency": 0.2852039660560685,
        "relevance": 0.277456315603189,
        "overall": 0.25277823269548344,
        "Generated": "383",
        "Gold": "Cons: The main problem with this paper is that the Q* value now lives in a much higher dimensional space, levelling any advantage a subsequent heuristic might give. Statements as 'Although this work is superficially similar to ours, it differs a lot in the details' The authors are diligent about trying to explain what type of regularities are exploited by each of UVFAs and GPI, and how this can be combined in USFAs. Top of p.4 says that GPI generalizes well when the policy pi(s) does well on task w’."
    },
    {
        "coherence": 0.2025436014509788,
        "consistency": 0.2611007581232343,
        "fluency": 0.2700044787458653,
        "relevance": 0.06394756719187934,
        "overall": 0.19939910137798944,
        "Generated": "384",
        "Gold": "A semi-supervised approach to salient object detection using a combination of pseudo-label prediction and adversarial noise reduction training. Strengths - The idea of using temporal information for generating semantically matching pseudo labels is an interesting approach. While optical flow base warping is generally used for temporal consistency, there are errors and drawbacks in optical flow based warping. However, using non-local attention based similarity mapping removes those errors for matching features to generate pseudo labels while reducing noise in the overall pseudo label generation."
    },
    {
        "coherence": 0.04056864669957047,
        "consistency": 0.07540924743284076,
        "fluency": 0.27715169365179537,
        "relevance": 0.03075767350372353,
        "overall": 0.10597181532198253,
        "Generated": "385",
        "Gold": "I didn't quite understand the model setup and the motivation. In my understanding, the model learned the latent domain labels via k-mesans. And, then, based on this pseudo domain labels, model is fine-tuned with Residual Adapter applied. Was I understood the model set-up and motivation? d. With PACS dataset, you have experimented with the model (k-means+RA)."
    },
    {
        "coherence": 0.16729005778650077,
        "consistency": 0.2135684019060608,
        "fluency": 0.24439826682427326,
        "relevance": 0.012869525926068895,
        "overall": 0.15953156311072594,
        "Generated": "386",
        "Gold": "CellPose: a generalist algorithm for cellular segmentation. Nature Methods, 100–106 (2020). Title: Cell Pose: A Generalist Algorithm for Cell Tracking. However, the results are not very good and the authors use a lot of tools from the literature to achieve their goal. They claim that in the conclusion that the proposed This is a very good application for detecting holograms in images."
    },
    {
        "coherence": 0.3497514571204075,
        "consistency": 0.3281522401539659,
        "fluency": 0.3129087013334776,
        "relevance": 0.10104827256175901,
        "overall": 0.2729651677924025,
        "Generated": "387",
        "Gold": "This paper proposes a variational inference method named Support Decomposition Variational Inference (SDVI) that targets probabilistic programs with stochastic support. The main idea is that each variational component is optimized independently for each sub-program. Finally, the approach is compared to baseline algorithms on a Gaussian model with stocchastic control flow, on an infinite Gaustian mixture model (IGMM) with a stochistic number of clusters, and on inferring the kernel structure of a Galois Process."
    },
    {
        "coherence": 0.4502983273124059,
        "consistency": 0.47936190054716576,
        "fluency": 0.23445352246691722,
        "relevance": 0.11498375628776543,
        "overall": 0.31977437665356356,
        "Generated": "388",
        "Gold": "A must-know baseline for vision-and-language research: using CLIP as a visual encoder. I am doubtful whether this paper opens any new directions to the community other than the general tendency I've mentioned. The paper conducts extensive experiments to study the generalizability of CLIP pretraining across various tasks. However, there are also some notable weaknesses: - It appears that CLIP only works well with grid-like features extracted from convolutional neural networks (i.e., no experiment on regional features and worse results when combined with a Visual Transformer)."
    },
    {
        "coherence": 0.3822479469356605,
        "consistency": 0.45430558267226756,
        "fluency": 0.30836909010420155,
        "relevance": 0.13293709619140026,
        "overall": 0.31946492897588247,
        "Generated": "389",
        "Gold": "The authors propose a Frank-Wolfe-based algorithm for solving partially stochastic composite minimization problems over general convex constraints which admit a regularized linear minimization oracle. This general algorithm is applied to the case where the static part of the objective is a finite sum. There are numerical results on least squares with L1-norm constraint and matrix factorization. a new accelerated gradient method for solving a smoothed dual problem over a compact converx set 2. The main requirement of this technique is that this 'minibatching' is done in parallel."
    },
    {
        "coherence": 0.230763726276178,
        "consistency": 0.20817121988071716,
        "fluency": 0.351807607446625,
        "relevance": 0.06886586100498661,
        "overall": 0.2149021036521267,
        "Generated": "390",
        "Gold": "This paper leverages the recent breakthrough of Masked AutoEncoders (MAE) for the domain of robotics. It shows that visual knowledge can be transferred from different domains such as ego4d into specific robotics domains thanks to MAE pretraining. strengths Evaluation of pretrained vision encoders using masked autoencoderons for real world robotics tasks, with variations of everyday scenes, as well as novel objects. # Strengths Strong accept. - While considerable work has investigated visual pre training to improve robot policy learning and for example behavior cloning, the key thing about this work compared to prior works is that it can benefit from pre training on just a big pile of images, with no assumptions about them other than that they cover a wide, diverse set of images. # It is indeed also notable that they show improvements of using ViT-Large, a 307M parameter model. For those"
    },
    {
        "coherence": 0.3117259750102978,
        "consistency": 0.45022747106046973,
        "fluency": 0.18404121879181318,
        "relevance": 0.07489417696416775,
        "overall": 0.25522221045668714,
        "Generated": "391",
        "Gold": "A metric-learning loss on top of a MLP adapter for finetuning on target datasets is a promising approach for improving the group strength of the foundation models. The idea of \"contrastive adapter\" is novel. The paper is well written and easy to follow with a clear goal (improving group robustness in foundational models). The method is easy to implement and to apply to a large variety of scenarios."
    },
    {
        "coherence": 0.300089318106964,
        "consistency": 0.19709584820051831,
        "fluency": 0.38728593095132297,
        "relevance": 0.23034856674607707,
        "overall": 0.2787049160012206,
        "Generated": "392",
        "Gold": "Summary: Strengths: - a clear explanation of the problem - clear explanations of training and resulting hyperparameters - Section 2 criticises Mikolov & co.'s skip-gram model on the grounds that it introduces noisy entities because it ignores context structure. Yet, the skip gram model is a very strong work. - A clear description of the problems and their solutions."
    },
    {
        "coherence": 0.02579458157682296,
        "consistency": 0.1129825889040553,
        "fluency": 0.17101246677578125,
        "relevance": 0.060146101442585205,
        "overall": 0.09248393467481118,
        "Generated": "393",
        "Gold": "The paper is a good introduction with a very clear formulation of the problem at hand and a clear motivation for the proposed method. However, the experimental results are not convincing enough to justify the use of this method in a large-scale cross-sectional MRI study. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][30][36][37][38][39][40][61][62][65][66][67][68][69][70][71][76][77][78][79][80]"
    },
    {
        "coherence": 0.1648061743900884,
        "consistency": 0.25770126882996647,
        "fluency": 0.1862051538132872,
        "relevance": 0.22281126159298725,
        "overall": 0.20788096465658232,
        "Generated": "394",
        "Gold": "Cons and Questions: This paper provides a new perspective and used mathematically tools of Hermite expansion etc to inspire and proposed new method for variance reduction which prevent dying unity problem in Hierarchical VAE (in term of implementation) seems very simple. Thus, the method looks easy to adopt and has solid analysis on variance reduction. The authors focus on reducing the variance of the functions parameterizing the variational distribution of each layer using a layer-wise smoothing operator based on the Ornstein-Uhlenbeck semigroup (parameterized by a parameter $rho$). epsilon_1$ has a very small probability of a 1-ep = z_i-1$; in the complement set $mu_i-2$, it is possible to achieve a low KL in the original ELBO, by using an $mi_i+1$ which only has sensible values in the"
    },
    {
        "coherence": 0.22277467552015887,
        "consistency": 0.2184503015264388,
        "fluency": 0.1904199194149669,
        "relevance": 0.07586586789134912,
        "overall": 0.17687769108822843,
        "Generated": "395",
        "Gold": "A framework for unsupervised learning of graphs. The goal is to build graph representation using an encoder that is useful for downstream tasks such as graph classification. The decoder $D$ is tasked with minimizing the conditional entropy of the perturbation $Delta A = A-tildeA$ when given the two representations $H=E(X,A)$, but also perturbation"
    },
    {
        "coherence": 0.07726075258866756,
        "consistency": 0.09468757195628527,
        "fluency": 0.5050207402893293,
        "relevance": 0.13278959982822786,
        "overall": 0.20243966616562747,
        "Generated": "396",
        "Gold": "This paper proposes a framework for detecting misclassification errors, based on regression of target confidence scores and application of a Gaussian process for uncertainty in predicted confidence scores. However, it is not demonstrated whether RED would outperform other confidence scoring and OOD detection methods mentioned in the related work section, such as temperature scaling (or the related method ODIN, proposed in Liang, S., Li, Y., and Srikant, R., 2017. Enhancing the reliability of out-of-distribution image detection in neural networks."
    },
    {
        "coherence": 0.33423805248265176,
        "consistency": 0.2144398115769868,
        "fluency": 0.23951640584923464,
        "relevance": 0.09637880641547986,
        "overall": 0.22114326908108828,
        "Generated": "397",
        "Gold": "This paper proposes a bootstrap framework to study the generalization problem of deep learning, by decomposing the traditional test error into an ‘Ideal World’ test error plus the gap between. Empirically, it demonstrates that such gap (soft-error) is small in supervised image classification for typical deep learning model architectures. However, the main claim of the paper is a pure experimental claim. This paper defines a metric of generalization gap between the ideal world and real world via 'bootstrap approximation', and seeks to use his gap to explain some phenomena."
    },
    {
        "coherence": 0.36554392925538626,
        "consistency": 0.45669670848193455,
        "fluency": 0.3892966147315785,
        "relevance": 0.11795731175156764,
        "overall": 0.33237364105511674,
        "Generated": "398",
        "Gold": "This paper studies how to ensure optimization monotonicity of learning an accurate dynamics model for MBRL. They derive a lower bound for the derived policy performance improvement that depends on the one-step dynamics prediction error of the current model, constraint by the model shift. This paper focuses on the monotonic improvement for model-based reinforcement learning which is an extremely important problem due to the inherently entangled nature of the multi-level optimization problem - policy optimization and model learning. The primary objective is to show that $||VM1|pi_1 - VM2|pu_2||geq C$ which can guarantee monotonic improvements under the updating dynamics."
    },
    {
        "coherence": 0.37767065543157613,
        "consistency": 0.2941552972861847,
        "fluency": 0.41257368833255564,
        "relevance": 0.05293497221749085,
        "overall": 0.2843336533169518,
        "Generated": "399",
        "Gold": "I was not able to interpret what is being said. A curriculum learning in RL that combines goal-oriented RL and episodic RL in order to achieve a sample efficient exploration-exploitation trade-off. ### Contributions - The paper designs an intrinsic reward to encourage exploration while avoiding local optima policies. - Exploitation is leveraged through a region-based memory. This paper presents a novel and interesting idea that is sufficiently supported by the empirical experiments. The challenges that the new methods attempts to address and important and central to good performance in deep RL, thus the paper's results would be of interest to RL experimental"
    },
    {
        "coherence": 0.12294084016519802,
        "consistency": 0.27049228134915015,
        "fluency": 0.21265344009624612,
        "relevance": 0.07852290605180282,
        "overall": 0.17115236691559926,
        "Generated": "400",
        "Gold": "This paper is well-written and clear. The authors do a good job clearly stating the motivation for the work and differences from prior work. The key idea is to fit a distribution over policies, observation models, and transition models using an EM-like method. Offline experiments on a healthcare dataset show that the method learns interpretable decision dynamics, recovers biased internal models, accurately predicts actions relative to prior methods."
    },
    {
        "coherence": 0.07806948152950598,
        "consistency": 0.23458252152998738,
        "fluency": 0.06004121195998897,
        "relevance": 0.11563624325488062,
        "overall": 0.12208236456859073,
        "Generated": "401",
        "Gold": "This paper presents a novel solution to an interesting problem - when KBs are automatically expanded user feedback is crucial to identify incorrect and missing entity attributes and relations. More specifically, in the case of entity identity uncertainty, enabling the user feedback as part of the entity resolution mentions, appears novel and important. The paper makes an algorithmic contribution. The contribution is limited from the perspective of human computation. The experiment uses simulated user feedback for evaluating the method."
    },
    {
        "coherence": 0.047204634040955505,
        "consistency": 0.13233882112862233,
        "fluency": 0.2365462608495637,
        "relevance": 0.11153312769636257,
        "overall": 0.13190571092887604,
        "Generated": "402",
        "Gold": "#### Weaknesses - This work is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution as it draws heavily from prior methods. Unlike previous works that consider unsupervised/self-supervised pre-training for few-shot learning, this work provides some theoretical justification for its method. A theoretical analysis of the relationship between supervised learning and self-supervised learning in the context of few shot learning."
    },
    {
        "coherence": 0.16079050290214517,
        "consistency": 0.31665279328784773,
        "fluency": 0.1512759949575657,
        "relevance": 0.026800269514809936,
        "overall": 0.16387989016559215,
        "Generated": "403",
        "Gold": "This is an interesting paper on a topic with real-world application: anomaly detection. The author's organization is, at times quite confusing: - the introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. - section 2.1 starts quite abruptly with he two Lemmas 7 & Theorem 3 (which, in fact, is Theoriem 1). The second, intuitive part of 2.1 is extremely helpful. This paper presents a new approach to active anomaly detectors. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections."
    },
    {
        "coherence": 0.12416310870929922,
        "consistency": 0.30572325997361127,
        "fluency": 0.26960069821894267,
        "relevance": 0.06312835232191094,
        "overall": 0.19065385480594102,
        "Generated": "404",
        "Gold": "This paper proposes a new method - Attention-Based Clustering (ABC) that incorporates context to learn a metric in the form of an embedding and kernel similarity layer (predefined). The embedded layer uses repeated self attention blocks (SABs) from the transformer architecture and is theoretically shown to make the clusters more condensed. I am concerned that the idea seems very incremental and simply uses a combination of techniques that have been proposed in the literature. I would also recommend adding a few more data sets to the results."
    },
    {
        "coherence": 0.09001773116768558,
        "consistency": 0.1229059862492682,
        "fluency": 0.05070311475566239,
        "relevance": 0.04499261829384444,
        "overall": 0.07715486261661515,
        "Generated": "405",
        "Gold": "This paper introduces the \"Confident Adaptive Language Modeling\" framework, which contains very detailed study about the source of errors of doing early exiting during the decoding process of Transformer based language model, and proposes a method to calibrate well about local and global predictions to overcome these errors. Several empirical results, on text summarization, machine translation and Squad demonstrated the usefulness of the proposed method."
    },
    {
        "coherence": 0.12574774238530215,
        "consistency": 0.20886350666571454,
        "fluency": 0.17245751259934144,
        "relevance": 0.04363228680123007,
        "overall": 0.13767526211289707,
        "Generated": "406",
        "Gold": "This paper proposes a novel architecture and regularization technique for RNNs, where the hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. The authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks. This paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown."
    },
    {
        "coherence": 0.0766704725989454,
        "consistency": 0.20840733087618127,
        "fluency": 0.15383686802190039,
        "relevance": 0.072946209007248,
        "overall": 0.12796522012606876,
        "Generated": "407",
        "Gold": "This paper presents a learning-based approach to find the optimal boundary values for domain-decomposed PDE solvers. The major contribution is the application of TAGConv to solving PDEs, and a new numerical method to compute the loss defined on the spectral radius of the difference matrix. This paper only tested their method on Helmholtz equation, which is the major limitation. But the authors also have mentioned this limitation and further validation remains for future works."
    },
    {
        "coherence": 0.08071519032876646,
        "consistency": 0.16217517132116147,
        "fluency": 0.07789322494047936,
        "relevance": 0.062068817941904816,
        "overall": 0.09571310113307803,
        "Generated": "408",
        "Gold": "I really like the main aim of the paper for I think XAI deserves better evaluation metrics. However, I have some concerns that explains my vote. First of all, I am not fully convinced that the assumption of restricting to similar examples with the *same* class is really good for giving a relevant explanation. I believe that a good explanation will need both positive and negative examples, particularly near the decision frontier. Therefore this tempers the interest of the proposal made in this paper. Second, I had a hard time figuring out how and when the third test (with subclasses) can be used, and how it has been implemented in the presented experiments."
    },
    {
        "coherence": 0.22573390970866541,
        "consistency": 0.2456945947504176,
        "fluency": 0.06798785079946315,
        "relevance": 0.04667199621106207,
        "overall": 0.14652208786740206,
        "Generated": "409",
        "Gold": "A 3D mesh stylization method that performs both appearance and geometric styles by optimizing with a differentiable rendering pipeline. It adopts a more physics-aware rendering model that explicitly models geometry (normal map), BRDF (diffuse, roughness, specular), and lighting (SG), which is claimed to generate more realistic results (better shading appearance and details) and allows for editing such as relighting. Results are compared with Text2Mesh and achieves better performance."
    },
    {
        "coherence": 0.26293487783040187,
        "consistency": 0.3930562413993843,
        "fluency": 0.0733212173848929,
        "relevance": 0.12480366342693455,
        "overall": 0.21352900001040342,
        "Generated": "410",
        "Gold": "Weaknesses: 1. Lack of ablations: The authors' proposed method is comprised of multiple moving parts (e.g., the redefined post-processing function $Phi$, the masking functions, and the aggregation of multiple attribution maps), and it's not clear to me exactly which component(s) led to the improvements over previous methods. 2. Influence of the ReLU in aggregations: A new attribution based DNN explanation method that distills input features using weak and extremely positive contributor masks. It combines the advantages of both global IG and local FG approaches via a new sequential feature distillation algorithm, distilling irrelevant features from the input. However, none of the comments (or blames) made on the prior work makes sense to me. I will elaborate in detail."
    },
    {
        "coherence": 0.16005821732083167,
        "consistency": 0.207255474380331,
        "fluency": 0.06918069850049395,
        "relevance": 0.04151508296907052,
        "overall": 0.1195023682926818,
        "Generated": "411",
        "Gold": "This paper provides a theoretical analysis on the existence of over-smothing in transformer models such as BERT. It proposes a hierarchical fusion method as a solution to Oversmoothing. Can authors provide further experiments with different datasets or applications to demonstrate the effectiveness of the proposed method? The authors revealed that layer normalization in the transformer blocks contributed to the Over-SmoothING issue both theoretically and empirically. Moreover, the authors proposed to improve the over smoothing issue by Hierarchical Fusion Strategies."
    },
    {
        "coherence": 0.14870806785020804,
        "consistency": 0.19308325243436766,
        "fluency": 0.07225316748968745,
        "relevance": 0.07026973043482232,
        "overall": 0.12107855455227137,
        "Generated": "412",
        "Gold": "U-WILDS is a solid step towards the use of real-world datasets for benchmarking unsupervised domain adaptation algorithms, and would be a valuable contribution to the conference. The authors introduce an extension to the new, but popular, data shift benchmark WILDS data sets called U-WHILDS. The main finding is that most of the techniques performed poorly in more realistic domain adaptation problems. This paper proposes an extended version of the popular, but commonly used, U-HILDS benchmark dataset by augmenting the different domain data with additional unlabeled examples."
    },
    {
        "coherence": 0.1443205437724719,
        "consistency": 0.19760935922226566,
        "fluency": 0.06041012000466291,
        "relevance": 0.06752930460437244,
        "overall": 0.11746733190094323,
        "Generated": "413",
        "Gold": "This paper provides an algorithm for training neural networks in the over-parameterized regime. The authors go to great lengths to show that under good implementation which exploits various structural properties (low rank in particular), the runtime complexity of the algorithm should make it practical. It does not provide empirical validation of the method, or an implementation. The proposed algorithm can find the global solution within the time that is subquadratic in the network width. My first concern is regarding the intuition of this paper. The author claims in the introduction that “it is increasingly evident that first-order methods are becoming a real bottleneck for many practical applications” because the convergence rate is typically slow, i.e., requiring $mathrmpoly(n, L, log(1/epsilon))$ to converge, while second-order method enjoys a much faster convergence rate ($log(1 / m)$, while"
    },
    {
        "coherence": 0.3325083738012046,
        "consistency": 0.4061264034942181,
        "fluency": 0.09267654703976688,
        "relevance": 0.05753714054595999,
        "overall": 0.22221211622028741,
        "Generated": "414",
        "Gold": "This paper is not ready for publication. The exposition is not at all clear and needs substantial rewriting. Additionally, the evaluation done in the paper isn't well-justified. Furthermore, I expected to see actual impact of the retrieved results on downstream QA performance of a system like Chen et al.'s, or Clark and Gardner 2018. Even if you have a slightly better ranking of retrieved paragraphs, it's not clear to me that this will improve performance, if the downstream method is properly calibrated to handle multiple paragraphs. A retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight"
    },
    {
        "coherence": 0.1220153119574791,
        "consistency": 0.17323767262737025,
        "fluency": 0.10274054840621732,
        "relevance": 0.07691237769698638,
        "overall": 0.11872647767201326,
        "Generated": "415",
        "Gold": "This paper introduces a new dataset for multi-agent reinforcement learning in tracking tasks. The authors present not only tracking tasks but also tasks conveyed by mobile agents, enabling cooperative, competitive, and mixed tasks to be played with ease. Contributions include: A new OpenAI Gym environment for the partially observable two-team target coverage problem. A new Multi-Agent Tracking Environment (MATE) for Target Coverage Control Problems."
    },
    {
        "coherence": 0.08942671169029573,
        "consistency": 0.13090352288068238,
        "fluency": 0.14520435756572492,
        "relevance": 0.026167729641809096,
        "overall": 0.09792558044462803,
        "Generated": "416",
        "Gold": "I think it is an interesting paper, but I still have some concerns about the significance of the new task, the novelty of the technical contributions, and the experiments as well. The action labels are ambiguous sometimes. As shown by the case in Figure 2, the action “put pot on oven” may have multiple plausible moving trajectories over several timestamps, and may finally lead to many plausible positions of the pot on the oven. All these futuristic future frames are reasonable since they reflect the uncertainty of the action labels."
    },
    {
        "coherence": 0.07232004006068053,
        "consistency": 0.1479172476179056,
        "fluency": 0.07469711058077581,
        "relevance": 0.041931472729523245,
        "overall": 0.0842164677472213,
        "Generated": "417",
        "Gold": "This paper studies why optimization-based adversarial attacks have close to zero target transferability in attack real-world ASR pipelines. The control experiments are carefully executed and provide clear takeaway insights. # Weaknesses - Lack of in-depth root causes analysis on the findings. The paper provides some discussion in each ablation study. However, the current discussion does not go beyond describing the observations of the experimental results. For measurement papers, the readers are more interested (and will be more appreciated) if an in -depth analysis of the fundamental reasons why such feature affects the transferability issue. This is not an easy task, where additional small-scale experiments may also be required as the authors generate hypotheses."
    },
    {
        "coherence": 0.223803114218315,
        "consistency": 0.42073457407992715,
        "fluency": 0.17080485327112283,
        "relevance": 0.08150239226250326,
        "overall": 0.22421123345796704,
        "Generated": "418",
        "Gold": "A novel method for debiasing neural networks by generating synthetic data in a manner which confuses the network while being close (in $l_2$ norm) to the original training data. A method for OOD generalization is proposed. To prevent a model (Explorer Model) from learning \"shortcut features\" (simple features which do not work in OOD settingspurious correlations), an autoencoder is trained, which tries to fool a pretrained model (Blocked Model). The explorer model is then trained on the reconstructed image, from which some of the shortcut features were hopefully removed by AE to fool the Blocked Model."
    },
    {
        "coherence": 0.18247455898876935,
        "consistency": 0.2686001320165692,
        "fluency": 0.11248715767853923,
        "relevance": 0.04021126653079544,
        "overall": 0.1509432788036683,
        "Generated": "419",
        "Gold": "This paper proposes and analyzes an algorithm for reward maximization in a Multi-Armed Bandit, with the additional fairness constraint that each arm should be pulled for at least a fraction of time. Results about the asymptotic satisfaction (or not) of the fairness constraints depending on the relationship between $A_i$ and the suboptimality gap of each arm are given, as well as results about the gap-dependent and gap-independent regret of the studied algorithm under a modified reward with the threshold."
    },
    {
        "coherence": 0.21768302289946703,
        "consistency": 0.36409190042136136,
        "fluency": 0.04855392441293057,
        "relevance": 0.07181782047905638,
        "overall": 0.17553666705320384,
        "Generated": "420",
        "Gold": "This paper proposes interesting approaches to review popular static word embeddings. The authors try to reconstruct many interesting concepts and ideas in different fields like geography that covers spatial relationships and corresponding philosophy. (Main Contributions) This paper tries to study if we can learn about the human's process of generating new ideas or concepts with the help of generative models. However, it does not provide a good enough solution to the problem to meet the bar of the ICLR. This paper studies an interesting problem, to analyze structure evolution from the geometry in encoding space."
    },
    {
        "coherence": 0.07656903852979911,
        "consistency": 0.18210967435530134,
        "fluency": 0.18420136747855353,
        "relevance": 0.035751744128149986,
        "overall": 0.11965795612295099,
        "Generated": "421",
        "Gold": "This paper proposes a method of compressing neural radience fields by learning mappings from latent codes to model parameters such that both distortion/reconstruction quality and the rate get minimized. While maintaining the same level of quality, this method is able to compress NeRF models for more efficient sender-receiver transmission. This paper proposed to add entropy penalized reparametrization (Oktay et al. (2020)) technique into Nerf (Mildenhall ed. ( 2020)), with some improvements by extending the nerf to multiple scenes. However, the multiple scenes experiments are not performed well to demonstrate the effectiveness of the proposed method."
    },
    {
        "coherence": 0.06710448708307629,
        "consistency": 0.11651786628999596,
        "fluency": 0.2805542917562262,
        "relevance": 0.0983301192665088,
        "overall": 0.14062669109895182,
        "Generated": "422",
        "Gold": "This paper provides a very interesting perspective on the optimization process of SGD. The authors provide several examples for which SGD behaves in an unintuitive way, e.g. converging to a global maximum while using GD on the population loss of the same objective will not. This raises many interesting questions regarding the effect of noise in optimization and is a novel contribution as I understand it. The experimental section is also nice and"
    },
    {
        "coherence": 0.10792455178466542,
        "consistency": 0.2072227650301003,
        "fluency": 0.15276131429468234,
        "relevance": 0.07246649442716104,
        "overall": 0.13509378138415226,
        "Generated": "423",
        "Gold": "This paper compares the performance of a disembodied third person camera vs. place the camera on the robot's hand/gripper. The authors find that the hand camera improves generalization and training performance in the cases where a hand camera still reveals enough information to complete the task. When the hand Camera does not reveal enough information, the third Person Camera is still needed and the authors propose to use an information bottleneck to reduce the amount of information used from the third person Camera, thereby improving generalization even when it's needed."
    },
    {
        "coherence": 0.11338470121157894,
        "consistency": 0.20704604367800086,
        "fluency": 0.31589022565954733,
        "relevance": 0.03002439198013272,
        "overall": 0.16658634063231495,
        "Generated": "424",
        "Gold": "This paper addresses the problem of predicting links and time-stamps in a temporal knowledge graph, and proposes a novel neural Hawkes model that uses the continuous neural hawkes formulation as its basis. Key assumption the authors make is that the object entity interacts (forms an temporal link) with a predicate s only based on past links involving the same Is the proposed model significantly better than other baseline methods? Can you run the significance test on the results? Yes, the paper is well written and the model is well motivated. The experiments show improvement. The results from different models are very close to each other. A graph Hawke's Neural Network for Event and Time Prediction on Temporal Knowledge Graphs"
    },
    {
        "coherence": 0.20411169152937375,
        "consistency": 0.739386629592045,
        "fluency": 0.3154258694624563,
        "relevance": 0.05329652548057616,
        "overall": 0.3280551790161128,
        "Generated": "425",
        "Gold": "The TAP-Vid dataset is a benchmark for long-term arbitrary point tracking tasks. The proposed task, i.e., tracking any point (TAP), can benefit the development of embodied agents. A novel baseline method TAP -Net is proposed to deal with the problem and gets superior performance on the proposed TAP Vid series point tracking datasets in terms of occlusion accuracy, average thresholded position accuracy, and average Jaccard thresholds."
    },
    {
        "coherence": 0.06890546332888386,
        "consistency": 0.08702148367736029,
        "fluency": 0.31468198262080643,
        "relevance": 0.04354842496308948,
        "overall": 0.12853933864753503,
        "Generated": "426",
        "Gold": "Pros: This paper addresses the problem of event sequence prediction in a contrastive self-supervised learning framework. However, the authors have minor contributions in these individual methods. The authors propose a random slices subsequence generation strategy (CoLES) for contrastive learning in event sequences. The proposed CoLES algorithm is a data augmentation strategy that randomly selects subsets from the full event sequence from each user. This algorithm is tested with several publicly available datasets:"
    },
    {
        "coherence": 0.1148576252790223,
        "consistency": 0.16837993555547287,
        "fluency": 0.3467774833802348,
        "relevance": 0.0671411003495309,
        "overall": 0.1742890361410652,
        "Generated": "427",
        "Gold": "New tool for inferring underlying properties of partially observed spiking neural networks (generalization of the fully observable solution by Rene et al), based on mean field modeling on net effect of unobserved neurons This paper is clearly written and the authors define the posed problem nicely. The work is technical sound and the experiments are worked out thoroughly. The proposed latent model is an interesting approach to bridge the gap between pure statistical models and more biologically interpretable models. However, there are several weaknesses: ### Weaknesses 1. The authors test their setup only on one parameter set, it is not clear that the results are robust to different network configurations. The authors share therefor the same inductive biases whereas the other three models could be advantageous on differently generated data."
    },
    {
        "coherence": 0.04799697029187719,
        "consistency": 0.12074169919775479,
        "fluency": 0.27511857520035676,
        "relevance": 0.10299920119571193,
        "overall": 0.13671411147142518,
        "Generated": "428",
        "Gold": "A novel meta-learning method that decouples feature extraction from feature aggregation & few-shot reasoning within the given task. The authors conduct experiments to show that their method is competitive with a few other methods from the literature on a number of datasets. I recommend a reject for this paper, on the grounds that the architecture design needs further analysis (as explained above). Minor details: 1. In Section 4.2, maybe recalling the definition of sufficient statistics would clarify lemma 2. 2. Typo in Eq 4: r instead"
    },
    {
        "coherence": 0.19518102286568462,
        "consistency": 0.35221580078610587,
        "fluency": 0.21685649010611438,
        "relevance": 0.0504436869464251,
        "overall": 0.2036742501760825,
        "Generated": "429",
        "Gold": "Summary: DN-CFR is a novel MC samping strategy. It is based on a neural network function approximation algorithm. It can be applied to a large number of games, and can be used for evaluation. - It does not seem necessary to predict cumulative mixture policies (ASN network). One could train a mixture policy network to directly predict the current policy along with a cumulative regret policy. \"Care Algorithm Design\" is subclass of \"care algorithm design\", which means that it is designed in a way that minimizes memory requirements"
    },
    {
        "coherence": 0.12496025442887763,
        "consistency": 0.28734760424922445,
        "fluency": 0.05783452428009029,
        "relevance": 0.1258397614071307,
        "overall": 0.14899553609133076,
        "Generated": "430",
        "Gold": "Pros This paper takes one important issue of current speech synthesis area: TTS adaptation to new voice. Its multi-phonetic-level acoustic condition modeling approach seem technically new and interesting, &lt;b&gt;...&nbsp;. ...and focused on the task of low-resource, robust, and low-dimensional speaker adaptation. The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the encoder during pretraining and adaptable to new recordings at fine-tuning/inference."
    },
    {
        "coherence": 0.05516755211640911,
        "consistency": 0.21654669572547836,
        "fluency": 0.11228026112009185,
        "relevance": 0.05607565253554075,
        "overall": 0.11001754037438001,
        "Generated": "431",
        "Gold": "Impact and Recommendation This is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios get the method off the ground. The paper proposes a modification of the PPO algorithm which can accommodate a single task demonstration with the goal of faster learning in sparse-reward tasks. There are 4 tasks proposed by the paper, inspired by Animal-AI Olympics. The proposed method outperforms those baselines."
    },
    {
        "coherence": 0.11181879641972886,
        "consistency": 0.14799760239445833,
        "fluency": 0.3227361546044029,
        "relevance": 0.05273064822158312,
        "overall": 0.1588208004100433,
        "Generated": "432",
        "Gold": "Summary: Pros -------- * It is not clear what is the novelty of the theoretical results in this paper when compared to the regret bounds by Chowdhury & Gopalan (2019), who provide both frequentist and Bayesian bounds when the transitions and rewards are assumed to be sampled from Gaussian processes or sampled according to a GP. [...] The current paper mentions a $HSAsqrtT$ bound in [...] A model-based reinforcement learning algorithm named MPC-PSRL. Theoretically, the authors provide regret analysis of the proposed algorithm. [...]"
    },
    {
        "coherence": 0.1033730831137996,
        "consistency": 0.15755858373416676,
        "fluency": 0.1609214236195076,
        "relevance": 0.047200472171021086,
        "overall": 0.11726339065962377,
        "Generated": "433",
        "Gold": "This paper presents a fully integrated pipeline to diagnose common thoracic diseases using a recurrent neural network model dedicated to natural language processing (specifically a an attentive LSTM model). The paper reads well and the methodology is relatively clear. The results of the system are not well analysed or compared fairly with the literature. Only a single example of a system output is shown with no discussion of cases where various elements of this system fail or limitations."
    },
    {
        "coherence": 0.11062151225387623,
        "consistency": 0.30944360518983605,
        "fluency": 0.14542024534117898,
        "relevance": 0.0609425072783491,
        "overall": 0.1566069675158101,
        "Generated": "434",
        "Gold": "This is a highly novel paper which tackles an excellent problem: how to properly tune regularization parameters for common algorithms. The standard approach to tuning ridge-regression, LASSO, or the elastic net methods is Grid-Sample Complexity, rather than example-sample complexity. This paper seems to be theoretically sound and follows a logical progression. Section 2.1 and Lemma 2.1 seem to be related to the work in [1]. Section 4 does not seem to address a useful problem."
    },
    {
        "coherence": 0.08253221146829147,
        "consistency": 0.12279330153804234,
        "fluency": 0.20063001673167152,
        "relevance": 0.025602426085581078,
        "overall": 0.10788948895589659,
        "Generated": "435",
        "Gold": "This paper introduces a new family of non-negative Bregman divergences for density ratio estimation with flexible models that aims at solving the train-loss hacking problem. The contribution is foremost theoretical, but includes an experimental validation on benchmark problems. A new loss/objective based on the Bregmann divergence's empirical approximation while modeling the density ratio function $r*$ by a flexible hypothesis family. The authors propose a simple yet practical and principled algorithm for DRE, providing a Non- Negative Correction, with the Non-Negative Correction."
    },
    {
        "coherence": 0.0887332902126715,
        "consistency": 0.1367787943601966,
        "fluency": 0.31194150467205156,
        "relevance": 0.09607409234320995,
        "overall": 0.15838192039703242,
        "Generated": "436",
        "Gold": "This is a very good paper. It is easy to follow and has a lot of good examples. It also has some limitation of classic optimization of the lewis game (i.e. the lack of feedback). I would have been curious to add even a toyish experiment to test the optimization procedure, and obtain an empirical insight of the system dynamics. This is an excellent paper. I would like to see more examples of cooperative language acquisition."
    },
    {
        "coherence": 0.08060207726187853,
        "consistency": 0.23863040572067415,
        "fluency": 0.2264650993636017,
        "relevance": 0.09639764331952179,
        "overall": 0.16052380641641906,
        "Generated": "437",
        "Gold": "The Active-Passive SimStereo dataset evaluates the generalizability of 10 stereo matching algorithms on active stereo images. This paper proposes a study of domain transfer and generalization abilities of different state-of-the-art deep learning-based stereo architectures when presented with active, instead of passive, stereo images, and provides insights on limiting and positive features of competing architectures using their dataset. The authors report an ablation study of fine-tuned stereo methods on the ActivePassive StereoSim dataset."
    },
    {
        "coherence": 0.10077046478897554,
        "consistency": 0.3054460361454139,
        "fluency": 0.28137662518259,
        "relevance": 0.033369384957005345,
        "overall": 0.1802406277684962,
        "Generated": "438",
        "Gold": "This paper proposes a self-supervised idea for unsupervised anomaly detection. Specifically, this framework enables high-performance AD without any labels via SRR, which is an ensemble approach to propose candidate anomaly samples that are refined from training. This way allows more robust fitting of the anomaly decision boundaries and also better learning of data representations. Labeled data, which are not the case with most AD methods (see Chandola et al. (2009) and the recent reviews by Ruff ed al. (2021)). the IEEE International Conference on Computer Vision. 2015. [2] Beggel, Laura, Michael Pfeiffer, and Bernd Bischl. \"Robust anomaly detector in images using adversarial autoencoders.\" arXiv:1901.06355 (2019)"
    },
    {
        "coherence": 0.14770073033761863,
        "consistency": 0.24131939544492997,
        "fluency": 0.24513313390973296,
        "relevance": 0.043129931714138425,
        "overall": 0.16932079785160503,
        "Generated": "439",
        "Gold": "Personalized PageRank Vectors with Differential Privacy in Graph Representation Learning with a Sensitivity-Bounded Version of PushFlowCap and a DPPushFlowCap Algorithm. The paper studies an interesting problem, and is relatively well written. However, there are several major issues with the paper. The main technical idea is quite nice, and pretty natural. The sensitivity analysis of the Push-Flow and PUSHFLOWCAP algorithms is technically interesting, though not particularly involved. The statements in the final results are somewhat complicated since the utility analysis holds under some conditions. These statements would become stronger if a lower bound could be shown. The authors could simplify the notation and present it in a way that the symbols are easier to parse."
    },
    {
        "coherence": 0.13212728473658308,
        "consistency": 0.39423439601851884,
        "fluency": 0.10212512495502037,
        "relevance": 0.07101646036000892,
        "overall": 0.1748758165175328,
        "Generated": "440",
        "Gold": "I like the idea of the paper and its results, while there are some unclear parts. I am not sure if one can directly compare the GLD and GLD bounds in the paper with those in [Negrea et al., 2019] . However, the first terms in RHS in Theorem 4.1 and Lemma 3.2 are different since they bound different quantities. This paper demonstrates a new discrete data-dependent prior to the PAC-Bayesian framework, and prove a high probability generalization based on Floored GD. The empirical results showed that the presented bound could be non-vacuous due to the $eta C_etha$ multiplicative factor if the learning algorithms cannot achieve nearly zero training error."
    },
    {
        "coherence": 0.05320135402477311,
        "consistency": 0.1366218427019951,
        "fluency": 0.10951929389560881,
        "relevance": 0.0466439736436938,
        "overall": 0.0864966160665177,
        "Generated": "441",
        "Gold": "Pros: This work provides a novel extension to the state-of-the-art approach (SPARTA) to solving Dec-POMDPs, a co-operative variant of partially observable stochastic games. Its main idea relies on the segmentation of action-observation histories into more manageable public-private factors and training belief models to predict the belief on a trajecotory."
    },
    {
        "coherence": 0.06312892972283489,
        "consistency": 0.1555403922993342,
        "fluency": 0.2702470846070369,
        "relevance": 0.027330545978024076,
        "overall": 0.12906173815180752,
        "Generated": "442",
        "Gold": "This paper proposes a new cross-entropy loss function to improve the transferability of adversarial attacks. It focuses on enlarging logit margins which consequently reduce saturation and enable longer optimization and iterations. This paper is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods. This method needs more effort for tuning or need combining logit calibrations to achieve better performance. This can make the method less attractive to the community."
    },
    {
        "coherence": 0.03283517751085328,
        "consistency": 0.10968121355447952,
        "fluency": 0.1516001233710284,
        "relevance": 0.05139223636663899,
        "overall": 0.08637718770075005,
        "Generated": "443",
        "Gold": "Pros: This is a very good paper. Cons: The method does not implement the optimal transport map. The Wasserstein gradient is discretized via Euler methods to obtain the proposed algorithm. The authors describe a statistical methodology to compute this gradient flow to the target distribution. They also prove a bound for it and the simulations are promising. However, the authors may need to rephrase these results as an introduction to the background knowledge of this area, so that the novel theoretical contributions can be highlighted."
    },
    {
        "coherence": 0.029039999251053952,
        "consistency": 0.07611134741134344,
        "fluency": 0.2705608406785246,
        "relevance": 0.007192943445344388,
        "overall": 0.09572628269656659,
        "Generated": "444",
        "Gold": "I am leaning towards rejection of the paper. Overall, while the paper contains some interesting results, overall the presentation and flow can be improved. --- After rebuttal: The authors made several improvements to the flow and structure of this paper , as suggested in my review . Overall , although the paper has a strong theoretical analysis framework , it seems to be a novel approach to the problem of convergence of gradient flows of two-layer neural networks ."
    },
    {
        "coherence": 0.08872968654269164,
        "consistency": 0.19206701222975756,
        "fluency": 0.23675604767856215,
        "relevance": 0.024550809734751513,
        "overall": 0.13552588904644072,
        "Generated": "445",
        "Gold": "Prompt Tuning-Based Transfer Learning for Lifelong Few-Shot Language Learning Using a Pre-Trained Language Model. Empirical Results Show Superior Results Over Existing Methods Like EWC and MAS when evaluated on text classification, NER, and summarization tasks. This paper aims to formally define the lifelong Few - Shot Language Learning problem and benchmark a strong pre-trained LM (T5) on 3 tasks (NER, Text Classification, and Summarization) over 9 different datasets"
    },
    {
        "coherence": 0.06411800223805385,
        "consistency": 0.13614086982888476,
        "fluency": 0.2945493971731235,
        "relevance": 0.02908251800109993,
        "overall": 0.1309726968102905,
        "Generated": "446",
        "Gold": "This paper proposes a new benchmark for natural language understanding: GLUE. Models will be evaluated based on a diverse set of existing language understanding tasks which encourages the models to learn shared knowledge across different tasks. The authors empirically show that models trained with multiple tasks perform better than models that focused on one specific task. They also point out existing methods are not able achieve good performance in this dataset and request for more general natural language understand system. The author should provide more detailed analysis and interpretable explanations for the results as opposed to simply stating that the overall performance is better. This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLE will be an impactful resource for the NLP community."
    },
    {
        "coherence": 0.04650431330547194,
        "consistency": 0.08707764056474514,
        "fluency": 0.20889036723448778,
        "relevance": 0.033451764150377074,
        "overall": 0.09398102131377048,
        "Generated": "447",
        "Gold": "The physical model is well explained, but the motivation and motivation of the proposed method need to be better explained. The main weakness is the data employed, which is a rather simple simulation in a healthy setting. This is probably enough to prove the concept, but authors should explain more clearly how different these simulations are from real data, wht differences can be expected when switching to real images, and what is required to actually do that."
    },
    {
        "coherence": 0.1198721024243989,
        "consistency": 0.16394421099033726,
        "fluency": 0.33334908902263477,
        "relevance": 0.05396634204879943,
        "overall": 0.1677829361215426,
        "Generated": "448",
        "Gold": "A new benchmark for the community event forecasting task on continuous-time dynamic graphs, this paper proposes a joint model composed of graph neural networks and marked temporal point process. The authors propose to factorize the joint prediction problem into three easier conditional probability modeling problems. The TPP related works cite only two related works, while the literature is quite rich in this field as described in [28] and [29] ."
    },
    {
        "coherence": 0.1411369743104201,
        "consistency": 0.29797711403247107,
        "fluency": 0.22747368881111976,
        "relevance": 0.12704580181143588,
        "overall": 0.1984083947413617,
        "Generated": "449",
        "Gold": "### 4, Typo or misconfiguration? This paper describes a technique for reducing the size and computation of a Transformer model by projecting and factoring weight matrices. Experiments on MT, summarization, and language modeling show improved results over competing techniques, and even over standard Transformers, despite using significantly fewer parameters and less computation. The core “dictionary” technique isn’t really explained at a high level before the paper plunges into the details. DictFormer: a Modification of the Transformer Architecture by replacing attention layers and layers in its Feed-Forward Networks across all of its blocks with learned shared dictionaries."
    },
    {
        "coherence": 0.1305979860940259,
        "consistency": 0.23735554816336799,
        "fluency": 0.11321737588467536,
        "relevance": 0.12503984889362127,
        "overall": 0.1515526897589226,
        "Generated": "450",
        "Gold": "A well-written and easy-to-follow paper on RL methods for estimating poisoning attacks, and a good introduction to heuristic algorithms for transferring a bi-level model to an optimization problem. This is a very good paper, with a strong empirical basis and an empirical comparison to more involved methods than Random and Random DOPE. The baseline method is too weak and does not show how good DOPE really is. The empirical results are thoroughly done. The behavior policy is close to the evaluation policy policy policy. It would be interesting to study larger values of epsilon to see how the results change. Page 3 has some typos. \"Behavior policy is parameterized by theta\""
    },
    {
        "coherence": 0.18457200110129146,
        "consistency": 0.257627359999423,
        "fluency": 0.10841823811033915,
        "relevance": 0.04493164406824378,
        "overall": 0.14888731081982434,
        "Generated": "451",
        "Gold": "I'm always concerned when papers compare their novel approach to a baseline that is obtained by castrating their own approach -- this essentially only proves non-inferiority. Yes, the MRF-Unet model may improve segmentation performance compared to Unet, but it is unclear based on Fig. 4 whether the model is really better, as there is very large overlap between Unet and MRF prior model results."
    },
    {
        "coherence": 0.047943943973776944,
        "consistency": 0.0756475829685759,
        "fluency": 0.0740456923320055,
        "relevance": 0.016298510275138713,
        "overall": 0.05348393238737426,
        "Generated": "452",
        "Gold": "I have read the authors' rebuttal. My decision stays unchanged. In my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for ICLR. Additionally, the authors did not update their submission to reflect the changes. The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks. While the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence. Additional justification for why (or when) to use this should be described."
    },
    {
        "coherence": 0.12846495595238794,
        "consistency": 0.27213731713788775,
        "fluency": 0.11928574447685672,
        "relevance": 0.04088107517066951,
        "overall": 0.1401922731844505,
        "Generated": "453",
        "Gold": "The authors present a topology analysis improvement to GCN, using persistent homology to capture global information regarding the topology of the graph. The authors conduct several experiments, from graph to node classification, and also introduce two novel data sets to exemplify the importance of topology. In most cases, the proposed method outperforms other baseline methods, as well as other topology aware methods. Review the paper. It is technically sound and relatively novel as it introduces persistent homologie, which is more powerful than the WL test from the view of topological awareness (Theorem 2), and provides an a rational framework to incorporate it. Comments: The authors use the DeepSets embedding function as in section 4.1 to embed different graph signals into high-dimension space. However, the paper contains no empirical study to validate the choice."
    },
    {
        "coherence": 0.07384219981716218,
        "consistency": 0.1299590319209771,
        "fluency": 0.13406277053335497,
        "relevance": 0.05611933851793666,
        "overall": 0.09849583519735773,
        "Generated": "454",
        "Gold": "A graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks. It incorporates a denoising component based on graph wavelet transforms. The authors also provide new results (i.e., graph generation tasks) to address the Q.4 in my comments. A novel method to learn graph embedding in a encoder-decoder framework for graph classification and social recommendation. A de-noising step in wavelet domain to remove the amplified noise."
    },
    {
        "coherence": 0.1145094898992509,
        "consistency": 0.18912159841326964,
        "fluency": 0.20081740401298723,
        "relevance": 0.07440912575740351,
        "overall": 0.14471440452072784,
        "Generated": "455",
        "Gold": "This is a very good paper. I would like to see a more detailed evaluation of the proposed method. The tasks of model interpretation and model explanation (justifying the predictions made by a given model) seem to be used interchangeably. The authors propose a novel method, called Consensus, using an ensemble of deep learning architectures, to study the interpretability of models, when ground truth of interpretations is not available."
    },
    {
        "coherence": 0.11378560799057982,
        "consistency": 0.24214526551910384,
        "fluency": 0.17757878302551322,
        "relevance": 0.02166281923369669,
        "overall": 0.1387931189422234,
        "Generated": "456",
        "Gold": "This paper aims to improve the generalization power of meta-learning. It claims to propose an effective strategy named meta-dropout, which is applied to the transferable knowledge generalized from base categories to novel categories. However, after reading this paper, I felt the contribution claimed is not supported well and thus, the contribution of this paper is not very clearly stated. The detailed reasons are as follows: 1. In section 3, the authors formulated episode-based meta learning and pretrain fine tune based meta -learning, and form a unified meta learning framework. 2. The authors claim to have three contributions: 1. Utilize the idea of meta learning to integrate two very different streams of few-shot learning methods, i.e., the episodic meta- learning-based and pre-train finetune-based few - shot learning. 3. Propose a simple yet effective strategy, named meta dropout,"
    },
    {
        "coherence": 0.30754972620711957,
        "consistency": 0.2330742184605831,
        "fluency": 0.12664542691080763,
        "relevance": 0.04147845126963666,
        "overall": 0.17718695571203674,
        "Generated": "457",
        "Gold": "I think you should cite \"Neural Sequence Chunker\" by J. Schmidhuber and Unsupervised Real-Time Control through Variational Empowerment by M. Karl et al. I believe the paper has a very nice core idea, and a well thought out --------- Event prediction (specifically CE). Though the paper does say in words what D_st and D_ch are (which is much appreciated), the fact that their comparison is a reasonable way to define a predictable event is not clear to me."
    },
    {
        "coherence": 0.14795912345828102,
        "consistency": 0.24180997507145613,
        "fluency": 0.25395614744026507,
        "relevance": 0.08354023631595822,
        "overall": 0.1818163705714901,
        "Generated": "458",
        "Gold": "### pros - The authors found that the diffusion model can actually be divided into a first half and a second half, with the first half being used to roughly generate the noisy images, and the second half being utilized for denoising. The authors also found that solving the first part with VAE is more effective than solving it with diffusion, and they verified this experimentally. The author's findings also mean that 80-90% of the total diffusion steps can be replaced by a single pass VAE, which would also make the synthesis more efficient."
    },
    {
        "coherence": 0.29347260697160366,
        "consistency": 0.41676060136886073,
        "fluency": 0.29503145749665355,
        "relevance": 0.07018955218637304,
        "overall": 0.26886355450587274,
        "Generated": "459",
        "Gold": "This paper is a good introduction to relational representations for reward learning, but the empirical results are too small-scale and too small to apply to real-robotics tasks. This paper uses Symbolic Reward Machines (SRMs) to define symbolic states, transitions, and rewards. It uses adversarial imitation learning to classify expert- and agent-trajectories. It's a very good paper, but it's not a perfect fit for the real world."
    },
    {
        "coherence": 0.07349268807141451,
        "consistency": 0.2298604470848005,
        "fluency": 0.1245087672688352,
        "relevance": 0.04695212657993689,
        "overall": 0.11870350725124676,
        "Generated": "460",
        "Gold": "Pros: This paper provides interesting results for neural architecture search. In particular, this paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA-NAS), using importance sampling, which can be applied to micro/macro, constrained/unconstrained search problems. GA-NAS beats the state-of-the-art search algorithms proposed for NSA on public benchmarks, including NAS-Bench-101, NAS Bench-201, and NAS - Bench-301. Also, on the EfficientNet macro search space, GA NAS finds a new architecture with higher ImageNet accuracy and a lower number of parameters than EficientNet-B0."
    },
    {
        "coherence": 0.15745388068529995,
        "consistency": 0.22686079909149592,
        "fluency": 0.16379277022357874,
        "relevance": 0.046894892163782675,
        "overall": 0.14875058554103932,
        "Generated": "461",
        "Gold": "I am not convinced of its fit for XAIP. There is an attempt in the Related Work section to establish a connection to contrastive explanations. However, a mere property that distinguishes two plans is not automatically an explanation for \"why certain events occur.\" For instance, consider two plans both fulfilling goal p. The first plan results in p, q, the second plan in not-q. Thus, eventually(q) distinguished both plans. Why-question is this formula an explanation to?"
    },
    {
        "coherence": 0.04077609123906975,
        "consistency": 0.105890778763119,
        "fluency": 0.24788997856383507,
        "relevance": 0.05870605751068432,
        "overall": 0.11331572651917703,
        "Generated": "462",
        "Gold": "This paper proposes an accelerated first-order optimization algorithm for smooth and (strongly) geodesically-convex functions over a compact and geometrically convex set in Hadamard manifolds. It is proven in this paper that this Riemannian algorithm enjoys the same convergence rate as the Euclidean AGD algorithm without constraints. Though there exist some relavent works, none of them fully generalize the Euclindean aGD convergence results to the Riemannnian setting."
    },
    {
        "coherence": 0.12802959814805784,
        "consistency": 0.2202346500383739,
        "fluency": 0.1829634905586461,
        "relevance": 0.03815788092361215,
        "overall": 0.14234640491717251,
        "Generated": "463",
        "Gold": "A review of a paper that proposes a learnable bloom filter architecture. It is a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact than regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query."
    },
    {
        "coherence": 0.07814629166382636,
        "consistency": 0.25669303438272006,
        "fluency": 0.2993014367402622,
        "relevance": 0.03011612738378197,
        "overall": 0.16606422254264766,
        "Generated": "464",
        "Gold": "A RL environment based on the multi-year, multi-crop crop grow model (CGM) Cycles for open field agriculture. Agronomists and researchers can try different reinforcement learning or ML strategies. The authors only evaluate some variants of a single RL algorithm, PPO-1. There is no real-world impact to the RL community, and it's not clear that this environment opens up new avenues of exploration."
    },
    {
        "coherence": 0.18417105938403913,
        "consistency": 0.22244946921779102,
        "fluency": 0.16603866479267473,
        "relevance": 0.06968835199629515,
        "overall": 0.16058688634770002,
        "Generated": "465",
        "Gold": "Cross-domain semantic parsing/question answering is a very important problem because of the broad applicability of the technique. It is largely a recombination of known techniques for a simpler version of a widely-studied problem. Synthetic question generation based on a knowledge graph with a set of RP submodules. The empirical gains are modest, but consistent. This paper is well written and motivated. The results are convincing."
    },
    {
        "coherence": 0.1796837646711841,
        "consistency": 0.2574508588528216,
        "fluency": 0.39027595559185124,
        "relevance": 0.05548791744047354,
        "overall": 0.22072462413908261,
        "Generated": "466",
        "Gold": "This paper describes experiments that inject linguistic information into BERT and measure improvements in correlation with FMRI measurements of humans reading an underlying sentence (which is also analyzed by BERT) The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure through fine-tuning can improve brain decoding performance. The authors would be improved by experimenting with language models other than BERT, as it is not clear at the moment whether the produced results are generalizable to different language models or are BERT-specific. Comparison of Bert to GPT and XLNet would emphasize the advantages/disadvantages of autoencoder-based vs autoregressive models and could potentially provide additional insight on how attention is represented in human brain"
    },
    {
        "coherence": 0.3894361535080709,
        "consistency": 0.4705752271336147,
        "fluency": 0.2791530909192238,
        "relevance": 0.03956767079751303,
        "overall": 0.2946830355896056,
        "Generated": "467",
        "Gold": "The E2W is a convex regularization algorithm for Monte-Carlo Tree Search (MCTS). It is compared empirically with PUCT as a policy search in Alpha-go style MCTS. This paper essentially explores the duality of policies and corresponding (regularized) values. The paper provides theoretical analysis of three instantiations of the normalizations. Finally, the paper provides some empirical gains on certain toy domains and some atari games. A general framework for regularized Monte Carlo tree search."
    },
    {
        "coherence": 0.07088893703256047,
        "consistency": 0.15230283230991357,
        "fluency": 0.3282101304732281,
        "relevance": 0.049216358193985336,
        "overall": 0.15015456450242187,
        "Generated": "468",
        "Gold": "This paper proposes a modification of the CTC training loss to cope with incomplete transcripts in the training set, where the actual transcription of the beginning or of the end is missing. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. W-CTC is a simple and efficient modification in CTC algorithm to support partial labels (continuous) for a large percentage of label corruption with significance testing. It is empirically proved to significantly improve performance over CTC even if up to 40-70% label sequence is missed (overall performance similar to the complete label case)."
    },
    {
        "coherence": 0.16545126665380752,
        "consistency": 0.2383463966148591,
        "fluency": 0.280892211983436,
        "relevance": 0.10543443397148267,
        "overall": 0.19753107730589634,
        "Generated": "469",
        "Gold": "This paper builds useful insights and novel methods for graph convolutional networks, based on the Lanczos algorithm for efficient computations involving the graph Laplacian matrices induced by the neighbor edge structure of graph networks. The authors are obviously very knowledgeable in graph convnets, graph signal processing, and optimisation. However, there are really too many things in this paper, which leads to numerous shortcuts, and some time confusion."
    },
    {
        "coherence": 0.1892513188155388,
        "consistency": 0.2792460790456108,
        "fluency": 0.22982478973465778,
        "relevance": 0.09960757890801601,
        "overall": 0.19948244162595583,
        "Generated": "470",
        "Gold": "AdaAug is an efficient exploition-exploration workflow that searches adaptive augmentation policies in a class-dependent and potentially instance-dependent manner to improve the generalisation capability of deep learning models. The originality may not be overwhelmed since this work may be the first trial to *automatically* learn instances-dependent augmentations. This is not a major strength but a favorable bonus that allows better exploration for augmentation search. A straightforward way is to evaluate a randomly initialized $h_gamma$ (not trained ever) on downstream experiments while keeping other *diversity parameters* the same."
    },
    {
        "coherence": 0.06678953000229149,
        "consistency": 0.20597008386433555,
        "fluency": 0.24222261233940653,
        "relevance": 0.03962296200131695,
        "overall": 0.13865129705183765,
        "Generated": "471",
        "Gold": "Summary: This paper is a good example of a computational linguistics approach. It's not clear what the overall goal of the work is, but it's interesting to see what the authors are trying to achieve. The main problem with this paper is that it doesn't seem to have a clear goal in mind. There are a number of problems with this work, but I'm not sure what's the overall aim of this work."
    },
    {
        "coherence": 0.2124814048378626,
        "consistency": 0.15628744746653414,
        "fluency": 0.36221368891756306,
        "relevance": 0.10792155205466816,
        "overall": 0.20972602331915696,
        "Generated": "472",
        "Gold": "A novel deep learning model that integrates a variational auto encoder and a generative adversarial network to generate synthetic images conditioned by synthetic labels. The method is novel, however I do have some concerns regarding the method to ensure the consistency between the MRI used as a style image and the generated cardiac shape. The authors proposed a complicated model but the segmentation results are quite similar or even slightly worse than the data augmentation. Also GAN based synthesise was proposed and widely tested before."
    },
    {
        "coherence": 0.13251775671451438,
        "consistency": 0.24462410933440565,
        "fluency": 0.10118134293033212,
        "relevance": 0.08391072610883185,
        "overall": 0.14055848377202101,
        "Generated": "473",
        "Gold": "This is an interesting paper that takes a less common approach to RNNs: provably stable and net-of-nets. The results are at places more difficult to read, but overall it is clear. This paper is primarily a theoretical contribution to the construction of assemblies of recurrent neural networks. We know that combinations of learned modular components can be powerful and far more tractable than learning bespoke models from scratch, particularly in applied domains (e.g. AlphaGo). Yet so far, we have no theoretical guarantees that these combinations will actually remain stable. The authors then show that they can best SOTA algorithms on some of the common (albeit easier) benchmarks in the field, even under (and perhaps because of) these constraints."
    },
    {
        "coherence": 0.19416641000956153,
        "consistency": 0.2896851836552858,
        "fluency": 0.1660480384515993,
        "relevance": 0.04480231827520792,
        "overall": 0.17367548759791362,
        "Generated": "474",
        "Gold": "A novel family of concept bottleneck models that address the issue of conceptual bottlenecks. Strength: This paper tries to solve a challenging research question: to design XAI models which are good at task performance. Weaknesses: - CEMs assume that the datasets contain annotations of concepts which is not valid in practice and are often quite expensive to obtain. This paper introduces a model named concept embedding model (CEM) based on Concept Bottleneck Model architecture. It compared to CBM, CEM contains an embeddining generator layer that considers two embeddding representations (one for activate and one for inactivate) and then produces an embeddicing representation for one concept in c_hat."
    },
    {
        "coherence": 0.1681348095902607,
        "consistency": 0.2472052716425983,
        "fluency": 0.27602497170603485,
        "relevance": 0.03744293765006525,
        "overall": 0.1822019976472398,
        "Generated": "475",
        "Gold": "A method for identifying controllable (learned) features of a state that can be used to explore in RL, when confronted to a sparse reward task. This paper investigates the problem of extracting a meaningful state representation to help with exploration in A2C. The core idea consists in identifying c  n  (c - n) ."
    },
    {
        "coherence": 0.3266287653997491,
        "consistency": 0.30389463075777023,
        "fluency": 0.2997792960271944,
        "relevance": 0.09158910332545793,
        "overall": 0.2554729488775429,
        "Generated": "476",
        "Gold": "This paper introduces a new method EMIX - which improves upon QMIX by adding a global surprise minimisation objective to the centralised-value function training. This paper also demonstrates improved performance than baselines on most games within Starcraft II and through ablations demonstrates the necessity for both EBM approximations and surprise reward shaping. The authors propose incorporating surprise minimization in multi-agent reinforcement learning through temporal energy models for capturing the uncertainty across agents."
    },
    {
        "coherence": 0.15696452952535891,
        "consistency": 0.22135054215169042,
        "fluency": 0.20499406091956324,
        "relevance": 0.07865506667165864,
        "overall": 0.16549104981706783,
        "Generated": "477",
        "Gold": "This paper is of the highest technical standard, and to the best of my knowledge the first attempt at using Wasserstein Gradient flows in a computationally attractive way for the purposes of variational inference. Its originality lies not so much in the end goal of the algorithm---there are plenty of other variational methods using Gaussian families/mixtures of Gaussians---but in using an area of mathematics that has hitherto seen little use in the context of variable inference.\""
    },
    {
        "coherence": 0.2633335579541536,
        "consistency": 0.33240968119844655,
        "fluency": 0.3120887562575986,
        "relevance": 0.053338808944091806,
        "overall": 0.24029270108857262,
        "Generated": "478",
        "Gold": "This paper proposes a GNN-based method for objective space decomposition (ODA) in IP problems. The method is strictly designed for the ODA approach to be solved, which allows it to gain the performance improvements, but does perhaps not leave much room for further generalization. However, since ODA is already a general problem, this should be okay. The approach itself is well motivated and intuitive enough to understand, adopt and potentially reproduce. An advantage is that the method is independent of the solver itself."
    },
    {
        "coherence": 0.2635256503602312,
        "consistency": 0.28880298540086097,
        "fluency": 0.19223527008266444,
        "relevance": 0.054168184447407584,
        "overall": 0.19968302257279102,
        "Generated": "479",
        "Gold": "A recursive disentanglement network for the learning of disentangled VAEs from information theoretic perspective. The results appear convincing and beating relevant baselines (though with the disclaimer that mostly these baselines seem rather old by now; some of the relevant comparisons from the last 2 years might be missing, but I cannot name any). The experimental results show RecurD outperforms some existing baselines on two benchmark datasets."
    },
    {
        "coherence": 0.1958729599090642,
        "consistency": 0.2022339922588847,
        "fluency": 0.07401613607601713,
        "relevance": 0.06273490427456395,
        "overall": 0.1337144981296325,
        "Generated": "480",
        "Gold": "This paper proposes a method for mean-variance trade-off optimization in RL. The method, named EQUMRL, tries to optimize the Pareto efficiency by maximizing the quadratic utility function. The proposed method mitigates the double sampling issue appeared in prior work thus simplifies the optimization procedure. The authors also provided experiments to demonstrate the benefits of the algorithm using both synthetic datasets and real world data."
    },
    {
        "coherence": 0.19153771159476235,
        "consistency": 0.3081040145892946,
        "fluency": 0.12250302833863042,
        "relevance": 0.18336614084347702,
        "overall": 0.2013777238415411,
        "Generated": "481",
        "Gold": "# Paper strengths - This paper presents a novel segmentation scheme that combines the block motion vectors for feature warping, bi-directional propagation, and feature fusion. However, I still have several concerns: 1. As the blocks are generally rough estimation, it may damage the performance of the tasks. The authors should further clarify how the imperfect estimation influence the performance, e.g., the Blocking artifacts. 2. The authors are expected to conduct more comprehensive experiments."
    },
    {
        "coherence": 0.353481805105749,
        "consistency": 0.2900003808851903,
        "fluency": 0.274481935491139,
        "relevance": 0.0651810149952986,
        "overall": 0.24578628411934422,
        "Generated": "482",
        "Gold": "I believe the paper is solving a very interesting problem. The results and proofs all look correct to me, and the simplicity of the algorithm to construct the dual coreset is positively surprising. The numerical experiments look very limited compared to the theoretical richness of the paper. The authors adequately addressed the limitations and potential negative societal impact of their work. This paper proposes a method to reduce the complexity of Wasserstein distributionally robust optimization (WDRO) in machine learning via coresets. The theoretical routine for coreset construction seems to be an extension of previous methods [1, 2]. Clarity: The originality of this paper is the first to employ the coreset treatment for the worst-case distribution in WDRO problems. The proof section could be hard to follow without the re-stated theorems and the placement of TheoreM 1 does not follow the order of introduction in the main paper."
    },
    {
        "coherence": 0.3096322714987731,
        "consistency": 0.3390900977097987,
        "fluency": 0.15030966203946491,
        "relevance": 0.11749082327105849,
        "overall": 0.2291307136297738,
        "Generated": "483",
        "Gold": "This paper proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending"
    },
    {
        "coherence": 0.09845351291194522,
        "consistency": 0.18708172266149523,
        "fluency": 0.16161369567326575,
        "relevance": 0.053552589571277565,
        "overall": 0.12517538020449592,
        "Generated": "484",
        "Gold": "This paper proposes a siamese network for visual tracking, namely SiamCAN, which utilizes cross channel attention, spatial attention, and anchor-free regression head to learn more specific feature, using template's channel information to help the search branch to learn deeper correlation, and utilizing Anchor-Free regression branch with Diou-loss to deal with the clutters during the tracking procedure. This paper introduces cross-channel attrention mechanism and the Anchor-free box regression branch."
    },
    {
        "coherence": 0.03874482366480276,
        "consistency": 0.06878389948524316,
        "fluency": 0.10873752246122942,
        "relevance": 0.09092505449633265,
        "overall": 0.076797825026902,
        "Generated": "485",
        "Gold": "This paper introduces an improvement of the \"Hyperbolic Neural Networks\" (HNN) proposed by Ganea et al. and published at Neurips 2018. The authors consider in particular the multinomial logistic regression problem. The probability score of a category is determined by a linear separator and a bias term. In the vanilla HNN, two sets of parameters, each of dimensionality n (i.e. 2n in total), are learned to adapt the problem to hyperbolic geometry."
    },
    {
        "coherence": 0.4079600769746847,
        "consistency": 0.4733137075831976,
        "fluency": 0.11673355023491777,
        "relevance": 0.027108500274784335,
        "overall": 0.2562789587668961,
        "Generated": "486",
        "Gold": "This paper proposes/adopts a simple positive sampling scheme in metric learning: only sampling the easiest positive for each anchor. Authors give a theoretical analysis of how the proposed sampling scheme can reduce class collapse. Experiments on fine-grain retrieval datasets show the effectiveness of the sampling scheme. [1] Xuan H, Stylianou A, Pless R. Improved Embeddings with Easy Positive Triplet Mining[C], /The Conference on Advanced Computer Science, p. 69-70."
    },
    {
        "coherence": 0.18697959965373254,
        "consistency": 0.2127050732904185,
        "fluency": 0.18715505344583072,
        "relevance": 0.04572924448501714,
        "overall": 0.15814224271874971,
        "Generated": "487",
        "Gold": "I found the paper to be very well-written and the core ideas are well-explained. However, I would like to indicate that I haven’t checked the proofs in a very detailed manner and just went over them at a high-level. The paper seems to be original in the sense that it extends the previously proposed VE formalism by replacing equialities with error tolerances and that it develops a whole variety of new theoretical results out of this. This is the main strength of the paper."
    },
    {
        "coherence": 0.12246956982163544,
        "consistency": 0.19087866093199043,
        "fluency": 0.1448609758753269,
        "relevance": 0.08070478697049221,
        "overall": 0.13472849839986126,
        "Generated": "488",
        "Gold": "CLEVR-based compositionality is not a particularly groundbreaking study, but it is a good one. However, the novelty of the dataset as a whole is limited, as it is effectively a modification of CLE VR. It is not yet clear to me what the benefit of the audio modality is. A few-shot meta-learning problem to measure compositional concept learning under uncertainty. The authors mention that the concept space is unbounded and uncertain. Therefore, there could potentially be an infinite number of concepts where the positive samples are satisfied. Is it possible to make sure that the positive cases and the negative cases can prune all other concepts to make the actual concept tested in $D_supp$ is unique? If not, during testing, how to make certain a ''negative' sample is truly negative rather than coincidentally satisfying the concept?"
    },
    {
        "coherence": 0.08546965013835131,
        "consistency": 0.11972618772019128,
        "fluency": 0.16012298360869243,
        "relevance": 0.07388284280584524,
        "overall": 0.10980041606827007,
        "Generated": "489",
        "Gold": "This paper proposes a way to incorporate task information into multi-task learning. The hope is that more explicit knowledge of task information will improve MTL, especially in cases where the model seems to confuse tasks. The paper argues that providing OHV text labels is not always possible for tasks involving non-text modalities such as speech and images. But, I think it is possible to learn simple joint space task embeddings presentations for tasks from multimodalities. This paper is a promising direction, but it is not very exciting."
    },
    {
        "coherence": 0.12001730256225507,
        "consistency": 0.16966880318540903,
        "fluency": 0.18340951579770048,
        "relevance": 0.09437858002525627,
        "overall": 0.1418685503926552,
        "Generated": "490",
        "Gold": "Using Bayesian Optimisation + Hyperband approach to find an optimal combination of regularizers, this paper provides empirical evidence that regularization cocktails outperform a single regularizer, and that the benefits of regularization improves for smaller datasets. This paper provides previously-lacking in-depth empirical evidence to better understand the importance of combining different mechanisms for regularization, one of the most fundamental Concepts in Machine Learning."
    },
    {
        "coherence": 0.08542558186239449,
        "consistency": 0.15336952512714036,
        "fluency": 0.10712485715320504,
        "relevance": 0.06327318591003234,
        "overall": 0.10229828751319306,
        "Generated": "491",
        "Gold": "This paper proposes a novel conditioning-based regularization of GAN training to improve generated samples fidelity and stabilize training. The experimental results support the authors' argument about the improved stability and results in terms of objective metrics. Another more serious issue is that the method is strictly limited to spectrogram reconstruction. It is claimed that this conditioning will not limit the exploration of all modes of real data distribution. The proposed method is novel and interesting in some sense. The experiments and evaluation don't seem to sufficiently support the main arguments. Furthermore, another advantage of the proposed method was its ability to generate the samples with high diversity. This too was not demonstrated with experiments."
    },
    {
        "coherence": 0.2418570640698817,
        "consistency": 0.2415454860488399,
        "fluency": 0.24459412423538548,
        "relevance": 0.05853131091931103,
        "overall": 0.19663199631835454,
        "Generated": "492",
        "Gold": "I think this is a solid paper, with an interesting and reasonable approach to quantifying compositionality, and a fairly compelling set of results. The reported experiments cover reasonable ground in terms of questions relevant to compositionality (relationship to representation compression, generalization), and I appreciate the comparison to human judgments, which lends credibility to applicability of the framework. The authors start with a very clean example, that can potentially facilitate clarifying in a visual way the process of obtaining the measure. However, I feel that clarity is being traded-off for formality. It needs several reads to really distill the idea that essentially the authors are simply learning vectors of primitives that when added should resemble the representation of the input. Nevertheless, it's a good first step towards studying the problem of compositionality in vector representations."
    },
    {
        "coherence": 0.14537411506981135,
        "consistency": 0.17289598317140897,
        "fluency": 0.14080975090724526,
        "relevance": 0.055230927023725424,
        "overall": 0.12857769404304775,
        "Generated": "493",
        "Gold": "A general MCMC algorithm for approximating the joint posterior distribution of model parameters and true/denoised data, given a corrupted data set that has been privatized using an $epsilon$-differential privacy mechanism. A method for noise-aware DP Bayesian inference under DP, that handles the unobserved private data as an additional augmented latent variable to the model."
    },
    {
        "coherence": 0.12896552087943214,
        "consistency": 0.1384791718179487,
        "fluency": 0.09560744768797343,
        "relevance": 0.047406446080741814,
        "overall": 0.102614646616524,
        "Generated": "494",
        "Gold": "A systematic approach to address an important problem faced by multi-task learning (MTL) and propose a hierarchy in which to structure primary and auxiliary tasks in a model to improve primary task’s performance. This paper tackles conversational analysis problem and more specifically the Primary Multi-Task Learning. The main issue is that this approach adopts a specific neural network method. So how can we make sure that the conclusions always hold then?"
    },
    {
        "coherence": 0.17968438590015276,
        "consistency": 0.20730947642836936,
        "fluency": 0.10732344090406659,
        "relevance": 0.05067144962527305,
        "overall": 0.13624718821446544,
        "Generated": "495",
        "Gold": "MorphTE is a new word embedding compression method with morphological augmentation, which significantly outperforms similar word encoding methods. The authors conduct experiments on machine translation, question answering, and natural language inference tasks and show that Morphete can compress word embeddding parameters by a lot without performance loss. Weaknesses: The authors show that even with a 40x compression ratio, they can outprove previous baselines on a 20x scale."
    },
    {
        "coherence": 0.41283302806803546,
        "consistency": 0.3801559214429164,
        "fluency": 0.37233292285023883,
        "relevance": 0.038161223919201744,
        "overall": 0.3008707740700981,
        "Generated": "496",
        "Gold": "A paper on deep learning for extractive telegraphic summaries, which extracts text fragments (e.g., fragments of a sentence) as summarization, but the paper has little novelty. It's a good paper, but it's not as good as Malireddy et al.'s work, which is very similar to this paper, and has a lot of discussion about abstractive summary."
    },
    {
        "coherence": 0.22451963707037892,
        "consistency": 0.30713925243368534,
        "fluency": 0.1262555046280868,
        "relevance": 0.0877905222839461,
        "overall": 0.1864262291040243,
        "Generated": "497",
        "Gold": "This paper proposes a framework for federated learning that adaptively adjusts the frequency of the communication rounds and the quantization efficiency in a synergistic way. The idea of adaptation of quantization and frequency of communications is very nice; however, the experimental section was impossible to follow, but I think that minor modifications of the figures can resolve this problem. This paper proposed combining two orthogonal algorithms -- the lazily aggregated gradient(LAQ)method and adaptive quantization (AdaQuantFL)--to reduce communication complexity in  Federationed Learning."
    },
    {
        "coherence": 0.26058687325382796,
        "consistency": 0.38977523291888444,
        "fluency": 0.2120751922048242,
        "relevance": 0.06312403678474746,
        "overall": 0.231390333790571,
        "Generated": "498",
        "Gold": "This paper proposes a method called optimizer grafting. It uses two optimizers in one training session. One is to decide the update direction of parameters, and the other is to determine the update stride of parameters. This paper proposed a new optimizing mode and take a large amount of experiment exploration. S1. The paper structure is strange. I recommend to read some published proceedings to try to make this paper more clearly. W2. The theory is not reasonable. In other word, you just tell me you do it like this but not why it’s reasonable. Actually, I don’t think ADAM#SGD will be better than ADAM. ADAM calculates the update directions according to loss function. In a multi-dimensional space, this direction is composed of the value of each gradient and positive or negative(symbol) of every gradient. However, you change the symbol of some parameter’s gradient according to SGD Optimiz"
    },
    {
        "coherence": 0.09072868176854401,
        "consistency": 0.22335535236685003,
        "fluency": 0.2695309618680855,
        "relevance": 0.05200377493515819,
        "overall": 0.15890469273465943,
        "Generated": "499",
        "Gold": "Reinforcement Learning for Liver Tumor Ablation Planning is a promising approach for the treatment of liver tumors. It is well executed and has an interesting application of reinforcement learning to liver tumor planning. The authors use data from a public source. The results are comparable to a conventional method while being much faster. Novelty is limited as the paper present the use of existing methods on a different problem. It also appears to be a modified re-implementation without much detail provided which makes the results hard to judge."
    },
    {
        "coherence": 0.3315237343342486,
        "consistency": 0.3269774932963235,
        "fluency": 0.2458858937473548,
        "relevance": 0.09465152665201947,
        "overall": 0.2497596620074866,
        "Generated": "500",
        "Gold": "I like this paper, but I have some comments as follows: 1. The matrix M in Eq(1) can be represented as M = LT L. The Mahalanobis distance is equal to x_1-x_2 _M =((x_1/x_2) )T LLT (x_0/x_2). I wonder whether the similar modifications on the fully connected layers can achieve similar performance. If not, please give the reason. 2. There are some proposed SOTA methods like [1] for a fair comparison. 3. I suggest the authors to pass this paper through someone that can do the edits to fix the grammatic and typo errors that seem glaring"
    },
    {
        "coherence": 0.11758099146969048,
        "consistency": 0.22312532971688986,
        "fluency": 0.08211663981085673,
        "relevance": 0.0979857185413373,
        "overall": 0.1302021698846936,
        "Generated": "501",
        "Gold": "p.1: \"There are two main assumptions ... \" -- Are these assumptions or are these inputs to the problem? - or right before the conclusion in section 6 it would help to summarize that the last lemma is the final piece of the puzzle to construct M_1 and M_2 and that they have the right properties that are needed to prove the Theorem. G-identifiability is an important concept for the field of causality and it is therefore important to get it right; certainly in practice the issue of positive often gets overlooked; the correction here seems very pertinent and a new identification algorithm is provided. It seems to me that such an assumption of positivity is quite fundamental to many aspects of causal inference, and it would indeed be suspicious and need justification if a methods claims that it can do without this assumption"
    },
    {
        "coherence": 0.06461333710222295,
        "consistency": 0.18234227693668548,
        "fluency": 0.17216859292449313,
        "relevance": 0.06522001442853387,
        "overall": 0.12108605534798386,
        "Generated": "502",
        "Gold": "1). Literature review: it seemed to me that a literature review could be more complete. In particular, one of the most active authors in the field of compressed optimization is P Richtarik. This is important not only to note richtarik's contributions, but also to compare your results with those available in the literature. 2). Lower bounds: I think it is important here to clarify which factors are important to us when deriving lower bounds."
    },
    {
        "coherence": 0.1098703742498807,
        "consistency": 0.30136836714354787,
        "fluency": 0.08490382491590168,
        "relevance": 0.02033156875037832,
        "overall": 0.12911853376492716,
        "Generated": "503",
        "Gold": "I think the paper is technically sound and well-written, yet it operates under strong (though clearly stated) assumptions, which makes its contributions of interest only to a relatively narrow audience. The authors have compared their method against 5 different baselines and shown that their approach outperforms others. Appendix A: The paper claims their method (NResilient) “has better or similar RD than each baseline at all noise levels” (336-338) — unless I’m drastically misinterpreting figure 10b in the paper. The supplementary material is sparse and lacks a lot of detail. I found the introduction to be too abstract, despite references to specific work. Consider adding some specific examples to ground the discussion."
    },
    {
        "coherence": 0.10309461854845847,
        "consistency": 0.20534833829453594,
        "fluency": 0.1762919141995952,
        "relevance": 0.03168740469825119,
        "overall": 0.1291055689352102,
        "Generated": "504",
        "Gold": "Summary: A method for structure learning from observational and interventional data that uses a continuous optimization method. A three-phase score-based, iterative procedure is proposed to learn a causal structure over a set of categorical variables. It is based on the widely accepted do-formalism. The authors propose a 3-phase heuristic algorithm that fits the network before interventions, simulates the intervention on the fitted network and then again assigns a likelihood score to the network parameters."
    },
    {
        "coherence": 0.037752908749745274,
        "consistency": 0.10077698289756651,
        "fluency": 0.24274952877487788,
        "relevance": 0.026591429899337587,
        "overall": 0.10196771258038181,
        "Generated": "505",
        "Gold": "A baseline for detecting misclassified and out-of-distribution examples in neural networks. [1] Hendrycks, Dan, and Kevin Gimpel. I found the paper to be interesting on several aspects. I would have liked to see one other method for OOD detection presented, e.g., ensembling several models, adding test-time dropout, etc.."
    },
    {
        "coherence": 0.12134996994158306,
        "consistency": 0.2462192007820611,
        "fluency": 0.11366839030782694,
        "relevance": 0.06013072484174218,
        "overall": 0.13534207146830332,
        "Generated": "506",
        "Gold": "This paper addresses a not so well explored topic, but nevertheless important and interesting topic that would justify more research. The logic flow takes us through the author's thinking and reasoning clearly. The experiments and results are clearly presented with some helpful color-coding to aid the process. The authors do a great job evaluating the performance quantitatively and qualitatively. However, the model is still of very limited use since it only works on synthetic datasets."
    },
    {
        "coherence": 0.1685433995112953,
        "consistency": 0.32211213922136817,
        "fluency": 0.11405216901633859,
        "relevance": 0.0445189319729451,
        "overall": 0.1623066599304868,
        "Generated": "507",
        "Gold": "This paper provides a novel and well-designed architecture that both utilises latest advances in deep-learning and generates human-interpretable sketch graphs. The authors proposed a method to decompose CAD sketches into sub modular concepts, for design intent parsing. Specifically, the task is treated as a problem of program library induction. The goal is to discover sketch concepts (modular structures) from raw sketches in the format of sketches. DSL is formulated to enable a concise way to represent sketch concepts."
    },
    {
        "coherence": 0.056574499958994724,
        "consistency": 0.14421711051615466,
        "fluency": 0.06380143293476012,
        "relevance": 0.018266338522023598,
        "overall": 0.07071484548298328,
        "Generated": "508",
        "Gold": "A two-stage approach to restoring degraded speech signals. However, it does not provide the best picture of the performance of the proposed method, and the paper does not propose any novel methods for each stage. The authors propose a single model-based GSR task for removing multiple distortions simultaneously and a two stage-based generative framework called VoiceFixer to address the general speech restoration task. The author report that VoiceFiXer outperforms the GSR approach in the experimental results. 1. I believe that the paper has great potential if some concerns are addressed."
    },
    {
        "coherence": 0.06660663643602587,
        "consistency": 0.13805052256272404,
        "fluency": 0.04151226075908664,
        "relevance": 0.04893137310712681,
        "overall": 0.07377519821624084,
        "Generated": "509",
        "Gold": "This paper describes a computational method to construct ideal counterfactuals and isofactuals via invertible CNNs, and uses it to reveal biases in three different datasets. The directional derivative dx/dw, in the model, is the gradient of x and dw. The gradient of the x is the dynectogram of the gradient, which is the sum of the independent factors."
    },
    {
        "coherence": 0.16869771032557904,
        "consistency": 0.21412572724587048,
        "fluency": 0.07434811292115366,
        "relevance": 0.1539587936286743,
        "overall": 0.15278258603031938,
        "Generated": "510",
        "Gold": "Summary: This paper is a good introduction to the problem of risk-averse problem that is important in RL scenarios. The authors should consider using a more end-to-end method to replace the negative entropy as the regularizer, since you do not really care about the online gradient here. This part deservesto be better organized. The max value of t in line 2 of algorithm 1 (for t = 1, 2, ... . ) should be defined"
    },
    {
        "coherence": 0.11864791442889712,
        "consistency": 0.2486311655331942,
        "fluency": 0.016637540093148523,
        "relevance": 0.0201971052650382,
        "overall": 0.10102843133006952,
        "Generated": "511",
        "Gold": "This paper addresses the goal recognition design problem. It provides the formal framework of the extended GRD setting and presents a simple BFS approach that uses the k-planner transformation into classical planning problems for solving GRD-APK problems. A search-based solution method is presented and a formally sound pruning procedure is presented to make this search more efficient. This paper fits in the scope of the Workshop on Goal Recognition Design."
    },
    {
        "coherence": 0.17291794994737333,
        "consistency": 0.19786784423429854,
        "fluency": 0.09631640756492545,
        "relevance": 0.06966415116275552,
        "overall": 0.1341915882273382,
        "Generated": "512",
        "Gold": "This paper proposes a new conceptual framework to assess the safety of model via maximum deviation from a reference model over a pre-specified certification dataset. This paper then demonstrates the computation of deviation or bounds on deviation for trees, GAMs and tree ensembles along with a broader class of piecewise Lipschitz functions. This helps in understanding the practicality of this framework under different situations. Strengths: 1) The conceptual framework proposed in this paper is intuitive and helps reason about the safety a predictive model. 2) The case studies show how this framework can be useful not only in the assessment of regions within feature space that are problematic, but also in choosing hyperparameters such that the models are safe w.r.t this deviation metric. 3) The case of a more general piecewise lipschitz function and show how interpretable models can lead to more efficient computation of their metric, D(y, y_"
    },
    {
        "coherence": 0.03983364073685948,
        "consistency": 0.12764180462779645,
        "fluency": 0.047720648708137726,
        "relevance": 0.10142418828241767,
        "overall": 0.07915507058880283,
        "Generated": "513",
        "Gold": "Cons: This is a very good paper. I would like to know how many trials it takes to plot similarity figures (e.g., Figure 1 (a)-(b)). It would be better to try many times and give the mean and variance to avoid coincidence. Besides, will other parameters such as the number of samples and dimensions of features affect Hypothesis I? 3. Wide-ResNet28 trained with 200 epochs under suggest settings cannot converge to zero after about 200 apoks training, and the angles are approximately 10 degrees."
    },
    {
        "coherence": 0.09464945675921231,
        "consistency": 0.13963470509339712,
        "fluency": 0.06195555594232635,
        "relevance": 0.06248094559308442,
        "overall": 0.08968016584700504,
        "Generated": "514",
        "Gold": "A Reinforcement-Learning-based method to search for a non-uniform pruning strategy that will replace the uniform compression ratios used in other state-of-the-art methods. This paper leverages AutoML strategies (i.e., RL-based hyper-parameter search) to solve the network pruning problem. The overall problem is interesting, however, I am not sure how novel it is especially in the field of AutoML since there're many papers on automatically pruning the networks. The impact here is mainly on how to combine it with adversarial training."
    },
    {
        "coherence": 0.11848404129883755,
        "consistency": 0.20598323015127507,
        "fluency": 0.08892494952763526,
        "relevance": 0.06419542081038965,
        "overall": 0.11939691044703438,
        "Generated": "515",
        "Gold": "A review of four task-agnostic metrics for evaluating reinforcement learning agents: human similarity, curiosity, empowerment and information gain. This paper is well written with clarity and includes all experimental details for reproducibility. The methods/metrics proposed in this paper are not novel and have been used in various existing RL algorithms alongside task reward for training agents. The analysis of these metrics' correlation with human data is still an interesting piece of result but is not significant enough to become the sole contribution of an ICLR paper."
    },
    {
        "coherence": 0.21779663704297827,
        "consistency": 0.3449209677842848,
        "fluency": 0.10881605165138475,
        "relevance": 0.13617528409563354,
        "overall": 0.20192723514357033,
        "Generated": "516",
        "Gold": "This paper presents a new standardized dataset for predicting multiple proteomic properties using protein sequence data. This task is useful for determining which amino acid molecules exist in a sample and to what abundance. The authors clearly define two relevant machine learning tasks that this dataset could be used for. This includes the experimental procedure for generating the data as well as the annotations. The evaluation metrics are well suited for their specific tasks."
    },
    {
        "coherence": 0.0987565423292942,
        "consistency": 0.1300039908049208,
        "fluency": 0.04513619438630645,
        "relevance": 0.01392841414880094,
        "overall": 0.0719562854173306,
        "Generated": "517",
        "Gold": "**Clarity:** This paper proposes an Anchor-changing Regularized Natural Policy Gradient (ARNPG) framework, which can systematically incorporate ideas from well-performing first-order methods into the design of policy optimization algorithms for multi-objective MDP problems. This paper is an important contribution to our understanding of the problem and I therefore recommend this paper for acceptance. Theoretical Analysis of Policy Optimization in Multi-Objective Markov Decision Processes."
    },
    {
        "coherence": 0.11808170388208596,
        "consistency": 0.23081967403660578,
        "fluency": 0.09244282937480794,
        "relevance": 0.06981425985095496,
        "overall": 0.12778961678611367,
        "Generated": "518",
        "Gold": "I think it is a good idea, but I doubt it will be able to be implemented in MT standard evaluations, as all sentences used during training. A novel method of automatically generating high-quality test sets for machine translation evaluation and publishes the generated test sets to promote reproducibility and to allow researchers to use the generated tests for evaluating machine translation models. A technique to filter machine translation test sets in order to keep a set of instances"
    },
    {
        "coherence": 0.03940740604972193,
        "consistency": 0.08351465204287203,
        "fluency": 0.06197184989584418,
        "relevance": 0.028195368194904168,
        "overall": 0.05327231904583558,
        "Generated": "519",
        "Gold": "This paper presents a label-invariant augmentation strategy for graph contrastive learning by involving labels in the downstream task to guide the contrastive augmentation. It conducts the augmentation in the representation space and generates the most difficult sample while keeping the label of the augmented sample the same as the original sample. Experimental results on eight benchmark graph-structured data show that the proposed metho outperforms classical GNN-based methods and recent graph comparative learning."
    },
    {
        "coherence": 0.16242949778745733,
        "consistency": 0.3513975850658017,
        "fluency": 0.1697727773515238,
        "relevance": 0.142069547597437,
        "overall": 0.20641735195055494,
        "Generated": "520",
        "Gold": "This paper provides guidelines to design strong adaptive attacks to assess the robustness of robust graph neural networks that counteract adversarial attacks. The authors have conducted extensive experiments to show how adaptive attacks, which include global/local attacks under evasion/poisoning setting, can break several representative defense GANN models. The lessons and guidelines for adaptive attacking are reasonable. The proposed robustness unit test set is useful for evaluating new defense methods."
    },
    {
        "coherence": 0.0445805932562986,
        "consistency": 0.07049489244323719,
        "fluency": 0.17050600078279696,
        "relevance": 0.06156996371800037,
        "overall": 0.08678786255008328,
        "Generated": "521",
        "Gold": "Pros: 1. The authors' approach to fine-tuning pre-trained models and generalization under distribution shifts is a very simple one. 2. The authors suggest a two-stage approach where linear probing is followed by end-to-end fine tuning. 3. The authors propose a mitigation strategy based on two-layer networks. 4. The authors would very much appreciate clarity on this issue, is LP-FT something that has been used as a fine tuning heuristic?"
    },
    {
        "coherence": 0.17601246201085755,
        "consistency": 0.2931489687496899,
        "fluency": 0.2040964351474386,
        "relevance": 0.0687253396861743,
        "overall": 0.18549580139854008,
        "Generated": "522",
        "Gold": "The authors present a clever trick to combine the curvature of the loss function with a quadratic term that appear in the standard regret analysis of online learning algorithms. This paper shows that when variance is large, it is possible to achieve negative regret, i.e., outperform the best expert, otherwise, variance is small (bounded by O(log K)$ regret bound). Further, they use this concept (with the algorithm and its bound) in several other settings and prove regret bounds, essentially trying to make algorithms make use of large observed variance in the loss."
    },
    {
        "coherence": 0.09848221101466316,
        "consistency": 0.1706211504868161,
        "fluency": 0.1256982903903345,
        "relevance": 0.029500408898834166,
        "overall": 0.10607551519766197,
        "Generated": "523",
        "Gold": "This paper proposes a novel conditional manipulation method based on a single image, which is new in this area. The authors addressed this problem by proposing to learn the mapping between a set of primitive representations, which consists of edges and segmentation masks, and an image. They also adopted a thin-plate-splines transformation as augmentation which enables the model to robustly manipulate an image by editing primitives. The method requires a professional editing ability for editing edges of a super primitive. The generation of primitives also highly depends on the accuracy of semantic segmentation."
    },
    {
        "coherence": 0.0721408446273632,
        "consistency": 0.12278287046056283,
        "fluency": 0.2009200566865022,
        "relevance": 0.04001071347163513,
        "overall": 0.10896362131151584,
        "Generated": "524",
        "Gold": "--------- This paper presents a sound approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and - importantly - external inputs. This approach is enabled by using the outcome of an optimization algorithm (iLQR) in the recognition model, recently enabled by other work in the field. This paper demonstrates the use of this approach on simulated data and real neural data, and compares it to other classic and contemporary models of nonlinear dynamical systems."
    },
    {
        "coherence": 0.06787180337311287,
        "consistency": 0.19840838794217666,
        "fluency": 0.2042161885158112,
        "relevance": 0.01760034000585498,
        "overall": 0.12202417995923892,
        "Generated": "525",
        "Gold": "GOL is the first attempt to design an embedding space in which the direction and distance between objects represent their order and metric relations between their ranks, by enforcing two geometric constraints: the order/rank constraint and the metric constraint reflects rank difference. Estimates a test object rank are achieved by kNN, and a metric called discriminative ratio for ranking (DRR) estimates the quality of rank estimation embeddding spaces and yields rank estimation performances. Weaknesses: - The proposed GOP method rely on reference points, it is not clear how to select these reference points and how they affect the performance of GOP."
    },
    {
        "coherence": 0.10268915689538138,
        "consistency": 0.14041945813870815,
        "fluency": 0.19753395907742874,
        "relevance": 0.026946602466503194,
        "overall": 0.11689729414450538,
        "Generated": "526",
        "Gold": "This paper presents an experiment of safe reinforcement on a 2D grid-word where the safety constraints are specified in natural language instead of being specified formally. The authors defined three types of constraints to restrict agents to visit certain states: 1. Budgetary constraints 2. Relational constraints 3. Sequential constraints 4. Natural language does not play any role in step 2; and step 1 is just a classification of natural language into a pre-defined structured representation (the mask and the threshold)."
    },
    {
        "coherence": 0.25389193027707124,
        "consistency": 0.42196417678356396,
        "fluency": 0.2525109731253388,
        "relevance": 0.06789610661743896,
        "overall": 0.24906579670085324,
        "Generated": "527",
        "Gold": "This paper proves the existence of strong lottery tickets and further develops a framework to plant and hide winning lottery tickets with desirable properties in randomly initialized networks to help analyze the ability of state-of-the-art pruning methods to identify tickets of extreme sparsity. A novel take on the ability to find good sparse subnetwork do not perform as well as those for finding good \"weak\" (after training), even in for these toy problems."
    },
    {
        "coherence": 0.1825027751370372,
        "consistency": 0.3511720290224374,
        "fluency": 0.18097869614242135,
        "relevance": 0.07852957534496827,
        "overall": 0.19829576891171605,
        "Generated": "528",
        "Gold": "This paper introduces a method for embedding the hard-constraints for boundary conditions for physics-informed neural networks. It introduces the \"extra fields\" to transform the original PDE into equivalent forms so that the BCs can become linear. This paper addresses a critical problem of PINNs since training for boundary condition frequently collides with training for governing equations. Their method solves the problem by designing the general solutions to boundary conditions and proves several forthcoming facts."
    },
    {
        "coherence": 0.19121333521333236,
        "consistency": 0.22760904115005892,
        "fluency": 0.07360786450423507,
        "relevance": 0.033605052455969155,
        "overall": 0.13150882333089886,
        "Generated": "529",
        "Gold": "This paper proposes a perceptual reward function r(s, s_g) where 's' and 'g' are current and goal observations respectively. Its two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space. Overall, the idea looks very original and promissing, but the methods are quite difficult to get, and the authors could do a better job in that respect."
    },
    {
        "coherence": 0.2613671061003617,
        "consistency": 0.4894734056294349,
        "fluency": 0.0535065066854453,
        "relevance": 0.052487489229529664,
        "overall": 0.21420862691119288,
        "Generated": "530",
        "Gold": "GRAFF is a gradient flow based graph neural network architecture that can be seen as a re-parameterization of a learnable energy function. This paper introduces a new family of GNN models, GRAFF, on graphs wherein graph features are transformed according to a dynamical system given by the negative gradient of an energy function, which is parameterized and learned, instead of parameterizing the itself, as the most widely used GNN architectures do. This relation is properly studied in Section 4, where they show that GRAFF still includes many prior GNN model (up to the perhaps critically important matter of the non-linearity). However, this re..."
    },
    {
        "coherence": 0.10445002142467415,
        "consistency": 0.26619218721551996,
        "fluency": 0.0808570453984387,
        "relevance": 0.0851227355315624,
        "overall": 0.13415549739254878,
        "Generated": "531",
        "Gold": "This is a very good paper. It shows that all previous Bayesian approaches to DIP show overfitting. However, many previous studies including [1] are not shown in the results. This paper is clearly motivated to improve performance on this task. The proposed method is based on the concept of deep image priors, in this case an untrained image-generator network, and outputs both the reconstructed image and the uncertainty estimates of the weights of the network given the data."
    },
    {
        "coherence": 0.04505443536063376,
        "consistency": 0.10412383766474154,
        "fluency": 0.167489823694146,
        "relevance": 0.05094112191589315,
        "overall": 0.09190230465885361,
        "Generated": "532",
        "Gold": "Spectral coefficients of a convection-diffusion PDE using a neural network and the polynomial chaos expansion technique are presented in a 1D test case with a single stochastic parameter. The paper is well written and suitable for the workshop. The choice of expansion, whether the choice is empirical or have any theories backbone, is a good paper, but I would like to know more about the choice of expanding."
    },
    {
        "coherence": 0.16090993527030076,
        "consistency": 0.280301110686908,
        "fluency": 0.1400234665878288,
        "relevance": 0.05123409069915114,
        "overall": 0.15811715081104719,
        "Generated": "533",
        "Gold": "This paper proposes a neural network based approach for doing quantile regression on possibly censored data, i.e. when the target variable is sometimes not directly observed and instead a lower or upper bound is known, as in the case of survival data. The authors first build on the linear approach on Portnoy 2003 and extend it straightforwardly to neural networks. However this approach requires the sequential optimization of a new NN for each quantile level to be predicted, which is computationally heavy. Then a novel method for simultaneous quantitative regression with NN is proposed. This is interpreted as a form of expectation maximization, allowing to estimate all the quantiles simultaneously."
    },
    {
        "coherence": 0.14340701762025015,
        "consistency": 0.25567474889223674,
        "fluency": 0.09444097165739607,
        "relevance": 0.014954654628269228,
        "overall": 0.12711934819953805,
        "Generated": "534",
        "Gold": "This paper proposes a new framework to detect out of distribution samples for deep neural networks without retraining or modifying the model. It also seems to work well as, based on results, it outperforms baseline methods in several cases being also much more computationally light. However, a problem of the manuscript is that it quite extensively uses appendices as extra space (the whole paper is 28 pages long) their framework is not completely described or can be understood by wider audience."
    },
    {
        "coherence": 0.04941814793381833,
        "consistency": 0.12084788064596412,
        "fluency": 0.1174270776821638,
        "relevance": 0.03626727769688133,
        "overall": 0.0809900959897069,
        "Generated": "535",
        "Gold": "This paper analyzes the multi-head attention in transformers and suggests to use collaboration instead of concatenation of multiple heads. Empirical results on WMT’16 English-German demonstrates that the proposed approach didn’t speed up the training, even though it gives an improvement in terms of FLOPS. This may limit to the large scale of BERT pre-training. This paper presents an interesting collaborative MHA to enable heads to share projections, which can be easily applied to most existing transformer-based models, including NMT. The proposed method seems to be not effective for pre-trained models, e.g. when the number of parameters is decreased from 108.5M to 96.6M, this reduction of model size is not that big, while the average score decreases from 83.2 to 77.6"
    },
    {
        "coherence": 0.13661041281616806,
        "consistency": 0.17514503938099324,
        "fluency": 0.23059263361437893,
        "relevance": 0.19896414501618098,
        "overall": 0.18532805770693028,
        "Generated": "536",
        "Gold": "Summary: This paper is well written. The algorithms and theoretical results are presented in an accessible way and discussed in detail. The results seem to be competitive. It should be noted in Table 1 that the complexity does not exactly match that of Karimireddy et al. [2020]. I am not sure when Assumption 4 actually holds in practice with a small alpha = o(n) sum_j w(j) for all agents. This means all epsilon could be dependent in some way and thus alpha should be the same order as n."
    },
    {
        "coherence": 0.04774934763713125,
        "consistency": 0.13458496937609296,
        "fluency": 0.13201060307744833,
        "relevance": 0.05221260538774422,
        "overall": 0.0916393813696042,
        "Generated": "537",
        "Gold": "This paper addresses the problem of unsupervised image-to-image translation without making any assumptions about the existence of input pairs or input sets. To solve this task, the authors utilize a guiding network that generates a pseudo label (domain label) as well as a style code which are then provided to a generator network. The generator given a source image produces an output that follows the structure of the source yet preserves the style and \"domain\" details of the reference image."
    },
    {
        "coherence": 0.16846305360859526,
        "consistency": 0.18851066333900623,
        "fluency": 0.1766585766256506,
        "relevance": 0.0205118880243715,
        "overall": 0.1385360453994059,
        "Generated": "538",
        "Gold": "A meta-RL algorithm that learns by updating its weights through interaction with the environment and its own current weight state. The authors limit to 10M steps for the MetaWorld ML1 and ML10 environments; but the original MetaWorld paper achieved much higher performance with RL2 and MAML after much longer training (300M steps). Missing x & y labels in Figure 6. Finally, I do not find Figure 1 very helpful. I may misunderstand something here, but in equation 5, it seems that computing $v(s)$ and $W(s). How about the forward pass?"
    },
    {
        "coherence": 0.09031559852942174,
        "consistency": 0.24291641936190977,
        "fluency": 0.12127077391232646,
        "relevance": 0.023372930246999605,
        "overall": 0.11946893051266438,
        "Generated": "539",
        "Gold": "A GAN-based architecture for synthesizing regular and irregular time series using neural ODEs and continuous time-flow processes, this paper proposes GT-GAN, a framework for synthetic time series with various techniques such as generative adversarial networks, auto-encoders, neural ordinary differential equations, continuous time flow processes, and a neural time series GAN architecture. It is based on a GAN framework that uses a variety of techniques to synthesize regular time series."
    },
    {
        "coherence": 0.2468957823040774,
        "consistency": 0.379776984830715,
        "fluency": 0.10077176002363887,
        "relevance": 0.040497118934959976,
        "overall": 0.1919854115233478,
        "Generated": "540",
        "Gold": "## Main strengths: 1) Providing detailed diagrams highlighting the differences between the proposed method and the baselines 2) Performing an experiment explaining to rebut some potential doubts about sources of improvements 3) Visualizing the attention between positional queries and positional keys, justifying the limitations of the previous works and their proposed method's advantage 4) Improving benchmark results over SOTA on the COCO '17 validation set.## Main Weaknesses: 5) Based on my understanding, the experiment visualized in figure 3, aims to justify that something is wrong with the way positional information is represented and it is not merely an issue with the optimization. 6) Hyperparameter study is limited to temperature and number of decoder layers. Effects of other hyperparameter choices are not clear to the reader. This paper introduces dynamic anchor boxes as a query formulation for detection transformer (DETR)."
    },
    {
        "coherence": 0.08515317146896584,
        "consistency": 0.22308817954446292,
        "fluency": 0.07029877038695992,
        "relevance": 0.03980359808705071,
        "overall": 0.10458592987185984,
        "Generated": "541",
        "Gold": "This paper proposes an algorithm to alter the structure of a graph by adding/deleting edges so as to degrade the global performance of node classification. The main idea is to use the idea of meta-gradients from meta-learning to solve the bilevel optimization problem in the discrete graph data using greedy selection approach. From the experimental results, this treatment is really effective in attacking the graph learning models (GCN, CLN, DeepWalk). However, the motivation in using meta- learning to solve this problem is not very clear to me, e.g., what are the advantages it can offer."
    },
    {
        "coherence": 0.12853731897268042,
        "consistency": 0.27720199370692256,
        "fluency": 0.17576351858681075,
        "relevance": 0.04455727416146001,
        "overall": 0.15651502635696843,
        "Generated": "542",
        "Gold": "Latent Embeddings of High-Density Time Series Data: A Novel Approach to the Problem of Learning Latent Representations based on Self-Organizing Maps (SOMs). This paper presents a novel approach to the problem of learning latent representations of high-dimensional time series data. The authors propose to include an extra reconstruction loss term w.r.t. the discrete representation."
    },
    {
        "coherence": 0.06914284657550704,
        "consistency": 0.10522941986951526,
        "fluency": 0.09298353899438837,
        "relevance": 0.0524534333976796,
        "overall": 0.07995230970927257,
        "Generated": "543",
        "Gold": "This paper proposes a new method applicable to a specific case in unsupervised learning: having access to small amount of labels during the training phase where label signal is not used. I am also curious about the final performance on ImageNet after training for 1000(500) epochs. This paper adopts supervised contrastive learning and discovers that supervision can help accelerate the pretraining stage. Despite it maybe practical to speed up the experimental cycle, the technique contribution is rather limited."
    },
    {
        "coherence": 0.06462433763295379,
        "consistency": 0.16848030182036172,
        "fluency": 0.08480658198749824,
        "relevance": 0.030168199288683754,
        "overall": 0.08701985518237437,
        "Generated": "544",
        "Gold": "This paper addresses the problem of meta-learning tasks in heterogeneous feature spaces. It proposes a ground-truth alignment matrix to align the tasks. It also proposes an initialization strategy for supervised training. Chameleon: Learning Model Initializations Across Tasks With Different Schemas ------ This paper uses few-shot learning vocabulary and techniques, including Reptile, but the methodology seems completely different from the few-shoot learning literature. In particular, there does not appear to be a split between meta-training and meta-test classes within a dataset, except for the EMNIST experiment. Even then, the pre-training of the \"chameleon\" alignment module seems to involve using examples of the meta- test classes."
    },
    {
        "coherence": 0.08552251435111972,
        "consistency": 0.16796210304315512,
        "fluency": 0.12487948743391768,
        "relevance": 0.04840411651617625,
        "overall": 0.1066920553360922,
        "Generated": "545",
        "Gold": "This paper provides a public dataset of stimulated Raman histology and SRH imaging of brain tumors, and a benchmark for two computer vision tasks: multiclass histologic brain tumor classification and patch-based contrastive representation learning. The authors provide strong motivation for their work, and the data and code are all available (to reviewers at the moment, and to the public in the near future), and the appendix provides meticulous documentation of the data."
    },
    {
        "coherence": 0.11325302396960137,
        "consistency": 0.1317981815659087,
        "fluency": 0.1675445399087781,
        "relevance": 0.030190780769953927,
        "overall": 0.11069663155356052,
        "Generated": "546",
        "Gold": "A method to learn a variable error bound epsilon_i$ to use when building piecewise linear segments for learned indices for databases. This additional learned component greatly improves the space/error trade-off, does not cost too much in terms of indexi building cost, and has similar query costs as existing methods. This paper focuses on learning index with dynamic prediction error. The major contribution of this paper is proposing an efficient, pluggable d learned index framework that is adaptive to dynamics. The current presentation can be improved with answers to the questions in the previous sections."
    },
    {
        "coherence": 0.029706313336956645,
        "consistency": 0.05945829303761247,
        "fluency": 0.125931148010253,
        "relevance": 0.0016194838787836196,
        "overall": 0.05417880956590144,
        "Generated": "547",
        "Gold": "SVRG and the greedy-GQ in the optimization community. It provides a finite-time analysis of the proposed algorithm in the off-policy and Markovian sampling setting (convergence to the stationary point) and improves the sample complexity from the order $epsilon-3$ to $eppsilón-2$ comparing with the vanilla greedy GQ (\"Online, incremental, with memory and per-time-step computation costs that are linear in the number of features\")."
    },
    {
        "coherence": 0.1339087345736614,
        "consistency": 0.17010863483059085,
        "fluency": 0.18943035580093767,
        "relevance": 0.026235632459348204,
        "overall": 0.12992083941613455,
        "Generated": "548",
        "Gold": "A method to verify local robustness networks with piecewise-linear activation functions, providing tighter bounds than existing approximate verification approaches while scaling to deeper networks than existing complete verifiers. FastLin version, but it's a good way to test the robustness of large neural networks with ReLU as activation. 3. The proposed method is geometrically justified and benefits from many heuristics to optimize efficiency. 4. It scales better than other methods to larger and deeper architectures when tested on models trained for verifiability (MMR, RS). A systematic search over the convex polyhedral regions on which the network is linear, to find the decision boundary, so to certify local $ell_p$ robustness."
    },
    {
        "coherence": 0.3656611740694733,
        "consistency": 0.4577813907350059,
        "fluency": 0.09050925639972654,
        "relevance": 0.04168469622273199,
        "overall": 0.23890912935673442,
        "Generated": "549",
        "Gold": "I found this paper unnecessarily confusing. There are many claims that could be substantiated or explained further. Section 4.2 starts off by saying that one NN, one GLM, and one GP has been trained and then make a jump to Figure 4a which shows decision boundaries that are not surprising; I am not sure how it validates the protocol proposed to measure EVR without confounds. In order to compute the measures, the values for the discriminant features and the distractor features are required"
    }
]