[
    {
        "coherence": 0.738237062732258,
        "consistency": 0.891520169073004,
        "fluency": 0.5469405203560244,
        "relevance": 0.769824440854759,
        "overall": 0.7366305482540114,
        "Generated": "1",
        "Gold": "This paper addresses a core issue of the popular conditional neural process. The predictions at each test point are conditionally independent given the conditioning set. This is an inappropriate modeling assumption for many real-world datasets. The authors propose to go beyond a non-diagonal Gaussian to describe the joint distribution. The main idea is to directly parameterize the mean and the covariance functions of a Gaussian predictive via neural networks. They also propose to use Copulae to handle non-Gaussian marginal distributions."
    },
    {
        "coherence": 0.5819447739326667,
        "consistency": 0.8477525941675585,
        "fluency": 0.601706586000976,
        "relevance": 0.1249087226276105,
        "overall": 0.5390781691822031,
        "Generated": "2",
        "Gold": "This paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \\epsilon-greedy) or at thelevel of experiments. This paper proposes the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions.\nThis paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \\epsilon-greedy) or at thelevel of experiments. This paper proposes the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions."
    },
    {
        "coherence": 0.4983565358089343,
        "consistency": 0.7929661064822594,
        "fluency": 0.4911684983857408,
        "relevance": 0.1256043475576654,
        "overall": 0.47702387205864993,
        "Generated": "3",
        "Gold": "This paper proposes a method for learning graph-level representation in an unsupervised contrastive way. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The major concern about this method is that it encourages the closeness of augmented views from the same graph instances. For example, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule.\nThis paper proposes a method for learning graph-level representation in an unsupervised contrastive way. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The major concern about this method is that it encourages the closeness of augmented views from the same graph instances. For example, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule."
    },
    {
        "coherence": 0.405762360055993,
        "consistency": 0.7801849598418509,
        "fluency": 0.4199853973001009,
        "relevance": 0.09654387674494747,
        "overall": 0.425619148485723,
        "Generated": "4",
        "Gold": "The authors provide a compelling theoretical explanation for a large class of adversarial examples. They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures. The paper does not cover the adversarially-augmented training based iterative attacks. It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. Given that the experimental results are also not particularly novel, I recommend rejection.\nThe authors provide a compelling theoretical explanation for a large class of adversarial examples. They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures. The paper does not cover the adversarially-augmented training based iterative attacks. It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. Given that the experimental results are also not particularly novel, I recommend rejection."
    },
    {
        "coherence": 0.2923309258974805,
        "consistency": 0.5410653767834249,
        "fluency": 0.3469293517976111,
        "relevance": 0.04240675152945857,
        "overall": 0.3056831015019938,
        "Generated": "5",
        "Gold": "This paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games. The main algorithm is a version of the classic fictitious play algorithm. It operates on the Q-values, but a key novelty is the use of a particular form of regularization. The paper is well written but I did not check everything in detail. I’m also curious about the reduction of the presented algorithm to matrix game. And what can we say about the convergence rate of the algorithm?\nThis paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games. The main algorithm is a version of the classic fictitious play algorithm. It operates on the Q-values, but a key novelty is the use of a particular form of regularization. The paper is well written but I did not check everything in detail. I’m also curious about the reduction of the presented algorithm to matrix game. And what can we say about the convergence rate of the algorithm?"
    },
    {
        "coherence": 0.19137378520138504,
        "consistency": 0.4758536427701419,
        "fluency": 0.3566773992005496,
        "relevance": 0.029511020893377038,
        "overall": 0.2633539620163634,
        "Generated": "6",
        "Gold": "The authors proposed a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment and feature consistency. The proposed method divides target samples into source-like and target-specific ones. Source-like samples are used for global class clustering. Target-specific samples are use for learning local structures. An adaptive contrastive learning strategy is used to achieve class-wise adaptation and local consistency.\nThe authors proposed a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment and feature consistency. The proposed method divides target samples into source-like and target-specific ones. Source-like samples are used for global class clustering. Target-specific samples are use for learning local structures. An adaptive contrastive learning strategy is used to achieve class-wise adaptation and local consistency."
    },
    {
        "coherence": 0.4000815350379078,
        "consistency": 0.6597483361844051,
        "fluency": 0.37755125001002116,
        "relevance": 0.05409079441800194,
        "overall": 0.372867978912584,
        "Generated": "7",
        "Gold": "The paper puts together several ideas from prior works (partial rejection control/SMC, variational inference, dice-enterprise) Empirical experiments suggest that the method outperforms previous work. It is not obvious that one can obtain unbiased estimators of the normalizing constant. The experiments include a mix of toy and more realistic examples. The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks.\nThe paper puts together several ideas from prior works (partial rejection control/SMC, variational inference, dice-enterprise) Empirical experiments suggest that the method outperforms previous work. It is not obvious that one can obtain unbiased estimators of the normalizing constant. The experiments include a mix of toy and more realistic examples. The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks."
    },
    {
        "coherence": 0.21860854609453126,
        "consistency": 0.5279958361831074,
        "fluency": 0.48932008754574774,
        "relevance": 0.033867540796992744,
        "overall": 0.3174480026550948,
        "Generated": "8",
        "Gold": "This paper broadly considers meta-learning, a.k.a. bilevel optimization. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters. Empirically, the new algorithm achieved a new state-of-the art for model-free agents on the Atari ALE benchmark. The paper is well-written and features exemplary empirical execution. I think this paper should be an oral, best-reviewed paper. It is arguably the best in my batch.\nThis paper broadly considers meta-learning, a.k.a. bilevel optimization. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters. Empirically, the new algorithm achieved a new state-of-the art for model-free agents on the Atari ALE benchmark. The paper is well-written and features exemplary empirical execution. I think this paper should be an oral, best-reviewed paper. It is arguably the best in my batch."
    },
    {
        "coherence": 0.2683050897213578,
        "consistency": 0.4204978970813536,
        "fluency": 0.4849557968899586,
        "relevance": 0.028619815645146953,
        "overall": 0.3005946498344543,
        "Generated": "9",
        "Gold": "The paper introduces an extension to existing pattern generation algorithms for PDB heuristics. The way the paper is currently written, by interleaving background on existing methods, makes it hard to follow. The write-up is not very good and could besignificantly improved. The experiment section is missing many essential comparisons. The limits of 50s and 75s for bin packing seem very large. The paper presents two algorithms but many details are left out. This makes it impossible to understand how the algorithms work.\nThe paper introduces an extension to existing pattern generation algorithms for PDB heuristics. The way the paper is currently written, by interleaving background on existing methods, makes it hard to follow. The write-up is not very good and could besignificantly improved. The experiment section is missing many essential comparisons. The limits of 50s and 75s for bin packing seem very large. The paper presents two algorithms but many details are left out. This makes it impossible to understand how the algorithms work."
    },
    {
        "coherence": 0.18821182254646476,
        "consistency": 0.471589724259171,
        "fluency": 0.37648540120294216,
        "relevance": 0.023753737255910824,
        "overall": 0.2650101713161222,
        "Generated": "10",
        "Gold": "Paper proposes a new method to infer environment labels that can then be used for invariant learning (such as IRM methods) Essentially, it uses a neural network for clustering data into environments with appropriate loss functions. The novel method is shown to perform extremely well when compared to its competitors, achieving close to the performance of IRM. In my opinion, the experimental results are sufficient but the presentation should be improved (see below for specific suggestions)\nPaper proposes a new method to infer environment labels that can then be used for invariant learning (such as IRM methods) Essentially, it uses a neural network for clustering data into environments with appropriate loss functions. The novel method is shown to perform extremely well when compared to its competitors, achieving close to the performance of IRM. In my opinion, the experimental results are sufficient but the presentation should be improved (see below for specific suggestions)"
    },
    {
        "coherence": 0.1063542571692905,
        "consistency": 0.3584925015436447,
        "fluency": 0.4229363772093041,
        "relevance": 0.03620626556971613,
        "overall": 0.23099735037298888,
        "Generated": "11",
        "Gold": "This paper introduces a benchmark for RL-based control of traffic lights. The main weakness of this paper is that its main contribution, the benchmark, is provided under a no-derivatives license. This contribution consists of an OpenAI Gym style wrapper around the SUMO traffic simulation package. Experiments show that independent PPO/DQN tend to do quite well at the end of training. The paper is well written, and the benchmarking tasks are carefully chosen to match the real-world scenarios.\nThis paper introduces a benchmark for RL-based control of traffic lights. The main weakness of this paper is that its main contribution, the benchmark, is provided under a no-derivatives license. This contribution consists of an OpenAI Gym style wrapper around the SUMO traffic simulation package. Experiments show that independent PPO/DQN tend to do quite well at the end of training. The paper is well written, and the benchmarking tasks are carefully chosen to match the real-world scenarios."
    },
    {
        "coherence": 0.0875256768060968,
        "consistency": 0.20243056151614508,
        "fluency": 0.39587979443556953,
        "relevance": 0.033054066725645745,
        "overall": 0.1797225248708643,
        "Generated": "12",
        "Gold": "Theory of logical composition is used to build a new model of learning. The new model can be applied to a variety of tasks. It can also be used to solve problems that are too complex to be solved in the real world. The theory is based on the idea that the best way to learn a new skill is to start with the best knowledge of the previous skill. The goal of the theory is to make it easier to learn the new skill.\nTheory of logical composition is used to build a new model of learning. The new model can be applied to a variety of tasks. It can also be used to solve problems that are too complex to be solved in the real world. The theory is based on the idea that the best way to learn a new skill is to start with the best knowledge of the previous skill. The goal of the theory is to make it easier to learn the new skill."
    },
    {
        "coherence": 0.062168310020042916,
        "consistency": 0.19196165023715578,
        "fluency": 0.5409813215257674,
        "relevance": 0.02350950665403141,
        "overall": 0.20465519710924937,
        "Generated": "13",
        "Gold": "This paper is motivated by using reinforcement learning (RL) methods in inventory control. It customizes Q-learning for a special one-sided feedback/full-feedback setting. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on size of state and action space. I'm not totally convinced that the assumptions in this paper is general and realistic. I still have some concerns about the technical soundness.\nThis paper is motivated by using reinforcement learning (RL) methods in inventory control. It customizes Q-learning for a special one-sided feedback/full-feedback setting. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on size of state and action space. I'm not totally convinced that the assumptions in this paper is general and realistic. I still have some concerns about the technical soundness."
    },
    {
        "coherence": 0.1338265837670234,
        "consistency": 0.2743298430648811,
        "fluency": 0.38039832940524176,
        "relevance": 0.0847067844483935,
        "overall": 0.21831538517138493,
        "Generated": "14",
        "Gold": "Backdoor attack is becoming increasingly relevant in ML applications. This paper introduces a benchmark for backdoor attacks and defenses in the context of NLP models. The OpenBackdoor framework can be used by future attackers or defenders coming up with new algorithms. The paper focuses on a potentially important research track textual backdoor learning, which may be relevant to broad deep learning researchers in the future. It also provides evaluation metrics not only considering ASR/CACC, but also stealthy and semantic-preserving of poisoned text."
    },
    {
        "coherence": 0.10991825555828005,
        "consistency": 0.33737776347239357,
        "fluency": 0.37194065955563155,
        "relevance": 0.017802556540938656,
        "overall": 0.20925980878181097,
        "Generated": "15",
        "Gold": "Low-degree polynomials as a computational model used to study (in)tractability of learning/inference problems seems to be a useful and important model. The main limitation I can see is that the tree structure that makes this analysis go through is somewhat limited. The topic of this paper is completely out of my area, and any technical comments I make will probably be unfair to the authors. The paper is overall well-written and easy to read.\nLow-degree polynomials as a computational model used to study (in)tractability of learning/inference problems seems to be a useful and important model. The main limitation I can see is that the tree structure that makes this analysis go through is somewhat limited. The topic of this paper is completely out of my area, and any technical comments I make will probably be unfair to the authors. The paper is overall well-written and easy to read."
    },
    {
        "coherence": 0.3693413625231913,
        "consistency": 0.7978926126718526,
        "fluency": 0.43362077508091795,
        "relevance": 0.021067759260756356,
        "overall": 0.40548062738417956,
        "Generated": "16",
        "Gold": "Imitation with Planning at Test-time (IMPLANT) is a new algorithm for imitation learning. It incorporates decision-time planning within an inverse reinforcement learning algorithm. IMPLANT outperforms BC and GAIL-based baselines in the ‘non-transfer’ setting of imitation with limited expert trajectories. The paper is well-written and very easy to follow. The reward function in equation (4) is just a restatement of the optimality principle.\nImitation with Planning at Test-time (IMPLANT) is a new algorithm for imitation learning. It incorporates decision-time planning within an inverse reinforcement learning algorithm. IMPLANT outperforms BC and GAIL-based baselines in the ‘non-transfer’ setting of imitation with limited expert trajectories. The paper is well-written and very easy to follow. The reward function in equation (4) is just a restatement of the optimality principle."
    },
    {
        "coherence": 0.0998075552821086,
        "consistency": 0.27941617468266744,
        "fluency": 0.3784657823618259,
        "relevance": 0.0201482323084759,
        "overall": 0.19445943615876943,
        "Generated": "17",
        "Gold": "The proposed method uses a classifier and a synthetic data model. The data is then used to create a differentially private set of data. The results are published as separate sets of data, rather than as a single set. The method is based on the idea of using data as a tool to learn more about a subject. It is not clear how the data will be used in the future, but it is possible to use it to learn about the subject.\nThe proposed method uses a classifier and a synthetic data model. The data is then used to create a differentially private set of data. The results are published as separate sets of data, rather than as a single set. The method is based on the idea of using data as a tool to learn more about a subject. It is not clear how the data will be used in the future, but it is possible to use it to learn about the subject."
    },
    {
        "coherence": 0.14596881272313553,
        "consistency": 0.3229190301117344,
        "fluency": 0.29426420619196875,
        "relevance": 0.022929390674052984,
        "overall": 0.1965203599252229,
        "Generated": "18",
        "Gold": "This paper presents a comparative study of the performance of non-recurrent models and deep recurrent models for time series outlier detection. Seven models are evaluated on multiple synthetic and real-world time series datasets. The strengths of this paper are as follows: It can be applied to many real- world applications. It is unclear whether the used models are capable of detecting collective outliers. Motivation has to be reconsidered. Many questions of this form: are CNNs better? Are subsequence methods better? are traditional vs modern approaches better?\nThis paper presents a comparative study of the performance of non-recurrent models and deep recurrent models for time series outlier detection. Seven models are evaluated on multiple synthetic and real-world time series datasets. The strengths of this paper are as follows: It can be applied to many real- world applications. It is unclear whether the used models are capable of detecting collective outliers. Motivation has to be reconsidered. Many questions of this form: are CNNs better? Are subsequence methods better? are traditional vs modern approaches better?"
    },
    {
        "coherence": 0.17805774421989976,
        "consistency": 0.26383671624277555,
        "fluency": 0.3457530558877978,
        "relevance": 0.03894116666682003,
        "overall": 0.2066471707543233,
        "Generated": "19",
        "Gold": "Federated Averaging can generalize to new tasks (after fine-tuning) Theoretical guarantees are presented for a multi-task linear regression setting. Further empirical results demonstrate the effectiveness of learning representations with image classification tasks. The main contribution of this paper is theoretical, while the assumptions are very strong. The work by Collins et al. (ICML 2021) on shared representations should be compared more deeply with the proposed analysis since the two setting bear some similarity.\nFederated Averaging can generalize to new tasks (after fine-tuning) Theoretical guarantees are presented for a multi-task linear regression setting. Further empirical results demonstrate the effectiveness of learning representations with image classification tasks. The main contribution of this paper is theoretical, while the assumptions are very strong. The work by Collins et al. (ICML 2021) on shared representations should be compared more deeply with the proposed analysis since the two setting bear some similarity."
    },
    {
        "coherence": 0.08208050061580206,
        "consistency": 0.23919504731619107,
        "fluency": 0.34852691224124616,
        "relevance": 0.016137792787306156,
        "overall": 0.17148506324013638,
        "Generated": "20",
        "Gold": "\"This manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues\" \"Reading the other comments online, the authors seem to have addressed those concerns as well\" \"The paper add value to the research community through thorough experimental study as well as in industry. The paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks\"\n\"This manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues\" \"Reading the other comments online, the authors seem to have addressed those concerns as well\" \"The paper add value to the research community through thorough experimental study as well as in industry. The paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks\""
    },
    {
        "coherence": 0.16278879443841893,
        "consistency": 0.37934184333161025,
        "fluency": 0.4608279690330365,
        "relevance": 0.0503824073500509,
        "overall": 0.2633352535382792,
        "Generated": "21",
        "Gold": "This study proposes a disentangled representation learning method called \"one at a time\" It is a VAE-GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner. The most impressive part is the one-at-a-time (OAT) factor learning approach. However, it has several issues, including inaccurate claims, and many typos. A similar idea has already been proposed in 'Robust Disentanglement of a Few Factors at a Time' The authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset.\nThis study proposes a disentangled representation learning method called \"one at a time\" It is a VAE-GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner. The most impressive part is the one-at-a-time (OAT) factor learning approach. However, it has several issues, including inaccurate claims, and many typos. A similar idea has already been proposed in 'Robust Disentanglement of a Few Factors at a Time' The authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset."
    },
    {
        "coherence": 0.0499309103131843,
        "consistency": 0.12535637337501623,
        "fluency": 0.4699218371412996,
        "relevance": 0.022005527417271253,
        "overall": 0.16680366206169284,
        "Generated": "22",
        "Gold": "This paper presents a new 2D animation dataset for pixel-wise and region-wise (segment matching) correspondence. This dataset is made by converting three high-quality 3D movies into 2DAnimation. Authors clearly explain he motivation behind this dataset and what makes it different from existing ones. The results show the effectiveness of AnimeRun and the limitations of existing methods. There seems to be a lack of explicit Limitations section in this work, and parts of the writing seem incorrect.\nThis paper presents a new 2D animation dataset for pixel-wise and region-wise (segment matching) correspondence. This dataset is made by converting three high-quality 3D movies into 2DAnimation. Authors clearly explain he motivation behind this dataset and what makes it different from existing ones. The results show the effectiveness of AnimeRun and the limitations of existing methods. There seems to be a lack of explicit Limitations section in this work, and parts of the writing seem incorrect."
    },
    {
        "coherence": 0.1073690284071755,
        "consistency": 0.33779718056273766,
        "fluency": 0.26958867080774956,
        "relevance": 0.0291799285583146,
        "overall": 0.18598370208399434,
        "Generated": "23",
        "Gold": "This paper studies Combinatorial Multi-Armed Bandit (CMAB) problems with probabilistically-Triggered arms. CMAB is a variant of the standard bandit setting where the learner chooses a subset of arms. The mean reward is a function of the chosen subset and of its component arms’ mean rewards. This formulation generalizes problems including cascading bandits for ranking search results. The paper is very well-written and provides clear intuition behind various ideas/assumptions.\nThis paper studies Combinatorial Multi-Armed Bandit (CMAB) problems with probabilistically-Triggered arms. CMAB is a variant of the standard bandit setting where the learner chooses a subset of arms. The mean reward is a function of the chosen subset and of its component arms’ mean rewards. This formulation generalizes problems including cascading bandits for ranking search results. The paper is very well-written and provides clear intuition behind various ideas/assumptions."
    },
    {
        "coherence": 0.22536763773430063,
        "consistency": 0.3924257134807737,
        "fluency": 0.12276808904451421,
        "relevance": 0.029531525665686845,
        "overall": 0.19252324148131886,
        "Generated": "24",
        "Gold": "The paper is generally well-written, however, there is some confusion listed as follows. The goal of their training scheme is to create an ensemble of models that has a high disagreement on the out-of-distribution (OOD) samples in the unlabeled set. The success of the method largely relies on the models in the ensemble having sufficiently different outputs on OOD samples. Some claims are vague in the submission, for example, the authors claimed that the Maximum discrepancy method (MCD) tends to result in ensembles that do not disagree enough on Ood data."
    },
    {
        "coherence": 0.2816892555600138,
        "consistency": 0.44295148857896066,
        "fluency": 0.2627733525961826,
        "relevance": 0.042020686154143505,
        "overall": 0.2573586957223251,
        "Generated": "25",
        "Gold": "This paper investigates the collapsing problem of contrastive learning. It attempts to attribute the collapsing phenomena to strong augmentation and implicit regularization, using simple linear network models. Based on the above analysis, the paper then propose a simple sub- vector based CL method called DirectCLR. Experiments on ImageNet demonstrate the effectiveness of the proposed method. This paper shows that contrastive methods also suffer from the “dimensionality collapse” phenomenon. Two underlying causes for this problem are proved in simplified settings of shallow, linear networks.\nThis paper investigates the collapsing problem of contrastive learning. It attempts to attribute the collapsing phenomena to strong augmentation and implicit regularization, using simple linear network models. Based on the above analysis, the paper then propose a simple sub- vector based CL method called DirectCLR. Experiments on ImageNet demonstrate the effectiveness of the proposed method. This paper shows that contrastive methods also suffer from the “dimensionality collapse” phenomenon. Two underlying causes for this problem are proved in simplified settings of shallow, linear networks."
    },
    {
        "coherence": 0.14956459558895244,
        "consistency": 0.1805189848000152,
        "fluency": 0.26189200138058044,
        "relevance": 0.022969313113374454,
        "overall": 0.15373622372073062,
        "Generated": "26",
        "Gold": "The paper proposes an \"ensemble\" of autoencoders of varying depths and widths called RobOD. RobOD is competitive at outlier detection against other models, while being significantly more robust to the hyperparameter choices. The authors discuss that their method is costlier than a single training model. The proposed ROBOD seems to be simple and easy-to-implement, but enjoys both good effectiveness and efficiency against other AE-based baselines.\nThe paper proposes an \"ensemble\" of autoencoders of varying depths and widths called RobOD. RobOD is competitive at outlier detection against other models, while being significantly more robust to the hyperparameter choices. The authors discuss that their method is costlier than a single training model. The proposed ROBOD seems to be simple and easy-to-implement, but enjoys both good effectiveness and efficiency against other AE-based baselines."
    },
    {
        "coherence": 0.14198042426869964,
        "consistency": 0.2742818180989117,
        "fluency": 0.3741750136250363,
        "relevance": 0.045772336678104444,
        "overall": 0.209052398167688,
        "Generated": "27",
        "Gold": "This paper studies the last-iterate convergence rate to a Nash equilibrium in monotone games. It extends previous work to analyze the convergence of no-regret learning algorithm with constant step size in (possibly) constrained games. The technique used for finding the potential function needed to prove the result is novel and interesting. This paper has no negative societal impact. The authors propose a novel notion, namely the tangent residual. Tangent residual is proposed as a new proximity measure to the Nash equilibrium.\nThis paper studies the last-iterate convergence rate to a Nash equilibrium in monotone games. It extends previous work to analyze the convergence of no-regret learning algorithm with constant step size in (possibly) constrained games. The technique used for finding the potential function needed to prove the result is novel and interesting. This paper has no negative societal impact. The authors propose a novel notion, namely the tangent residual. Tangent residual is proposed as a new proximity measure to the Nash equilibrium."
    },
    {
        "coherence": 0.1319040799852331,
        "consistency": 0.20502678932470683,
        "fluency": 0.3595369282406741,
        "relevance": 0.044987233366537475,
        "overall": 0.18536375772928787,
        "Generated": "28",
        "Gold": "This paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The authors also find ‘phase transitions’ where a small increase in capability results in qualitatively new reward hacking behavior. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking.\nThis paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The authors also find ‘phase transitions’ where a small increase in capability results in qualitatively new reward hacking behavior. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking."
    },
    {
        "coherence": 0.20695265934698875,
        "consistency": 0.3941422061429014,
        "fluency": 0.4022811443606963,
        "relevance": 0.048554212455399655,
        "overall": 0.2629825555764965,
        "Generated": "29",
        "Gold": "The paper proposes a method for accelerating distributed high-dimensional regression under sparsity constraints. The method combines Nesterov’s proximal gradient with consensus and gradient-tracking mechanisms. This method can estimate locally the gradient of the empirical loss while enforcing agreement on the local estimates. They show it converges globally at a linear rate, achieving both optimal iteration complexity and communication complexity. While there is sufficient contribution, many of the core technical ideas build upon classical approaches to decentralized optimization.\nThe paper proposes a method for accelerating distributed high-dimensional regression under sparsity constraints. The method combines Nesterov’s proximal gradient with consensus and gradient-tracking mechanisms. This method can estimate locally the gradient of the empirical loss while enforcing agreement on the local estimates. They show it converges globally at a linear rate, achieving both optimal iteration complexity and communication complexity. While there is sufficient contribution, many of the core technical ideas build upon classical approaches to decentralized optimization."
    },
    {
        "coherence": 0.17398264191361143,
        "consistency": 0.3209757540324336,
        "fluency": 0.27034532387265353,
        "relevance": 0.07844657078484897,
        "overall": 0.2109375726508869,
        "Generated": "30",
        "Gold": "This paper tackles the conformal prediction problem under adversarial perturbations. The proposed approach combines the non-conformity score with randomized smoothing and the standard conformal Prediction. The effectiveness of the proposed methods was demonstrated on the CifAR10, CIFAR100, and ImageNet datasets. For safety-critical applications, it is important to build robust ways of quantifying uncertainty. This paper is well-written and reach the acceptance bar of ICLR."
    },
    {
        "coherence": 0.3066228216747433,
        "consistency": 0.4001282777747594,
        "fluency": 0.3166866736967195,
        "relevance": 0.024240985000575973,
        "overall": 0.2619196895366996,
        "Generated": "31",
        "Gold": "This paper describes a NeRF-like representation for binaural room impulse responses in a given acoustic space. It is an implicit representation of listener position, orientation, and ear, as well as source position, time, and frequency. Experiments on the Soundspaces and MeshRIR datasets show that it is able to reproduce realistic effects in loudness maps. The paper is clear and well-written. The method appears to work well.\nThis paper describes a NeRF-like representation for binaural room impulse responses in a given acoustic space. It is an implicit representation of listener position, orientation, and ear, as well as source position, time, and frequency. Experiments on the Soundspaces and MeshRIR datasets show that it is able to reproduce realistic effects in loudness maps. The paper is clear and well-written. The method appears to work well."
    },
    {
        "coherence": 0.1018722440538023,
        "consistency": 0.16852569754227043,
        "fluency": 0.5424712887169288,
        "relevance": 0.026160313927499918,
        "overall": 0.2097573860601254,
        "Generated": "32",
        "Gold": "This paper reinterprets the theory of PPO-clip based on the hinge policy optimization. It also generalizes the algorithm to a new family of policy-based algorithms. The paper is very well-written both in terms of how the main parts of the paper are easy to follow. The only \"weakness\" is that PPO is a heavily used algorithm and the empirical results are not convincing enough. This paper suggests a generalization to the commonly used PPO(-clip) algorithm.\nThis paper reinterprets the theory of PPO-clip based on the hinge policy optimization. It also generalizes the algorithm to a new family of policy-based algorithms. The paper is very well-written both in terms of how the main parts of the paper are easy to follow. The only \"weakness\" is that PPO is a heavily used algorithm and the empirical results are not convincing enough. This paper suggests a generalization to the commonly used PPO(-clip) algorithm."
    },
    {
        "coherence": 0.12223798342256578,
        "consistency": 0.35436853215812647,
        "fluency": 0.5276183816292189,
        "relevance": 0.040594554482284634,
        "overall": 0.2612048629230489,
        "Generated": "33",
        "Gold": "The paper addresses the problem of sample-efficient inference for symbolic physical rules. Authors propose a generative model along with a symbolic regression framework. Forces are produced from a probabilistic context free grammar designed to mimic simple Newtonian physics. The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility. The paper proposes an Bayesian-symbolic physics (BSP), an intuitive physics model.\nThe paper addresses the problem of sample-efficient inference for symbolic physical rules. Authors propose a generative model along with a symbolic regression framework. Forces are produced from a probabilistic context free grammar designed to mimic simple Newtonian physics. The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility. The paper proposes an Bayesian-symbolic physics (BSP), an intuitive physics model."
    },
    {
        "coherence": 0.08616046370349072,
        "consistency": 0.28017682067136696,
        "fluency": 0.4237193423578137,
        "relevance": 0.02019264702862371,
        "overall": 0.20256231844032377,
        "Generated": "34",
        "Gold": "Automata was developed with the goal of accelerating the search for the optimal configuration in order to minimize the cost and environmental impact of the hyper-parameter tuning process. The key mechanism described by the authors as the main contributor to the savings and speed-ups achieved by Automata is DSS. The paper’s empirical work is very well done. It contains clear experiments setting, abundant real world examples, thoughtful baselines, detailed ablation studies and interesting analysis.\nAutomata was developed with the goal of accelerating the search for the optimal configuration in order to minimize the cost and environmental impact of the hyper-parameter tuning process. The key mechanism described by the authors as the main contributor to the savings and speed-ups achieved by Automata is DSS. The paper’s empirical work is very well done. It contains clear experiments setting, abundant real world examples, thoughtful baselines, detailed ablation studies and interesting analysis."
    },
    {
        "coherence": 0.09288487292143184,
        "consistency": 0.3026623790632038,
        "fluency": 0.4954146959032808,
        "relevance": 0.023741616149508104,
        "overall": 0.22867589100935615,
        "Generated": "35",
        "Gold": "Theorem 1 does not hold because an important condition is missing. The method is very ad-hoc, without any theoretical justification. It might also be computationally challenging to optimize the proposed loss for extreme multi-label datasets. Both the proposed methods outperform a host of highly competitive baselines on a variety of datasets by significant margins. It is based on the finding that the distribution of the norms of the learnt weight vectors also follows a power-law.\nTheorem 1 does not hold because an important condition is missing. The method is very ad-hoc, without any theoretical justification. It might also be computationally challenging to optimize the proposed loss for extreme multi-label datasets. Both the proposed methods outperform a host of highly competitive baselines on a variety of datasets by significant margins. It is based on the finding that the distribution of the norms of the learnt weight vectors also follows a power-law."
    },
    {
        "coherence": 0.1556749292468779,
        "consistency": 0.2435101368597249,
        "fluency": 0.3739738254675524,
        "relevance": 0.014360803416250569,
        "overall": 0.19687992374760144,
        "Generated": "36",
        "Gold": "The authors propose a novel churn reduction algorithm based on distillation. It involves the training of a classifier by minimizing a distilled loss and solving a convex program. The authors validate empirically their approach on 12 OpenML datasets. The primary limitation of this study is that, while the underlying problem framework is intriguing, it does not appear to make a significant impact. I encourage authors to make additional improvements, particularly for the experiments described in Section 5.\nThe authors propose a novel churn reduction algorithm based on distillation. It involves the training of a classifier by minimizing a distilled loss and solving a convex program. The authors validate empirically their approach on 12 OpenML datasets. The primary limitation of this study is that, while the underlying problem framework is intriguing, it does not appear to make a significant impact. I encourage authors to make additional improvements, particularly for the experiments described in Section 5."
    },
    {
        "coherence": 0.07733587026538319,
        "consistency": 0.16042088624906164,
        "fluency": 0.47572697364931005,
        "relevance": 0.05292380983957594,
        "overall": 0.1916018850008327,
        "Generated": "37",
        "Gold": "Draft is a vector extension of [1] on studying how to approximately solve the global optima of a two-layered Relu network. The key of the analysis is to enumerate all possible sign patterns of the ReLU unit generating from specific data. A Frank-Wolfe algorithm for finding the global optimum of the resulting convex program is proposed and evaluated on smaller datasets. Unfortunately in the general case the complexity is exponential in the rank of the data matrix. The notation in some critical parts of the paper is not clear and makes reading difficult."
    },
    {
        "coherence": 0.07115162959779174,
        "consistency": 0.2862571588877141,
        "fluency": 0.4491601882803334,
        "relevance": 0.024904079594673206,
        "overall": 0.20786826409012812,
        "Generated": "38",
        "Gold": "PCAI and PIC allow for more efficient handling of missing data. There are two use cases, fashion MNIST (imaging data) and Parkinson (voice recording) dataset. Main assumption is that introducing random missing is not acceptable without sufficient evidence and justification. The experiments indicate that the imputation presents low MSE and, when applied to classification tasks, results in similar or better accuracy values. Weaknesses: The authors only compared PCAI strategy with the traditional strategy.\nPCAI and PIC allow for more efficient handling of missing data. There are two use cases, fashion MNIST (imaging data) and Parkinson (voice recording) dataset. Main assumption is that introducing random missing is not acceptable without sufficient evidence and justification. The experiments indicate that the imputation presents low MSE and, when applied to classification tasks, results in similar or better accuracy values. Weaknesses: The authors only compared PCAI strategy with the traditional strategy."
    },
    {
        "coherence": 0.09322477419661714,
        "consistency": 0.1960993839192184,
        "fluency": 0.38170996758924075,
        "relevance": 0.02075026849019741,
        "overall": 0.17294609854881843,
        "Generated": "39",
        "Gold": "This paper presents a self-supervised learning with an information maximization criterion among alternative latent representations of the same input. It considers a second-order-statistics based mutual information measure, the log-determinant mutual information (LDMI), which is equivalent to Shannon mutual information under Gaussian distribution. In general, the paper is well written and reasonably understandable. I found the underlying theory to be very strong, however the presented experiments aren't sufficient to show the strength of the work.\nThis paper presents a self-supervised learning with an information maximization criterion among alternative latent representations of the same input. It considers a second-order-statistics based mutual information measure, the log-determinant mutual information (LDMI), which is equivalent to Shannon mutual information under Gaussian distribution. In general, the paper is well written and reasonably understandable. I found the underlying theory to be very strong, however the presented experiments aren't sufficient to show the strength of the work."
    },
    {
        "coherence": 0.16046328293200623,
        "consistency": 0.38445935448445784,
        "fluency": 0.4188469434451028,
        "relevance": 0.027159511284154846,
        "overall": 0.24773227303643044,
        "Generated": "40",
        "Gold": "The paper proposes a method to learn an environment field that predicts the distance from any location in the map to a query location. At the core of the method is an implicit neural representation of environment. The method seems to be performing similarly to baselines or slightly better, while being more efficient. The paper proposes modeling reaching distance between any start position and any goal (subject to obstacle avoidance) with a neural network. The network is trained in a supervised manner on data obtained from a traditional search method that assumes discrete states. The usefulness of the trained value network for navigation and its generalization properties are then experimentally validated.\nThe paper proposes a method to learn an environment field that predicts the distance from any location in the map to a query location. At the core of the method is an implicit neural representation of environment. The method seems to be performing similarly to baselines or slightly better, while being more efficient. The paper proposes modeling reaching distance between any start position and any goal (subject to obstacle avoidance) with a neural network. The network is trained in a supervised manner on data obtained from a traditional search method that assumes discrete states. The usefulness of the trained value network for navigation and its generalization properties are then experimentally validated."
    },
    {
        "coherence": 0.2113794836249545,
        "consistency": 0.45554416025467404,
        "fluency": 0.4455709383603653,
        "relevance": 0.02499769448879911,
        "overall": 0.28437306918219823,
        "Generated": "41",
        "Gold": "This paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. In order to disentangle the identity information from the attributes the paper proposes adversarial information factorization. The motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.\nThis paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. In order to disentangle the identity information from the attributes the paper proposes adversarial information factorization. The motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset."
    },
    {
        "coherence": 0.05619098769673813,
        "consistency": 0.18050893582589472,
        "fluency": 0.544491053443481,
        "relevance": 0.012571954663010183,
        "overall": 0.198440732907281,
        "Generated": "42",
        "Gold": "Surreal-GAN aims to create fake pathological data with a latent variable and a healthy input. It includes an inverse function that predicts the latent variable from a fake/real pathological data. The results show that the model has a higher c-index than NMF, LDA and FA predictions. The method frames the diseased brains as multiple disease-related features at various severity imposed on normal brains. The paper is well clearly organized, and the general idea of the method is very interesting.\nSurreal-GAN aims to create fake pathological data with a latent variable and a healthy input. It includes an inverse function that predicts the latent variable from a fake/real pathological data. The results show that the model has a higher c-index than NMF, LDA and FA predictions. The method frames the diseased brains as multiple disease-related features at various severity imposed on normal brains. The paper is well clearly organized, and the general idea of the method is very interesting."
    },
    {
        "coherence": 0.08331847592054907,
        "consistency": 0.24240876342593073,
        "fluency": 0.5157499867378195,
        "relevance": 0.01793381409916295,
        "overall": 0.21485276004586557,
        "Generated": "43",
        "Gold": "The paper is motivated by the need for a) thorough evaluation of data-driven approaches in scientific computing pipelines and b) the lack of standardized benchmarks in the literature. The work is interesting, and important for learning physical systems. However, the value of this benchmark needs to be more explained, please see Weaknesses. The paper is very concise, easy-to-follow and well-illustrated. The flexibility of their framework allows for the integration of other learning tasks or machine learning methods.\nThe paper is motivated by the need for a) thorough evaluation of data-driven approaches in scientific computing pipelines and b) the lack of standardized benchmarks in the literature. The work is interesting, and important for learning physical systems. However, the value of this benchmark needs to be more explained, please see Weaknesses. The paper is very concise, easy-to-follow and well-illustrated. The flexibility of their framework allows for the integration of other learning tasks or machine learning methods."
    },
    {
        "coherence": 0.05833881963142903,
        "consistency": 0.2787090905519979,
        "fluency": 0.25889571049468335,
        "relevance": 0.024560396961820455,
        "overall": 0.15512600440998267,
        "Generated": "44",
        "Gold": "The authors present a new benchmark for three different types of legal tasks (classification, judgement prediction, and summarization) in Korean. Existing legal tasks are largely English-oriented. Having legal NLP tasks in a diverse set of languages is important. The authors show that pretraining on a domain specific corpus result in performance gains across several of the tasks. The whole pipeline, from the preprocessing to the evaluation, lacks rigors and makes me doubt the pertinence of the contribution.\nThe authors present a new benchmark for three different types of legal tasks (classification, judgement prediction, and summarization) in Korean. Existing legal tasks are largely English-oriented. Having legal NLP tasks in a diverse set of languages is important. The authors show that pretraining on a domain specific corpus result in performance gains across several of the tasks. The whole pipeline, from the preprocessing to the evaluation, lacks rigors and makes me doubt the pertinence of the contribution."
    },
    {
        "coherence": 0.043462416744219384,
        "consistency": 0.1716557259137085,
        "fluency": 0.2904170493453858,
        "relevance": 0.02076852859833926,
        "overall": 0.13157593015041325,
        "Generated": "45",
        "Gold": "This is the first work to design a benchmark for graph neural architecture search. The benchmark can provide a database of GNN architectures for quick look-up. The code that was used to create the benchmark data is not open sourced. The studied problem is novel, but not important/interesting. Different NAS algorithms might have different performances on different problems/tasks/datasets. Concerns raised from other reviewers are either not convincing to me or adequately addressed by the authors.\nThis is the first work to design a benchmark for graph neural architecture search. The benchmark can provide a database of GNN architectures for quick look-up. The code that was used to create the benchmark data is not open sourced. The studied problem is novel, but not important/interesting. Different NAS algorithms might have different performances on different problems/tasks/datasets. Concerns raised from other reviewers are either not convincing to me or adequately addressed by the authors."
    },
    {
        "coherence": 0.048483021742955906,
        "consistency": 0.0984684380876226,
        "fluency": 0.388428190503257,
        "relevance": 0.0114405793430337,
        "overall": 0.1367050574192173,
        "Generated": "46",
        "Gold": "The paper presents a deep reinforcement learning model for portfolio optimization that harnesses both cross-asset dependencies and time dependencies. The proposed approach is evaluated on three sets of data from the Canadian and US stock markets. The results show that the proposed approach outperformed baselines in terms of different metrics. The paper develops the first CNN based portfolio optimization network named **WaveCorr** It is capable of capturing both temporal and cross-sectional correlation structure for the training data.\nThe paper presents a deep reinforcement learning model for portfolio optimization that harnesses both cross-asset dependencies and time dependencies. The proposed approach is evaluated on three sets of data from the Canadian and US stock markets. The results show that the proposed approach outperformed baselines in terms of different metrics. The paper develops the first CNN based portfolio optimization network named **WaveCorr** It is capable of capturing both temporal and cross-sectional correlation structure for the training data."
    },
    {
        "coherence": 0.18090051677193447,
        "consistency": 0.3537567575775485,
        "fluency": 0.37654658401827096,
        "relevance": 0.027115526260101126,
        "overall": 0.23457984615696378,
        "Generated": "47",
        "Gold": "The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature. This is motivated by a link between negatively curved edges and graph bottlenecks. Though the theoretical contribution is strong the empirical evidence is not strong. The paper is well-written and well-organized. The authors show an impressive understanding and familiarity with the related and classical work. It provides an analytical explanation for the over-squashing and GNN bottleneck phenomena.\nThe authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature. This is motivated by a link between negatively curved edges and graph bottlenecks. Though the theoretical contribution is strong the empirical evidence is not strong. The paper is well-written and well-organized. The authors show an impressive understanding and familiarity with the related and classical work. It provides an analytical explanation for the over-squashing and GNN bottleneck phenomena."
    },
    {
        "coherence": 0.17029483096468198,
        "consistency": 0.3823272317313899,
        "fluency": 0.22499892188784634,
        "relevance": 0.037726086474186765,
        "overall": 0.20383676776452622,
        "Generated": "48",
        "Gold": "The paper presents an empirical study of the role of various stages in recent extreme compression of Transformer architectures for NLP tasks. Authors find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on the study, they propose a new compression pipeline named XtrmC. The paper contains valuable engineering insights, yet its presentation is largely hampered by the careless writing and lack of basic proofreading. It lacks a detailed analyses of the computational complexity of the various methods.\nThe paper presents an empirical study of the role of various stages in recent extreme compression of Transformer architectures for NLP tasks. Authors find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on the study, they propose a new compression pipeline named XtrmC. The paper contains valuable engineering insights, yet its presentation is largely hampered by the careless writing and lack of basic proofreading. It lacks a detailed analyses of the computational complexity of the various methods."
    },
    {
        "coherence": 0.12886469008977366,
        "consistency": 0.23139911828585863,
        "fluency": 0.4381791269986617,
        "relevance": 0.047434693990930844,
        "overall": 0.2114694073413062,
        "Generated": "49",
        "Gold": "The paper makes the previously unreported finding that contrastive learning produces \"group disentangled\" representations and outperform other disentanglement models. It offers a nice literature review, exposition is clear and provides many implementation details that should help reproducibility. I like the simple and clear idea (to study disentangler properties of contrastive methods) and the comprehensive experimental results. The selection of the normalization function affects the results.\nThe paper makes the previously unreported finding that contrastive learning produces \"group disentangled\" representations and outperform other disentanglement models. It offers a nice literature review, exposition is clear and provides many implementation details that should help reproducibility. I like the simple and clear idea (to study disentangler properties of contrastive methods) and the comprehensive experimental results. The selection of the normalization function affects the results."
    },
    {
        "coherence": 0.04352760219284808,
        "consistency": 0.11880715685264867,
        "fluency": 0.34654569166633886,
        "relevance": 0.019228178729210746,
        "overall": 0.1320271573602616,
        "Generated": "50",
        "Gold": "This work explored how to use surrogate models to expand the existing (and limited) neural architecture search -- NAS -- benchmark. This surrogate model is able to predict all architectures in DARTS search space, which is about 10^18 possible architectures. All codes are open-sourced, which demonstrated the good reproducibility of this work. In general, this is a good NAS paper that explored a new direction. Overall, the paper is well-written but it does not contain significant contributions.\nThis work explored how to use surrogate models to expand the existing (and limited) neural architecture search -- NAS -- benchmark. This surrogate model is able to predict all architectures in DARTS search space, which is about 10^18 possible architectures. All codes are open-sourced, which demonstrated the good reproducibility of this work. In general, this is a good NAS paper that explored a new direction. Overall, the paper is well-written but it does not contain significant contributions."
    },
    {
        "coherence": 0.09446167215784664,
        "consistency": 0.25432056967726546,
        "fluency": 0.28881968307703915,
        "relevance": 0.026154548355255193,
        "overall": 0.1659391183168516,
        "Generated": "51",
        "Gold": "In this work, the authors first analyze the behavior of injecting backdoors into a well-trained clean model via fine-tuning it on a poisoned dataset. The authors propose to evaluate the consistency of the backdoor performance with both global and instance-wise consistency. They propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. In general, this paper is novel and solid, and I recommend a strong acceptance.\nIn this work, the authors first analyze the behavior of injecting backdoors into a well-trained clean model via fine-tuning it on a poisoned dataset. The authors propose to evaluate the consistency of the backdoor performance with both global and instance-wise consistency. They propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. In general, this paper is novel and solid, and I recommend a strong acceptance."
    },
    {
        "coherence": 0.1497081186664947,
        "consistency": 0.3240486239624158,
        "fluency": 0.4242881171918244,
        "relevance": 0.021384005449295487,
        "overall": 0.2298572163175076,
        "Generated": "52",
        "Gold": "The problem is shown to be not-submodular, and therefore the most standard methods don't quite work. The authors present a greedy algorithm to get a reasonable approximation. The stronger contribution in my view is the observation that this problem helps in the COVID vaccine design pipeline. A set of experiments suggests the proposed approach outperforms the formulations considered. The novelty of the proposed work is limited. An expert would likely be able to take the formulations in the paper and turn them into code.\nThe problem is shown to be not-submodular, and therefore the most standard methods don't quite work. The authors present a greedy algorithm to get a reasonable approximation. The stronger contribution in my view is the observation that this problem helps in the COVID vaccine design pipeline. A set of experiments suggests the proposed approach outperforms the formulations considered. The novelty of the proposed work is limited. An expert would likely be able to take the formulations in the paper and turn them into code."
    },
    {
        "coherence": 0.09296714184449112,
        "consistency": 0.23950652870923908,
        "fluency": 0.47166603430398896,
        "relevance": 0.021628707654042408,
        "overall": 0.20644210312794042,
        "Generated": "53",
        "Gold": "Batch augmentation is a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample. This will increase the size of the batch by M. The authors claim the proposed method has better generalization performance. The improvement on test errors does not look significant. The paper shows that training with large batch size serves as an effective regularization method for deep networks.\nBatch augmentation is a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample. This will increase the size of the batch by M. The authors claim the proposed method has better generalization performance. The improvement on test errors does not look significant. The paper shows that training with large batch size serves as an effective regularization method for deep networks."
    },
    {
        "coherence": 0.10544015010529374,
        "consistency": 0.2558319073829386,
        "fluency": 0.2841380423937991,
        "relevance": 0.012856042190369676,
        "overall": 0.16456653551810024,
        "Generated": "54",
        "Gold": "This paper presents a dynamic inference paradigm based on selective inference of convolutions on the spatial dimension. For each convolutional block, it uses a masker layer to predict the masked region of the feature map. Such convolution has less computation than a full convolution. Experiments are performed on image classification and demonstrate latency reduction of 23 to 45% depending on the hardware superiority. This submission introduces a spatially dynamic neural network approach, developed in a latency-aware manner that can generate realistic speed-ups during inference.\nThis paper presents a dynamic inference paradigm based on selective inference of convolutions on the spatial dimension. For each convolutional block, it uses a masker layer to predict the masked region of the feature map. Such convolution has less computation than a full convolution. Experiments are performed on image classification and demonstrate latency reduction of 23 to 45% depending on the hardware superiority. This submission introduces a spatially dynamic neural network approach, developed in a latency-aware manner that can generate realistic speed-ups during inference."
    },
    {
        "coherence": 0.1260314277233979,
        "consistency": 0.2650220085708447,
        "fluency": 0.204756341735833,
        "relevance": 0.021238538483397122,
        "overall": 0.15426207912836817,
        "Generated": "55",
        "Gold": "The approach is benchmarked against important baseline approaches, U-Net and DeepLabv3. The method has a lot of overlap with the DoD-Net paper so I feel the technical novelty of this paper is limited. Is it possible to publicly release (part of) the data and set up a challenge around this? The problem of having a multiple class segmentation method from partially labeled datasets is of value to the community. No current solution is satisfactory.\nThe approach is benchmarked against important baseline approaches, U-Net and DeepLabv3. The method has a lot of overlap with the DoD-Net paper so I feel the technical novelty of this paper is limited. Is it possible to publicly release (part of) the data and set up a challenge around this? The problem of having a multiple class segmentation method from partially labeled datasets is of value to the community. No current solution is satisfactory."
    },
    {
        "coherence": 0.1037245073838929,
        "consistency": 0.21077707585646765,
        "fluency": 0.3318270448725663,
        "relevance": 0.014387108514545588,
        "overall": 0.1651789341568681,
        "Generated": "56",
        "Gold": "The proposed learnable audio frontend (LEAF) is a generalization of a mel filterbank, used commonly in machine audition. The paper evaluates it on several tasks in the audio domain such as acoustic event classification, speaker identification, keyword spotting, language identification, music classification etc. It is reported to yield on par or better classification results than the other methods on several of these tasks using single- or multi-task learning. This paper is well written and easy to follow. It lacks humility in its story-telling and its style.\nThe proposed learnable audio frontend (LEAF) is a generalization of a mel filterbank, used commonly in machine audition. The paper evaluates it on several tasks in the audio domain such as acoustic event classification, speaker identification, keyword spotting, language identification, music classification etc. It is reported to yield on par or better classification results than the other methods on several of these tasks using single- or multi-task learning. This paper is well written and easy to follow. It lacks humility in its story-telling and its style."
    },
    {
        "coherence": 0.14086142785941372,
        "consistency": 0.39106015084427076,
        "fluency": 0.16793136653106588,
        "relevance": 0.05324940140317595,
        "overall": 0.18827558665948158,
        "Generated": "57",
        "Gold": "This paper introduces two online sequential tests of equality and contrasts, for Bernoulli and Poisson process. The new tests show the speed up in decision making and reduce the opportunity cost in data collection process. Strengths: The design of thetest is technically solid. Weakness: The assumption on the parameters (line 52 to line 56) about the alternative may not be accurate. This paper proposes new sequential hypothesis tests and corresponding confidence sets and intervals for multin coefficients.\nThis paper introduces two online sequential tests of equality and contrasts, for Bernoulli and Poisson process. The new tests show the speed up in decision making and reduce the opportunity cost in data collection process. Strengths: The design of thetest is technically solid. Weakness: The assumption on the parameters (line 52 to line 56) about the alternative may not be accurate. This paper proposes new sequential hypothesis tests and corresponding confidence sets and intervals for multin coefficients."
    },
    {
        "coherence": 0.31180712682368145,
        "consistency": 0.48033180463601766,
        "fluency": 0.1000241155574418,
        "relevance": 0.03776531331801556,
        "overall": 0.23248209008378914,
        "Generated": "58",
        "Gold": "The authors consider the problem of finding an approximate extensive-form correlated equilibrium (EFCE) of a finite general-sum multiplayer extensive- form game. They propose an accelerated version via optimism of the algorithm by Farina et al. (2019a) The paper is well written (see specific comments and the concern below) To me this is a good and important theoretical contribution to the existing literature. It is concluded by an experimental evaluation of the. algorithm that shows that it is superior or performs on par with the algorithm of Farina. et. al.\nThe authors consider the problem of finding an approximate extensive-form correlated equilibrium (EFCE) of a finite general-sum multiplayer extensive- form game. They propose an accelerated version via optimism of the algorithm by Farina et al. (2019a) The paper is well written (see specific comments and the concern below) To me this is a good and important theoretical contribution to the existing literature. It is concluded by an experimental evaluation of the. algorithm that shows that it is superior or performs on par with the algorithm of Farina. et. al."
    },
    {
        "coherence": 0.17440721975204634,
        "consistency": 0.32035770411866266,
        "fluency": 0.19134036519693187,
        "relevance": 0.04222165554897947,
        "overall": 0.1820817361541551,
        "Generated": "59",
        "Gold": "This submission reproduces the work \"Transformer interpretability beyond attention visualization’ by Chefer et al. (2021) The authors then go beyond the work by integrating the resulting relevance scores into the pixel affinity propagation framework of Ahn and Kwak (2018) Although the experiments do not surpass the state-of-the-art, this experiment shows promising preliminary results. It would have been better to present a more detailed discussion on hyper-parameter tuning.\nThis submission reproduces the work \"Transformer interpretability beyond attention visualization’ by Chefer et al. (2021) The authors then go beyond the work by integrating the resulting relevance scores into the pixel affinity propagation framework of Ahn and Kwak (2018) Although the experiments do not surpass the state-of-the-art, this experiment shows promising preliminary results. It would have been better to present a more detailed discussion on hyper-parameter tuning."
    },
    {
        "coherence": 0.07190817789333595,
        "consistency": 0.2772024696286184,
        "fluency": 0.32159733997608153,
        "relevance": 0.026869992625929578,
        "overall": 0.17439449503099136,
        "Generated": "60",
        "Gold": "The paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results. This is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in \"Technical Approach\" is quite minimilistic and scarce in details.\nThe paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results. This is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in \"Technical Approach\" is quite minimilistic and scarce in details."
    },
    {
        "coherence": 0.17131747365717442,
        "consistency": 0.38456501759477985,
        "fluency": 0.17641920483776843,
        "relevance": 0.03491990489680554,
        "overall": 0.19180540024663204,
        "Generated": "61",
        "Gold": "The paper studies the acceleration of unitary neural network training. The main strategy is to obtain a low rank approximation of the gradient update and then use Riemannian gradient descent. This approximation allows the proposed method to be essentially a factor of O(n) faster than its competitors. The paper is well-motivated and clearly-written. The proposed methods are simple-yet-effective, and the theoretical results are strong. Comparison with prior works looks comprehensive.\nThe paper studies the acceleration of unitary neural network training. The main strategy is to obtain a low rank approximation of the gradient update and then use Riemannian gradient descent. This approximation allows the proposed method to be essentially a factor of O(n) faster than its competitors. The paper is well-motivated and clearly-written. The proposed methods are simple-yet-effective, and the theoretical results are strong. Comparison with prior works looks comprehensive."
    },
    {
        "coherence": 0.13848782849594404,
        "consistency": 0.29950201866162646,
        "fluency": 0.2124434476020013,
        "relevance": 0.025852686250107237,
        "overall": 0.16907149525241977,
        "Generated": "62",
        "Gold": "The paper studies the problem of robust envy-free rent division. In the conventional problem variant, given n individuals and their evaluations for n rooms, the goal is to find a rule to allocate the rooms and fairly split the rent such that everyone is satisfied. This paper proposes three robust approaches for the envy- free rent division problem. The robustness and the computation times of the proposed approaches are demonstrated on user data collected from Spliddit.\nThe paper studies the problem of robust envy-free rent division. In the conventional problem variant, given n individuals and their evaluations for n rooms, the goal is to find a rule to allocate the rooms and fairly split the rent such that everyone is satisfied. This paper proposes three robust approaches for the envy- free rent division problem. The robustness and the computation times of the proposed approaches are demonstrated on user data collected from Spliddit."
    },
    {
        "coherence": 0.13621233091059695,
        "consistency": 0.28704054936542445,
        "fluency": 0.16486236852219374,
        "relevance": 0.023304521282416564,
        "overall": 0.1528549425201579,
        "Generated": "63",
        "Gold": "The paper proposes a learning framework for compositional representations of goals for goal-conditioned RL. The goals reside in a low-dimensional representation space that is obtained from high-dimensional sensory data. The method is simple to implement in existing architectures and uses established approaches that have been developed for some time now. The paper is overall very well written, and the ideas and results are communicated clearly. It shows favorable performance over several baselines on multiple environments.\nThe paper proposes a learning framework for compositional representations of goals for goal-conditioned RL. The goals reside in a low-dimensional representation space that is obtained from high-dimensional sensory data. The method is simple to implement in existing architectures and uses established approaches that have been developed for some time now. The paper is overall very well written, and the ideas and results are communicated clearly. It shows favorable performance over several baselines on multiple environments."
    },
    {
        "coherence": 0.27041211892248823,
        "consistency": 0.4433543351420404,
        "fluency": 0.3514517317102857,
        "relevance": 0.12468102033853676,
        "overall": 0.2974748015283378,
        "Generated": "64",
        "Gold": "This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers.\nThis work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers."
    },
    {
        "coherence": 0.1501609571977254,
        "consistency": 0.31636871977735265,
        "fluency": 0.29340911727094854,
        "relevance": 0.026952257712575183,
        "overall": 0.19672276298965044,
        "Generated": "65",
        "Gold": "The paper uses three techniques to \"sparsify\" a dense network over training. The authors show SOTA performance for the high sparsity-regime on ResNet-50 for Imagenet. The proposed method allows for one-cycle unstructured pruning that facilitates rewiring during training. It adopts a soft-threshold pruning strategy and progressively increase the sparsity ratio of the network along the training iterations.\nThe paper uses three techniques to \"sparsify\" a dense network over training. The authors show SOTA performance for the high sparsity-regime on ResNet-50 for Imagenet. The proposed method allows for one-cycle unstructured pruning that facilitates rewiring during training. It adopts a soft-threshold pruning strategy and progressively increase the sparsity ratio of the network along the training iterations."
    },
    {
        "coherence": 0.14053931788423657,
        "consistency": 0.2751601780530362,
        "fluency": 0.15523681596277425,
        "relevance": 0.021956740835614733,
        "overall": 0.14822326318391543,
        "Generated": "66",
        "Gold": "The paper introduces a “predict-and-denoise’ model for structured prediction. This framework allows leveraging of unlabelled output data to train the denoiser. The paper demonstrates the performance of this model on two tasks - font image generation and pseudocode-to-code translation. For the SPoC task they show an improvement of 3-5% over a simple transformer baseline. I believe that this is an interesting idea, and practically useful in the cases where data is sparse.\nThe paper introduces a “predict-and-denoise’ model for structured prediction. This framework allows leveraging of unlabelled output data to train the denoiser. The paper demonstrates the performance of this model on two tasks - font image generation and pseudocode-to-code translation. For the SPoC task they show an improvement of 3-5% over a simple transformer baseline. I believe that this is an interesting idea, and practically useful in the cases where data is sparse."
    },
    {
        "coherence": 0.4422333594894833,
        "consistency": 0.8408444799750564,
        "fluency": 0.17812612514773607,
        "relevance": 0.03706910136331419,
        "overall": 0.37456826649389746,
        "Generated": "67",
        "Gold": "The authors introduce the TGEA 2.0 dataset which is a Chinese dataset where the examples are generated by various pretrained language models. The dataset has been annotated such that the machine-authored texts can be assessed on various tasks within the broad categories of diagnosis tasks and pathology mitigation tasks. A substantial subset of the texts were manually annotated at a fine-grained level and corrected for text quality issues. The main issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data.\nThe authors introduce the TGEA 2.0 dataset which is a Chinese dataset where the examples are generated by various pretrained language models. The dataset has been annotated such that the machine-authored texts can be assessed on various tasks within the broad categories of diagnosis tasks and pathology mitigation tasks. A substantial subset of the texts were manually annotated at a fine-grained level and corrected for text quality issues. The main issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data."
    },
    {
        "coherence": 0.11021363304766615,
        "consistency": 0.21439140111369412,
        "fluency": 0.1892782037136228,
        "relevance": 0.024131856133113746,
        "overall": 0.13450377350202422,
        "Generated": "68",
        "Gold": "Theorem 1 and Theorem 2 leave something to be desired. The plots corresponding to the results are too small. The motivation for avoiding storing instances on the server is not well fleshed out. In this case, the algorithm resembles SBL with larger batch size but at cost of tolerating less Byzantine learning. The algorithm does not converge to a stationary point. It does converge to the point that the algorithm can be used to compute a more robust model.\nTheorem 1 and Theorem 2 leave something to be desired. The plots corresponding to the results are too small. The motivation for avoiding storing instances on the server is not well fleshed out. In this case, the algorithm resembles SBL with larger batch size but at cost of tolerating less Byzantine learning. The algorithm does not converge to a stationary point. It does converge to the point that the algorithm can be used to compute a more robust model."
    },
    {
        "coherence": 0.2953363981089662,
        "consistency": 0.5209076228560747,
        "fluency": 0.20744008876240033,
        "relevance": 0.030666296090191437,
        "overall": 0.2635876014544082,
        "Generated": "69",
        "Gold": "This paper proposes a Network-Wise Quantization (NWQ) approach. NWQ arises overfitting and discrete optimization problems. To solve the combinatorial optimization problem, the authors use Annealing Softmax and Anealing Mixup. The insight of this paper is straight-forward but very sensible. The proposed method seem to contain too many hyper-parameters. They conduct thorough ablation studies and experiments on several mainstream networks.\nThis paper proposes a Network-Wise Quantization (NWQ) approach. NWQ arises overfitting and discrete optimization problems. To solve the combinatorial optimization problem, the authors use Annealing Softmax and Anealing Mixup. The insight of this paper is straight-forward but very sensible. The proposed method seem to contain too many hyper-parameters. They conduct thorough ablation studies and experiments on several mainstream networks."
    },
    {
        "coherence": 0.24371325281793033,
        "consistency": 0.38301252649349143,
        "fluency": 0.3273340281934221,
        "relevance": 0.029806921470195036,
        "overall": 0.24596668224375973,
        "Generated": "70",
        "Gold": "This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance. While overall the writing quality of the paper is high, the paper itself is a strong rejection. This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE) By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IBfunctional training for DNN which is shown to have better prediction accuracy.\nThis work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance. While overall the writing quality of the paper is high, the paper itself is a strong rejection. This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE) By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IBfunctional training for DNN which is shown to have better prediction accuracy."
    },
    {
        "coherence": 0.16943296170890107,
        "consistency": 0.3821517875732398,
        "fluency": 0.2593792915033215,
        "relevance": 0.03485649700427281,
        "overall": 0.2114551344474338,
        "Generated": "71",
        "Gold": "Theorem 4.3 shows that in the 1D setting the $\\arg\\max$ oracle has the same complexity as the label comparison oracle. Theorem 5.2 for the active pool-based setting shows that the sample complexity of label comparison is actually smaller than the lower bound. Figures are not necessarily close to their description. Algorithms are referenced in the text as a whole, without a line by line clear cut descriptions.\nTheorem 4.3 shows that in the 1D setting the $\\arg\\max$ oracle has the same complexity as the label comparison oracle. Theorem 5.2 for the active pool-based setting shows that the sample complexity of label comparison is actually smaller than the lower bound. Figures are not necessarily close to their description. Algorithms are referenced in the text as a whole, without a line by line clear cut descriptions."
    },
    {
        "coherence": 0.14120568272521797,
        "consistency": 0.41684935205288187,
        "fluency": 0.47301812401617666,
        "relevance": 0.021059950292585772,
        "overall": 0.26303327727171555,
        "Generated": "72",
        "Gold": "Counterfactual Generative Network (CGN) can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism. CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting. The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al. in Causalgan.\nCounterfactual Generative Network (CGN) can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism. CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting. The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al. in Causalgan."
    },
    {
        "coherence": 0.2755311491380517,
        "consistency": 0.4994967609621574,
        "fluency": 0.2208206744859776,
        "relevance": 0.02784180332117259,
        "overall": 0.25592259697683983,
        "Generated": "73",
        "Gold": "The method for the most part makes sense, but there are some areas that need to be improved. The results of the study were published in the open-source journal, The Open Knowledge Project, which is published by Oxford University Press. The full report can be found at: http://www.openknowledgeproject.org/news/2013/09/07/the-results-of-the-study-on-deep-networking-and-data-augmentation.html.\nThe method for the most part makes sense, but there are some areas that need to be improved. The results of the study were published in the open-source journal, The Open Knowledge Project, which is published by Oxford University Press. The full report can be found at: http://www.openknowledgeproject.org/news/2013/09/07/the-results-of-the-study-on-deep-networking-and-data-augmentation.html."
    },
    {
        "coherence": 0.20560067593645362,
        "consistency": 0.5419190284701517,
        "fluency": 0.22881253933499449,
        "relevance": 0.030740016446958205,
        "overall": 0.2517680650471395,
        "Generated": "74",
        "Gold": "The paper first introduces a quantity, the cosine similarity of sample-wise local optima to evaluate the model performance at the initialization. They theoretically proved that their proposed quantity is the upper bound of both the training and generalization error under certain assumptions. This measure can then be optimized using and iterative procedure denoted as NIO. Extensive experiments are conducted to verify the efficacy of this approach. It is to substitute the previous network initialization algorithm, such as Kaiming's method.\nThe paper first introduces a quantity, the cosine similarity of sample-wise local optima to evaluate the model performance at the initialization. They theoretically proved that their proposed quantity is the upper bound of both the training and generalization error under certain assumptions. This measure can then be optimized using and iterative procedure denoted as NIO. Extensive experiments are conducted to verify the efficacy of this approach. It is to substitute the previous network initialization algorithm, such as Kaiming's method."
    },
    {
        "coherence": 0.1524599721529344,
        "consistency": 0.2090079937978592,
        "fluency": 0.3840264242398233,
        "relevance": 0.027558837170749334,
        "overall": 0.19326330684034154,
        "Generated": "75",
        "Gold": "The paper proposes GONs which seek to build a generative model with an “implicit” encoder. The main idea being that existing generative models with an encoder are “redundant” in that the decoder itself has the ability to compute the gradient. The paper is very-well written and interesting. The method seems to be getting very good results,. Still, the paper seems to have been rushed.\nThe paper proposes GONs which seek to build a generative model with an “implicit” encoder. The main idea being that existing generative models with an encoder are “redundant” in that the decoder itself has the ability to compute the gradient. The paper is very-well written and interesting. The method seems to be getting very good results,. Still, the paper seems to have been rushed."
    },
    {
        "coherence": 0.11712388591803012,
        "consistency": 0.21184190511340822,
        "fluency": 0.23573374706812805,
        "relevance": 0.03862123342766848,
        "overall": 0.1508301928818087,
        "Generated": "76",
        "Gold": "The work presents a benchmark suite of tasks for evaluating the performance of learned visual representations for use in echocardiography. This paper shows powerful usage in specific clinical tasks like Cardiac struct identification, estimation, and clinical predictions. Translating the clinical evaluation protocols into ML-friendly tasks will bridge the gap in deep learning applications in the medical domain. The use of AI in medicine requires a higher threshold of security and testing than other use cases because the difference between right and wrong output can mean life or death.\nThe work presents a benchmark suite of tasks for evaluating the performance of learned visual representations for use in echocardiography. This paper shows powerful usage in specific clinical tasks like Cardiac struct identification, estimation, and clinical predictions. Translating the clinical evaluation protocols into ML-friendly tasks will bridge the gap in deep learning applications in the medical domain. The use of AI in medicine requires a higher threshold of security and testing than other use cases because the difference between right and wrong output can mean life or death."
    },
    {
        "coherence": 0.2710334125543731,
        "consistency": 0.5577070343195787,
        "fluency": 0.2819585557574758,
        "relevance": 0.03724032030655319,
        "overall": 0.2869848307344952,
        "Generated": "77",
        "Gold": "The paper tackles the problem of bot detection on Twitter with graph-based methods. TwiBot-22 is a heterogeneous information network, including 4 entities and 14 relation types. It also uses a weak supervision learning strategy to generate high-quality annotations. The primary strength of the paper is the dataset which contains over 100,000 bot users. The authors perform extensive evaluation using latest methods and re-implemented previous baselines. There are also some experiments on the importance of graphs in several models.\nThe paper tackles the problem of bot detection on Twitter with graph-based methods. TwiBot-22 is a heterogeneous information network, including 4 entities and 14 relation types. It also uses a weak supervision learning strategy to generate high-quality annotations. The primary strength of the paper is the dataset which contains over 100,000 bot users. The authors perform extensive evaluation using latest methods and re-implemented previous baselines. There are also some experiments on the importance of graphs in several models."
    },
    {
        "coherence": 0.21169245279220378,
        "consistency": 0.523576419414135,
        "fluency": 0.15687957664116237,
        "relevance": 0.02051256258609499,
        "overall": 0.22816525285839903,
        "Generated": "78",
        "Gold": "DollarStreet is a dataset that of everyday household items from around the world with a variety of countries and incomes. The dataset includes 38,479 images of 289 common household objects photographed from 404 homes across 63 nations. Each image has full demographic information such as global region, country name and monthly income. The lack of standardization in the photos (regarding angle, lighting, etc) may not make this a very suitable dataset for training machine learning algorithms.\nDollarStreet is a dataset that of everyday household items from around the world with a variety of countries and incomes. The dataset includes 38,479 images of 289 common household objects photographed from 404 homes across 63 nations. Each image has full demographic information such as global region, country name and monthly income. The lack of standardization in the photos (regarding angle, lighting, etc) may not make this a very suitable dataset for training machine learning algorithms."
    },
    {
        "coherence": 0.3352408625152902,
        "consistency": 0.41579370225821444,
        "fluency": 0.26156609556161114,
        "relevance": 0.07535495367965925,
        "overall": 0.27198890350369376,
        "Generated": "79",
        "Gold": "The paper considers standard linear algebra problems with two twists on the input. It is a dynamic setting where, at every point of time, exactly one of the matrices undergoes arbitrary changes. They show that in this dynamic setting, by maintaining a binary tree of sketches of tensor products, their algorithms can achieve runtimes $q$ times better than the naive algorithm. The authors propose a tree-based data structure to maintain the sketch of the data.\nThe paper considers standard linear algebra problems with two twists on the input. It is a dynamic setting where, at every point of time, exactly one of the matrices undergoes arbitrary changes. They show that in this dynamic setting, by maintaining a binary tree of sketches of tensor products, their algorithms can achieve runtimes $q$ times better than the naive algorithm. The authors propose a tree-based data structure to maintain the sketch of the data."
    },
    {
        "coherence": 0.19174551200266668,
        "consistency": 0.3649080257633306,
        "fluency": 0.40611616261816436,
        "relevance": 0.036472287048244484,
        "overall": 0.2498104968581015,
        "Generated": "80",
        "Gold": "This paper proposes leveraging contrastive learning to action-labeled trajectories. The learned representation will connect to the goal-conditioned value functions. This work may have the potential to benefit the RL, representation learning, and robotics communities. The proposed method simplifies the representation learning + planning in RL problem, by using the inner product of learnt representations as a correlate for the Q-value function. The authors well motivate their approach, compare it extensively with related work on learning goal- Conditioned policies.\nThis paper proposes leveraging contrastive learning to action-labeled trajectories. The learned representation will connect to the goal-conditioned value functions. This work may have the potential to benefit the RL, representation learning, and robotics communities. The proposed method simplifies the representation learning + planning in RL problem, by using the inner product of learnt representations as a correlate for the Q-value function. The authors well motivate their approach, compare it extensively with related work on learning goal- Conditioned policies."
    },
    {
        "coherence": 0.1511097937495499,
        "consistency": 0.3998310351331824,
        "fluency": 0.20306754617867973,
        "relevance": 0.0336448359431228,
        "overall": 0.19691330275113372,
        "Generated": "81",
        "Gold": "The study is well-motivated and tackles an important problem that would occur in real-world applications. The design of the proposed method seems reasonable, and it works well in the experiments with several datasets. Some points remain unclear or are not convincing, which makes my score a bit conservative. The proposed method is end to end and achieves significant improvement on ImageNet dataset compared to baseline methods. The authors propose Open World Semi-Supervised Learning (ORCA) The method learns to classify previously seen classes in the labeled data and novel class in the unlabeled data.\nThe study is well-motivated and tackles an important problem that would occur in real-world applications. The design of the proposed method seems reasonable, and it works well in the experiments with several datasets. Some points remain unclear or are not convincing, which makes my score a bit conservative. The proposed method is end to end and achieves significant improvement on ImageNet dataset compared to baseline methods. The authors propose Open World Semi-Supervised Learning (ORCA) The method learns to classify previously seen classes in the labeled data and novel class in the unlabeled data."
    },
    {
        "coherence": 0.16052251903744486,
        "consistency": 0.1972670471587314,
        "fluency": 0.2924264164918088,
        "relevance": 0.036312822668432665,
        "overall": 0.1716322013391044,
        "Generated": "82",
        "Gold": "This work claims to propose a general methodology for approximating offline algorithms in online settings. The motivation of bridging the gap between offline algorithms and their online counterparts is clear and practical. The main methodology is to predict an offline algorithm’s actions in the real time future via learning behavioral structures of the offline algorithm. Results on one-dimensional synthetic and stock-market data show that the predictive behavior of this method matches our intuitions. It is most accurate when explaining the data and least accurate when predicting into the future.\nThis work claims to propose a general methodology for approximating offline algorithms in online settings. The motivation of bridging the gap between offline algorithms and their online counterparts is clear and practical. The main methodology is to predict an offline algorithm’s actions in the real time future via learning behavioral structures of the offline algorithm. Results on one-dimensional synthetic and stock-market data show that the predictive behavior of this method matches our intuitions. It is most accurate when explaining the data and least accurate when predicting into the future."
    },
    {
        "coherence": 0.14838272283271556,
        "consistency": 0.2681957606976117,
        "fluency": 0.2858376903191393,
        "relevance": 0.04776284635603598,
        "overall": 0.18754475505137563,
        "Generated": "83",
        "Gold": "The paper introduces Ask4Help, a method for augmenting an existing policy with the ability to fall back to an expert policy during an episode. This is achieved without retraining the existing pre-trained agent by introducing a meta-controller that will select whether to follow the agent or the expert at every timestep. The proposed method is evaluated on two tasks, namely object navigation and room rearrangement. The experiments on RoboTHOR show that the proposed method can achieve higher success rates using fewer expert queries.\nThe paper introduces Ask4Help, a method for augmenting an existing policy with the ability to fall back to an expert policy during an episode. This is achieved without retraining the existing pre-trained agent by introducing a meta-controller that will select whether to follow the agent or the expert at every timestep. The proposed method is evaluated on two tasks, namely object navigation and room rearrangement. The experiments on RoboTHOR show that the proposed method can achieve higher success rates using fewer expert queries."
    },
    {
        "coherence": 0.18814316660261163,
        "consistency": 0.35467109325985563,
        "fluency": 0.146214194499844,
        "relevance": 0.04198445423379022,
        "overall": 0.18275322714902537,
        "Generated": "84",
        "Gold": "This work studies learnability under a targeted poisoning attack. It is fundamentally based on the ensemble-based robustness proposed by Levine and Feizi [2020] The writing overall was clear, the number of typos was a problem. The paper is well-organized, but there are many typos which make the reading challenging at times. This work shows that under the realizable case the adversarial risk bound is tight in terms of corruption fraction and VC dimension of the hypothesis set.\nThis work studies learnability under a targeted poisoning attack. It is fundamentally based on the ensemble-based robustness proposed by Levine and Feizi [2020] The writing overall was clear, the number of typos was a problem. The paper is well-organized, but there are many typos which make the reading challenging at times. This work shows that under the realizable case the adversarial risk bound is tight in terms of corruption fraction and VC dimension of the hypothesis set."
    },
    {
        "coherence": 0.05370608584411938,
        "consistency": 0.11319586526772345,
        "fluency": 0.327328725995568,
        "relevance": 0.06542940996816332,
        "overall": 0.13991502176889353,
        "Generated": "85",
        "Gold": "The paper proses a new adversarial (poisoning) defense based on a known graph reweighting scheme known as the ricci curvature. It is not clear to the reader if the sampling scheme/training scheme is the major reason for adversarial robustness. The paper lacks clarity at some points and has inconsistencies in notation. The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?)"
    },
    {
        "coherence": 0.12427392783179554,
        "consistency": 0.3063155285876874,
        "fluency": 0.14585321613251792,
        "relevance": 0.024540968251231497,
        "overall": 0.1502459102008081,
        "Generated": "86",
        "Gold": "This paper aims to solve the rigid registration of 3D point clouds using a deep neural network. The key difference from previous methods is that this paper proposes a region-conditioned transformation. This paper also has many flaws, such as suspicious implementations, insufficient experiments and unclear descriptions. The paper is not well organized and presented. Many proposed modules are not technically sound and are not clear to follow. The method is evaluated on ShapeNet40, showing good results w.r.t. prior art.\nThis paper aims to solve the rigid registration of 3D point clouds using a deep neural network. The key difference from previous methods is that this paper proposes a region-conditioned transformation. This paper also has many flaws, such as suspicious implementations, insufficient experiments and unclear descriptions. The paper is not well organized and presented. Many proposed modules are not technically sound and are not clear to follow. The method is evaluated on ShapeNet40, showing good results w.r.t. prior art."
    },
    {
        "coherence": 0.20857190840136017,
        "consistency": 0.37946888854761146,
        "fluency": 0.14905127583856595,
        "relevance": 0.038887165258726854,
        "overall": 0.19399480951156609,
        "Generated": "87",
        "Gold": "This work serves to point out that the current methods of evaluating reward function algorithms do not appear to be sound. In particular, the paper shows both parameter and reward based comparisons to ground truth reward functions can fail to give a reasonable sense of how good a given solution is. The paper is an unusual contribution for a robotics conference. It does not directly contribute to advancing understanding of robot control or learning. It presents a critical analysis of the measures used to assess the quality of policies learned via HRI driven reward learning.\nThis work serves to point out that the current methods of evaluating reward function algorithms do not appear to be sound. In particular, the paper shows both parameter and reward based comparisons to ground truth reward functions can fail to give a reasonable sense of how good a given solution is. The paper is an unusual contribution for a robotics conference. It does not directly contribute to advancing understanding of robot control or learning. It presents a critical analysis of the measures used to assess the quality of policies learned via HRI driven reward learning."
    },
    {
        "coherence": 0.1567131465468329,
        "consistency": 0.3599322274849736,
        "fluency": 0.07858500984873043,
        "relevance": 0.015716428802865386,
        "overall": 0.15273670317085059,
        "Generated": "88",
        "Gold": "SwinTrack adopts the classic siamese structure, and uses Swin Transformer for both feature extraction and fusion of template and search region. A special motion token which encodes the past trajectory is introduced to further boost the tracking performance. SwinTrack is evaluated on major SOT benchmarks and achieves SOTA performance. The idea to use SwinTransformer instead of a CNN in the branches seems to be novel. The experiments show state-of-the-art results on large-scale tracking datasets.\nSwinTrack adopts the classic siamese structure, and uses Swin Transformer for both feature extraction and fusion of template and search region. A special motion token which encodes the past trajectory is introduced to further boost the tracking performance. SwinTrack is evaluated on major SOT benchmarks and achieves SOTA performance. The idea to use SwinTransformer instead of a CNN in the branches seems to be novel. The experiments show state-of-the-art results on large-scale tracking datasets."
    },
    {
        "coherence": 0.33245279610011635,
        "consistency": 0.5140932557370474,
        "fluency": 0.1291755440789064,
        "relevance": 0.036158213583693234,
        "overall": 0.25296995237494085,
        "Generated": "89",
        "Gold": "This paper presents a probing analysis on clean (non-robust) vs. adversarially trained robust models. It only employs PGD asdversarial training for obtaining a \"robust\" model. Robust representations are less specialized and distant layers are more similar in robust networks. Early layers converge faster and later layers overfit to local minima. The paper is well-written and experiments clearly described. It is based on the perspective of representation similarity. Such a perspective provides an interesting direction to delve deeper into the properties of robust representation learning.\nThis paper presents a probing analysis on clean (non-robust) vs. adversarially trained robust models. It only employs PGD asdversarial training for obtaining a \"robust\" model. Robust representations are less specialized and distant layers are more similar in robust networks. Early layers converge faster and later layers overfit to local minima. The paper is well-written and experiments clearly described. It is based on the perspective of representation similarity. Such a perspective provides an interesting direction to delve deeper into the properties of robust representation learning."
    },
    {
        "coherence": 0.40248467722967124,
        "consistency": 0.443811874674234,
        "fluency": 0.2582160407958563,
        "relevance": 0.07739813987825482,
        "overall": 0.29547768314450407,
        "Generated": "90",
        "Gold": "The paper proposes a new curriculum for RL method. The basic idea is to \"learn\" a residual for each task, modeling in this way how the tasks differ. The method is tested on four different problems and is compared with two 3 baseline methods. The manuscript contributes interesting ideas and the proposed method seems to have good performance. The idea of using the sum of residuals to model the state-action function approximation was interesting. The algorithm learns on Bellman residuals between the current task and that of the previous task."
    },
    {
        "coherence": 0.20467084762448487,
        "consistency": 0.3684380623179538,
        "fluency": 0.10681154212600376,
        "relevance": 0.027202424744441026,
        "overall": 0.17678071920322086,
        "Generated": "91",
        "Gold": "The paper shows that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While there are some doubtful statements, overall the paper is well written and easy to read. The authors failed to discuss their paper with \"ON THE QUANTITATIVE ANALYSIS OF DECODERBASED GENERATIVE MODELS\", which uses AIS to estimate the likelihood. The assumption made by the authors that \"generator is injective\" is problematic or even wrong.\nThe paper shows that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While there are some doubtful statements, overall the paper is well written and easy to read. The authors failed to discuss their paper with \"ON THE QUANTITATIVE ANALYSIS OF DECODERBASED GENERATIVE MODELS\", which uses AIS to estimate the likelihood. The assumption made by the authors that \"generator is injective\" is problematic or even wrong."
    },
    {
        "coherence": 0.17964288314632346,
        "consistency": 0.32987639706366656,
        "fluency": 0.2153391656963877,
        "relevance": 0.03298765450274761,
        "overall": 0.18946152510228134,
        "Generated": "92",
        "Gold": "The paper builds on the following two observations: KGs are incomplete often lacking facts that would be needed for reasoning to answer a question. Current methods over-retrieves facts (edges) from the KG leading to a lot of unrelated facts that potentially makes reasoning noisier and harder. The proposed model then sparsifies the graph by learning edge weights via a two-step message passing process. The experiments, however, only show modest (yet still positive) empirical gains.\nThe paper builds on the following two observations: KGs are incomplete often lacking facts that would be needed for reasoning to answer a question. Current methods over-retrieves facts (edges) from the KG leading to a lot of unrelated facts that potentially makes reasoning noisier and harder. The proposed model then sparsifies the graph by learning edge weights via a two-step message passing process. The experiments, however, only show modest (yet still positive) empirical gains."
    },
    {
        "coherence": 0.15146014190439405,
        "consistency": 0.3533139098562826,
        "fluency": 0.16246206524511106,
        "relevance": 0.023110163265843937,
        "overall": 0.17258657006790792,
        "Generated": "93",
        "Gold": "This paper introduces a new distributed value-based multi-agent reinforcement learning framework. It divides the system into two parts, multiple containers, and one centralizer. Containers are trained with trajectories generated by their own actors interacting with the environment. In contrast, the centralizer is trained with high-priority samples sent by all containers. By grouping actors, replay buffers, learners, and a queue manager into containers CMARL can be used to scale learning to the requirements of the multi- agent setting.\nThis paper introduces a new distributed value-based multi-agent reinforcement learning framework. It divides the system into two parts, multiple containers, and one centralizer. Containers are trained with trajectories generated by their own actors interacting with the environment. In contrast, the centralizer is trained with high-priority samples sent by all containers. By grouping actors, replay buffers, learners, and a queue manager into containers CMARL can be used to scale learning to the requirements of the multi- agent setting."
    },
    {
        "coherence": 0.10764951140554421,
        "consistency": 0.2630298298677413,
        "fluency": 0.0738316985065013,
        "relevance": 0.01689621150690603,
        "overall": 0.1153518128216732,
        "Generated": "94",
        "Gold": "This paper designed a hypothesis testing procedure for detecting changes in episode sequential data. The method is demonstrated based on a non-iid and non-Gaussian setting for reward signals. The strength of this paper would be significantly boosted if the proposed method can be used to solve an non-stationary RL problem. I believe that a paper requires a notable refactoring to improve the presentation. The provided theoretical grounds looks as non-enough for publication @ ICLR.\nThis paper designed a hypothesis testing procedure for detecting changes in episode sequential data. The method is demonstrated based on a non-iid and non-Gaussian setting for reward signals. The strength of this paper would be significantly boosted if the proposed method can be used to solve an non-stationary RL problem. I believe that a paper requires a notable refactoring to improve the presentation. The provided theoretical grounds looks as non-enough for publication @ ICLR."
    },
    {
        "coherence": 0.1536151738836097,
        "consistency": 0.2808869142118718,
        "fluency": 0.09941360997776785,
        "relevance": 0.02594434699394537,
        "overall": 0.13996501126679867,
        "Generated": "95",
        "Gold": "Theoretical analysis is provided to calibrate gradients of both features and CAG blocks in the backward process. The number of clusters $k$ is tied to the bit, then the performance degradation of extreme cases including 1-bit or 2-bit. The novelty of capturing such channel-dimension structured information is good. The existing communication-optimized methods either are insufficient in reducing the traffic or may degrade the model performance. This work proposed a hierarchical compression algorithm for feature data, called Stripe-wise Group Quantization.\nTheoretical analysis is provided to calibrate gradients of both features and CAG blocks in the backward process. The number of clusters $k$ is tied to the bit, then the performance degradation of extreme cases including 1-bit or 2-bit. The novelty of capturing such channel-dimension structured information is good. The existing communication-optimized methods either are insufficient in reducing the traffic or may degrade the model performance. This work proposed a hierarchical compression algorithm for feature data, called Stripe-wise Group Quantization."
    },
    {
        "coherence": 0.04229261042082702,
        "consistency": 0.05916577697439877,
        "fluency": 0.17856215562808905,
        "relevance": 0.023305085440317157,
        "overall": 0.07583140711590801,
        "Generated": "96",
        "Gold": "AutoMunge converts NLP into features suitable for NNs. The authors of the library have put a lot of thought into its construction, and it looks very useful. However, ICLR is about /learning/ representations, not about feature engineering. It is a tool to help people with this type of analysis. It can be used for other types of analysis as well. The software package is so complex that a well-designed implementation in itself is of scientific significance.\nAutoMunge converts NLP into features suitable for NNs. The authors of the library have put a lot of thought into its construction, and it looks very useful. However, ICLR is about /learning/ representations, not about feature engineering. It is a tool to help people with this type of analysis. It can be used for other types of analysis as well. The software package is so complex that a well-designed implementation in itself is of scientific significance."
    },
    {
        "coherence": 0.1334860122122539,
        "consistency": 0.26925064439301705,
        "fluency": 0.07221681304235224,
        "relevance": 0.03554383342483756,
        "overall": 0.12762432576811517,
        "Generated": "97",
        "Gold": "This article proposes to include an attention mechanism to Deep RL to provide a visual \"explanation\" of the learned policy. There is solid evidence that attention is an important aspect in perception and learning. The approach is straightforward and similar to attention maps used in, eg, image captioning. The motivation and goal on the inverting gaze area are less clear. The paper has some confusing details. This article proposes an interpretable RL agent architecture that uses attention masks.\nThis article proposes to include an attention mechanism to Deep RL to provide a visual \"explanation\" of the learned policy. There is solid evidence that attention is an important aspect in perception and learning. The approach is straightforward and similar to attention maps used in, eg, image captioning. The motivation and goal on the inverting gaze area are less clear. The paper has some confusing details. This article proposes an interpretable RL agent architecture that uses attention masks."
    },
    {
        "coherence": 0.09426243841744064,
        "consistency": 0.17866628185069658,
        "fluency": 0.09902768457292797,
        "relevance": 0.036053912132941957,
        "overall": 0.10200257924350178,
        "Generated": "98",
        "Gold": "The idea to deal with label noise by transforming noisy class labels into noisy similarity labels seems to be novel. In most cases the proposed Class2Simi can improve the accuracy compared with baselines. But the improvement is not significant in many cases like those on MNIST and CIFAR10. The proposed method has been evaluated on multiple datasets with consistent improvement across all noise ratios. The paper empirically demonstrates that the proposed method works well on several synthetic datasets and a large-scale real-world dataset.\nThe idea to deal with label noise by transforming noisy class labels into noisy similarity labels seems to be novel. In most cases the proposed Class2Simi can improve the accuracy compared with baselines. But the improvement is not significant in many cases like those on MNIST and CIFAR10. The proposed method has been evaluated on multiple datasets with consistent improvement across all noise ratios. The paper empirically demonstrates that the proposed method works well on several synthetic datasets and a large-scale real-world dataset."
    },
    {
        "coherence": 0.0675509547867251,
        "consistency": 0.2347524164870132,
        "fluency": 0.2971902216978,
        "relevance": 0.031565521719698425,
        "overall": 0.1577647786728092,
        "Generated": "99",
        "Gold": "This paper is the first to achieve the optimal convergence rate under the NTK regime. They show that smooth target functions are learned rapidly at faster convergence rate. The paper is technically sound and the proof techniques are different from existing literatures. This result is not a *plug and play* based on the current kernelized-SGD litterature. The quality of the paper is undeniable and fills the gap between optimality of NTK literature. Overall, I vote for accepting.\nThis paper is the first to achieve the optimal convergence rate under the NTK regime. They show that smooth target functions are learned rapidly at faster convergence rate. The paper is technically sound and the proof techniques are different from existing literatures. This result is not a *plug and play* based on the current kernelized-SGD litterature. The quality of the paper is undeniable and fills the gap between optimality of NTK literature. Overall, I vote for accepting."
    },
    {
        "coherence": 0.18614271514312833,
        "consistency": 0.1428345862839865,
        "fluency": 0.34349496837177534,
        "relevance": 0.04630158384617607,
        "overall": 0.17969346341126655,
        "Generated": "100",
        "Gold": "This paper addresses contact prediction for PPI by proposing a geometric deep learning framework. They use a k nearest neighbor representation for each protein and compute geometric-based attention scores to convolute the messages over this graph. They validate this approach on the PPI task, using the DIPS-plus dataset. The key contribution of this paper is not clear. The proposed architecture seems not novel, and I have some serious concerns regarding the experiments. This paper proposed a novel geometry-evolving graph transformer for protein interface contact prediction.\nThis paper addresses contact prediction for PPI by proposing a geometric deep learning framework. They use a k nearest neighbor representation for each protein and compute geometric-based attention scores to convolute the messages over this graph. They validate this approach on the PPI task, using the DIPS-plus dataset. The key contribution of this paper is not clear. The proposed architecture seems not novel, and I have some serious concerns regarding the experiments. This paper proposed a novel geometry-evolving graph transformer for protein interface contact prediction."
    },
    {
        "coherence": 0.0834410941172555,
        "consistency": 0.16787798334652904,
        "fluency": 0.5058384844083711,
        "relevance": 0.022880599468640874,
        "overall": 0.19500954033519913,
        "Generated": "101",
        "Gold": "The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Authors empirically evaluate the method over a set of visual Mujoco tasks and Atari games. The method is novel and the results are strong. However, it would benefit from a more detailed study in order to understand where the reported gains come from. The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward.\nThe paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Authors empirically evaluate the method over a set of visual Mujoco tasks and Atari games. The method is novel and the results are strong. However, it would benefit from a more detailed study in order to understand where the reported gains come from. The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward."
    },
    {
        "coherence": 0.08693215812630091,
        "consistency": 0.16681757868848462,
        "fluency": 0.29471909370636207,
        "relevance": 0.03313834358078619,
        "overall": 0.14540179352548344,
        "Generated": "102",
        "Gold": "The authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The experiments seem to show performance in par with the FullCRF on decoupled training. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. The paper is well written with many relevant references and easy to read.\nThe authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The experiments seem to show performance in par with the FullCRF on decoupled training. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. The paper is well written with many relevant references and easy to read."
    },
    {
        "coherence": 0.06826152071587634,
        "consistency": 0.1605417229823342,
        "fluency": 0.189842286239905,
        "relevance": 0.06112403521739675,
        "overall": 0.11994239128887807,
        "Generated": "103",
        "Gold": "This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties than the identity STE. The discussions for most theoretical results are very short or not organized well. The paper demonstrates that training with the latter against the population loss is guaranteed to converge to a critical point. This paper is the first to use Ste to train convolutional networks with binary activations and non overlapping patches."
    },
    {
        "coherence": 0.106685822037255,
        "consistency": 0.18196857464878813,
        "fluency": 0.12935407460994994,
        "relevance": 0.0681259345797996,
        "overall": 0.12153360146894816,
        "Generated": "104",
        "Gold": "The authors propose a role diversity metric to quantify the difference between agents in multi-agent reinforcement learning. They also find that the error bound in MARL can be decomposed into three parts that have a strong relation to the role diversity. To evaluate the proposed method, they conduct some experiments on MPE and SMAC environments. The metric is used to inform the use of communication, parameter sharing, and other MARL algorithmic decisions. The discovered guidelines about the choice of training strategies will benefit the community. The theoretical analysis is not well-motivated."
    },
    {
        "coherence": 0.25359200386631275,
        "consistency": 0.43092010864143987,
        "fluency": 0.1582944520941628,
        "relevance": 0.14256277173759574,
        "overall": 0.24634233408487782,
        "Generated": "105",
        "Gold": "The paper proposes a novel factorisation approach and uses recurrent networks. The qualitative experiment is interesting, but there is no information given about the level of musical training the participants had. The cross entropy definition is missing important details. There are a few points that could be better clarified. We conclude with a list of the pros and cons of this work. The paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field."
    },
    {
        "coherence": 0.2214548826204964,
        "consistency": 0.3511567406275323,
        "fluency": 0.13232650850830294,
        "relevance": 0.023625788178608807,
        "overall": 0.18214097998373513,
        "Generated": "106",
        "Gold": "OmniVL is a vision-language foundation model that supports both image-language and video-language pretraining in a unified framework. The model allows for evaluation of vision-only tasks, multimodal retrieval, and multimodals based tasks like captioning and VQA. The authors argue they achieve a good performance on many different types of tasks. The limitations are briefly discussed while the potential societal impact is left for future studies.\nOmniVL is a vision-language foundation model that supports both image-language and video-language pretraining in a unified framework. The model allows for evaluation of vision-only tasks, multimodal retrieval, and multimodals based tasks like captioning and VQA. The authors argue they achieve a good performance on many different types of tasks. The limitations are briefly discussed while the potential societal impact is left for future studies."
    },
    {
        "coherence": 0.23868182322531994,
        "consistency": 0.5374251133393751,
        "fluency": 0.12531813809560802,
        "relevance": 0.02384406218351252,
        "overall": 0.2313172842109539,
        "Generated": "107",
        "Gold": "This paper presents a neural deformation method, which utilize the deformation priors in a large-scale dataset. The proposed method first deforms the input model back to the canonical space, and then deforms it to the target that satisfies the constraints given by the user. This paper is introduced an approach to learn mesh deformations of dynamic bodies from user-provided handles. To manipulate the source meshes into different poses, the shape deformations are learned via canonicalization.\nThis paper presents a neural deformation method, which utilize the deformation priors in a large-scale dataset. The proposed method first deforms the input model back to the canonical space, and then deforms it to the target that satisfies the constraints given by the user. This paper is introduced an approach to learn mesh deformations of dynamic bodies from user-provided handles. To manipulate the source meshes into different poses, the shape deformations are learned via canonicalization."
    },
    {
        "coherence": 0.19895129517174243,
        "consistency": 0.38471165306574484,
        "fluency": 0.14997065101256205,
        "relevance": 0.021138644458358728,
        "overall": 0.18869306092710203,
        "Generated": "108",
        "Gold": "In this paper, the problem of catastrophic forgetting for continual learning is explored. The core idea is to benefit from data compression to reduce the space to store data samples and then replay the reconstructed data points. Extensive experiments show that with the proposed method, a naive compression method can achieve the SOTA on several continual learning benchmarks. This paper studies the problems of classification and object detection from natural image datasets in a continual learning setting. The strategy used by the paper is to fill this memory buffer with compressed data samples.\nIn this paper, the problem of catastrophic forgetting for continual learning is explored. The core idea is to benefit from data compression to reduce the space to store data samples and then replay the reconstructed data points. Extensive experiments show that with the proposed method, a naive compression method can achieve the SOTA on several continual learning benchmarks. This paper studies the problems of classification and object detection from natural image datasets in a continual learning setting. The strategy used by the paper is to fill this memory buffer with compressed data samples."
    },
    {
        "coherence": 0.22113434423476516,
        "consistency": 0.5417304967988522,
        "fluency": 0.2031464340166618,
        "relevance": 0.02161063473821604,
        "overall": 0.2469054774471238,
        "Generated": "109",
        "Gold": "This paper proposes a novel surround-view depth estimation, and project features of different views into a volumetric feature spaces. The pipeline also enables novel-view Depth synthesis. The problem setting may be useful for autonomous driving. The proposed model has the unique capability of predicting depth maps for views not among the inputs. The authors propose a self-supervised approach for the task of surround- view depth estimation. The method consists of three parts: a feature fusion module, a depth fusion module and a global motion estimation module.\nThis paper proposes a novel surround-view depth estimation, and project features of different views into a volumetric feature spaces. The pipeline also enables novel-view Depth synthesis. The problem setting may be useful for autonomous driving. The proposed model has the unique capability of predicting depth maps for views not among the inputs. The authors propose a self-supervised approach for the task of surround- view depth estimation. The method consists of three parts: a feature fusion module, a depth fusion module and a global motion estimation module."
    },
    {
        "coherence": 0.3073417391948098,
        "consistency": 0.45378604116094673,
        "fluency": 0.27667313275784533,
        "relevance": 0.03819500192529033,
        "overall": 0.26899897875972306,
        "Generated": "110",
        "Gold": "The authors present a second-generation of the soundscape tool. The tool is designed to simulate realistic acoustic reverberation from 3-dimensional room/scene models. Such a tool facilitates large-scale rendering of audio for simulated scenes. The platform can be very useful for the research community as it supports audio-visual rendering and can be used in any environment. The dataset and the benchmarking results are also good to have for the community. The methods used in benchmarking are limited.\nThe authors present a second-generation of the soundscape tool. The tool is designed to simulate realistic acoustic reverberation from 3-dimensional room/scene models. Such a tool facilitates large-scale rendering of audio for simulated scenes. The platform can be very useful for the research community as it supports audio-visual rendering and can be used in any environment. The dataset and the benchmarking results are also good to have for the community. The methods used in benchmarking are limited."
    },
    {
        "coherence": 0.1600496147389118,
        "consistency": 0.34681043837512143,
        "fluency": 0.11472152623665358,
        "relevance": 0.024643833848442567,
        "overall": 0.16155635329978235,
        "Generated": "111",
        "Gold": "FlowSelect is based on normalizing flows to select features in a controlled fashion, meaning that the false discovery rate (FDR) is limited. Empirically, FLOW SELECT consistently controls the FDR on both synthetic and semi-synthetic benchmarks. Asymptotically,  the proposed method computes valid p-values. The paper considers the problem of multiple hypothesis testing with the FDR control. The work is situated in the model-X setting. The presentation of this paper can be improved.\nFlowSelect is based on normalizing flows to select features in a controlled fashion, meaning that the false discovery rate (FDR) is limited. Empirically, FLOW SELECT consistently controls the FDR on both synthetic and semi-synthetic benchmarks. Asymptotically,  the proposed method computes valid p-values. The paper considers the problem of multiple hypothesis testing with the FDR control. The work is situated in the model-X setting. The presentation of this paper can be improved."
    },
    {
        "coherence": 0.0917757589490042,
        "consistency": 0.2984330233848407,
        "fluency": 0.16058907277509488,
        "relevance": 0.011715430331888625,
        "overall": 0.1406283213602071,
        "Generated": "112",
        "Gold": "Spectrum Random Masking (SRM) is a data augmentation method by random masking in the frequency domain. SRM can be plugged into existing policy generalization algorithms like DrQ and SVEA. The only benchmark demonstrated in the paper is Distracting DMControl, which is a set of toy environments. The experiments and ablation studies are sufficient to show the effectiveness of the proposed method. The authors have claimed some limitations.\nSpectrum Random Masking (SRM) is a data augmentation method by random masking in the frequency domain. SRM can be plugged into existing policy generalization algorithms like DrQ and SVEA. The only benchmark demonstrated in the paper is Distracting DMControl, which is a set of toy environments. The experiments and ablation studies are sufficient to show the effectiveness of the proposed method. The authors have claimed some limitations."
    },
    {
        "coherence": 0.12398661454848718,
        "consistency": 0.2273090475496352,
        "fluency": 0.16410896840520886,
        "relevance": 0.025810748358699436,
        "overall": 0.1353038447155077,
        "Generated": "113",
        "Gold": "This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words for that topic. The latent codes in InfoGAN correspond to the latent topics in topic modelling. The author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss.\nThis paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. The model basically combines two steps: first to generate words (bag-of-words) for a topic, then second to generate the sequence of the words for that topic. The latent codes in InfoGAN correspond to the latent topics in topic modelling. The author draws lessons from the infoGAN and designed a creative object function with reconstruction loss and categorical loss."
    },
    {
        "coherence": 0.2806485527826687,
        "consistency": 0.4313307016366122,
        "fluency": 0.10888608406097279,
        "relevance": 0.02063094432471771,
        "overall": 0.21037407070124284,
        "Generated": "114",
        "Gold": "Perceptual sensitivity is correlated with the probability of an image in its close neighborhood. Distances induced by AEs are correlated with Prob(Trng data), as well as with human perception. The paper specifies that perceptual distances do not always lead to noticeable gains in performance over Euclidean distance. The most interesting contribution is the training of auto-encoders with noise and a perceptual similarity distance. I think it is more a review paper and there are not important enough contributions.\nPerceptual sensitivity is correlated with the probability of an image in its close neighborhood. Distances induced by AEs are correlated with Prob(Trng data), as well as with human perception. The paper specifies that perceptual distances do not always lead to noticeable gains in performance over Euclidean distance. The most interesting contribution is the training of auto-encoders with noise and a perceptual similarity distance. I think it is more a review paper and there are not important enough contributions."
    },
    {
        "coherence": 0.13361333557034707,
        "consistency": 0.30409492860151466,
        "fluency": 0.2893036014718745,
        "relevance": 0.0235035416376759,
        "overall": 0.18762885182035302,
        "Generated": "115",
        "Gold": "Temporal Quantile Adjustment (TQA) is proposed for time-series data with a focus on constructing efficient and valid prediction intervals. TQA can improve longitudinal coverage and preserve cross-sectional coverage for the prediction interval built for regression on cross-section time series data. The theory part supports the validity of coverages. The underlying problem studied in the paper is extremely relevant to the machine learning community. The authors give error bounds for coverage and also support their claim empirically.\nTemporal Quantile Adjustment (TQA) is proposed for time-series data with a focus on constructing efficient and valid prediction intervals. TQA can improve longitudinal coverage and preserve cross-sectional coverage for the prediction interval built for regression on cross-section time series data. The theory part supports the validity of coverages. The underlying problem studied in the paper is extremely relevant to the machine learning community. The authors give error bounds for coverage and also support their claim empirically."
    },
    {
        "coherence": 0.1212104197240957,
        "consistency": 0.31173539269722633,
        "fluency": 0.17571657626512488,
        "relevance": 0.0232655885111478,
        "overall": 0.15798199429939866,
        "Generated": "116",
        "Gold": "This work proposes LECO, i.e. to get the state visitation count via learning the VQ-VAE model. The count-based reward is combined with the task-specific modulation, which is learned through the LIRPG framework. Experiments in MiniGrid environments and two DMLab tasks shows that their method generally outperforms or is on par with several baselines. This paper studies a learnable hash function for count- based exploration.\nThis work proposes LECO, i.e. to get the state visitation count via learning the VQ-VAE model. The count-based reward is combined with the task-specific modulation, which is learned through the LIRPG framework. Experiments in MiniGrid environments and two DMLab tasks shows that their method generally outperforms or is on par with several baselines. This paper studies a learnable hash function for count- based exploration."
    },
    {
        "coherence": 0.16078787471293307,
        "consistency": 0.2615325192275711,
        "fluency": 0.2303674460688221,
        "relevance": 0.024475099866796586,
        "overall": 0.16929073496903072,
        "Generated": "117",
        "Gold": "This paper aims to tackle the matrix completion problem by drawing connection from prior work in image completion domain. It seems to be a combination of prior work: Multi-graph convolution combined with Dirichlet energy on row and column graph laplacian. The grammar and style of writing are not as high as expected. Several typos and lowercase/uppercase misuse throughout the whole paper. Use of the adjectives “surprising” and “spectacular” for the state-of-the-art methods is uncustomary language.\nThis paper aims to tackle the matrix completion problem by drawing connection from prior work in image completion domain. It seems to be a combination of prior work: Multi-graph convolution combined with Dirichlet energy on row and column graph laplacian. The grammar and style of writing are not as high as expected. Several typos and lowercase/uppercase misuse throughout the whole paper. Use of the adjectives “surprising” and “spectacular” for the state-of-the-art methods is uncustomary language."
    },
    {
        "coherence": 0.11406123615564215,
        "consistency": 0.3453953574149815,
        "fluency": 0.21538604385377627,
        "relevance": 0.023018573506567313,
        "overall": 0.1744653027327418,
        "Generated": "118",
        "Gold": "The paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks.\nThe paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks."
    },
    {
        "coherence": 0.18547139048432876,
        "consistency": 0.3685069232324953,
        "fluency": 0.14532649550437657,
        "relevance": 0.02088471797646529,
        "overall": 0.18004738179941648,
        "Generated": "119",
        "Gold": "This paper study the non-stationary problem for transformer based forecasting model. They find that the nomarlization step in pre-process will lead transformers to generate indistinguishable temporal attention. The proposed strategy can be easily integrate with different transformer backbone. Empirical results show consistent improvement. The motivation is clear and the writing is easy to follow. The theoretical part is not very rigorous. The authors are a bit pretentious at times, but other than that it is all right.\nThis paper study the non-stationary problem for transformer based forecasting model. They find that the nomarlization step in pre-process will lead transformers to generate indistinguishable temporal attention. The proposed strategy can be easily integrate with different transformer backbone. Empirical results show consistent improvement. The motivation is clear and the writing is easy to follow. The theoretical part is not very rigorous. The authors are a bit pretentious at times, but other than that it is all right."
    },
    {
        "coherence": 0.14413270656631158,
        "consistency": 0.30047625557454705,
        "fluency": 0.32536562901852767,
        "relevance": 0.026724806764140095,
        "overall": 0.1991748494808816,
        "Generated": "120",
        "Gold": "In this paper, the problem of learning the optimal threshold policy for Markov Decision Processes (MDPs) is considered. Using the monotonicity property of threshold policies, an off-policy actor-critic algorithm is proposed to learn the optimal policy. Extensive simulation results are presented to demonstrate that the proposed algorithms outperform other algorithms in the literature. The main contribution is based on the use of threshold policy and simplification of the policy gradient.\nIn this paper, the problem of learning the optimal threshold policy for Markov Decision Processes (MDPs) is considered. Using the monotonicity property of threshold policies, an off-policy actor-critic algorithm is proposed to learn the optimal policy. Extensive simulation results are presented to demonstrate that the proposed algorithms outperform other algorithms in the literature. The main contribution is based on the use of threshold policy and simplification of the policy gradient."
    },
    {
        "coherence": 0.10264709913637371,
        "consistency": 0.21621474943196484,
        "fluency": 0.1663240398663635,
        "relevance": 0.033872534662490225,
        "overall": 0.12976460577429805,
        "Generated": "121",
        "Gold": "Transformers are capable of in-context learning all of the simple function classes tested in the paper to a surprising extend. Carefully designed experiments show that this capability isn't explained by simple memorization of training examples. The paper is well written, with a clearly defined (and well motivated) experimental protocol. The empirical results are intriguing and will be of interest to the broader ML community. I think the paper can easily be expanded to include complex functions.\nTransformers are capable of in-context learning all of the simple function classes tested in the paper to a surprising extend. Carefully designed experiments show that this capability isn't explained by simple memorization of training examples. The paper is well written, with a clearly defined (and well motivated) experimental protocol. The empirical results are intriguing and will be of interest to the broader ML community. I think the paper can easily be expanded to include complex functions."
    },
    {
        "coherence": 0.14830205050733752,
        "consistency": 0.2944121152511493,
        "fluency": 0.19075294556238928,
        "relevance": 0.02298421198964281,
        "overall": 0.16411283082762973,
        "Generated": "122",
        "Gold": "This paper studies ways of subsampling tall-and-dense p-norm regression involving structured Vandermonde matrices. It shows that in this setting with additional structure, sampling by Lewis weights produces poly(p) * d row sized samples for all values of p. The motivation towards this technique is not well presented. This paper creates a coreset for solving Lp regression on structured input. The algorithms, theorems and lemmas are very well written.\nThis paper studies ways of subsampling tall-and-dense p-norm regression involving structured Vandermonde matrices. It shows that in this setting with additional structure, sampling by Lewis weights produces poly(p) * d row sized samples for all values of p. The motivation towards this technique is not well presented. This paper creates a coreset for solving Lp regression on structured input. The algorithms, theorems and lemmas are very well written."
    },
    {
        "coherence": 0.10237931014468475,
        "consistency": 0.2741585837590734,
        "fluency": 0.18135382884077675,
        "relevance": 0.01550057750419697,
        "overall": 0.14334807506218297,
        "Generated": "123",
        "Gold": "Theoretic advantages of the proposed framework such as asympototic unbiasedness seem interesting. There is a lot hyperparamters or customiazations possiblei in the framework. The importance weighting may not work in some cases, and some conditions may be required to be satisfied. The method proposed is based on the importance of reweighting. It seems that in this framework, all sampled data should be saved and used in the calculation of P, Q, and a_t.\nTheoretic advantages of the proposed framework such as asympototic unbiasedness seem interesting. There is a lot hyperparamters or customiazations possiblei in the framework. The importance weighting may not work in some cases, and some conditions may be required to be satisfied. The method proposed is based on the importance of reweighting. It seems that in this framework, all sampled data should be saved and used in the calculation of P, Q, and a_t."
    },
    {
        "coherence": 0.1654100578496441,
        "consistency": 0.4405760668444014,
        "fluency": 0.055183541954178676,
        "relevance": 0.017504652527099027,
        "overall": 0.1696685797938308,
        "Generated": "124",
        "Gold": "This paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The experiments are a proof-of-concept on simple illustrative toy examples. The resulting algorithm from the derived PDE seems not having much practical advantage over SGHMC (a stochastic version of 2nd order Langevin dynamics) The proposed methods either rely on strong Gaussian assumptions, or  \"density estimations\"\nThis paper introduces a functional extension of the Bregman Lagrangian framework of Wibisono et al. 2016. The experiments are a proof-of-concept on simple illustrative toy examples. The resulting algorithm from the derived PDE seems not having much practical advantage over SGHMC (a stochastic version of 2nd order Langevin dynamics) The proposed methods either rely on strong Gaussian assumptions, or  \"density estimations\""
    },
    {
        "coherence": 0.14119496542318238,
        "consistency": 0.22199135272818882,
        "fluency": 0.12174471643710162,
        "relevance": 0.05159095841127218,
        "overall": 0.13413049824993625,
        "Generated": "125",
        "Gold": "The paper studies multi-task generalization in model-based reinforcement learning (MBRL) One of the standard ways to approach the problem is given by inferring a latent variable Z encoding each task. The authors propose to encode segments of state-action trajectories into Z vectors and maximize similarity between Zs from same trajectory. They introduce a set of auxiliary losses based on relational intervention and causal reasoning to encourage the inferred context to be the same.\nThe paper studies multi-task generalization in model-based reinforcement learning (MBRL) One of the standard ways to approach the problem is given by inferring a latent variable Z encoding each task. The authors propose to encode segments of state-action trajectories into Z vectors and maximize similarity between Zs from same trajectory. They introduce a set of auxiliary losses based on relational intervention and causal reasoning to encourage the inferred context to be the same."
    },
    {
        "coherence": 0.12535057640722724,
        "consistency": 0.22578271690057067,
        "fluency": 0.23244227203695123,
        "relevance": 0.030841307090348137,
        "overall": 0.15360421810877434,
        "Generated": "126",
        "Gold": "This paper proposes a different look at why neural networks generalize despite optimizing to zero training error, over-parameterization, etc. The paper focuses on classification with CIFAR10 and ImageNet with AlexNet, VGG, and ResNet models. The idea could be potentially interesting, but I found the writing and presentation is a bit confusing. This is an entirely empirical work. The empirical findings reported in the paper are not insightful and the paper makes claims that are heuristic and not justified.\nThis paper proposes a different look at why neural networks generalize despite optimizing to zero training error, over-parameterization, etc. The paper focuses on classification with CIFAR10 and ImageNet with AlexNet, VGG, and ResNet models. The idea could be potentially interesting, but I found the writing and presentation is a bit confusing. This is an entirely empirical work. The empirical findings reported in the paper are not insightful and the paper makes claims that are heuristic and not justified."
    },
    {
        "coherence": 0.1374476446316817,
        "consistency": 0.17813201485375724,
        "fluency": 0.38011341619573835,
        "relevance": 0.06547823426741457,
        "overall": 0.19029282748714796,
        "Generated": "127",
        "Gold": "The paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018. The RHS of the equation  in Lemma 2 misses terms related with B_{d,2}.\nThe paper provides a generalization bound for multi-layered deep neural networks in terms of dimensions rather than norms. The bound is derived by controlling Rademacher complexity of the Ramp loss under the Lipschitzness of the network. It is not clear whether the bound is vacuous or not, as discussed in Arora et al. 2018. The RHS of the equation  in Lemma 2 misses terms related with B_{d,2}."
    },
    {
        "coherence": 0.09465432457347872,
        "consistency": 0.15776506286846625,
        "fluency": 0.3962269785340113,
        "relevance": 0.027258590701078392,
        "overall": 0.16897623916925866,
        "Generated": "128",
        "Gold": "The paper introduces a new normalization method named pixel-wise tensor normalization which improves both accuracy and robustness of the model. The results shows somewhat improvement, but not significant. The paper still needs further polishment and is not ready for publication at the moment. These two techniques are easy to understand, and bring improved robustness compared with the baseline. Here are some suggestions for the authors to improve the paper's robustness and accuracy.\nThe paper introduces a new normalization method named pixel-wise tensor normalization which improves both accuracy and robustness of the model. The results shows somewhat improvement, but not significant. The paper still needs further polishment and is not ready for publication at the moment. These two techniques are easy to understand, and bring improved robustness compared with the baseline. Here are some suggestions for the authors to improve the paper's robustness and accuracy."
    },
    {
        "coherence": 0.18167761041340483,
        "consistency": 0.3229393755834859,
        "fluency": 0.24234227205066763,
        "relevance": 0.05591942555026935,
        "overall": 0.20071967089945691,
        "Generated": "129",
        "Gold": "This paper proposes a method to learn a state representation for RL using the Laplacian. One use-case of the learnt state representation is for reward-shaping that is said to accelerate the training of standard goal-driven RL algorithms. The method is also well illustrated and compared with other methods, showing the efficiency of the proposed method. Overall the authors make a nice contribution. This works proposes a scalable way of approximating the eigenvectors of the La Placian in RL by optimizing the graph drawing objective on limited sampled states.\nThis paper proposes a method to learn a state representation for RL using the Laplacian. One use-case of the learnt state representation is for reward-shaping that is said to accelerate the training of standard goal-driven RL algorithms. The method is also well illustrated and compared with other methods, showing the efficiency of the proposed method. Overall the authors make a nice contribution. This works proposes a scalable way of approximating the eigenvectors of the La Placian in RL by optimizing the graph drawing objective on limited sampled states."
    },
    {
        "coherence": 0.06268412069258063,
        "consistency": 0.1351690509461975,
        "fluency": 0.26522957557978516,
        "relevance": 0.02931739875922796,
        "overall": 0.12310003649444781,
        "Generated": "130",
        "Gold": "This paper introduces “Cross domain schemas” (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the user’s high-level intent. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset. The paper totally disregard the work on Semantic Role Labelling and on languages for expressing the general meaning of language.\nThis paper introduces “Cross domain schemas” (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the user’s high-level intent. The model outperforms other multitask Seq2Seq models on the Snips (Goo et al., 2018) dataset. The paper totally disregard the work on Semantic Role Labelling and on languages for expressing the general meaning of language."
    },
    {
        "coherence": 0.2330025908586864,
        "consistency": 0.42524296439283976,
        "fluency": 0.1658136971223629,
        "relevance": 0.021572444305413673,
        "overall": 0.2114079241698257,
        "Generated": "131",
        "Gold": "This work fully single-loop stochastic algorithms for bilevel optimization problems where the inner problem is strongly convex. The authors propose 2 methods: SOBA and SABA. SOBA updates $x, z, v$ similarly to single-level SGD and uses two-timescales decreasing step-sizes. SABA uses variance reduction, similarly to SAGA. The framework involves only 3 unbiased estimation in each iteration.\nThis work fully single-loop stochastic algorithms for bilevel optimization problems where the inner problem is strongly convex. The authors propose 2 methods: SOBA and SABA. SOBA updates $x, z, v$ similarly to single-level SGD and uses two-timescales decreasing step-sizes. SABA uses variance reduction, similarly to SAGA. The framework involves only 3 unbiased estimation in each iteration."
    },
    {
        "coherence": 0.3904493986038579,
        "consistency": 0.4068574478715971,
        "fluency": 0.31210817019478276,
        "relevance": 0.04631028585743931,
        "overall": 0.2889313256319193,
        "Generated": "132",
        "Gold": "The authors propose an approximate MCMC method for sampling a posterior distribution of weights in a Bayesian neural network. While the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance. This paper considers a new learning paradigm for Bayesian Neuron Networks (BNN): learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a \"functional space\"\nThe authors propose an approximate MCMC method for sampling a posterior distribution of weights in a Bayesian neural network. While the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance. This paper considers a new learning paradigm for Bayesian Neuron Networks (BNN): learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a \"functional space\""
    },
    {
        "coherence": 0.09893255767670364,
        "consistency": 0.2917413892640686,
        "fluency": 0.10874773392068575,
        "relevance": 0.02218612975996052,
        "overall": 0.13040195265535465,
        "Generated": "133",
        "Gold": "The paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning (RL) agent. The proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects. The method is tested on SafeLife Suite, and its performance can match task-specific safe learning baselines. This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects.\nThe paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning (RL) agent. The proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects. The method is tested on SafeLife Suite, and its performance can match task-specific safe learning baselines. This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects."
    },
    {
        "coherence": 0.2566632183267104,
        "consistency": 0.4001535036433198,
        "fluency": 0.27625212203489563,
        "relevance": 0.030743209883290195,
        "overall": 0.24095301347205403,
        "Generated": "134",
        "Gold": "This paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. The benefits over existing models are the structured, 3D object representations which allows to manipulate objects in the scenes. This paper also shows that the inferred object representations can be used in a visual reasoning task. The method receives a single input image (with the camera coordinates though these are fixed) and extracts a set of slots - one slot for each object.\nThis paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. The benefits over existing models are the structured, 3D object representations which allows to manipulate objects in the scenes. This paper also shows that the inferred object representations can be used in a visual reasoning task. The method receives a single input image (with the camera coordinates though these are fixed) and extracts a set of slots - one slot for each object."
    },
    {
        "coherence": 0.27677700908670144,
        "consistency": 0.3101152861482274,
        "fluency": 0.462515359186587,
        "relevance": 0.021534480213930424,
        "overall": 0.26773553365886155,
        "Generated": "135",
        "Gold": "The paper proposes the algorithm \"AdaRL\" a transfer method for reinforcement learning for different domains. The method is based on learning a latent representation with \" domain shared\" and \"domain specific\" components. The transfer is done by collecting some data in target domain and estimating \" domain specific\" variable of the target domain. Instead of implicitly updating the policy using data from the source domain, learn a particularly structured latent model and the elements of variation and learn a policy that performs pretty well.\nThe paper proposes the algorithm \"AdaRL\" a transfer method for reinforcement learning for different domains. The method is based on learning a latent representation with \" domain shared\" and \"domain specific\" components. The transfer is done by collecting some data in target domain and estimating \" domain specific\" variable of the target domain. Instead of implicitly updating the policy using data from the source domain, learn a particularly structured latent model and the elements of variation and learn a policy that performs pretty well."
    },
    {
        "coherence": 0.2713368629695235,
        "consistency": 0.4259552478174254,
        "fluency": 0.2123725590298795,
        "relevance": 0.066816725030962,
        "overall": 0.2441203487119476,
        "Generated": "136",
        "Gold": "The paper studies the degradation in performance of deep learning models under both adversarial as well as natural perturbations or contamination. The driving question of this paper seems to be whether naturalperturbations hurt generalization as much as adversarial examples. It is not always clear if the reported results are over training or testing samples. The notation throughout this paper is at times confusing. The authors propose a technique to “standardize” the robustification process across different perturbation.\nThe paper studies the degradation in performance of deep learning models under both adversarial as well as natural perturbations or contamination. The driving question of this paper seems to be whether naturalperturbations hurt generalization as much as adversarial examples. It is not always clear if the reported results are over training or testing samples. The notation throughout this paper is at times confusing. The authors propose a technique to “standardize” the robustification process across different perturbation."
    },
    {
        "coherence": 0.13452146914968094,
        "consistency": 0.26893461259574575,
        "fluency": 0.1637229748984753,
        "relevance": 0.02015897655898776,
        "overall": 0.14683450830072245,
        "Generated": "137",
        "Gold": "The authors propose a method of removing the bias in semi-supervised learning: debiased SSL (DeSSL) The paper provides generalization error bounds for the proposed methods based on the missing completely at random (MCAR) assumption. The paper has good clarity on the theory presented and generally sound approaches and good results. This paper's presentation is very poor and very hard to follow. Theoretical ananysis about the generalization are discussed.\nThe authors propose a method of removing the bias in semi-supervised learning: debiased SSL (DeSSL) The paper provides generalization error bounds for the proposed methods based on the missing completely at random (MCAR) assumption. The paper has good clarity on the theory presented and generally sound approaches and good results. This paper's presentation is very poor and very hard to follow. Theoretical ananysis about the generalization are discussed."
    },
    {
        "coherence": 0.19711941060879082,
        "consistency": 0.3539151290170112,
        "fluency": 0.11023615488288892,
        "relevance": 0.038573582303909025,
        "overall": 0.17496106920315002,
        "Generated": "138",
        "Gold": "The paper is well written and sound; the topic is interesting and fits UAI's domains. The approach is original. Further developments and experiments are necessary, but if they confirm the significance of the approach, the result is valuable. I am missing evidence that the problem studied is truly useful. There does not seem to be an experiment that would work on some (semi- realistic) data to solve a meaningful problem. I felt the authors covered a lot of ground and also explained many of the complicated ideas fairly clearly.\nThe paper is well written and sound; the topic is interesting and fits UAI's domains. The approach is original. Further developments and experiments are necessary, but if they confirm the significance of the approach, the result is valuable. I am missing evidence that the problem studied is truly useful. There does not seem to be an experiment that would work on some (semi- realistic) data to solve a meaningful problem. I felt the authors covered a lot of ground and also explained many of the complicated ideas fairly clearly."
    },
    {
        "coherence": 0.07095694830597232,
        "consistency": 0.29440147622300683,
        "fluency": 0.16435258146764065,
        "relevance": 0.016330666662593654,
        "overall": 0.13651041816480336,
        "Generated": "139",
        "Gold": "The proposed SCL-WC method outperforms state-of-the-art weakly-supervised whole-slide image classification studies. The application of contrastive learning, i.e., MoCov3, in histopathological image classification is very common. The proposed method consists of two main steps: a domain-specific self-super supervised feature extractor and task-specific feature aggregation modules. Ablation studies are conducted to help understand the importance of each proposed component.\nThe proposed SCL-WC method outperforms state-of-the-art weakly-supervised whole-slide image classification studies. The application of contrastive learning, i.e., MoCov3, in histopathological image classification is very common. The proposed method consists of two main steps: a domain-specific self-super supervised feature extractor and task-specific feature aggregation modules. Ablation studies are conducted to help understand the importance of each proposed component."
    },
    {
        "coherence": 0.21127768587425463,
        "consistency": 0.3147319379609494,
        "fluency": 0.349557503002696,
        "relevance": 0.02390727132518592,
        "overall": 0.22486859954077146,
        "Generated": "140",
        "Gold": "This submission presents a detailed numerical study of gradient flow of feedforward neural networks. Different layers of the three-layer architecture exhibit changes to their parameters. To the best of my knowledge, this submission gives new insight into training dynamics of MLPs with more than one hidden layer. Despite a few typos the paper is well presented and easy to read. The most important part of the paper, where the parameters gamma2 and gamma3 are derived, is rushed.\nThis submission presents a detailed numerical study of gradient flow of feedforward neural networks. Different layers of the three-layer architecture exhibit changes to their parameters. To the best of my knowledge, this submission gives new insight into training dynamics of MLPs with more than one hidden layer. Despite a few typos the paper is well presented and easy to read. The most important part of the paper, where the parameters gamma2 and gamma3 are derived, is rushed."
    },
    {
        "coherence": 0.13302173188598837,
        "consistency": 0.36844210608885747,
        "fluency": 0.13586036921318012,
        "relevance": 0.012757878737973637,
        "overall": 0.1625205214814999,
        "Generated": "141",
        "Gold": "The paper tests the method on three datasets, a synthetic 40 dimensional spiral dataset, the venerable MNIST dataset and a scaled down CELEB A dataset. It applies a prior encoder to construct an implicit prior, which is more flexible. The paper also proposes a graph-based algorithm for minimizing the pathwise energy to achieve the manifold walking. It proposes EPSWAE as a solution to the issues of representation learning with VAEs and proposes a new algorithm.\nThe paper tests the method on three datasets, a synthetic 40 dimensional spiral dataset, the venerable MNIST dataset and a scaled down CELEB A dataset. It applies a prior encoder to construct an implicit prior, which is more flexible. The paper also proposes a graph-based algorithm for minimizing the pathwise energy to achieve the manifold walking. It proposes EPSWAE as a solution to the issues of representation learning with VAEs and proposes a new algorithm."
    },
    {
        "coherence": 0.19781937385381052,
        "consistency": 0.278121103240435,
        "fluency": 0.2971926990469593,
        "relevance": 0.025020483586826917,
        "overall": 0.19953841493200794,
        "Generated": "142",
        "Gold": "This paper presents a genetic algorithm framework for molecular optimization. It produces valid molecules through the crossover and mutation operations along with appropriate fitness functions. The main claim of the paper (benefit of two-stage procedure) is not supported by empirical results. The experiments in the paper are limited and not very relevant to real-world drug discovery. It would be more interesting to explore a variety of more complex tasks. The reward is a combination of several objectives like Eqn.\nThis paper presents a genetic algorithm framework for molecular optimization. It produces valid molecules through the crossover and mutation operations along with appropriate fitness functions. The main claim of the paper (benefit of two-stage procedure) is not supported by empirical results. The experiments in the paper are limited and not very relevant to real-world drug discovery. It would be more interesting to explore a variety of more complex tasks. The reward is a combination of several objectives like Eqn."
    },
    {
        "coherence": 0.21423189296479775,
        "consistency": 0.4626700296661088,
        "fluency": 0.16172180454186527,
        "relevance": 0.021116565026005233,
        "overall": 0.21493507304969425,
        "Generated": "143",
        "Gold": "Circulant MinHash (C-MinHash) to approximate the Jaccard similarity in massive binary data. Compared with MinHash, C-Min Hash only requires two (or maybe one in practice) random permutations in a circulant manner for approximation. Extensive experiments validate the effectiveness of C- MinHash. The benefits of using only two permutation is not well justified. The storage cost may not be a concern as one can generate random permutation on the fly without storing them.\nCirculant MinHash (C-MinHash) to approximate the Jaccard similarity in massive binary data. Compared with MinHash, C-Min Hash only requires two (or maybe one in practice) random permutations in a circulant manner for approximation. Extensive experiments validate the effectiveness of C- MinHash. The benefits of using only two permutation is not well justified. The storage cost may not be a concern as one can generate random permutation on the fly without storing them."
    },
    {
        "coherence": 0.07306322177017857,
        "consistency": 0.162674509506828,
        "fluency": 0.14181989238735526,
        "relevance": 0.018100397533118075,
        "overall": 0.09891450529936999,
        "Generated": "144",
        "Gold": "This is an exciting research problem, and could be of broad interest in robotics. The problem posed, and associated data sets and simulation code, could be an interesting and novel source of challenge to machine learning researchers. The proposed task of cross-modal inference is an interesting task. I however hardly find any significance of the proposed method. Overall, the paper seems to require a significant improvement. Authors ought to better define the image description space of the objects. Concepts have to be clearly defined prior to their use.\nThis is an exciting research problem, and could be of broad interest in robotics. The problem posed, and associated data sets and simulation code, could be an interesting and novel source of challenge to machine learning researchers. The proposed task of cross-modal inference is an interesting task. I however hardly find any significance of the proposed method. Overall, the paper seems to require a significant improvement. Authors ought to better define the image description space of the objects. Concepts have to be clearly defined prior to their use."
    },
    {
        "coherence": 0.3495559345018851,
        "consistency": 0.3621968012762827,
        "fluency": 0.26580123623325386,
        "relevance": 0.01920770434961758,
        "overall": 0.24919041909025982,
        "Generated": "145",
        "Gold": "The paper shows that normalization is critical for zero-shot learning (ZSL) In the ZSL randomization is coming from the two sources, attribute and feature. Normalization of the two source helps to reduce the variance. Paper also extend ZSL framework to the continual learning ZSL setup. It also shows that two normalization tricks are not enough to variance control in a deep architecture. The experiments for CZSL are performed in two datasets, CUB and SUN.\nThe paper shows that normalization is critical for zero-shot learning (ZSL) In the ZSL randomization is coming from the two sources, attribute and feature. Normalization of the two source helps to reduce the variance. Paper also extend ZSL framework to the continual learning ZSL setup. It also shows that two normalization tricks are not enough to variance control in a deep architecture. The experiments for CZSL are performed in two datasets, CUB and SUN."
    },
    {
        "coherence": 0.22734787476729637,
        "consistency": 0.4320801624669358,
        "fluency": 0.24405964184055667,
        "relevance": 0.021247669990554968,
        "overall": 0.23118383726633593,
        "Generated": "146",
        "Gold": "Falconn++ combines the LSF property with the LSH to achieve a lower query time complexity beyond the vanilla LSH schemes such as Falconn. Extensive experiments validated the superior performance of Falconn++. The paper is well organized and easy to follow. The method improves upon the original FALCON and claims to be comparable to HNSW. It is not yet clear if Falconn will be used for recommender and retrieval systems.\nFalconn++ combines the LSF property with the LSH to achieve a lower query time complexity beyond the vanilla LSH schemes such as Falconn. Extensive experiments validated the superior performance of Falconn++. The paper is well organized and easy to follow. The method improves upon the original FALCON and claims to be comparable to HNSW. It is not yet clear if Falconn will be used for recommender and retrieval systems."
    },
    {
        "coherence": 0.18660123592017147,
        "consistency": 0.2270822536305915,
        "fluency": 0.3377335171847828,
        "relevance": 0.029733902737522452,
        "overall": 0.19528772736826702,
        "Generated": "147",
        "Gold": "This paper studies learning in stochastic games, which are extensions of Markov decision processes (MDPs) from the single-agent setup to the multi-agent one. The main contribution is a model-based algorithm called Nash-VI. The goal is to learn an \\epsilon approximate of the Nash equilibrium while minimizing the sample complexity. This rate matches the lower bound of order \\Omega(H^3S(A+B) up to a factor min(A,B)\nThis paper studies learning in stochastic games, which are extensions of Markov decision processes (MDPs) from the single-agent setup to the multi-agent one. The main contribution is a model-based algorithm called Nash-VI. The goal is to learn an \\epsilon approximate of the Nash equilibrium while minimizing the sample complexity. This rate matches the lower bound of order \\Omega(H^3S(A+B) up to a factor min(A,B)"
    },
    {
        "coherence": 0.15910032354145492,
        "consistency": 0.16852409279481187,
        "fluency": 0.21638620756906815,
        "relevance": 0.019541926309445946,
        "overall": 0.14088813755369523,
        "Generated": "148",
        "Gold": "This paper shows the proposed loss functions do not incentivise the learner to represent the uncertainty in a faithful way. The strength of this paper is that it is relevant and important in the literature. The weakness is that a limited amount of numerical results may restrict the understanding of readers. This paper discusses the difficulty in learning a predictor that exhibits the epistemic uncertainty given labeled examples as a training dataset. The analysis is limited in scope, applying only to a certain losses proposed in prior work.\nThis paper shows the proposed loss functions do not incentivise the learner to represent the uncertainty in a faithful way. The strength of this paper is that it is relevant and important in the literature. The weakness is that a limited amount of numerical results may restrict the understanding of readers. This paper discusses the difficulty in learning a predictor that exhibits the epistemic uncertainty given labeled examples as a training dataset. The analysis is limited in scope, applying only to a certain losses proposed in prior work."
    },
    {
        "coherence": 0.2984522974609248,
        "consistency": 0.3908732360248079,
        "fluency": 0.2836033269899907,
        "relevance": 0.0641641702164596,
        "overall": 0.25927325767304576,
        "Generated": "149",
        "Gold": "The paper studies tail behaviors of bandit problems. It proves that any algorithm that achieves instance-dependent consistency (logarithmic regret) must exhibit heavy-tailed regret. The authors consider a novel problem of characterizing the tail behavior of the random regret for algorithms that are designed to solve the multi-armed bandit problem. They show that the distribution of regret has a heavy tail in the large T regime for the standard policies. It shows that it is possible to achieve light-tailed and worst-case optimal regret at the same time.\nThe paper studies tail behaviors of bandit problems. It proves that any algorithm that achieves instance-dependent consistency (logarithmic regret) must exhibit heavy-tailed regret. The authors consider a novel problem of characterizing the tail behavior of the random regret for algorithms that are designed to solve the multi-armed bandit problem. They show that the distribution of regret has a heavy tail in the large T regime for the standard policies. It shows that it is possible to achieve light-tailed and worst-case optimal regret at the same time."
    },
    {
        "coherence": 0.3358051695466011,
        "consistency": 0.5193216421549534,
        "fluency": 0.32299958460995687,
        "relevance": 0.017223686260366102,
        "overall": 0.29883752064296937,
        "Generated": "150",
        "Gold": "Nocturne is a new 2D driving simulator, with a focus on multi-agent coordination under partial observability. The simulator is run on real-world data at a high-frequency rate, providing an accurate account of driving situations. It first reconstructs maps and replay objects' trajectories contained in real- world datasets. After that, traffic vehicles are turned into controllable agents with partial observable. Compared to previous works, Nocturne has more efficient environment interaction efficiency.\nNocturne is a new 2D driving simulator, with a focus on multi-agent coordination under partial observability. The simulator is run on real-world data at a high-frequency rate, providing an accurate account of driving situations. It first reconstructs maps and replay objects' trajectories contained in real- world datasets. After that, traffic vehicles are turned into controllable agents with partial observable. Compared to previous works, Nocturne has more efficient environment interaction efficiency."
    },
    {
        "coherence": 0.2980712805479526,
        "consistency": 0.41053176061931107,
        "fluency": 0.19961592577632709,
        "relevance": 0.13839156531415558,
        "overall": 0.2616526330644366,
        "Generated": "151",
        "Gold": "The paper provides a detailed discussion of the main characteristics of harmful text that are, or can be, generated by (large) language models. While the paper offers many critical insights and has many noteworthy merits, it might still benefit from a second round of reviews. The authors fail to provide clear definitions of key terms that are used in the paper such as harm, bias, and toxic. Overall, the paper has merit, as it promotes a discussion about important issues."
    },
    {
        "coherence": 0.339693734628108,
        "consistency": 0.5042658209343777,
        "fluency": 0.31512667978668535,
        "relevance": 0.02842449461267142,
        "overall": 0.29687768249046065,
        "Generated": "152",
        "Gold": "This paper aims to speed up NAS with a training-free performance estimation strategy. It estimates an architecture’s performance from two perspectives: (1) trainability, and (2) expressivity. The method can search within 30 minutes (on CIFAR-10) and 4 hours (on ImageNet) The metrics for trainability and expressivity seem to be the direct application of deep learning theories. The major contributions of this work, the two network measurement methods, were proposed by previous works.\nThis paper aims to speed up NAS with a training-free performance estimation strategy. It estimates an architecture’s performance from two perspectives: (1) trainability, and (2) expressivity. The method can search within 30 minutes (on CIFAR-10) and 4 hours (on ImageNet) The metrics for trainability and expressivity seem to be the direct application of deep learning theories. The major contributions of this work, the two network measurement methods, were proposed by previous works."
    },
    {
        "coherence": 0.24079570105132977,
        "consistency": 0.3881644015904063,
        "fluency": 0.25347012526020873,
        "relevance": 0.013777457610084616,
        "overall": 0.22405192137800736,
        "Generated": "153",
        "Gold": "This paper presents a modification to existing information theoretic feature selection algorithms. It adds a strong relevance term estimated using a k-nn MI estimator. Experimental results show that MIBFS with UR consistently outperform their unboosted conterparts. The work is easy to follow. However, the perspectives and methods are not novel. And there is a technical flaw in the analysis. It's a slightly expanded copy of a paper published at IEEE International Symposium on Information Theory 2020, referenced as \"Exploring unique relevance for mutual information based feature selection\"\nThis paper presents a modification to existing information theoretic feature selection algorithms. It adds a strong relevance term estimated using a k-nn MI estimator. Experimental results show that MIBFS with UR consistently outperform their unboosted conterparts. The work is easy to follow. However, the perspectives and methods are not novel. And there is a technical flaw in the analysis. It's a slightly expanded copy of a paper published at IEEE International Symposium on Information Theory 2020, referenced as \"Exploring unique relevance for mutual information based feature selection\""
    },
    {
        "coherence": 0.21938797733526672,
        "consistency": 0.5098737600112488,
        "fluency": 0.24159829036559913,
        "relevance": 0.030364264909954673,
        "overall": 0.25030607315551734,
        "Generated": "154",
        "Gold": "The authors show that larger-sized language models memorize training data faster. This memorization happens before the overfitting of language modeling. Specific part-of-speech-tagged tokens are memorized faster like nouns and numbers. Larger models also tends to forget less, and they seem to memorize unique parts of speech tokens. The authors have not discussed the limitations of their work thoroughly. The paper measures memorization as the proportion of correctly predicted labels (which corresponds to predicting the correct next/masked token) and forgetting as the decline in memorization.\nThe authors show that larger-sized language models memorize training data faster. This memorization happens before the overfitting of language modeling. Specific part-of-speech-tagged tokens are memorized faster like nouns and numbers. Larger models also tends to forget less, and they seem to memorize unique parts of speech tokens. The authors have not discussed the limitations of their work thoroughly. The paper measures memorization as the proportion of correctly predicted labels (which corresponds to predicting the correct next/masked token) and forgetting as the decline in memorization."
    },
    {
        "coherence": 0.14481578416205818,
        "consistency": 0.25384591931955247,
        "fluency": 0.2443955705842212,
        "relevance": 0.028468762865696856,
        "overall": 0.1678815092328822,
        "Generated": "155",
        "Gold": "The paper addresses the KG refinement task, aiming to improve KGs that were built via imperfect automated processes. Key insight is to augment KG embeddings not only with implicit type information, but also with explicit types produced by an ontology-based system. The resulting algorithm, TypeE-X, leverages the benefits of structured (often human-crafted) information and the versatility of continuous embeddeds. The experiments show substantial improvement on datasets with rich ontologies (not wordnet) The effects of iteration are minimal, so it is not clear that they are useful.\nThe paper addresses the KG refinement task, aiming to improve KGs that were built via imperfect automated processes. Key insight is to augment KG embeddings not only with implicit type information, but also with explicit types produced by an ontology-based system. The resulting algorithm, TypeE-X, leverages the benefits of structured (often human-crafted) information and the versatility of continuous embeddeds. The experiments show substantial improvement on datasets with rich ontologies (not wordnet) The effects of iteration are minimal, so it is not clear that they are useful."
    },
    {
        "coherence": 0.3964798021429546,
        "consistency": 0.40120323206002506,
        "fluency": 0.19823216686147047,
        "relevance": 0.04113097320122131,
        "overall": 0.2592615435664179,
        "Generated": "156",
        "Gold": "Adversarial neural networks can cause the complete verifier to produce imprecise results in floating point arithmetics. They also showed it is possible to insert a backdoor to the network such that the backdoor is missed by the verifier while it can trigger some behavior desired by the attacker. A defence to this behavior is proposed, making all network parameters slightly noisy. The paper has several weaknesses related to presentation. The authors show how to make their networks look a bit less suspicious. They show a method to create neural networks that, due to floating-point error, lead to wrong robustness certifications.\nAdversarial neural networks can cause the complete verifier to produce imprecise results in floating point arithmetics. They also showed it is possible to insert a backdoor to the network such that the backdoor is missed by the verifier while it can trigger some behavior desired by the attacker. A defence to this behavior is proposed, making all network parameters slightly noisy. The paper has several weaknesses related to presentation. The authors show how to make their networks look a bit less suspicious. They show a method to create neural networks that, due to floating-point error, lead to wrong robustness certifications."
    },
    {
        "coherence": 0.49094595882163905,
        "consistency": 0.6609500905719112,
        "fluency": 0.27529008031466506,
        "relevance": 0.0376865870502413,
        "overall": 0.3662181791896142,
        "Generated": "157",
        "Gold": "This paper proposes a deep state space model for videos. A latent variable z is introduced for explaining content and kept fixed for all frames. Experiments are carried out on Sprites and MUG to demonstrate the efficacy. The authors propose a method called Halo that allows to disentangle the content from the motion in image sequences, in the VAE framework. They do so by separating the latent space in two spaces: 1) the content space, a global content vector that summarizes the image sequence, and 2) the motion space.\nThis paper proposes a deep state space model for videos. A latent variable z is introduced for explaining content and kept fixed for all frames. Experiments are carried out on Sprites and MUG to demonstrate the efficacy. The authors propose a method called Halo that allows to disentangle the content from the motion in image sequences, in the VAE framework. They do so by separating the latent space in two spaces: 1) the content space, a global content vector that summarizes the image sequence, and 2) the motion space."
    },
    {
        "coherence": 0.4891431649500584,
        "consistency": 0.5979519734968639,
        "fluency": 0.1638215652454474,
        "relevance": 0.037146213093033204,
        "overall": 0.3220157291963507,
        "Generated": "158",
        "Gold": "This paper introduced a new generative modelling method for structured data. Authors experiment on polyphonic music modeling and language modeling. Authors propose a diffusion model that adds a mask as an additional inputs to noisy data. The organization of the writing is very loose, making it very confusing for the readers. The analysis is not very convincing both quantitatively and qualitatively. The authors did not really cover too much about limitations. This idea is very close to SUNDAE proposed by DeepMind this year.\nThis paper introduced a new generative modelling method for structured data. Authors experiment on polyphonic music modeling and language modeling. Authors propose a diffusion model that adds a mask as an additional inputs to noisy data. The organization of the writing is very loose, making it very confusing for the readers. The analysis is not very convincing both quantitatively and qualitatively. The authors did not really cover too much about limitations. This idea is very close to SUNDAE proposed by DeepMind this year."
    },
    {
        "coherence": 0.49253106813329944,
        "consistency": 0.7000034213551452,
        "fluency": 0.23329403185525294,
        "relevance": 0.019440616616657608,
        "overall": 0.3613172844900888,
        "Generated": "159",
        "Gold": "This paper addressed the problem “entity monitoring” Based on the problem formulation, it is very related to the object re-identification and instance retrieval problem. The architecture is built to mimic components typically found in DAF systems. The resulting system is evaluated on several different tasks using synthetic data. All experiments are performed with synthetic dataset. This does not expose much real-world challenges. The paper proposes an end-to-end system for the data association and filtering (DAF) problem.\nThis paper addressed the problem “entity monitoring” Based on the problem formulation, it is very related to the object re-identification and instance retrieval problem. The architecture is built to mimic components typically found in DAF systems. The resulting system is evaluated on several different tasks using synthetic data. All experiments are performed with synthetic dataset. This does not expose much real-world challenges. The paper proposes an end-to-end system for the data association and filtering (DAF) problem."
    },
    {
        "coherence": 0.3433512373706261,
        "consistency": 0.4370853249428781,
        "fluency": 0.31381503518959764,
        "relevance": 0.09049903985233458,
        "overall": 0.29618765933885915,
        "Generated": "160",
        "Gold": "The proposed method outperforms six benchmarks with a large margin. The major concern of the proposed method is the label model. It is suggested that the authors should provide the graphs. The paper is somewhat presented in atelegraphic style. It's not clear how much the details add given that you don't give a big empirical exploration of the tradeoffs you derive. \"Suppose that data $x, y, \\lambda$ follows the model in (1).\" is a big assumption."
    },
    {
        "coherence": 0.06481599485923228,
        "consistency": 0.14858363601418367,
        "fluency": 0.07722574060470432,
        "relevance": 0.034449996093597904,
        "overall": 0.08126884189292954,
        "Generated": "161",
        "Gold": "The paper tackles a typically disregarded problem in contrastive learning. It shows the importance of selecting difficult negatives to obtain stronger unsupervised representations. The Ring model results in a simple method that can be applied to any contrastive algorithm resulting in a better representation. The paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles. This sampling strategy improves the contrastivelearning methods (IR, CMC, MoCO)\nThe paper tackles a typically disregarded problem in contrastive learning. It shows the importance of selecting difficult negatives to obtain stronger unsupervised representations. The Ring model results in a simple method that can be applied to any contrastive algorithm resulting in a better representation. The paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles. This sampling strategy improves the contrastivelearning methods (IR, CMC, MoCO)"
    },
    {
        "coherence": 0.11192757785228213,
        "consistency": 0.2513389508280952,
        "fluency": 0.15306207873299826,
        "relevance": 0.01927700618483901,
        "overall": 0.13390140339955364,
        "Generated": "162",
        "Gold": "ODConv combines two prior ideas, i.e. filter recalibration with attention in SENet and additive kernels in CondConv/DyConv. The method is overall quite efficient without too much overhead, due to separate attention weights for different dimensions. The proposed method can be plugged into most existing CNN architectures and has good performance on public datasets, such as ImageNet and MS-COCO. But the weakness of this paper is too experimental-oriented without thorough analysis on the results.\n ODConv combines two prior ideas, i.e. filter recalibration with attention in SENet and additive kernels in CondConv/DyConv. The method is overall quite efficient without too much overhead, due to separate attention weights for different dimensions. The proposed method can be plugged into most existing CNN architectures and has good performance on public datasets, such as ImageNet and MS-COCO. But the weakness of this paper is too experimental-oriented without thorough analysis on the results."
    },
    {
        "coherence": 0.20964228633157553,
        "consistency": 0.3689870121262039,
        "fluency": 0.11294016228477972,
        "relevance": 0.021980640717458772,
        "overall": 0.1783875253650045,
        "Generated": "163",
        "Gold": "The authors propose an evolutionary ensemble method for decoding neural signals. The method involves initially training an ensemble model via minimizing a squared error loss on training data. The overall model prediction is then made from a weighted combination of ensemble members. The main advantage of the method is that the model can be adaptive to the variation of neural signals through evolution in the Bayesian framework. Unfortunately the extremely confused description makes it impossible to understand, much less reproduce the method.\nThe authors propose an evolutionary ensemble method for decoding neural signals. The method involves initially training an ensemble model via minimizing a squared error loss on training data. The overall model prediction is then made from a weighted combination of ensemble members. The main advantage of the method is that the model can be adaptive to the variation of neural signals through evolution in the Bayesian framework. Unfortunately the extremely confused description makes it impossible to understand, much less reproduce the method."
    },
    {
        "coherence": 0.3379805076062132,
        "consistency": 0.43087140318854006,
        "fluency": 0.24065203097610272,
        "relevance": 0.07860380722882337,
        "overall": 0.27202693724991983,
        "Generated": "164",
        "Gold": "The paper aims to make strides towards a theoretical understanding of Deep neural networks, which remains elusive to date. They show that their proposed Lyapunov loss converges faster than the L1 and L2 losses. Can a similar loss function for MLPs on classification tasks be easily derived? The authors in this paper make an attempt in providing finite time convergence guarantees of the training process of neural networks. The idea is to come up with a weight update rule which guarantees the convergence rate.\nThe paper aims to make strides towards a theoretical understanding of Deep neural networks, which remains elusive to date. They show that their proposed Lyapunov loss converges faster than the L1 and L2 losses. Can a similar loss function for MLPs on classification tasks be easily derived? The authors in this paper make an attempt in providing finite time convergence guarantees of the training process of neural networks. The idea is to come up with a weight update rule which guarantees the convergence rate."
    },
    {
        "coherence": 0.13574418154378254,
        "consistency": 0.22789394235168067,
        "fluency": 0.1693082679081708,
        "relevance": 0.031258870266470784,
        "overall": 0.1410513155175262,
        "Generated": "165",
        "Gold": "The paper provides a solution for multi-cloud configuration problem. It tries to provide cloud customers with an optimal configuration to minimise runtime and cost. The paper also presents a dataset, for offline benchmarking, comprising of 60 different multi- cloud configuration tasks across 3 cloud service providers. Overall, the paper is well written but several aspects of the problem were ignored. The dataset presented could be meaningful but currently it lacks the rigor in generating the dataset.\nThe paper provides a solution for multi-cloud configuration problem. It tries to provide cloud customers with an optimal configuration to minimise runtime and cost. The paper also presents a dataset, for offline benchmarking, comprising of 60 different multi- cloud configuration tasks across 3 cloud service providers. Overall, the paper is well written but several aspects of the problem were ignored. The dataset presented could be meaningful but currently it lacks the rigor in generating the dataset."
    },
    {
        "coherence": 0.1319641701475416,
        "consistency": 0.21089684707803488,
        "fluency": 0.10967596151817272,
        "relevance": 0.01502557831689316,
        "overall": 0.11689063926516059,
        "Generated": "166",
        "Gold": "The paper presents a new, more complex, dataset for the use of disentangled representation learning. The dataset is based on real and simulated images of the trifinger robot platform. The authors also present a new neural architecture to scale disentanglement on more complex datasets. The work has sufficient quality, and is sufficiently clear and original. For detail, please see the below pros and cons of the paper. Back to Mail Online home. back to the page you came from.\nThe paper presents a new, more complex, dataset for the use of disentangled representation learning. The dataset is based on real and simulated images of the trifinger robot platform. The authors also present a new neural architecture to scale disentanglement on more complex datasets. The work has sufficient quality, and is sufficiently clear and original. For detail, please see the below pros and cons of the paper. Back to Mail Online home. back to the page you came from."
    },
    {
        "coherence": 0.23571312828452096,
        "consistency": 0.39578966134914173,
        "fluency": 0.4324870505265651,
        "relevance": 0.03853081960935258,
        "overall": 0.2756301649423951,
        "Generated": "167",
        "Gold": "A new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown. How these benefits touch and sight association tasks are unclear.\nA new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown. How these benefits touch and sight association tasks are unclear."
    },
    {
        "coherence": 0.18225286793142803,
        "consistency": 0.3567138911005241,
        "fluency": 0.2138293073915403,
        "relevance": 0.023953337795691386,
        "overall": 0.19418735105479595,
        "Generated": "168",
        "Gold": "This paper develops a novel way to compare two kernels, based on optimal transport between feature representations of the tree representation of the kernel. The paper comes up with a good idea for kernel search, inspired from references [10,4] This paper proposes a new way to select a kernel (covariance function) for Gaussian processes (GP) The authors borrow ideas from NN architecture search to propose a so called symbolical-optimal-transport (SOT) kernel.\nThis paper develops a novel way to compare two kernels, based on optimal transport between feature representations of the tree representation of the kernel. The paper comes up with a good idea for kernel search, inspired from references [10,4] This paper proposes a new way to select a kernel (covariance function) for Gaussian processes (GP) The authors borrow ideas from NN architecture search to propose a so called symbolical-optimal-transport (SOT) kernel."
    },
    {
        "coherence": 0.13532293046211313,
        "consistency": 0.1583727038900157,
        "fluency": 0.2785824958561386,
        "relevance": 0.09493592017150729,
        "overall": 0.1668035125949437,
        "Generated": "169",
        "Gold": "This paper proposes a new way to model discrete state diffusion models. The method is based on a 'continuous time' model. It uses a technique called 'tau-leaping' to test the model. While there is still a (relatively small) performance gap between this work and continuous diffusion models, I think it is an important work. I'd like to further raise my score if my concerns are addressed. The paper proposes to sample from the CT diffusion model via tau-Leaping, a well-known technique in chemical physics."
    },
    {
        "coherence": 0.07580177401287824,
        "consistency": 0.14440905888158587,
        "fluency": 0.42209722031153624,
        "relevance": 0.02412139199829639,
        "overall": 0.1666073613010742,
        "Generated": "170",
        "Gold": "The paper describes a bit-flipping white-box attack on deployed neural network classifiers. Given a model, find a perturbation of the parameters bits such that the model with misclassify one specific example, while maintaining high accuracy on other examples. Experiments CIFAR-10 and ImageNet show that the proposed method outperforms the SOTA. The proposed method is mathematically sound. It empirically outperforms or at least is comparable with previous state-of-the-art methods on undefended models.\nThe paper describes a bit-flipping white-box attack on deployed neural network classifiers. Given a model, find a perturbation of the parameters bits such that the model with misclassify one specific example, while maintaining high accuracy on other examples. Experiments CIFAR-10 and ImageNet show that the proposed method outperforms the SOTA. The proposed method is mathematically sound. It empirically outperforms or at least is comparable with previous state-of-the-art methods on undefended models."
    },
    {
        "coherence": 0.0975866198924468,
        "consistency": 0.2558196576181294,
        "fluency": 0.10393692814617464,
        "relevance": 0.028062026879697378,
        "overall": 0.12135130813411205,
        "Generated": "171",
        "Gold": "The goal of this paper is to evaluate methods for detecting out-of-distribution samples in a more comprehensive fashion than prior work. Using a pre-trained network improves OOD detection (in line with previous results), but one particular method does. The results show that cosine similarity consistently outperforms other methods across all sub-tasks. There is no real discussion or insights into why certain methods work in some scenarios and not others. Fine-tuning/pre-training gives a strong performance boost for OoD detection.\nThe goal of this paper is to evaluate methods for detecting out-of-distribution samples in a more comprehensive fashion than prior work. Using a pre-trained network improves OOD detection (in line with previous results), but one particular method does. The results show that cosine similarity consistently outperforms other methods across all sub-tasks. There is no real discussion or insights into why certain methods work in some scenarios and not others. Fine-tuning/pre-training gives a strong performance boost for OoD detection."
    },
    {
        "coherence": 0.3773682885377982,
        "consistency": 0.5237989551276261,
        "fluency": 0.40345185898422065,
        "relevance": 0.0470329089339603,
        "overall": 0.3379130028959013,
        "Generated": "172",
        "Gold": "Adversarial training in linear neural networks is a corollary to Theorem 5. The paper's results place a milestone in the theory of adversarial robustness. The proof of Theorem 2 may have some potential errors. The results, to be honest, are not surprising, given previous works on standard training. But I believe the rigorous justification presented in the paper is of importance. It shows the implicit bias in a simplified setting with a deep linear neural network and linearly separable data.\nAdversarial training in linear neural networks is a corollary to Theorem 5. The paper's results place a milestone in the theory of adversarial robustness. The proof of Theorem 2 may have some potential errors. The results, to be honest, are not surprising, given previous works on standard training. But I believe the rigorous justification presented in the paper is of importance. It shows the implicit bias in a simplified setting with a deep linear neural network and linearly separable data."
    },
    {
        "coherence": 0.17974528673708837,
        "consistency": 0.17509704340692026,
        "fluency": 0.14701576606530314,
        "relevance": 0.030145975509194258,
        "overall": 0.1330010179296265,
        "Generated": "173",
        "Gold": "The paper aims at justifying the performance gain that is acquired by the use of \"composite\" neural networks. The analysis done in the paper is that of a simple linear mixture of the outputs produced by each component. The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., \"components\") in the input of a network then you have \"more information\", and this cannot be bad.\nThe paper aims at justifying the performance gain that is acquired by the use of \"composite\" neural networks. The analysis done in the paper is that of a simple linear mixture of the outputs produced by each component. The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., \"components\") in the input of a network then you have \"more information\", and this cannot be bad."
    },
    {
        "coherence": 0.19216108007257315,
        "consistency": 0.38022296317705867,
        "fluency": 0.2989637900056545,
        "relevance": 0.02551439328437468,
        "overall": 0.22421555663491527,
        "Generated": "174",
        "Gold": "The authors proposed a Sparse Low-Dimensional Decision \"SLDD-Model\" for image classification. The model ensures a low-dimensional feature space by performing feature selection prior to the last (linear) layer of a neural network. To understand the meaning of the selected features, the authors used posthoc feature alignment with human interpretable concepts. The results are strong, the discovered features are interesting, and the qualitative examples are sensible.\nThe authors proposed a Sparse Low-Dimensional Decision \"SLDD-Model\" for image classification. The model ensures a low-dimensional feature space by performing feature selection prior to the last (linear) layer of a neural network. To understand the meaning of the selected features, the authors used posthoc feature alignment with human interpretable concepts. The results are strong, the discovered features are interesting, and the qualitative examples are sensible."
    },
    {
        "coherence": 0.21264933897693336,
        "consistency": 0.37805224672891313,
        "fluency": 0.5293015351033722,
        "relevance": 0.0340723399519854,
        "overall": 0.288518865190301,
        "Generated": "175",
        "Gold": "A more general variant of the predictive coding (PC) algorithm for learning deep neural networks is introduced. Unlike traditional PC, this generalized PC is also effective in training highly complex neural architectures such as transformer networks. The paper is well written with impressive results, although some notation and exposition can be improved. It is comparable to backpropagation in the classification of MNSIT and CIFAR-10 datasets. The authors apply this to supervised classification, VAE training, and training transformers.\nA more general variant of the predictive coding (PC) algorithm for learning deep neural networks is introduced. Unlike traditional PC, this generalized PC is also effective in training highly complex neural architectures such as transformer networks. The paper is well written with impressive results, although some notation and exposition can be improved. It is comparable to backpropagation in the classification of MNSIT and CIFAR-10 datasets. The authors apply this to supervised classification, VAE training, and training transformers."
    },
    {
        "coherence": 0.24924692262242268,
        "consistency": 0.36268182428874585,
        "fluency": 0.4576648843878136,
        "relevance": 0.04219844303009618,
        "overall": 0.2779480185822696,
        "Generated": "176",
        "Gold": "This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. The novelty is limited and not well explained. It is not clear how much data is used to train the super- resolution model. The paper is addressing an important problem but I still have several concerns.\nThis paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. The novelty is limited and not well explained. It is not clear how much data is used to train the super- resolution model. The paper is addressing an important problem but I still have several concerns."
    },
    {
        "coherence": 0.1107386592747193,
        "consistency": 0.3167591518222163,
        "fluency": 0.21565747569435958,
        "relevance": 0.029858606326073688,
        "overall": 0.16825347327934223,
        "Generated": "177",
        "Gold": "This paper aims to improve the performance of the Nondeterministic Stack RNN (NS-RNN) Using unnormalized positive weights instead of probabilities for stack actions and allowing the model to directly observe the state. The paper also uses the new NS-Rnn for a language modelling task on the Penn Treebank by introducing a memory-limiting technique. The ideas are technically sound and empirically proved. But the novelty and impact of this paper are not significant enough.\nThis paper aims to improve the performance of the Nondeterministic Stack RNN (NS-RNN) Using unnormalized positive weights instead of probabilities for stack actions and allowing the model to directly observe the state. The paper also uses the new NS-Rnn for a language modelling task on the Penn Treebank by introducing a memory-limiting technique. The ideas are technically sound and empirically proved. But the novelty and impact of this paper are not significant enough."
    },
    {
        "coherence": 0.1482278032283003,
        "consistency": 0.26885672458246584,
        "fluency": 0.14153970357971762,
        "relevance": 0.03143232256444212,
        "overall": 0.14751413848873146,
        "Generated": "178",
        "Gold": "Review of SLDVBF, a paper showing how an RNN-based inference procedure fits complex dynamical systems. The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches. This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The proposed model has a clear graphical representation, but its formal counterpart is more difficult to grasp.\nReview of SLDVBF, a paper showing how an RNN-based inference procedure fits complex dynamical systems. The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches. This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The proposed model has a clear graphical representation, but its formal counterpart is more difficult to grasp."
    },
    {
        "coherence": 0.27355667415581386,
        "consistency": 0.4047006689419226,
        "fluency": 0.4251702684356279,
        "relevance": 0.04508242970449929,
        "overall": 0.2871275103094659,
        "Generated": "179",
        "Gold": "This paper proposes to solve high-dimensional nonlinear parabolic partial differential equations by using an Actor-Critic neural network architecture. The potential for the proposed idea is difficult to realize because the paper lacks essential details. The authors introduce the abbreviations D(BSDE) without ever defining them of explicitly explaining what they refer too. The discussion of the method itself could be made clearer. The proposed work claims to decrease the number of trainable parameters, hyperparameters, and convergence rate.\nThis paper proposes to solve high-dimensional nonlinear parabolic partial differential equations by using an Actor-Critic neural network architecture. The potential for the proposed idea is difficult to realize because the paper lacks essential details. The authors introduce the abbreviations D(BSDE) without ever defining them of explicitly explaining what they refer too. The discussion of the method itself could be made clearer. The proposed work claims to decrease the number of trainable parameters, hyperparameters, and convergence rate."
    },
    {
        "coherence": 0.10142740127789693,
        "consistency": 0.1671648672886201,
        "fluency": 0.3099697570782018,
        "relevance": 0.0323720573603127,
        "overall": 0.15273352075125787,
        "Generated": "180",
        "Gold": "The paper asks how we should interpolate in the latent space. The key idea is to derive a natural interpolant from the prior distribution p(z), where z is the latent variable. The authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property. I have some concerns about the idea itself (see below), yet find that the paper brings an interesting new idea to the table.\nThe paper asks how we should interpolate in the latent space. The key idea is to derive a natural interpolant from the prior distribution p(z), where z is the latent variable. The authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property. I have some concerns about the idea itself (see below), yet find that the paper brings an interesting new idea to the table."
    },
    {
        "coherence": 0.1045241948923454,
        "consistency": 0.27890051644650876,
        "fluency": 0.04210530096005431,
        "relevance": 0.03526423384171255,
        "overall": 0.11519856153515526,
        "Generated": "181",
        "Gold": "The authors hope to explore a more efficient/reasonable masking strategy for training masked autoencoders (MAEs) Their intuition is that current masking strategies (e.g., random masking) cannot utilize semantic information contained in images. To achieve this, an iBOT-pretrained ViT is used to extract the features, and then StyleGAN-based decoder is trained to learn semantic parts. The paper claims that semantic parts of objects are the visual analogue of words.\nThe authors hope to explore a more efficient/reasonable masking strategy for training masked autoencoders (MAEs) Their intuition is that current masking strategies (e.g., random masking) cannot utilize semantic information contained in images. To achieve this, an iBOT-pretrained ViT is used to extract the features, and then StyleGAN-based decoder is trained to learn semantic parts. The paper claims that semantic parts of objects are the visual analogue of words."
    },
    {
        "coherence": 0.16778578055731386,
        "consistency": 0.2581931713220705,
        "fluency": 0.08565996998174044,
        "relevance": 0.025498687027338283,
        "overall": 0.13428440222211577,
        "Generated": "182",
        "Gold": "This is a well written paper proposing a quantitative framework to expand the inferences that can be done in the study of the causal effects of task stimuli on brain signals. The considered problem is interesting and important. The proposed metrics are intuitively reasonable. Although the considered problem has some \"causal flavour\", I feel that the proposed method is rather irrelevant to causality. I am not sure whether this paper would be a good fit for this conference.\nThis is a well written paper proposing a quantitative framework to expand the inferences that can be done in the study of the causal effects of task stimuli on brain signals. The considered problem is interesting and important. The proposed metrics are intuitively reasonable. Although the considered problem has some \"causal flavour\", I feel that the proposed method is rather irrelevant to causality. I am not sure whether this paper would be a good fit for this conference."
    },
    {
        "coherence": 0.43945291172113016,
        "consistency": 0.48351160009252026,
        "fluency": 0.33170385048844697,
        "relevance": 0.039282705727642635,
        "overall": 0.323487767007435,
        "Generated": "183",
        "Gold": "The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. The motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to. The proposal provides an easy-to-implement drop-in regularizer framework. The paper is head over heels. It can be caricatured as extreme obfuscation. I encourage more simulation studies and take more GAN structures into consideration.\nThe paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. The motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to. The proposal provides an easy-to-implement drop-in regularizer framework. The paper is head over heels. It can be caricatured as extreme obfuscation. I encourage more simulation studies and take more GAN structures into consideration."
    },
    {
        "coherence": 0.2714740778098747,
        "consistency": 0.3878229061514138,
        "fluency": 0.14759248080743761,
        "relevance": 0.03131665456087067,
        "overall": 0.2095515298323992,
        "Generated": "184",
        "Gold": "The paper proposes a min-min-max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The paper has shown that the proposed method can successfully reduce the test performance of the model (adversarially) trained by generating robust-error-minimization noise. This paper is well-written and the experiments are extensive. However, the proposed REM has the following issues: Unrealistic assumptions.\nThe paper proposes a min-min-max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The paper has shown that the proposed method can successfully reduce the test performance of the model (adversarially) trained by generating robust-error-minimization noise. This paper is well-written and the experiments are extensive. However, the proposed REM has the following issues: Unrealistic assumptions."
    },
    {
        "coherence": 0.04679548208991433,
        "consistency": 0.11167438610457812,
        "fluency": 0.17653174356497364,
        "relevance": 0.04599133191360715,
        "overall": 0.09524823591826831,
        "Generated": "185",
        "Gold": "The method is well organized and well written. Only weakness that I can think of is that improvement may be only in a context of 2.5D vs 3D. Methodological development is well motivated and clearly written. Experiments are well defined, and results are presented well. The paper is missing comparison against a V-Net architecture trained on patches used for learning global representation. The method uses a randomized scheme for extracting patches, self-enforcing and spatial ensembling.\nThe method is well organized and well written. Only weakness that I can think of is that improvement may be only in a context of 2.5D vs 3D. Methodological development is well motivated and clearly written. Experiments are well defined, and results are presented well. The paper is missing comparison against a V-Net architecture trained on patches used for learning global representation. The method uses a randomized scheme for extracting patches, self-enforcing and spatial ensembling."
    },
    {
        "coherence": 0.28661486335616165,
        "consistency": 0.3821883975429025,
        "fluency": 0.24371604190183588,
        "relevance": 0.09060392123577125,
        "overall": 0.2507808060091678,
        "Generated": "186",
        "Gold": "This paper investigates the regret analysis of a particular form of reward shaping. The shaping function is an approximation of the optimal state value function. This reward shaping is integrated into an extension of the UCBVI algorithm. The paper addresses a very relevant and useful question, and it provides a convincing and competent answer. The theoretical investigation in this paper is relevant and timely. Empirical evaluation on three domains is also provided, and the experiments confirm the merit of the contribution."
    },
    {
        "coherence": 0.2965626662273278,
        "consistency": 0.3820815253598696,
        "fluency": 0.1431270191333373,
        "relevance": 0.023462963490270855,
        "overall": 0.21130854355270143,
        "Generated": "187",
        "Gold": "The problem of testing the closeness of distributions asks to distinguish two cases for input distributions $P,Q$ The most natural setting for a testing algorithm is to only allow black-box sample access to $P$ and $Q$. However, it is known that this requires the sample complexity to be exponential in $n$. In this submission, the authors present an algorithm, Pacoco, that has sample complexity $tilde{O} ($n - 11.6 \\epsilon) + n / \\eta^2)\nThe problem of testing the closeness of distributions asks to distinguish two cases for input distributions $P,Q$ The most natural setting for a testing algorithm is to only allow black-box sample access to $P$ and $Q$. However, it is known that this requires the sample complexity to be exponential in $n$. In this submission, the authors present an algorithm, Pacoco, that has sample complexity $tilde{O} ($n - 11.6 \\epsilon) + n / \\eta^2)"
    },
    {
        "coherence": 0.23520134510819596,
        "consistency": 0.3090450447658084,
        "fluency": 0.13433261867252264,
        "relevance": 0.05371609544191723,
        "overall": 0.18307377599711108,
        "Generated": "188",
        "Gold": "This paper studies implicit (and continuous) neural representations (INRs) of data defined on discrete lattices. The paper introduces a novel signal processing framework named INSP-Net that directly operates on INRs without explicit decoding. Theoretically, the authors prove continuous convolution filter can be uniformly approximated by a linear combination of high-order differential operators. It is helpful for the community to be aware of the huge potential of INR of images.\nThis paper studies implicit (and continuous) neural representations (INRs) of data defined on discrete lattices. The paper introduces a novel signal processing framework named INSP-Net that directly operates on INRs without explicit decoding. Theoretically, the authors prove continuous convolution filter can be uniformly approximated by a linear combination of high-order differential operators. It is helpful for the community to be aware of the huge potential of INR of images."
    },
    {
        "coherence": 0.18438395962009124,
        "consistency": 0.2896119735553547,
        "fluency": 0.12116353529658729,
        "relevance": 0.018163703500502584,
        "overall": 0.15333079299313393,
        "Generated": "189",
        "Gold": "This paper proposes a novel VIO method which estimates 5 DoF poes (with the elevation fixed) from monocular camera and IMU. The proposed method adopt Unscented Kalman Filter (UKF) which is natually differentiable for the state estimation. This paper proposed an interpretable architecture for VIO, which admits differentiable training. Experimental results on two domains demonstrate that the proposed approach is effective and competitive against recent deep learning-based architectures.\nThis paper proposes a novel VIO method which estimates 5 DoF poes (with the elevation fixed) from monocular camera and IMU. The proposed method adopt Unscented Kalman Filter (UKF) which is natually differentiable for the state estimation. This paper proposed an interpretable architecture for VIO, which admits differentiable training. Experimental results on two domains demonstrate that the proposed approach is effective and competitive against recent deep learning-based architectures."
    },
    {
        "coherence": 0.2349704167776716,
        "consistency": 0.30219379811692787,
        "fluency": 0.14358855499240492,
        "relevance": 0.04802610796838622,
        "overall": 0.18219471946384766,
        "Generated": "190",
        "Gold": "The main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors. The main result from the theorem is that the error gap of norm constrained embeddeddings scales as O(d^-0.5(lnn)0. 5) The authors find that the term in the bound that represents the function complexity involves the norm of the learnt coordinates.\nThe main contribution of this paper is that it shows both theoretically and empirically that in linear graph embeddings, the generalization error is bounded by the norms of the embedding vectors. The main result from the theorem is that the error gap of norm constrained embeddeddings scales as O(d^-0.5(lnn)0. 5) The authors find that the term in the bound that represents the function complexity involves the norm of the learnt coordinates."
    },
    {
        "coherence": 0.19311691214385052,
        "consistency": 0.30811006124249074,
        "fluency": 0.06265204334543864,
        "relevance": 0.008599402226747889,
        "overall": 0.14311960473963195,
        "Generated": "191",
        "Gold": "This work considers a regularized IRL setup, where instead of the entropy regularization used in maximum entropy IRL, an arbitrary convex regularizer $Omega$ is used. The experimental results shown in Figure 3 is interesting, but I have a few concerns. In some cases, the averaged Bregman divergence of RAIRL-NSM was larger than that of Random. Some things are a bit unclear to me which is preventing me from providing a more detailed analysis of the work.\nThis work considers a regularized IRL setup, where instead of the entropy regularization used in maximum entropy IRL, an arbitrary convex regularizer $Omega$ is used. The experimental results shown in Figure 3 is interesting, but I have a few concerns. In some cases, the averaged Bregman divergence of RAIRL-NSM was larger than that of Random. Some things are a bit unclear to me which is preventing me from providing a more detailed analysis of the work."
    },
    {
        "coherence": 0.1745313062489351,
        "consistency": 0.18860762919131774,
        "fluency": 0.07966397736380489,
        "relevance": 0.01283335051191818,
        "overall": 0.11390906582899397,
        "Generated": "192",
        "Gold": "CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots. Below are multiple summaries of a paper's reviews. This week's gallery includes a paper on the convergence rate of the stochastic proximal point algorithm (SPPA) for convex, finite-sum functions.\nCNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots. Below are multiple summaries of a paper's reviews. This week's gallery includes a paper on the convergence rate of the stochastic proximal point algorithm (SPPA) for convex, finite-sum functions."
    },
    {
        "coherence": 0.1142841802811013,
        "consistency": 0.2439754842634631,
        "fluency": 0.0985378146890283,
        "relevance": 0.15566321046715692,
        "overall": 0.1531151724251874,
        "Generated": "193",
        "Gold": "There is substantial interest in encoding ordinal relationships between classes into the training process of neural networks. The approach is validated with almost 200 CT scans. The paper addresses a common challenge faced when training models with labels which are coarse and could benefit from domain knowledge to more correctly define them. The method seems overly complicated in it's presentation. The evaluations are not very convincing. The problem being solved does not seem to be particularly common or clinically relevant. It is always better to include a data table for clarity.\nThere is substantial interest in encoding ordinal relationships between classes into the training process of neural networks. The approach is validated with almost 200 CT scans. The paper addresses a common challenge faced when training models with labels which are coarse and could benefit from domain knowledge to more correctly define them. The method seems overly complicated in it's presentation. The evaluations are not very convincing. The problem being solved does not seem to be particularly common or clinically relevant. It is always better to include a data table for clarity."
    },
    {
        "coherence": 0.15804172649359569,
        "consistency": 0.2087000583724117,
        "fluency": 0.10069103069757744,
        "relevance": 0.021289378842715243,
        "overall": 0.12218054860157503,
        "Generated": "194",
        "Gold": "This paper presents a comprehensive analysis of spectral clustering based approaches for node classification in graph-based data. The main weaknesses of the paper are: (1) a lack of direction and (2) an insufficient distinction from previous works. The experimental results are convincing, though they could be made more complete. It is enlightening to observe such performance of from a spectral based methods. An interesting paper which could open further avenues of research at the intersection of spectral methods and neural-based approaches.\nThis paper presents a comprehensive analysis of spectral clustering based approaches for node classification in graph-based data. The main weaknesses of the paper are: (1) a lack of direction and (2) an insufficient distinction from previous works. The experimental results are convincing, though they could be made more complete. It is enlightening to observe such performance of from a spectral based methods. An interesting paper which could open further avenues of research at the intersection of spectral methods and neural-based approaches."
    },
    {
        "coherence": 0.14810045595077415,
        "consistency": 0.24614209161172565,
        "fluency": 0.10125543597617465,
        "relevance": 0.03252868775464991,
        "overall": 0.1320066678233311,
        "Generated": "195",
        "Gold": "This paper propose a point process with a non-stationary kernel to model complex event data. Authors compared their method against state-of-the-art and baseline on synthetic and real datasets. Theoretical results appears to have very few links with empirical results. Limitations of the proposed approach are not investigated. The paper could be a good paper with more complete work. But I think the authors missed an extremely important influential paper by Feng Chen and Peter Hall (2013)\nThis paper propose a point process with a non-stationary kernel to model complex event data. Authors compared their method against state-of-the-art and baseline on synthetic and real datasets. Theoretical results appears to have very few links with empirical results. Limitations of the proposed approach are not investigated. The paper could be a good paper with more complete work. But I think the authors missed an extremely important influential paper by Feng Chen and Peter Hall (2013)"
    },
    {
        "coherence": 0.29100373776057314,
        "consistency": 0.420742612153068,
        "fluency": 0.2753854985581076,
        "relevance": 0.03533949173370673,
        "overall": 0.2556178350513639,
        "Generated": "196",
        "Gold": "The paper builds on the idea of small loss criteria, which favors learning on certain samples in the beginning of the learning process. It proposes a novel type of distributional normalization based on Wasserstein distance. This process is done with a particle based stochastic dynamics. Empirical results on CIFAR-10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches. The clarity of the manuscript is not quite good.\nThe paper builds on the idea of small loss criteria, which favors learning on certain samples in the beginning of the learning process. It proposes a novel type of distributional normalization based on Wasserstein distance. This process is done with a particle based stochastic dynamics. Empirical results on CIFAR-10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches. The clarity of the manuscript is not quite good."
    },
    {
        "coherence": 0.16970138752115074,
        "consistency": 0.17188638657105101,
        "fluency": 0.31122331486420973,
        "relevance": 0.03091085015940843,
        "overall": 0.17093048477895498,
        "Generated": "197",
        "Gold": "The paper introduces a new intrinsic reward for MARL, representing the causal influence of an agent’s action on another agent counterfactually. This reward is related to maximising the mutual information between the agents’ actions. The behaviour of agents using this reward is tested in a set of social dilemmas, where it leads to increased cooperation and communication protocols. The paper focuses on cooperative environments which is an underfocused area in RL.\nThe paper introduces a new intrinsic reward for MARL, representing the causal influence of an agent’s action on another agent counterfactually. This reward is related to maximising the mutual information between the agents’ actions. The behaviour of agents using this reward is tested in a set of social dilemmas, where it leads to increased cooperation and communication protocols. The paper focuses on cooperative environments which is an underfocused area in RL."
    },
    {
        "coherence": 0.17240956924155074,
        "consistency": 0.25197609457549003,
        "fluency": 0.23750613528719164,
        "relevance": 0.029430741663423615,
        "overall": 0.17283063519191402,
        "Generated": "198",
        "Gold": "The paper introduces an extension of Hindsight Experience Replay (HER) called Hindsight Curriculum Generation (HCG) HCG is demonstrated to learn faster in multi-goal RL benchmarks. The paper seems to be rushed, making it hard to follow the ideas presented in the paper. The word usage and sentence structure is unnatural, and I find myself guessing at what exactly the authors mean. The related works section can be greatly improved. Theorem 1 is not trivial and there is indeed doubt in my mind.\nThe paper introduces an extension of Hindsight Experience Replay (HER) called Hindsight Curriculum Generation (HCG) HCG is demonstrated to learn faster in multi-goal RL benchmarks. The paper seems to be rushed, making it hard to follow the ideas presented in the paper. The word usage and sentence structure is unnatural, and I find myself guessing at what exactly the authors mean. The related works section can be greatly improved. Theorem 1 is not trivial and there is indeed doubt in my mind."
    },
    {
        "coherence": 0.29114495368582033,
        "consistency": 0.3788317610876854,
        "fluency": 0.19828846132840916,
        "relevance": 0.03361386937828562,
        "overall": 0.22546976137005012,
        "Generated": "199",
        "Gold": "The authors convert a differential to a contour integral in the complex plane and then interpret this integral as a time integral of an oscillating teaching signal. They show that this holomorphic EP surpasses EP in performance especially in the presence of noise. However, the authors do not provide any pointers as to how this may be implemented in biology except mentioning that there may be connections to theta neurons. It is not clear if the results here are actually useful for understanding neural computation.\nThe authors convert a differential to a contour integral in the complex plane and then interpret this integral as a time integral of an oscillating teaching signal. They show that this holomorphic EP surpasses EP in performance especially in the presence of noise. However, the authors do not provide any pointers as to how this may be implemented in biology except mentioning that there may be connections to theta neurons. It is not clear if the results here are actually useful for understanding neural computation."
    },
    {
        "coherence": 0.2933393918022073,
        "consistency": 0.35407383167501083,
        "fluency": 0.143868050978976,
        "relevance": 0.04642729023831427,
        "overall": 0.2094271411736271,
        "Generated": "200",
        "Gold": "The paper studies an online allocation setting where arriving items are allocated among $n$ agents. As opposed to the standard setting, the planner does not know the realized valuation at each period. The paper proposes an interesting mechanism to overcome the ignorance of the valuation distribution. However, this approach is hampered by its reliance on the iid assumption. This work is thus the first to propose an approximate BIC policy, with sublinear regret, when neither of these conditions hold.\nThe paper studies an online allocation setting where arriving items are allocated among $n$ agents. As opposed to the standard setting, the planner does not know the realized valuation at each period. The paper proposes an interesting mechanism to overcome the ignorance of the valuation distribution. However, this approach is hampered by its reliance on the iid assumption. This work is thus the first to propose an approximate BIC policy, with sublinear regret, when neither of these conditions hold."
    },
    {
        "coherence": 0.1328715324794839,
        "consistency": 0.2970238232190046,
        "fluency": 0.08115323285291166,
        "relevance": 0.02198916417000261,
        "overall": 0.13325943818035071,
        "Generated": "201",
        "Gold": "The paper proposes a knowledge distillation method for face recognition. The method inherits the teacher’s classifier as the student’S classifier and then optimizes the student model with advanced loss functions. Since optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart. The ProxylessKD adopts a shared classifier for two models. Shared classifier yields better aligned embedding space.\nThe paper proposes a knowledge distillation method for face recognition. The method inherits the teacher’s classifier as the student’S classifier and then optimizes the student model with advanced loss functions. Since optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart. The ProxylessKD adopts a shared classifier for two models. Shared classifier yields better aligned embedding space."
    },
    {
        "coherence": 0.1025080434457235,
        "consistency": 0.11548850101782199,
        "fluency": 0.06413782303069109,
        "relevance": 0.015656739007547958,
        "overall": 0.07444777662544613,
        "Generated": "202",
        "Gold": "NormGrad is a saliency detection method used in medical image analysis. The authors intended to demonstrate the superiority of normGrad for IQA in chest x-Rays versus other techniques. The paper is generally well written, the literature review is expansive and covers the state of the art well. The evaluation of hyperparameters as well as the comparison of NormGrad to other methods is sound and extensive. It would be great to see more examples in the appendix.\nNormGrad is a saliency detection method used in medical image analysis. The authors intended to demonstrate the superiority of normGrad for IQA in chest x-Rays versus other techniques. The paper is generally well written, the literature review is expansive and covers the state of the art well. The evaluation of hyperparameters as well as the comparison of NormGrad to other methods is sound and extensive. It would be great to see more examples in the appendix."
    },
    {
        "coherence": 0.11087917085647289,
        "consistency": 0.2016872356616098,
        "fluency": 0.08600866443700077,
        "relevance": 0.028816598623785588,
        "overall": 0.10684791739471726,
        "Generated": "203",
        "Gold": "The paper presents an interesting mathematical derivation that drives the subsequent approach based on reducing classifier discrepancy. The theoretical parts of the paper are very poorly written and peppered with unexplained notations. Despite those problems, the logical flow is good so the paper is acceptably readable. The proposed method, Reducing Classifier Discrepancy, has two training phases. Empirical results show the benefits of the proposed method on three few-shot datasets.\nThe paper presents an interesting mathematical derivation that drives the subsequent approach based on reducing classifier discrepancy. The theoretical parts of the paper are very poorly written and peppered with unexplained notations. Despite those problems, the logical flow is good so the paper is acceptably readable. The proposed method, Reducing Classifier Discrepancy, has two training phases. Empirical results show the benefits of the proposed method on three few-shot datasets."
    },
    {
        "coherence": 0.20604881637401656,
        "consistency": 0.32482214251547153,
        "fluency": 0.12723152797622672,
        "relevance": 0.027633877696587456,
        "overall": 0.17143409114057556,
        "Generated": "204",
        "Gold": "This paper proposes to reverse-engineer the trojan in the feature space of DNNs. The proposed method can be used to detect feature-based trojan or dynamic trojan that is input dependent. Results on four datasets and seven different attacks demonstrate that the proposed method effectively defends both input-space and feature-space Trojans. This paper provides a unique insight that Trojan features will form a hyperplane in the high dimensional space.\nThis paper proposes to reverse-engineer the trojan in the feature space of DNNs. The proposed method can be used to detect feature-based trojan or dynamic trojan that is input dependent. Results on four datasets and seven different attacks demonstrate that the proposed method effectively defends both input-space and feature-space Trojans. This paper provides a unique insight that Trojan features will form a hyperplane in the high dimensional space."
    },
    {
        "coherence": 0.258029895271507,
        "consistency": 0.43438407053145117,
        "fluency": 0.05785281957762582,
        "relevance": 0.03193669894750451,
        "overall": 0.19555087108202213,
        "Generated": "205",
        "Gold": "The paper considers the Hierarchical Reinforcement Learning setting, Options in particular. It proposes an algorithm that allows to learn both the high-level and low-level (option) policies at once. The paper is quite difficult to follow, and requires several attentive reads to be understood. The method is well-motivated in general, but the notation is a little bit unclear. It provides empirical results in a variety of domains, demonstrating that the algorithm can improve data efficiency.\nThe paper considers the Hierarchical Reinforcement Learning setting, Options in particular. It proposes an algorithm that allows to learn both the high-level and low-level (option) policies at once. The paper is quite difficult to follow, and requires several attentive reads to be understood. The method is well-motivated in general, but the notation is a little bit unclear. It provides empirical results in a variety of domains, demonstrating that the algorithm can improve data efficiency."
    },
    {
        "coherence": 0.20793224948677422,
        "consistency": 0.34481276690480284,
        "fluency": 0.08344748565438873,
        "relevance": 0.029646557060597396,
        "overall": 0.1664597647766408,
        "Generated": "206",
        "Gold": "Author proposes a variant of differentiable multilayer tree ensembles with annealing mechanism. They use a specific types of trees, called ferns, which are constrained to have the same boolean function for all nodes at the same level. Training is done by softening non-differentiable parts of f Ferns. They show that their performance is somewhat comparable to other commonly used methods like CatBoost, TabNet and NODE.\nAuthor proposes a variant of differentiable multilayer tree ensembles with annealing mechanism. They use a specific types of trees, called ferns, which are constrained to have the same boolean function for all nodes at the same level. Training is done by softening non-differentiable parts of f Ferns. They show that their performance is somewhat comparable to other commonly used methods like CatBoost, TabNet and NODE."
    },
    {
        "coherence": 0.17895408350484332,
        "consistency": 0.3723301926736777,
        "fluency": 0.07987865002545479,
        "relevance": 0.07558403003354613,
        "overall": 0.17668673905938048,
        "Generated": "207",
        "Gold": "The paper studies how the diversity of tasks in the training phase affects the performance of meta-learning algorithms. The paper finds negative evidence, which is consistent with Setlur et al. (2021) Compared with the existing work, the paper performs more extensive experiments with different algorithms, task samplers, and different datasets. The findings indicate that increasing task diversity during the meta-training process does not boost performance. It does not do a good job in explaining why some of the sampling methods work better than others.\nThe paper studies how the diversity of tasks in the training phase affects the performance of meta-learning algorithms. The paper finds negative evidence, which is consistent with Setlur et al. (2021) Compared with the existing work, the paper performs more extensive experiments with different algorithms, task samplers, and different datasets. The findings indicate that increasing task diversity during the meta-training process does not boost performance. It does not do a good job in explaining why some of the sampling methods work better than others."
    },
    {
        "coherence": 0.31236024066129703,
        "consistency": 0.3272172198043295,
        "fluency": 0.09321734097412997,
        "relevance": 0.034251162430639595,
        "overall": 0.19176149096759904,
        "Generated": "208",
        "Gold": "This paper suggests to model outliers using a CVAE. It then suggests simple scoring functions for incorporating these simulated outlier into a classifier. A numerical comparison shows this method is able to outperform CSI on a couple of datasets. In the majority of setups, CGA shows competitive performance against the conventional OOD detection methods. The paper is well written, covers the related works, and importantly, is well motivated. The authors proposed to use a Conditional VAE in order to generate pseudo OOD data, which can be used to improve OOD Detection.\nThis paper suggests to model outliers using a CVAE. It then suggests simple scoring functions for incorporating these simulated outlier into a classifier. A numerical comparison shows this method is able to outperform CSI on a couple of datasets. In the majority of setups, CGA shows competitive performance against the conventional OOD detection methods. The paper is well written, covers the related works, and importantly, is well motivated. The authors proposed to use a Conditional VAE in order to generate pseudo OOD data, which can be used to improve OOD Detection."
    },
    {
        "coherence": 0.2189751401521103,
        "consistency": 0.2193272525170757,
        "fluency": 0.18458054094136225,
        "relevance": 0.024472886076440735,
        "overall": 0.16183895492174724,
        "Generated": "209",
        "Gold": "Bayes Augmented with Memory (BAM) is a new approach to learning. It uses selection variables to allow an agent to adaptively choose past experiences to forget. The paper aims to solve the downside of the posterior shrinkage of Bayesian online learning when applied in a non-stationary environment. While the method seems technically sound, the method still falls short of the following points. An immediate improvement would be to use a fixed memory.\nBayes Augmented with Memory (BAM) is a new approach to learning. It uses selection variables to allow an agent to adaptively choose past experiences to forget. The paper aims to solve the downside of the posterior shrinkage of Bayesian online learning when applied in a non-stationary environment. While the method seems technically sound, the method still falls short of the following points. An immediate improvement would be to use a fixed memory."
    },
    {
        "coherence": 0.15905816429925598,
        "consistency": 0.170788578399459,
        "fluency": 0.06018835115816132,
        "relevance": 0.055855544886742346,
        "overall": 0.11147265968590465,
        "Generated": "210",
        "Gold": "This paper proposes a novel approach to improve self-attention through by bridging the attention maps from different layers via a chain of convolution-based prediction modules. The experiments on GLUE are also weak and could be a result of variance over the existing BERT model. This paper proposed a modification to the classical transformer architecture. It demonstrated significant performance gain on multiple benchmark tasks. The main contribution of this paper is the introduction of CNN-based attention prediction to enhance model predictions.\nThis paper proposes a novel approach to improve self-attention through by bridging the attention maps from different layers via a chain of convolution-based prediction modules. The experiments on GLUE are also weak and could be a result of variance over the existing BERT model. This paper proposed a modification to the classical transformer architecture. It demonstrated significant performance gain on multiple benchmark tasks. The main contribution of this paper is the introduction of CNN-based attention prediction to enhance model predictions."
    },
    {
        "coherence": 0.14908222644884989,
        "consistency": 0.3092015244662322,
        "fluency": 0.05085637162428369,
        "relevance": 0.03500612608482407,
        "overall": 0.13603656215604748,
        "Generated": "211",
        "Gold": "This paper describes extensions on top of RML to support Excel spreadsheets. It represents a solid piece of Java engineering work, although the chosen semantic representation approaches are not optimal. A demo page http://www.dfki.uni-kl.de/~mschroeder/demo/excel-rml/ is provided. The paper does not give many insights except for introducing those extensions. The authors have created the corresponding code to deal with this, and added some specific functions."
    },
    {
        "coherence": 0.23534863826238622,
        "consistency": 0.46858938543864725,
        "fluency": 0.06494779103778807,
        "relevance": 0.029554061215898805,
        "overall": 0.19960996898868008,
        "Generated": "212",
        "Gold": "This paper proposes a new family of monotone deep Boltzmann machines. The pairwise potentials satisfy a monotonicity condition, giving rise to efficient mean-field iteration with provable convergence guarantees. Small-scale experiments are done as proof of concept. The paper is very well-written and easy to read. The significance of this paper is not high, and evaluation is weak. I am happy to increase my score if the above my concerns are properly addressed."
    },
    {
        "coherence": 0.17586457777504383,
        "consistency": 0.3093038901708494,
        "fluency": 0.09449956935218051,
        "relevance": 0.02567779896641073,
        "overall": 0.15133645906612114,
        "Generated": "213",
        "Gold": "First Hitting Diffusion Models (FHDM) are a class of generative models that generalize the fixed time diffusion probabilistic models. Learning and inference are achieved by approximating the drift term in the corresponding Ito diffusion process with a neural network. Experiments show that FHDM is able to generate high-quality samples for general data distributions. The authors also propose a fast sampling algorithm for the rotational symmetric domain. The method seems hard to scale to large and complex images.\nFirst Hitting Diffusion Models (FHDM) are a class of generative models that generalize the fixed time diffusion probabilistic models. Learning and inference are achieved by approximating the drift term in the corresponding Ito diffusion process with a neural network. Experiments show that FHDM is able to generate high-quality samples for general data distributions. The authors also propose a fast sampling algorithm for the rotational symmetric domain. The method seems hard to scale to large and complex images."
    },
    {
        "coherence": 0.2038311749040883,
        "consistency": 0.22529145366872255,
        "fluency": 0.08863161660376438,
        "relevance": 0.032049351134418126,
        "overall": 0.13745089907774832,
        "Generated": "214",
        "Gold": "This paper proposes reducing slip during dynamic motions by adapting the trajectory, specifically decreasing the velocity. This is in contrast to previous papers which normally focus on moderating gripping force. Reactive Slip Control (RSC) outperforms PSC, since it can plan to prevent slip before it occurs. The key idea is to learn a slip prediction model that takes tactile sensor data and planned future actions as input and outputs an estimation of whether the object is going to slip.\nThis paper proposes reducing slip during dynamic motions by adapting the trajectory, specifically decreasing the velocity. This is in contrast to previous papers which normally focus on moderating gripping force. Reactive Slip Control (RSC) outperforms PSC, since it can plan to prevent slip before it occurs. The key idea is to learn a slip prediction model that takes tactile sensor data and planned future actions as input and outputs an estimation of whether the object is going to slip."
    },
    {
        "coherence": 0.07864179260176632,
        "consistency": 0.21402941088638758,
        "fluency": 0.15751015366097376,
        "relevance": 0.013281340435135878,
        "overall": 0.11586567439606589,
        "Generated": "215",
        "Gold": "AlignKGC is a neural model that performs knowledge graph completion (KGC) on multilingual knowledge graphs that have entity and relation overlap. The paper is extremely clear, and results on DBP5L show significant improvements over the baselines. Authors propose a system for jointly learning to complete multiple monolingual KGs, and aligning their entities and relations. It proposes a model which jointly optimizes the three tasks using a linear combination of their losses. KGC is evaluated on each of 5 languages for which a KG is available.\nAlignKGC is a neural model that performs knowledge graph completion (KGC) on multilingual knowledge graphs that have entity and relation overlap. The paper is extremely clear, and results on DBP5L show significant improvements over the baselines. Authors propose a system for jointly learning to complete multiple monolingual KGs, and aligning their entities and relations. It proposes a model which jointly optimizes the three tasks using a linear combination of their losses. KGC is evaluated on each of 5 languages for which a KG is available."
    },
    {
        "coherence": 0.17765322189983848,
        "consistency": 0.27761543267383415,
        "fluency": 0.12379246361004935,
        "relevance": 0.050945166744641036,
        "overall": 0.15750157123209074,
        "Generated": "216",
        "Gold": "The paper study the effects of adding random features (RF) to graph neural networks (GNN) Theoretical aspect seems novel and quite surprising, especially since it shows that adding RF makes GNN more expressive than k-GCN (or k-IGN) for any k. The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability. The authors design two datasets wich require 2-WL distinguishing power (which is higher than the ones MPNNs have)\nThe paper study the effects of adding random features (RF) to graph neural networks (GNN) Theoretical aspect seems novel and quite surprising, especially since it shows that adding RF makes GNN more expressive than k-GCN (or k-IGN) for any k. The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability. The authors design two datasets wich require 2-WL distinguishing power (which is higher than the ones MPNNs have)"
    },
    {
        "coherence": 0.30976634503483796,
        "consistency": 0.37554941217364185,
        "fluency": 0.10461074739528946,
        "relevance": 0.04647770958838984,
        "overall": 0.20910105354803976,
        "Generated": "217",
        "Gold": "This paper examines the impact of forcing units in a CNN to be more or less “class-selective” The approach taken is to include a regularizer in the loss that directly penalizes or encourages class selectivity in individual units. The authors report that penalizing classSelectivity at intermediate layers has little-to-no effect on classification performance, and in some cases mildly improves performance. They observe that discouraging selectivity can have a small benefit and generally doesn't harm performance even at very high values.\nThis paper examines the impact of forcing units in a CNN to be more or less “class-selective” The approach taken is to include a regularizer in the loss that directly penalizes or encourages class selectivity in individual units. The authors report that penalizing classSelectivity at intermediate layers has little-to-no effect on classification performance, and in some cases mildly improves performance. They observe that discouraging selectivity can have a small benefit and generally doesn't harm performance even at very high values."
    },
    {
        "coherence": 0.22451988564552194,
        "consistency": 0.36686165263227477,
        "fluency": 0.2039803563100307,
        "relevance": 0.02541431446618063,
        "overall": 0.20519405226350199,
        "Generated": "218",
        "Gold": "The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups. The potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth) The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits. The paper provided a detailed analysis of time consumption on only a success-case.\nThe authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups. The potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth) The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits. The paper provided a detailed analysis of time consumption on only a success-case."
    },
    {
        "coherence": 0.23751804396040674,
        "consistency": 0.3398314879173957,
        "fluency": 0.1812280496821821,
        "relevance": 0.027610212242109192,
        "overall": 0.19654694845052345,
        "Generated": "219",
        "Gold": "This paper describes a few-shot learning approach that takes into account the semantic relatedness of the ground truth labels. In principal adding textual data that is semantically less ambiguous than image data helps. Experimental results are provided on four benchmark datasets that demonstrate that the proposed approach outperforms existing approaches. The effect of using a pre-trained feature extractor is unclear. The explanation of the proposed method seems unnecessarily redundant. The experiments are unconvincing.\nThis paper describes a few-shot learning approach that takes into account the semantic relatedness of the ground truth labels. In principal adding textual data that is semantically less ambiguous than image data helps. Experimental results are provided on four benchmark datasets that demonstrate that the proposed approach outperforms existing approaches. The effect of using a pre-trained feature extractor is unclear. The explanation of the proposed method seems unnecessarily redundant. The experiments are unconvincing."
    },
    {
        "coherence": 0.07431651727019088,
        "consistency": 0.16803648941171323,
        "fluency": 0.32813080222748314,
        "relevance": 0.013216601994854135,
        "overall": 0.14592510272606035,
        "Generated": "220",
        "Gold": "XMixup is a strategy for improving transfer learning in neural networks. It consists of mixup applied between target samples and source samples from the class pre-determined to be closest to target sample’s class. Experiments conducting transfer learning from pre-trained ImageNet to 6 smaller image classification datasets demonstrate XMixup to outperform the baseline approaches. I vote for rejecting the paper due to limited novelty and lack of convincing experiments.\nXMixup is a strategy for improving transfer learning in neural networks. It consists of mixup applied between target samples and source samples from the class pre-determined to be closest to target sample’s class. Experiments conducting transfer learning from pre-trained ImageNet to 6 smaller image classification datasets demonstrate XMixup to outperform the baseline approaches. I vote for rejecting the paper due to limited novelty and lack of convincing experiments."
    },
    {
        "coherence": 0.06674074199081047,
        "consistency": 0.12094787291454642,
        "fluency": 0.28238180632792587,
        "relevance": 0.04140698911801468,
        "overall": 0.12786935258782436,
        "Generated": "221",
        "Gold": "The authors propose an adversarial attack strategy for graph neural networks based on influence maximization. The attack is (claimed to be) black-box (does not have direct access to the model), evasion-based, and limited to perturbations of the node attributes. Experiments show such attacks are effective compared to baselines. The paper is well-written and generally easy to follow. The entire analysis is based on Assumption 1, but the paper does not provide a formal description of the data flow in the network."
    },
    {
        "coherence": 0.155926526308803,
        "consistency": 0.3187338648727997,
        "fluency": 0.1082233596091179,
        "relevance": 0.021848397861960116,
        "overall": 0.1511830371631702,
        "Generated": "222",
        "Gold": "A new framework leverages information generated from counterfactual predictions of optimal joint action selection. The proposed method overperforms many baselines including QMIX. Experiments based on StarCraft II show the improvement of the proposed method. This paper aims to improve the joint-action Q-values learnt in a cooperative MARL setting. The authors aim to do so by first identifying the shortcomings of existing methods. To remedy the errors they allow for agents to communicate and explicitly encourage them to send messages.\nA new framework leverages information generated from counterfactual predictions of optimal joint action selection. The proposed method overperforms many baselines including QMIX. Experiments based on StarCraft II show the improvement of the proposed method. This paper aims to improve the joint-action Q-values learnt in a cooperative MARL setting. The authors aim to do so by first identifying the shortcomings of existing methods. To remedy the errors they allow for agents to communicate and explicitly encourage them to send messages."
    },
    {
        "coherence": 0.060069466226239564,
        "consistency": 0.0926194687091333,
        "fluency": 0.21980450846800503,
        "relevance": 0.028273337562073238,
        "overall": 0.10019169524136277,
        "Generated": "223",
        "Gold": "This paper tries to prove that there is a bottleneck in feature learning for long-tailed classification. Data augmentation can help relieve the issues in long-tail feature space. Only positive effects from data augmentation were shown, the reasons and mechanisms were not fully discussed. The experiments would have been more persuasive if the balanced training set is a variant of the long- tail training set. This work shows the effectiveness of intra-class compactness and inter-class separation on long-tails representation learning.\nThis paper tries to prove that there is a bottleneck in feature learning for long-tailed classification. Data augmentation can help relieve the issues in long-tail feature space. Only positive effects from data augmentation were shown, the reasons and mechanisms were not fully discussed. The experiments would have been more persuasive if the balanced training set is a variant of the long- tail training set. This work shows the effectiveness of intra-class compactness and inter-class separation on long-tails representation learning."
    },
    {
        "coherence": 0.22357173792871765,
        "consistency": 0.36583261180267984,
        "fluency": 0.1031818333371518,
        "relevance": 0.019907932367506095,
        "overall": 0.17812352885901384,
        "Generated": "224",
        "Gold": "This paper investigates the condensation of weights of neural networks during the initial training stage. It showed theoretically and empirically that the maximal number of condensed orientations is twice the multiplicity of the activation function. This condensation restricts the capacity of NNs at the beginning, working as implicit regularization. The paper presents several typos and unclear sentences in the current form. Broadly, this work is intriguing, but could stand to benefit from a few improvements, suggested below.\nThis paper investigates the condensation of weights of neural networks during the initial training stage. It showed theoretically and empirically that the maximal number of condensed orientations is twice the multiplicity of the activation function. This condensation restricts the capacity of NNs at the beginning, working as implicit regularization. The paper presents several typos and unclear sentences in the current form. Broadly, this work is intriguing, but could stand to benefit from a few improvements, suggested below."
    },
    {
        "coherence": 0.11236170543200764,
        "consistency": 0.25426260867937706,
        "fluency": 0.06194939862039045,
        "relevance": 0.03224127768906624,
        "overall": 0.11520374760521035,
        "Generated": "225",
        "Gold": "This is one of the first papers doing BERT-style pretraining for vision transformers. This approach is inspired by the contrastive self-supervised learning approach like DINO and the mask modelling approach like BeiT. The paper is quite well written, the method is simple and gives better performance than DINO. Nevertheless, the comparison between the different approaches does not seem to be complete enough. For more information on the iBOT method click here.\nThis is one of the first papers doing BERT-style pretraining for vision transformers. This approach is inspired by the contrastive self-supervised learning approach like DINO and the mask modelling approach like BeiT. The paper is quite well written, the method is simple and gives better performance than DINO. Nevertheless, the comparison between the different approaches does not seem to be complete enough. For more information on the iBOT method click here."
    },
    {
        "coherence": 0.09307173337762079,
        "consistency": 0.25528171034199443,
        "fluency": 0.36102011101807696,
        "relevance": 0.03332633205759263,
        "overall": 0.1856749716988212,
        "Generated": "226",
        "Gold": "Using O(n polylog ) images one can recover all private images in the InstaHide challenge. However, the running time of the algorithm is exponential in the number of private images. The authors provide a solid solution for a problem with limited scope. In the current form, I believe the value for the general readers are limited, therefore I would recommend for weak reject. The paper makes reasonable improvements to the sample complexity of the challenge.\nUsing O(n polylog ) images one can recover all private images in the InstaHide challenge. However, the running time of the algorithm is exponential in the number of private images. The authors provide a solid solution for a problem with limited scope. In the current form, I believe the value for the general readers are limited, therefore I would recommend for weak reject. The paper makes reasonable improvements to the sample complexity of the challenge."
    },
    {
        "coherence": 0.12362316470048546,
        "consistency": 0.15948167858027318,
        "fluency": 0.48872442849061226,
        "relevance": 0.10983051030315626,
        "overall": 0.22041494551863178,
        "Generated": "227",
        "Gold": "This paper provides tools for classifying the payoff dynamics in general-sum n-player games as Lyapunov chaotic. The main text chops on Theorem 11 without any discussion on this theorem after. The paper could benefit from additional discussion and explanation of the claims (see below)This paper studies the chaos phenomena of learning in general normal-form games beyond zero-sum and coordination games. It appears to extend earlier work by Cheung and Piliouras to more general- sum settings."
    },
    {
        "coherence": 0.2776237941742698,
        "consistency": 0.4379736030872446,
        "fluency": 0.3823579383918707,
        "relevance": 0.029824873378040965,
        "overall": 0.28194505225785654,
        "Generated": "228",
        "Gold": "MemUP is a method to learn long-term dependencies for recurrent models. It learns to keep useful past states for future use, so it saves both computation and memory. Empirical experiments on supervisied learning and reinforcement learning tasks show that MemUP significantly boost the results over its baselines. The idea behind the method is clear and interesting, but could have make a better job exploring it further. The authors leverage a memory model to predict future outcomes with high uncertainty. By skipping all states with lower uncertainty, the training process takes less computational cost in backpropagation.\nMemUP is a method to learn long-term dependencies for recurrent models. It learns to keep useful past states for future use, so it saves both computation and memory. Empirical experiments on supervisied learning and reinforcement learning tasks show that MemUP significantly boost the results over its baselines. The idea behind the method is clear and interesting, but could have make a better job exploring it further. The authors leverage a memory model to predict future outcomes with high uncertainty. By skipping all states with lower uncertainty, the training process takes less computational cost in backpropagation."
    },
    {
        "coherence": 0.2500659699060399,
        "consistency": 0.3938530422436038,
        "fluency": 0.2911584673940491,
        "relevance": 0.0362308920641792,
        "overall": 0.24282709290196797,
        "Generated": "229",
        "Gold": "A new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown. How these benefits touch and sight association tasks are unclear.\nA new dataset for multimodal visuo-tactile learning called Touch and Go is proposed. The data are collected by human in an “approximately egocentric” manner, which is thought to have higher diversity than those data collected by robots. The benefits of human-collected tactile data are outlined in the paper whereas the advantage of robotic data collecting remains unknown. How these benefits touch and sight association tasks are unclear."
    },
    {
        "coherence": 0.11977410722533484,
        "consistency": 0.18574145828682495,
        "fluency": 0.3168019479576048,
        "relevance": 0.028090889516444485,
        "overall": 0.16260210074655226,
        "Generated": "230",
        "Gold": "This paper proposes a method VAEL, which combines VAE and ProbLog. The main idea is to interpret a subset of the latent representation z of VAE as symbolic representation of the input. A neural network maps z_sym to probabilities of facts in a ProbLog program. VAEL is tested on two image generation tasks and is shown to have better generalization. This work propose to unify unify VAE with probabilistic logic programming, which is an original problem to investigate.\nThis paper proposes a method VAEL, which combines VAE and ProbLog. The main idea is to interpret a subset of the latent representation z of VAE as symbolic representation of the input. A neural network maps z_sym to probabilities of facts in a ProbLog program. VAEL is tested on two image generation tasks and is shown to have better generalization. This work propose to unify unify VAE with probabilistic logic programming, which is an original problem to investigate."
    },
    {
        "coherence": 0.2280266145964227,
        "consistency": 0.36532509354654114,
        "fluency": 0.1178478138306183,
        "relevance": 0.021567172784881652,
        "overall": 0.18319167368961592,
        "Generated": "231",
        "Gold": "This paper proposes a new sequential recommendation model with behavior pathways. The pathway attention using learned binary routes can effectively remove unnecessary items for a given user sequence. Experimental results show that the proposed model significantly outperforms existing models on seven benchmark datasets. It is characterized by its ability to capture three types of user pathways and predict user action sequences with high accuracy. The paper is generally easy to follow and the idea of using pathway in recommendation algorithms seems to be new.\nThis paper proposes a new sequential recommendation model with behavior pathways. The pathway attention using learned binary routes can effectively remove unnecessary items for a given user sequence. Experimental results show that the proposed model significantly outperforms existing models on seven benchmark datasets. It is characterized by its ability to capture three types of user pathways and predict user action sequences with high accuracy. The paper is generally easy to follow and the idea of using pathway in recommendation algorithms seems to be new."
    },
    {
        "coherence": 0.18837369425461226,
        "consistency": 0.2904023030178724,
        "fluency": 0.07826929296366082,
        "relevance": 0.02512642504343173,
        "overall": 0.1455429288198943,
        "Generated": "232",
        "Gold": "Recursive Gradient Optimization (RGO) modifies the gradient direction at each update by multiplying it with a projection matrix P. RGO is theoretically designed to prioritise performance on the current task and, among the optimal solutions, find the one that causes least interference with previous tasks. Experiments are run on a number of standard continual learning image classification benchmarks, demonstrating an extremely strong performance of RGO. The experiments seem comprehensive in range (many benchmarks, many architectures)\nRecursive Gradient Optimization (RGO) modifies the gradient direction at each update by multiplying it with a projection matrix P. RGO is theoretically designed to prioritise performance on the current task and, among the optimal solutions, find the one that causes least interference with previous tasks. Experiments are run on a number of standard continual learning image classification benchmarks, demonstrating an extremely strong performance of RGO. The experiments seem comprehensive in range (many benchmarks, many architectures)"
    },
    {
        "coherence": 0.2523251038495721,
        "consistency": 0.48341045203169475,
        "fluency": 0.09603954769683236,
        "relevance": 0.025135661139239898,
        "overall": 0.21422769117933477,
        "Generated": "233",
        "Gold": "This paper studies the problem of distribution shifts as out-of-distribution generalization. It formulates the OOD problem as invariant risk minimization under different environments. The main idea is to decompose the graph into a set of ego-graphs rooted at each node. In empirical experiments on multiple datasets, the approach (EERM) outperforms its counterpart baseline ERM. Please see the weakness and questions section for more information.\nThis paper studies the problem of distribution shifts as out-of-distribution generalization. It formulates the OOD problem as invariant risk minimization under different environments. The main idea is to decompose the graph into a set of ego-graphs rooted at each node. In empirical experiments on multiple datasets, the approach (EERM) outperforms its counterpart baseline ERM. Please see the weakness and questions section for more information."
    },
    {
        "coherence": 0.20937809294887605,
        "consistency": 0.4766902743042256,
        "fluency": 0.18211185059990773,
        "relevance": 0.052991749222922754,
        "overall": 0.23029299176898302,
        "Generated": "234",
        "Gold": "The paper argues that the existing way of using Translation Memory (TM) in neural machine translation (NMT) is sub-optimal. It proposes TMG-NMT, namely Translation Memory Guided NMT, which consists of two parts, a universal memory encoder and a TM guided decoder. The authors should conduct experiments beyond English-to-French. More languages pairs should be verified. The model and the experiments are not adequately described."
    },
    {
        "coherence": 0.23947300488437892,
        "consistency": 0.36170326779410145,
        "fluency": 0.2010545160842224,
        "relevance": 0.034431503682807035,
        "overall": 0.20916557311137746,
        "Generated": "235",
        "Gold": "The proposed approach is only marginally better than MLE-based model fitting. I would have liked to see an evaluation on more datasets. The claim that that the proposed approach constitutes the first time a scoring rule other than maximum likelihood seems too strong, unnecessary and irrelevant to the value of the presented work. It is not clear how did the authors handle the irregularity (in time) of EHR encounters in the context of an RNN specification.\nThe proposed approach is only marginally better than MLE-based model fitting. I would have liked to see an evaluation on more datasets. The claim that that the proposed approach constitutes the first time a scoring rule other than maximum likelihood seems too strong, unnecessary and irrelevant to the value of the presented work. It is not clear how did the authors handle the irregularity (in time) of EHR encounters in the context of an RNN specification."
    },
    {
        "coherence": 0.08597901832643044,
        "consistency": 0.16392240526835244,
        "fluency": 0.36918105091280745,
        "relevance": 0.019580215614419152,
        "overall": 0.15966567253050237,
        "Generated": "236",
        "Gold": "The authors introduce two datasets, Video Cognitive Empathy (VCE) and Video to Valence (V2V) The datasets contain over 60,000 short videos that were annotated with emotional content and general emotional valence by crowdworkers. The authors train machine learning models on the datasets and demonstrate that some models reach good performance. Authors train standard CNNs on these datasets, and observed that while these networks perform well, they do leave a significant performance gap to be fulfilled.\nThe authors introduce two datasets, Video Cognitive Empathy (VCE) and Video to Valence (V2V) The datasets contain over 60,000 short videos that were annotated with emotional content and general emotional valence by crowdworkers. The authors train machine learning models on the datasets and demonstrate that some models reach good performance. Authors train standard CNNs on these datasets, and observed that while these networks perform well, they do leave a significant performance gap to be fulfilled."
    },
    {
        "coherence": 0.0965522851562986,
        "consistency": 0.2418443190707488,
        "fluency": 0.2640949501834837,
        "relevance": 0.029126347198055978,
        "overall": 0.15790447540214678,
        "Generated": "237",
        "Gold": "This paper considers the problem of estimating the intensity function of a multi-dimensional Poisson process. Inspired by the generalized additive model, the authors propose a certain non-parametric form for the log-likelihood. Experiments on synthetic data as well as real data (New York taxis) are included. This approach is interesting but it would deserve more works to further develop the idea. The paper is not motivated and clear enough. The performance of the proposed method is extreme through synthetic examples and a real data analysis.\nThis paper considers the problem of estimating the intensity function of a multi-dimensional Poisson process. Inspired by the generalized additive model, the authors propose a certain non-parametric form for the log-likelihood. Experiments on synthetic data as well as real data (New York taxis) are included. This approach is interesting but it would deserve more works to further develop the idea. The paper is not motivated and clear enough. The performance of the proposed method is extreme through synthetic examples and a real data analysis."
    },
    {
        "coherence": 0.14677846875888279,
        "consistency": 0.25414501423001074,
        "fluency": 0.31849279499113503,
        "relevance": 0.03745585562626714,
        "overall": 0.18921803340157395,
        "Generated": "238",
        "Gold": "The authors proposed an improved deep-learning-based representation learning method. I was deeply impressed by the far above state-of-the-art values of evaluation metric. The evaluation is fairly detailed. The authors used datasets that are somewhat larger than often used in the literature. The inclusion of the study of the temperature parameter also helped clarify a few questions I had when reading it. The paper is well qritten and experimental results are good.\nThe authors proposed an improved deep-learning-based representation learning method. I was deeply impressed by the far above state-of-the-art values of evaluation metric. The evaluation is fairly detailed. The authors used datasets that are somewhat larger than often used in the literature. The inclusion of the study of the temperature parameter also helped clarify a few questions I had when reading it. The paper is well qritten and experimental results are good."
    },
    {
        "coherence": 0.08196934135750146,
        "consistency": 0.11220998387615985,
        "fluency": 0.23474416839360904,
        "relevance": 0.015255367277174796,
        "overall": 0.11104471522611128,
        "Generated": "239",
        "Gold": "This is a new type of attack that uses a warp-based trigger instead of patched triggers. The effect is to distort the global structure of the image, making the attack undetectable by humans. This is the first time this technique has been used in a real-world attack. It is intended to make it harder for humans to detect and stop the attack. This method is also known as a “noise’ mode”.\nThis is a new type of attack that uses a warp-based trigger instead of patched triggers. The effect is to distort the global structure of the image, making the attack undetectable by humans. This is the first time this technique has been used in a real-world attack. It is intended to make it harder for humans to detect and stop the attack. This method is also known as a “noise’ mode”."
    },
    {
        "coherence": 0.28339885547167776,
        "consistency": 0.2878914971270781,
        "fluency": 0.09275447979957398,
        "relevance": 0.05075582770002322,
        "overall": 0.17870016502458824,
        "Generated": "240",
        "Gold": "The paper proposes matching problem-independent (worst-case) upper and lower bounds for the tabular version of the problem described above. The observation that variance-aware algorithms lead to bounds that are order-wise better in this setting is an interesting one. The paper's main contribution is to find out that the hardest problem instances (where means are small) yield small variance, meaning that the estimation for these instances is naturally more accurate. This work studied the regret minimization in cascading bandits.\nThe paper proposes matching problem-independent (worst-case) upper and lower bounds for the tabular version of the problem described above. The observation that variance-aware algorithms lead to bounds that are order-wise better in this setting is an interesting one. The paper's main contribution is to find out that the hardest problem instances (where means are small) yield small variance, meaning that the estimation for these instances is naturally more accurate. This work studied the regret minimization in cascading bandits."
    },
    {
        "coherence": 0.12489613122200469,
        "consistency": 0.31931613362426464,
        "fluency": 0.04484249542457557,
        "relevance": 0.013305157706126317,
        "overall": 0.1255899794942428,
        "Generated": "241",
        "Gold": "This paper presents a unified sequence-to-sequence interface for 4 core vision tasks: object detection, instance segmentation, keypoint detection and image captioning. The model is designed with a single encoder-decoder architecture, with no task-specific heads. In my opinion, the results are promising as it could initiate a new way of unifying different vision tasks. The proposed method achieves state-of-the-art performance on 4 tasks.\nThis paper presents a unified sequence-to-sequence interface for 4 core vision tasks: object detection, instance segmentation, keypoint detection and image captioning. The model is designed with a single encoder-decoder architecture, with no task-specific heads. In my opinion, the results are promising as it could initiate a new way of unifying different vision tasks. The proposed method achieves state-of-the-art performance on 4 tasks."
    },
    {
        "coherence": 0.06849110489392757,
        "consistency": 0.1277832031577425,
        "fluency": 0.12915888668229994,
        "relevance": 0.025157692315092836,
        "overall": 0.08764772176226572,
        "Generated": "242",
        "Gold": "The paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. Unlike in dropout masks, the masks are kept fixed throughout training/inference. This shifts the focus of the network away from texture and towards shape features. The authors argue that by doing so, the CNN is encouraged to learn less from object texture and more on features such as shape.\nThe paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. Unlike in dropout masks, the masks are kept fixed throughout training/inference. This shifts the focus of the network away from texture and towards shape features. The authors argue that by doing so, the CNN is encouraged to learn less from object texture and more on features such as shape."
    },
    {
        "coherence": 0.0667495911920391,
        "consistency": 0.16699088445465152,
        "fluency": 0.08428972298544089,
        "relevance": 0.020713089442060136,
        "overall": 0.08468582201854791,
        "Generated": "243",
        "Gold": "The paper proposes to combine Normalizing Flows with generative Diffusion Models. The target data is first non-linearly transformed via the flow and then the distribution over latent embeddings is modeled with the diffusion model. Since the flow is invertible and defined by a deterministic function, it is possible to formally relate the combined flow and diffusion process to a non-linear diffusion directly in data space. The structure of PDM is very similar to LSGM mentioned in the related work.\nThe paper proposes to combine Normalizing Flows with generative Diffusion Models. The target data is first non-linearly transformed via the flow and then the distribution over latent embeddings is modeled with the diffusion model. Since the flow is invertible and defined by a deterministic function, it is possible to formally relate the combined flow and diffusion process to a non-linear diffusion directly in data space. The structure of PDM is very similar to LSGM mentioned in the related work."
    },
    {
        "coherence": 0.09043339305034745,
        "consistency": 0.22885732542239928,
        "fluency": 0.15961973454518166,
        "relevance": 0.01569839379323792,
        "overall": 0.12365221170279156,
        "Generated": "244",
        "Gold": "The paper describes how to constrain realisations of a Gaussian process to satisfy $A f = 0$, where $A$ is a linear, matrix-valued differential operator with constant coefficients. The Smith normal form also restricts the class of problems that can be tackled to linear, homogenous, ordinary differential equations with constant coefficient. The resulting LODE-GPs are compared to classic GPs in three experiments. The paper is very well-written and the method is presented in a clear way.\nThe paper describes how to constrain realisations of a Gaussian process to satisfy $A f = 0$, where $A$ is a linear, matrix-valued differential operator with constant coefficients. The Smith normal form also restricts the class of problems that can be tackled to linear, homogenous, ordinary differential equations with constant coefficient. The resulting LODE-GPs are compared to classic GPs in three experiments. The paper is very well-written and the method is presented in a clear way."
    },
    {
        "coherence": 0.20997909014635646,
        "consistency": 0.27661977113288283,
        "fluency": 0.25072162142693716,
        "relevance": 0.031664336337709945,
        "overall": 0.1922462047609716,
        "Generated": "245",
        "Gold": "The paper describes a novel implementation of RED, regularization by denoising. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy. Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in terms of running time. However, the novelty of the methods look incremental. I would appreciate a discussion on whether this is unavoidable or if this is an open question for future work.\nThe paper describes a novel implementation of RED, regularization by denoising. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy. Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in terms of running time. However, the novelty of the methods look incremental. I would appreciate a discussion on whether this is unavoidable or if this is an open question for future work."
    },
    {
        "coherence": 0.24354497061905836,
        "consistency": 0.374748945938028,
        "fluency": 0.2356487695701103,
        "relevance": 0.05303900807257649,
        "overall": 0.22674542354994326,
        "Generated": "246",
        "Gold": "Authors study the problem of large-batch optimization for various \"dense\" computer vision tasks, such as object detection or instance segmentation. They propose Adaptive Gradient Variance Modulator (AGVM) - an optimizer-agnostic technique. While i have many low-level concerns about the paper positioning and quality, I am strongly in favor of paper acceptance. The paper clearly claims the motivation, which is the high gradient variance in the different modules.\nAuthors study the problem of large-batch optimization for various \"dense\" computer vision tasks, such as object detection or instance segmentation. They propose Adaptive Gradient Variance Modulator (AGVM) - an optimizer-agnostic technique. While i have many low-level concerns about the paper positioning and quality, I am strongly in favor of paper acceptance. The paper clearly claims the motivation, which is the high gradient variance in the different modules."
    },
    {
        "coherence": 0.11770349216681002,
        "consistency": 0.2640885928117853,
        "fluency": 0.19735393398022663,
        "relevance": 0.01431096237795091,
        "overall": 0.14836424533419323,
        "Generated": "247",
        "Gold": "The new revision of the paper has clarified several issues. The addition of past work was not well integrated. The authors do not really tackle the difficult issue of distance in the users' reaction metrics to the different effects and magnitudes. The choice of visualizations in study 2 was based on their resemblance of scatterplots. They used 16 visualizations. Which ones??? I'd like to know exactly which forms were used. It would be good to further add the number of trials seen by participants.\nThe new revision of the paper has clarified several issues. The addition of past work was not well integrated. The authors do not really tackle the difficult issue of distance in the users' reaction metrics to the different effects and magnitudes. The choice of visualizations in study 2 was based on their resemblance of scatterplots. They used 16 visualizations. Which ones??? I'd like to know exactly which forms were used. It would be good to further add the number of trials seen by participants."
    },
    {
        "coherence": 0.1137722421835283,
        "consistency": 0.3536529771816484,
        "fluency": 0.27825028906449145,
        "relevance": 0.009194085679151089,
        "overall": 0.18871739852720482,
        "Generated": "248",
        "Gold": "Video Text Visual Question Answering (ViteVQA) extends the previous text-based visual question answering task into the video domain. The proposed task aims at answering questions by reasoning texts and visual information spatiotemporally in a video. According to the statistic and analysis results in Page 5, the lengths of the majority questions and answers are under 14 and 10. It seems that the questions as demonstrated in Figure 2 is very simple with a very simple format, namely “waht is XXXX”\nVideo Text Visual Question Answering (ViteVQA) extends the previous text-based visual question answering task into the video domain. The proposed task aims at answering questions by reasoning texts and visual information spatiotemporally in a video. According to the statistic and analysis results in Page 5, the lengths of the majority questions and answers are under 14 and 10. It seems that the questions as demonstrated in Figure 2 is very simple with a very simple format, namely “waht is XXXX”"
    },
    {
        "coherence": 0.18775140531560996,
        "consistency": 0.31884036892800527,
        "fluency": 0.18771786612129315,
        "relevance": 0.05005269122952166,
        "overall": 0.18609058289860753,
        "Generated": "249",
        "Gold": "This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. The gains are unfortunately much weaker than I would have hoped for.\nThis paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. The gains are unfortunately much weaker than I would have hoped for."
    },
    {
        "coherence": 0.2307483312809796,
        "consistency": 0.27070630075243185,
        "fluency": 0.26145283323867985,
        "relevance": 0.03427760590150595,
        "overall": 0.19929626779339932,
        "Generated": "250",
        "Gold": "This paper targets at weakly supervised semantic segmentation for both 2D and 3D domain. The paper doesn't demonstrate if the learned model is practical and can generalize to images or scenes outside the chosen dataset. The proposed method is effective and improves existing methods by a large margin. The authors introduce new loss functions to match the pre-pooling activations and the statistics on intermediate features of pixels and projected features of 3D points.\nThis paper targets at weakly supervised semantic segmentation for both 2D and 3D domain. The paper doesn't demonstrate if the learned model is practical and can generalize to images or scenes outside the chosen dataset. The proposed method is effective and improves existing methods by a large margin. The authors introduce new loss functions to match the pre-pooling activations and the statistics on intermediate features of pixels and projected features of 3D points."
    },
    {
        "coherence": 0.14824570135288098,
        "consistency": 0.34288027425932416,
        "fluency": 0.06144497492263344,
        "relevance": 0.01152679091062634,
        "overall": 0.14102443536136625,
        "Generated": "251",
        "Gold": "The aim is to reduce the computational cost and inference time while maximally maintaining the classification performance. In particular, SOSP-I employs Hessian approximation while SOSP -H employs exact Hessian. The proposed method can also be used to find and remove architectural bottlenecks, which further improves the performance of the pruned networks. The results of this work can be seen at the bottom of the page. For more information on this paper, visit the official website.\nThe aim is to reduce the computational cost and inference time while maximally maintaining the classification performance. In particular, SOSP-I employs Hessian approximation while SOSP -H employs exact Hessian. The proposed method can also be used to find and remove architectural bottlenecks, which further improves the performance of the pruned networks. The results of this work can be seen at the bottom of the page. For more information on this paper, visit the official website."
    },
    {
        "coherence": 0.08745511660512584,
        "consistency": 0.18816776592459666,
        "fluency": 0.12121231628390251,
        "relevance": 0.026221360705851876,
        "overall": 0.10576413987986921,
        "Generated": "252",
        "Gold": "The paper presents this need clearly and sets out to tackle the problem with a custom CNN architecture. The clinical challenge is relevant, and described very solidly. There is a sufficiently large dataset available for this research, including an annotated target. The paper is written with a clarity that makes it sufficiently easy to follow the path (apart from the weaknesses addressed next). The results fail to convince me that the method actually works. The results show that using longer sequences improve predictions, however there is no justification that even the best performing results are good enough.\nThe paper presents this need clearly and sets out to tackle the problem with a custom CNN architecture. The clinical challenge is relevant, and described very solidly. There is a sufficiently large dataset available for this research, including an annotated target. The paper is written with a clarity that makes it sufficiently easy to follow the path (apart from the weaknesses addressed next). The results fail to convince me that the method actually works. The results show that using longer sequences improve predictions, however there is no justification that even the best performing results are good enough."
    },
    {
        "coherence": 0.2387977317764377,
        "consistency": 0.2996199244593567,
        "fluency": 0.06616473050560082,
        "relevance": 0.04545245537173237,
        "overall": 0.1625087105282819,
        "Generated": "253",
        "Gold": "The paper proposes boosting (using Frank-Wolfe) weak RL learners to get a strong (in a concrete sense) policy. The main theoretical bound does not depend on the number of states, which indicates that the method should be competitive in very large state-space MDPs. The paper is very tersely written and not easy to understand. It is difficult to catch the main idea of this paper, and it is dubious if the proposed algorithm can be implemented in practice."
    },
    {
        "coherence": 0.12675675065513267,
        "consistency": 0.13134742020468307,
        "fluency": 0.11935645761709865,
        "relevance": 0.014910006124578738,
        "overall": 0.09809265865037328,
        "Generated": "254",
        "Gold": "The paper aims to provide a fast algorithm to derive sparse risk scores that scales to high-dimensional datasets. The main idea is to produce a pool of almost-optimal sparse continuous solutions using a beam-search algorithm. The algorithm is implemented in stand-alone Python code, which is advantageous over competitors that rely on mathematical programming solvers. My major concern is the scope of this study, which affects its quality and significance. The paper is focused on risk scores learning which are simple but efficient (in terms of performance) models.\nThe paper aims to provide a fast algorithm to derive sparse risk scores that scales to high-dimensional datasets. The main idea is to produce a pool of almost-optimal sparse continuous solutions using a beam-search algorithm. The algorithm is implemented in stand-alone Python code, which is advantageous over competitors that rely on mathematical programming solvers. My major concern is the scope of this study, which affects its quality and significance. The paper is focused on risk scores learning which are simple but efficient (in terms of performance) models."
    },
    {
        "coherence": 0.10498455319621695,
        "consistency": 0.1376014053652149,
        "fluency": 0.213053367267741,
        "relevance": 0.055152817815706134,
        "overall": 0.12769803591121975,
        "Generated": "255",
        "Gold": "Results may be en par with a MICCAI 2017 challenge winner. The paper seems to overall propose a 1:1 implementation of some arbitrary clinical criteria in a processing pipeline. The weight for the loss function is not mentioned. Metrics are confusing when putting two-class (whole) results and three-class results together. The distinction between deep and periventricular WMH is not made using a neural network, but by a relative simple rule-based processing.\nResults may be en par with a MICCAI 2017 challenge winner. The paper seems to overall propose a 1:1 implementation of some arbitrary clinical criteria in a processing pipeline. The weight for the loss function is not mentioned. Metrics are confusing when putting two-class (whole) results and three-class results together. The distinction between deep and periventricular WMH is not made using a neural network, but by a relative simple rule-based processing."
    },
    {
        "coherence": 0.0632206629329696,
        "consistency": 0.13090311022992607,
        "fluency": 0.09031998435240911,
        "relevance": 0.04377838177206679,
        "overall": 0.08205553482184288,
        "Generated": "256",
        "Gold": "The goal of the work is to associate semantic data types to table attributes. Existing methods mostly adopt regular expressions or dictionary-lookup or feature engineering. The paper introduces a new method, called DCoM that is different from existing methods. It adopts deep neural networks to process raw text tokens directly. Experimental results on the VizNet corpus show thatDCoM models with DistilBERT/ELECTRA outperform Sherlock and other non-Deep Learning models.\nThe goal of the work is to associate semantic data types to table attributes. Existing methods mostly adopt regular expressions or dictionary-lookup or feature engineering. The paper introduces a new method, called DCoM that is different from existing methods. It adopts deep neural networks to process raw text tokens directly. Experimental results on the VizNet corpus show thatDCoM models with DistilBERT/ELECTRA outperform Sherlock and other non-Deep Learning models."
    },
    {
        "coherence": 0.22599723147785802,
        "consistency": 0.24403133880272074,
        "fluency": 0.2126850131957607,
        "relevance": 0.053223107491058214,
        "overall": 0.18398417274184942,
        "Generated": "257",
        "Gold": "The paper proposes a method to incorporate Successor Features (SFs) in domains with continuous state and action spaces. It proposes an actor-critic architecture that learns disentangled representations for the environment dynamics and the tasks. The main contribution of this model is the enablement of the SFs for continuous domains without relying on the costly inference mechanism. The performance of the proposed method is very similar to the goal conditioned SAC baseline on meta world tasks.\nThe paper proposes a method to incorporate Successor Features (SFs) in domains with continuous state and action spaces. It proposes an actor-critic architecture that learns disentangled representations for the environment dynamics and the tasks. The main contribution of this model is the enablement of the SFs for continuous domains without relying on the costly inference mechanism. The performance of the proposed method is very similar to the goal conditioned SAC baseline on meta world tasks."
    },
    {
        "coherence": 0.3158942928445557,
        "consistency": 0.3315055299773887,
        "fluency": 0.28696248592414497,
        "relevance": 0.05665568424251654,
        "overall": 0.24775449824715148,
        "Generated": "258",
        "Gold": "This paper studied the convergence properties of the proximal-GDA algorithm for solving nonconvex-strongly-concave optimization problems. It focuses on understanding the simplest (although not the best) algorithm that can be applied to the saddle point objective. The minmax problem has gained considerable attention recently but is not yet fully developed. I have two main concerns. The first one is that required assumptions are quite restrictive. The second concern is that a big part of obtained results is very natural if not known already.\nThis paper studied the convergence properties of the proximal-GDA algorithm for solving nonconvex-strongly-concave optimization problems. It focuses on understanding the simplest (although not the best) algorithm that can be applied to the saddle point objective. The minmax problem has gained considerable attention recently but is not yet fully developed. I have two main concerns. The first one is that required assumptions are quite restrictive. The second concern is that a big part of obtained results is very natural if not known already."
    },
    {
        "coherence": 0.2391533788213809,
        "consistency": 0.26556592863077744,
        "fluency": 0.400341628674397,
        "relevance": 0.02760123603510083,
        "overall": 0.23316554304041404,
        "Generated": "259",
        "Gold": "This paper exploits depth information by estimating ground plane and fuses that information with image features via attention layers. Experimental results on KITTI are very strong, outperforming MonoDETR by 2-4points. The proposed method is intuitive and clearly presented. It is claimed to be top on monocular 3D image detection on KittI. It estimates the camera pose at first and constructs ground depth map to guide the depth estimation of objects.\nThis paper exploits depth information by estimating ground plane and fuses that information with image features via attention layers. Experimental results on KITTI are very strong, outperforming MonoDETR by 2-4points. The proposed method is intuitive and clearly presented. It is claimed to be top on monocular 3D image detection on KittI. It estimates the camera pose at first and constructs ground depth map to guide the depth estimation of objects."
    },
    {
        "coherence": 0.2234897311628707,
        "consistency": 0.17343906014151012,
        "fluency": 0.10289872985403058,
        "relevance": 0.05327494858624749,
        "overall": 0.13827561743616473,
        "Generated": "260",
        "Gold": "The authors propose a method to automatically build a Knowledge Base of opinions and implications between them. The KB is realized as a directed graph where nodes correspond to opinions in a canonical form (modifier, aspect) It is built by factorizing a matrix of item-opinion frequencies, and finding the top k neighbors of an opinion. The authors claim that opinions and their implications often do not co-occur within the same review and the annotation for such implication relation is expensive.\nThe authors propose a method to automatically build a Knowledge Base of opinions and implications between them. The KB is realized as a directed graph where nodes correspond to opinions in a canonical form (modifier, aspect) It is built by factorizing a matrix of item-opinion frequencies, and finding the top k neighbors of an opinion. The authors claim that opinions and their implications often do not co-occur within the same review and the annotation for such implication relation is expensive."
    },
    {
        "coherence": 0.1377268005853625,
        "consistency": 0.23219955506124207,
        "fluency": 0.22169330962386016,
        "relevance": 0.02127615861249278,
        "overall": 0.15322395597073937,
        "Generated": "261",
        "Gold": "Paper “AREA ATTENTION” extends the current attention models from word level to “area level”, i.e., the combination of adjacent words. Every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn. (3) and EqN. (4)Attention mechanisms are designed to focus on a single item in the entire memory.\nPaper “AREA ATTENTION” extends the current attention models from word level to “area level”, i.e., the combination of adjacent words. Every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn. (3) and EqN. (4)Attention mechanisms are designed to focus on a single item in the entire memory."
    },
    {
        "coherence": 0.07443440348217031,
        "consistency": 0.1580643541347556,
        "fluency": 0.16366957635299118,
        "relevance": 0.013940810685769706,
        "overall": 0.1025272861639217,
        "Generated": "262",
        "Gold": "This paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other Pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data. The results are sensible but confirm what we already strongly suspected. Language models are very effective on sequence tagging tasks (POS, CCG)\nThis paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other Pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data. The results are sensible but confirm what we already strongly suspected. Language models are very effective on sequence tagging tasks (POS, CCG)"
    },
    {
        "coherence": 0.26096109025235337,
        "consistency": 0.32857680038741793,
        "fluency": 0.23907502697221578,
        "relevance": 0.04813274879316744,
        "overall": 0.2191864166012886,
        "Generated": "263",
        "Gold": "This paper studies the stable points of gradient descent ascent with different step-sizes. The paper's main result is that for any fixed point for the GDA dynamics, that point is stable for GDA with a large enough timescale separation if and only if it is a local Stackelberg equilibrium. It seems to me that the authors have not qualitatively improved over (Jin et al. 2020) This work proves that we can get the same stability guarantees while keeping the learning rates of both players finite.\nThis paper studies the stable points of gradient descent ascent with different step-sizes. The paper's main result is that for any fixed point for the GDA dynamics, that point is stable for GDA with a large enough timescale separation if and only if it is a local Stackelberg equilibrium. It seems to me that the authors have not qualitatively improved over (Jin et al. 2020) This work proves that we can get the same stability guarantees while keeping the learning rates of both players finite."
    },
    {
        "coherence": 0.3346493480049845,
        "consistency": 0.35865881368810143,
        "fluency": 0.15237500062958312,
        "relevance": 0.04848563270539315,
        "overall": 0.22354219875701556,
        "Generated": "264",
        "Gold": "This paper presents a new framework to describe and understand the dynamics of RNNs inspired by quantum physics. The authors also propose a novel RNN architecture derived by their analysis. Despite this paper is mainly a theory paper, it would have a lot more strength if the authors provide some experiments to demonstrate the strength of the proposed architecture. The ultimate goal of this work is to develop an architecture that is better able to model long-term dependencies.\nThis paper presents a new framework to describe and understand the dynamics of RNNs inspired by quantum physics. The authors also propose a novel RNN architecture derived by their analysis. Despite this paper is mainly a theory paper, it would have a lot more strength if the authors provide some experiments to demonstrate the strength of the proposed architecture. The ultimate goal of this work is to develop an architecture that is better able to model long-term dependencies."
    },
    {
        "coherence": 0.047237761127321046,
        "consistency": 0.0874797171943526,
        "fluency": 0.16799382869248208,
        "relevance": 0.03272477450397311,
        "overall": 0.0838590203795322,
        "Generated": "265",
        "Gold": "Tiling over whole slide images (using small patches) is considered explicitly in this method to adopt the optimal transport to histopathology slides. The method facilitates using a publicly available dataset from different tissues for pre-training a model to apply to a small (local) dataset. I believe there is potential that the proposed distance can be useful in more difficult tasks and rare cancer datasets. The transfer learning study does not seem to support the authors' claim strongly.\nTiling over whole slide images (using small patches) is considered explicitly in this method to adopt the optimal transport to histopathology slides. The method facilitates using a publicly available dataset from different tissues for pre-training a model to apply to a small (local) dataset. I believe there is potential that the proposed distance can be useful in more difficult tasks and rare cancer datasets. The transfer learning study does not seem to support the authors' claim strongly."
    },
    {
        "coherence": 0.2246282978366778,
        "consistency": 0.3586546810476769,
        "fluency": 0.41758351045841935,
        "relevance": 0.08360812949600407,
        "overall": 0.27111865470969454,
        "Generated": "266",
        "Gold": "The paper studies over-parametrized neural networks when the size of the network goes to infinity. It is known that training the network via gradient descent type algorithms is equivalent to learning via the Neural Tangent Kernel (NTK) The paper is well-written and the results make non-trivial extensions over the state-of-the-art. The exposition of the concepts is very good in general, though some references and some (minor) aspects appear to be missing."
    },
    {
        "coherence": 0.18583892978490463,
        "consistency": 0.22858077314965983,
        "fluency": 0.35954141201957895,
        "relevance": 0.02841030305208272,
        "overall": 0.20059285450155653,
        "Generated": "267",
        "Gold": "This manuscript extends the Strong Lottery Ticket Hypothesis to binary networks. It proposed a new method to obtain compact binary networks with better robustness. Experiments on CifAR-10 and CIFAR-100 show that the proposed method outperforms previous competitive methods. The paper proposes adding a last layer batchnorm to stabilize the training of their compressed robust models. The authors claim that their work is generic and does not suffer from limitations or potential negative societal impact.\nThis manuscript extends the Strong Lottery Ticket Hypothesis to binary networks. It proposed a new method to obtain compact binary networks with better robustness. Experiments on CifAR-10 and CIFAR-100 show that the proposed method outperforms previous competitive methods. The paper proposes adding a last layer batchnorm to stabilize the training of their compressed robust models. The authors claim that their work is generic and does not suffer from limitations or potential negative societal impact."
    },
    {
        "coherence": 0.2631696310155997,
        "consistency": 0.30875849210602324,
        "fluency": 0.48731302780799196,
        "relevance": 0.021260545417032012,
        "overall": 0.27012542408666174,
        "Generated": "268",
        "Gold": "Many SOTA models usually overfit w.r.t. spurious correlations in the data. The proposed method tries to maximally filter out task-irrelevant information in the feature vectors. Experiments on several datasets show improved performance on both in-domain and out-of-domain datasets. The paper is presented well, and it's a good read. My major concern is on the novelty of the proposed method. How would the VIB framework work with a different PLM, e.g., XLM-Roberta or T5?\nMany SOTA models usually overfit w.r.t. spurious correlations in the data. The proposed method tries to maximally filter out task-irrelevant information in the feature vectors. Experiments on several datasets show improved performance on both in-domain and out-of-domain datasets. The paper is presented well, and it's a good read. My major concern is on the novelty of the proposed method. How would the VIB framework work with a different PLM, e.g., XLM-Roberta or T5?"
    },
    {
        "coherence": 0.4057723761363771,
        "consistency": 0.3743223659140317,
        "fluency": 0.3709225547100348,
        "relevance": 0.08283522515235352,
        "overall": 0.30846313047819923,
        "Generated": "269",
        "Gold": "This paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. The main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation. The proposed approach can work effectively in larger scale domains with more difficult biases. It begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias. It would be good if the authors could at least mention that “Boltzmann rational’ is a specific model of “systematic” bias for which much experimental support eith humans and animals exists.\nThis paper addresses the interesting and challenging problem of learning the reward function from demonstrators which have unknown biases. The main shortcomings of the paper are a lack of clarity at certain points and a limited experimental validation. The proposed approach can work effectively in larger scale domains with more difficult biases. It begs the question whether accounting for unknown systematic bias can indeed outperform the assumption of a particular inaccurate bias. It would be good if the authors could at least mention that “Boltzmann rational’ is a specific model of “systematic” bias for which much experimental support eith humans and animals exists."
    },
    {
        "coherence": 0.31160059983555666,
        "consistency": 0.5415757848283653,
        "fluency": 0.16127605957468774,
        "relevance": 0.029799426847635563,
        "overall": 0.26106296777156135,
        "Generated": "270",
        "Gold": "This paper proposes a distribution shift benchmark for unsupervised anomaly detection. The dataset is built over a network traffic data Kyoto-2006+ and the test data is split into three distributions IID, NEAR, and FAR chronologically. Some preliminary experiments also show the performance degradation of classic anomaly detection algorithms due to the data distribution shift. The proposed problem is practical and meaningful. The authors provide a long-time data created by using honeypots to detect anomalous traffic.\nThis paper proposes a distribution shift benchmark for unsupervised anomaly detection. The dataset is built over a network traffic data Kyoto-2006+ and the test data is split into three distributions IID, NEAR, and FAR chronologically. Some preliminary experiments also show the performance degradation of classic anomaly detection algorithms due to the data distribution shift. The proposed problem is practical and meaningful. The authors provide a long-time data created by using honeypots to detect anomalous traffic."
    },
    {
        "coherence": 0.1531580814225763,
        "consistency": 0.15036248075954078,
        "fluency": 0.2814834224116987,
        "relevance": 0.037505493249317436,
        "overall": 0.1556273694607833,
        "Generated": "271",
        "Gold": "The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The main idea is to treat the problem as a domain adaptation problem by considering the data shift between adversarial and clean distributions. Authors claim that the proposed method outperforms other methods when the target domain is the adversarial examples. The experiment section is not comprehensive. More recent SOTA methods are missing. The authors show that their proposed method provides: 1) higher accuracy on attacked data (more robustness), and 2) higher Accuracy on clean data.\nThe paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The main idea is to treat the problem as a domain adaptation problem by considering the data shift between adversarial and clean distributions. Authors claim that the proposed method outperforms other methods when the target domain is the adversarial examples. The experiment section is not comprehensive. More recent SOTA methods are missing. The authors show that their proposed method provides: 1) higher accuracy on attacked data (more robustness), and 2) higher Accuracy on clean data."
    },
    {
        "coherence": 0.14766740687911786,
        "consistency": 0.2935643430242312,
        "fluency": 0.1864918092880844,
        "relevance": 0.031110809524487623,
        "overall": 0.16470859217898026,
        "Generated": "272",
        "Gold": "WPipe is a novel pipeline parallelism technique for training large DNN models. Method aims to improve upon existing pipeline methods such as Pipedream-2BW. Key idea is to divide the model partitions into two parts groups and make the forward pass of the second part. Experimental evaluation shows that WPipe can achieve significant acceleration (1.4x) and lower memory footprint (36% smaller) when compared to state-of-the-art PipeDream-2 BW.\nWPipe is a novel pipeline parallelism technique for training large DNN models. Method aims to improve upon existing pipeline methods such as Pipedream-2BW. Key idea is to divide the model partitions into two parts groups and make the forward pass of the second part. Experimental evaluation shows that WPipe can achieve significant acceleration (1.4x) and lower memory footprint (36% smaller) when compared to state-of-the-art PipeDream-2 BW."
    },
    {
        "coherence": 0.10756667617942238,
        "consistency": 0.15198685436872514,
        "fluency": 0.14808990985181616,
        "relevance": 0.04033734581084587,
        "overall": 0.11199519655270239,
        "Generated": "273",
        "Gold": "The paper introduces four EEG signal transformations to model the real-world variability observable during deployment. Then, the paper proposes a multi-pronged approach to evaluate the robustness of healthcare ML models. The overall idea is well-motivated and addresses a unique and important challenge in ML for health. The paper is quite clear, but could do with some additional figures showing correlations between in-the-wild performance and the proposed measures. The highlights in Table 2 is misleading - when several numbers are equal, they should all be highlighted.\nThe paper introduces four EEG signal transformations to model the real-world variability observable during deployment. Then, the paper proposes a multi-pronged approach to evaluate the robustness of healthcare ML models. The overall idea is well-motivated and addresses a unique and important challenge in ML for health. The paper is quite clear, but could do with some additional figures showing correlations between in-the-wild performance and the proposed measures. The highlights in Table 2 is misleading - when several numbers are equal, they should all be highlighted."
    },
    {
        "coherence": 0.28186226152397437,
        "consistency": 0.24945114684220948,
        "fluency": 0.30195861981818256,
        "relevance": 0.0473843363386606,
        "overall": 0.22016409113075674,
        "Generated": "274",
        "Gold": "This paper focuses on Byzantine-robust federated learning, a learning paradigm where a centralized server coordinates learning across a data set partitioned across multiple worker nodes. In this setting, the main focus is on studying the setting where honest worker nodes own heterogeneous data sets. The paper is well-motivated, clearly written, and contains solid contributions. There are some minor inaccuracies in the proofs, small typos, and other minor weaknesses.\nThis paper focuses on Byzantine-robust federated learning, a learning paradigm where a centralized server coordinates learning across a data set partitioned across multiple worker nodes. In this setting, the main focus is on studying the setting where honest worker nodes own heterogeneous data sets. The paper is well-motivated, clearly written, and contains solid contributions. There are some minor inaccuracies in the proofs, small typos, and other minor weaknesses."
    },
    {
        "coherence": 0.28038267529971017,
        "consistency": 0.39559163267225683,
        "fluency": 0.26005388521084527,
        "relevance": 0.06827967821240286,
        "overall": 0.2510769678488038,
        "Generated": "275",
        "Gold": "Paper presents empirical analysis Multi-headed Self-Attention (MSA) as part of Vision Transformer (ViT) and its variants. Based on these observations, paper proposes a new architecture -- AlterNet, where CNN block is combined with MSA block. AlterNet is shown to outperform pure CNNs in both large and small data regimes. The paper then presents a hybrid CNN-ViT architecture that combines some of the benefits of ViTs with advantages of CNNs.\nPaper presents empirical analysis Multi-headed Self-Attention (MSA) as part of Vision Transformer (ViT) and its variants. Based on these observations, paper proposes a new architecture -- AlterNet, where CNN block is combined with MSA block. AlterNet is shown to outperform pure CNNs in both large and small data regimes. The paper then presents a hybrid CNN-ViT architecture that combines some of the benefits of ViTs with advantages of CNNs."
    },
    {
        "coherence": 0.2747139088468577,
        "consistency": 0.31996452702662426,
        "fluency": 0.42196691865385166,
        "relevance": 0.03282991634524355,
        "overall": 0.2623688177181443,
        "Generated": "276",
        "Gold": "The authors present a new approach for model-based reinforcement learning by training two models and using them separately. One model prioritizes accurate transitions and another has a weighting term for value- gradient, which they combine in a new algorithm directional derivative projection policy optimization. The theoretical results and experimental protocols seem sound, although the core change proposed by the method does not provide very large improvements over baselines. I am recommending to accept this paper because its algorithm construction is well-reasoned.\nThe authors present a new approach for model-based reinforcement learning by training two models and using them separately. One model prioritizes accurate transitions and another has a weighting term for value- gradient, which they combine in a new algorithm directional derivative projection policy optimization. The theoretical results and experimental protocols seem sound, although the core change proposed by the method does not provide very large improvements over baselines. I am recommending to accept this paper because its algorithm construction is well-reasoned."
    },
    {
        "coherence": 0.06895589683525571,
        "consistency": 0.09286654436391463,
        "fluency": 0.4031591208845574,
        "relevance": 0.02371093015680702,
        "overall": 0.1471731230601337,
        "Generated": "277",
        "Gold": "The paper is written clearly with no substantial formatting errors. The authors commit themselves to open-source the dataset and codes for reproducibility. The dataset involves instance-based annotations which would be valuable for the community. The paper was published in the open-access journal, Theoretical and Applied Statistics. It was written by a group of researchers at the University of California, San Diego. It is the first paper of its kind to be published in this way.\nThe paper is written clearly with no substantial formatting errors. The authors commit themselves to open-source the dataset and codes for reproducibility. The dataset involves instance-based annotations which would be valuable for the community. The paper was published in the open-access journal, Theoretical and Applied Statistics. It was written by a group of researchers at the University of California, San Diego. It is the first paper of its kind to be published in this way."
    },
    {
        "coherence": 0.21152005113765365,
        "consistency": 0.2337750044138497,
        "fluency": 0.46489753750560475,
        "relevance": 0.06749946933986997,
        "overall": 0.24442301559924454,
        "Generated": "278",
        "Gold": "The paper tackles a very relevant problem in histopathology image analysis. The proposed methodology (combination of classical data augmentation and GANs) is very straightforward and logical. The paper is not very clearly written and is not easy to follow. The presentation of the protocols the authors propose could have been clearer and better explained. The discussion relating to the results is somewhat superficial. HEDjitter behaves differently from the other techniques. DC-GAN cannot be considered state-of-the-art anymore.\nThe paper tackles a very relevant problem in histopathology image analysis. The proposed methodology (combination of classical data augmentation and GANs) is very straightforward and logical. The paper is not very clearly written and is not easy to follow. The presentation of the protocols the authors propose could have been clearer and better explained. The discussion relating to the results is somewhat superficial. HEDjitter behaves differently from the other techniques. DC-GAN cannot be considered state-of-the-art anymore."
    },
    {
        "coherence": 0.2608970167958294,
        "consistency": 0.26390321304972253,
        "fluency": 0.4063761335431803,
        "relevance": 0.04168132529049556,
        "overall": 0.24321442216980693,
        "Generated": "279",
        "Gold": "This work investigates how pre-training distributions induce robustness in image-text models. They use Contrastive Language-Image Pre-training (CLIP) as a case study. The empirical results suggest that no single training data provides consistent out-of-distribution performance. I think this paper, in its current form, already matches the quality of a typical publication at NeurIPS conference. The authors call for further study into dataset design.\nThis work investigates how pre-training distributions induce robustness in image-text models. They use Contrastive Language-Image Pre-training (CLIP) as a case study. The empirical results suggest that no single training data provides consistent out-of-distribution performance. I think this paper, in its current form, already matches the quality of a typical publication at NeurIPS conference. The authors call for further study into dataset design."
    },
    {
        "coherence": 0.3052369656275747,
        "consistency": 0.38261801363160397,
        "fluency": 0.151232858708243,
        "relevance": 0.03298535643717918,
        "overall": 0.21801829860115018,
        "Generated": "280",
        "Gold": "The paper presents a modification of the empirical risk minimization (ERM) framework in order to obtain more robust or fairer results, using tilted objective. Strong points: the article is entirely reproducible as the author furnishes a well documented code. Weak points: Use of 80% outliers in Table 1 without further explications. The paper provides nice discussions of a simple modification of ERM paradigm. The success of Term heavily rely on a *magical* tuning of the parameter $t$ depending on the application.\nThe paper presents a modification of the empirical risk minimization (ERM) framework in order to obtain more robust or fairer results, using tilted objective. Strong points: the article is entirely reproducible as the author furnishes a well documented code. Weak points: Use of 80% outliers in Table 1 without further explications. The paper provides nice discussions of a simple modification of ERM paradigm. The success of Term heavily rely on a *magical* tuning of the parameter $t$ depending on the application."
    },
    {
        "coherence": 0.17490759460640193,
        "consistency": 0.22299576026604156,
        "fluency": 0.16604486538853072,
        "relevance": 0.041483959462285044,
        "overall": 0.1513580449308148,
        "Generated": "281",
        "Gold": "The paper studies multi-fidelity best arm identification (BAI) On each round, a decision-maker should choose which arm to pull and which fidelity to pull the arm at. Lower fidelity evaluations are cheaper to evaluate, but provide a more biased estimate of the objective function. They present and analyse a new algorithm, IISE, that theoretically outperforms basic bandit algorithms that do no exploit the multi fidelity structure of the problem. The lower bound given in the paper is quantifiably better than the lower bound for regularBAI.\nThe paper studies multi-fidelity best arm identification (BAI) On each round, a decision-maker should choose which arm to pull and which fidelity to pull the arm at. Lower fidelity evaluations are cheaper to evaluate, but provide a more biased estimate of the objective function. They present and analyse a new algorithm, IISE, that theoretically outperforms basic bandit algorithms that do no exploit the multi fidelity structure of the problem. The lower bound given in the paper is quantifiably better than the lower bound for regularBAI."
    },
    {
        "coherence": 0.16220775935466905,
        "consistency": 0.5342499524643155,
        "fluency": 0.22370874544641878,
        "relevance": 0.03229661095105485,
        "overall": 0.23811576705411455,
        "Generated": "282",
        "Gold": "This paper proposes a method named zero-shot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. The proposed method can detect the unseen attributes via a decompose-and-reassemble manner. Results are demonstrated using the CUB dataset alone. The new attribute synthesis can be used to solve Zero-Shot Classification (ZSC) problems. The approach is compared to 4 other ZSC algorithms on theCUB dataset.\nThis paper proposes a method named zero-shot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. The proposed method can detect the unseen attributes via a decompose-and-reassemble manner. Results are demonstrated using the CUB dataset alone. The new attribute synthesis can be used to solve Zero-Shot Classification (ZSC) problems. The approach is compared to 4 other ZSC algorithms on theCUB dataset."
    },
    {
        "coherence": 0.3265341709899433,
        "consistency": 0.2840735761961154,
        "fluency": 0.29110660010365985,
        "relevance": 0.04516613943199056,
        "overall": 0.23672012168042728,
        "Generated": "283",
        "Gold": "The paper proposed a very interesting training algorithm for auto-regressive models. It does not require any MLE pre-training and can directly optimize from the sampling. The idea should also apply on many “incremental rewards”, for instance, BLEU scores in machine translation. The experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art.\nThe paper proposed a very interesting training algorithm for auto-regressive models. It does not require any MLE pre-training and can directly optimize from the sampling. The idea should also apply on many “incremental rewards”, for instance, BLEU scores in machine translation. The experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art."
    },
    {
        "coherence": 0.15571562392065838,
        "consistency": 0.2871117959202529,
        "fluency": 0.2929432442190007,
        "relevance": 0.027116258372087875,
        "overall": 0.19072173060799996,
        "Generated": "284",
        "Gold": "The paper is written, provides good description of the state-of-the-art and comprehensive results. The total idea is good, but the novelty is not much. A preference test using MOS or preference score should be conducted. The paper is well written, and also shows the effectiveness of the method. It is difficult to grasp without a good background on deep learning for speech enhancement. It's a good start, but there is a long way to go.\nThe paper is written, provides good description of the state-of-the-art and comprehensive results. The total idea is good, but the novelty is not much. A preference test using MOS or preference score should be conducted. The paper is well written, and also shows the effectiveness of the method. It is difficult to grasp without a good background on deep learning for speech enhancement. It's a good start, but there is a long way to go."
    },
    {
        "coherence": 0.2785967132170419,
        "consistency": 0.25810233553430584,
        "fluency": 0.2727186841041617,
        "relevance": 0.05113288355489134,
        "overall": 0.2151376541026002,
        "Generated": "285",
        "Gold": "The paper proposes a multi-document extractive machine reading model and algorithm. The model is composed of 3 distinct parts. First, the document retriever and the document reader. Then, the paper proposes to use a \"multi-step-reasoner\" which learns to reformulate the question into its latent space. The idea of multi-step & bi-directional interaction between the retriever & reader is novel enough. The paper contains enough literature studies on existing retriever-reader framework in open-domain setting.\nThe paper proposes a multi-document extractive machine reading model and algorithm. The model is composed of 3 distinct parts. First, the document retriever and the document reader. Then, the paper proposes to use a \"multi-step-reasoner\" which learns to reformulate the question into its latent space. The idea of multi-step & bi-directional interaction between the retriever & reader is novel enough. The paper contains enough literature studies on existing retriever-reader framework in open-domain setting."
    },
    {
        "coherence": 0.2907262858874,
        "consistency": 0.24798480033677686,
        "fluency": 0.2412437448508307,
        "relevance": 0.040799953918688826,
        "overall": 0.2051886962484241,
        "Generated": "286",
        "Gold": "This paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) The method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and no theoretical analysis is done. The paper uses similar ideas to those proposed by Matthews et. al (2018a) and Lee et al (2017)\nThis paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) The method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and no theoretical analysis is done. The paper uses similar ideas to those proposed by Matthews et. al (2018a) and Lee et al (2017)"
    },
    {
        "coherence": 0.19639185614382673,
        "consistency": 0.25499833042274933,
        "fluency": 0.28714929743554346,
        "relevance": 0.03244318741910905,
        "overall": 0.19274566785530714,
        "Generated": "287",
        "Gold": "DaSGD is an algorithm for large-scale large-batch training of deep neural networks. The algorithm combines Local SGD with delayed averaging steps to hide the communication overhead. Authors decided to decrease the font size of all important equations and plots to fit the conference size requirements. The paper is well-written and easy to understand. I have some concerns in the novelty. I did not find the improvement regarding the wall clock time by hiding the communication in the experiment.\nDaSGD is an algorithm for large-scale large-batch training of deep neural networks. The algorithm combines Local SGD with delayed averaging steps to hide the communication overhead. Authors decided to decrease the font size of all important equations and plots to fit the conference size requirements. The paper is well-written and easy to understand. I have some concerns in the novelty. I did not find the improvement regarding the wall clock time by hiding the communication in the experiment."
    },
    {
        "coherence": 0.19737085650646777,
        "consistency": 0.12659154727923072,
        "fluency": 0.23109295069682367,
        "relevance": 0.0316568975940244,
        "overall": 0.14667806301913666,
        "Generated": "288",
        "Gold": "The paper studies decentralized optimization algorithm over general communication weight matrix. The weight matrix has to be fixed over time, so it does not seem to apply to decentralized algorithms where the communication pattern is different in different steps. When the matrix is fixed, the paper proves the optimal convergence rate for nonconvex objectives. The main innovation is the new structure of the connection graph, which can be used to revisiting the lower bounds. The article is quite well written and easy to follow.\nThe paper studies decentralized optimization algorithm over general communication weight matrix. The weight matrix has to be fixed over time, so it does not seem to apply to decentralized algorithms where the communication pattern is different in different steps. When the matrix is fixed, the paper proves the optimal convergence rate for nonconvex objectives. The main innovation is the new structure of the connection graph, which can be used to revisiting the lower bounds. The article is quite well written and easy to follow."
    },
    {
        "coherence": 0.43712374223105455,
        "consistency": 0.4553385295320199,
        "fluency": 0.2698028545838237,
        "relevance": 0.08899851005650576,
        "overall": 0.312815909100851,
        "Generated": "289",
        "Gold": "This paper tackles the problem of controlled text generation by converting it into a conditional text generation similar to (Keskar et al.19). It proposes an architectural modification to the transformer LM used in GPT2. A CoCon layer is added as a separate transformer block in the middle allowing self-attention to be performed. The novelty of the CoCon method lies in its ability to condition on entire input sequences. The paper is well written, the method is intuitive and all components are well motivated.\nThis paper tackles the problem of controlled text generation by converting it into a conditional text generation similar to (Keskar et al.19). It proposes an architectural modification to the transformer LM used in GPT2. A CoCon layer is added as a separate transformer block in the middle allowing self-attention to be performed. The novelty of the CoCon method lies in its ability to condition on entire input sequences. The paper is well written, the method is intuitive and all components are well motivated."
    },
    {
        "coherence": 0.2273439416377007,
        "consistency": 0.3328719212508279,
        "fluency": 0.17295569551225842,
        "relevance": 0.05163603582717475,
        "overall": 0.19620189855699044,
        "Generated": "290",
        "Gold": "The paper considers the problem of continual learning on semantic segmentation. Experiments have been conducted on the PASCAL VOC dataset. The proposed solutions, although simple, does not give too much insight, more like some tricks. The authors draw inspiration from previous studies in the field and introduce wider convolution for final feature extraction as well as apply dropout to the output of the feature encoder. They show an increase in performance in specific setups using two proposed improvements.\nThe paper considers the problem of continual learning on semantic segmentation. Experiments have been conducted on the PASCAL VOC dataset. The proposed solutions, although simple, does not give too much insight, more like some tricks. The authors draw inspiration from previous studies in the field and introduce wider convolution for final feature extraction as well as apply dropout to the output of the feature encoder. They show an increase in performance in specific setups using two proposed improvements."
    },
    {
        "coherence": 0.14218393270565458,
        "consistency": 0.28378288082538355,
        "fluency": 0.1556293324944069,
        "relevance": 0.0426123205118567,
        "overall": 0.15605211663432544,
        "Generated": "291",
        "Gold": "The paper proposes constructing a pyramid-shaped input representation based on a pyramid attention mechanism. The pyramid attention combines three levels of looking at the source code (statement, token, and subtoken level) The levels are combined bottom up from subtoken to statement level, and also with a cross-granularity interaction. The proposed method is elegant, does not overcomplicate the process, and seems easily reproducible. Authors do not provide limitation section nor analyze the results qualitatively.\nThe paper proposes constructing a pyramid-shaped input representation based on a pyramid attention mechanism. The pyramid attention combines three levels of looking at the source code (statement, token, and subtoken level) The levels are combined bottom up from subtoken to statement level, and also with a cross-granularity interaction. The proposed method is elegant, does not overcomplicate the process, and seems easily reproducible. Authors do not provide limitation section nor analyze the results qualitatively."
    },
    {
        "coherence": 0.13954013393251635,
        "consistency": 0.21846979245866907,
        "fluency": 0.33466857670604516,
        "relevance": 0.13671582466487897,
        "overall": 0.2073485819405274,
        "Generated": "292",
        "Gold": "This paper incorporates a concept called hypergraph network into reinforcement learning. The idea of hypergraph is to extend edge to hyperedge where a set of vertices can be considered at the same time. Empirical results on a few atari games (29) and simulated physical control benchmarks clearly demonstrate the superiority of the approach. The paper is clearly written and well-organized. But in general, I find the technical contributions (borderline) to be incremental.\nThis paper incorporates a concept called hypergraph network into reinforcement learning. The idea of hypergraph is to extend edge to hyperedge where a set of vertices can be considered at the same time. Empirical results on a few atari games (29) and simulated physical control benchmarks clearly demonstrate the superiority of the approach. The paper is clearly written and well-organized. But in general, I find the technical contributions (borderline) to be incremental."
    },
    {
        "coherence": 0.23042806200533145,
        "consistency": 0.2173177744962059,
        "fluency": 0.1458280684984872,
        "relevance": 0.030822136187266303,
        "overall": 0.1560990102968227,
        "Generated": "293",
        "Gold": "The authors use a deep-learning pipeline to understand the nature of encoding in the salamander retina. They collect PSTHs from neurons in response to short natural movies, then the PSTHS are converted to images and used as the input to a U-Net. Curiously, the latent representation can be used to accurately decode time in the movie, even for completely novel movies. This suggests that the model learns a low-dimensional representation of elapsed time.\nThe authors use a deep-learning pipeline to understand the nature of encoding in the salamander retina. They collect PSTHs from neurons in response to short natural movies, then the PSTHS are converted to images and used as the input to a U-Net. Curiously, the latent representation can be used to accurately decode time in the movie, even for completely novel movies. This suggests that the model learns a low-dimensional representation of elapsed time."
    },
    {
        "coherence": 0.3064423752688161,
        "consistency": 0.3272502078486809,
        "fluency": 0.18773046933656706,
        "relevance": 0.029259486402765256,
        "overall": 0.21267063471420736,
        "Generated": "294",
        "Gold": "This paper presents a model, called PoET, for single-image pose estimation. They feed the image to a backbone object detector (Scaled-YOLO-v4), which produces multi-scale features and bounding boxes. The model then predicts rotation and translation, using geodesic loss and L2 distance. They evaluate the model on the YCB-V dataset, where their method gets better performance than many baselines.\nThis paper presents a model, called PoET, for single-image pose estimation. They feed the image to a backbone object detector (Scaled-YOLO-v4), which produces multi-scale features and bounding boxes. The model then predicts rotation and translation, using geodesic loss and L2 distance. They evaluate the model on the YCB-V dataset, where their method gets better performance than many baselines."
    },
    {
        "coherence": 0.15622875872528644,
        "consistency": 0.2027665053646752,
        "fluency": 0.238027035144257,
        "relevance": 0.04460077613565045,
        "overall": 0.1604057688424673,
        "Generated": "295",
        "Gold": "Lack of explanation on how this novel method can be applied in real-world scenarios. Good references to other research overall, but references missing at times. More explanation on prior methods, who derived them, what applications they have nowadays, and where they are being used in the AI world. The paper is not easy to follow, which makes it hard to pinpoint the exact contribution. The proposed approach is an interesting contribution to network Granger causal modelling.\nLack of explanation on how this novel method can be applied in real-world scenarios. Good references to other research overall, but references missing at times. More explanation on prior methods, who derived them, what applications they have nowadays, and where they are being used in the AI world. The paper is not easy to follow, which makes it hard to pinpoint the exact contribution. The proposed approach is an interesting contribution to network Granger causal modelling."
    },
    {
        "coherence": 0.5039054717375266,
        "consistency": 0.5362594066192796,
        "fluency": 0.3988547492310426,
        "relevance": 0.12054998807620651,
        "overall": 0.3898924039160138,
        "Generated": "296",
        "Gold": "The paper is concerned with selective classification in a stylised 'realisably noisy' data model. It is approached by designing a new loss for learning a selector given a predictor $f$, denoted $W(g;f, \\theta) The proposed method has remarkably better selective risk compared to the underlying classification literature. The problem of identifying uninformative data is interesting and very relevant in modern machine learning. The theory of the paper---though highly stylized and based on $0-1$ loss that cannot be optimized efficiently---is revealing and also a good starting point."
    },
    {
        "coherence": 0.33317644834439547,
        "consistency": 0.3542686441616769,
        "fluency": 0.22669275047830148,
        "relevance": 0.05328766291921197,
        "overall": 0.24185637647589647,
        "Generated": "297",
        "Gold": "This paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue. The paper seems hastily written with some grammar issues, typos, and sloppy use of LaTeX. It focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties.\nThis paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue. The paper seems hastily written with some grammar issues, typos, and sloppy use of LaTeX. It focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties."
    },
    {
        "coherence": 0.22840505197722527,
        "consistency": 0.26192417936439666,
        "fluency": 0.35272628956019153,
        "relevance": 0.035443189044415915,
        "overall": 0.21962467748655734,
        "Generated": "298",
        "Gold": "This paper wants to combine sequence (called Context) and AST (called Structure) representations of source code in a Transformer encoder model. Different pairwise relations based on ASTs and sequence ordering are computed and each of them is encoded as a separate distance matrix. This model is evaluated on the task of code summarization and compared against code2seq and GREAT models. The results show that Code Transformer achieves results better than these models. By adding positional relational information, such as shortest path length, and ancestor distance the transformers learn to better represent code.\nThis paper wants to combine sequence (called Context) and AST (called Structure) representations of source code in a Transformer encoder model. Different pairwise relations based on ASTs and sequence ordering are computed and each of them is encoded as a separate distance matrix. This model is evaluated on the task of code summarization and compared against code2seq and GREAT models. The results show that Code Transformer achieves results better than these models. By adding positional relational information, such as shortest path length, and ancestor distance the transformers learn to better represent code."
    },
    {
        "coherence": 0.20092553766805357,
        "consistency": 0.4311848816360562,
        "fluency": 0.4400630840554686,
        "relevance": 0.017938182269908825,
        "overall": 0.2725279214073718,
        "Generated": "299",
        "Gold": "Pretrained Transformer models trained on two different modalities (GPT-2 for language, and iGPT for image) are applied to Mujoco offline RL tasks - Hopper, HalfCheetah and Walker2D. The study looks at the differences in activations, parameters, and gradients as well as the impact of the context length. Pre-trained models also do not acquire more knowledge about the input or the label during fine-tuning.\nPretrained Transformer models trained on two different modalities (GPT-2 for language, and iGPT for image) are applied to Mujoco offline RL tasks - Hopper, HalfCheetah and Walker2D. The study looks at the differences in activations, parameters, and gradients as well as the impact of the context length. Pre-trained models also do not acquire more knowledge about the input or the label during fine-tuning."
    },
    {
        "coherence": 0.22105059365274865,
        "consistency": 0.2628565226119726,
        "fluency": 0.20493925023751638,
        "relevance": 0.05228848029554438,
        "overall": 0.1852837116994455,
        "Generated": "300",
        "Gold": "The main result is that ERM leads to small expected Moreau loss. The results are not really novel, but the literature review is quite thorough. The connections to existing literature are very interesting and show the generality of their results. The main issue is that some of the assumptions are not transparent and seem to be difficult to understand. Some results are limited to linear multi-index models and gaussian covariate proportions.\nThe main result is that ERM leads to small expected Moreau loss. The results are not really novel, but the literature review is quite thorough. The connections to existing literature are very interesting and show the generality of their results. The main issue is that some of the assumptions are not transparent and seem to be difficult to understand. Some results are limited to linear multi-index models and gaussian covariate proportions."
    },
    {
        "coherence": 0.2409282624970793,
        "consistency": 0.4189992391597097,
        "fluency": 0.09321930315083293,
        "relevance": 0.040815910660818945,
        "overall": 0.19849067886711022,
        "Generated": "301",
        "Gold": "This paper proposes an algorithm for Heteogeneously Observable Imitation Learning. It is expensive to obtain paired samples that depict corresponding imitator and demonstrator states. The authors propose the Importance Weighting with Rejection (IWRE) to solve the dynamics mismatch with importance sampling. They conduct experiments on Mujoco and Atari games and their method gets the best imitation performance. The work has a good level of originality on the theoretical side.\nThis paper proposes an algorithm for Heteogeneously Observable Imitation Learning. It is expensive to obtain paired samples that depict corresponding imitator and demonstrator states. The authors propose the Importance Weighting with Rejection (IWRE) to solve the dynamics mismatch with importance sampling. They conduct experiments on Mujoco and Atari games and their method gets the best imitation performance. The work has a good level of originality on the theoretical side."
    },
    {
        "coherence": 0.05318073184770891,
        "consistency": 0.14575297246098237,
        "fluency": 0.12449841578790692,
        "relevance": 0.012901621743002672,
        "overall": 0.08408343545990023,
        "Generated": "302",
        "Gold": "M-layer is a neural network \"layer\" that consists in affinely transforming the input to a matrix, applying its matrix exponential, and applying another affine transformation. A single M-Layer compares favorably to neural ODEs as well as a very simplistic RELU network. Theoretical discussion is generally clear although some places are a bit hard to follow. The robustness result is something, but it follows from a very basic result on matrix exponentials.\nM-layer is a neural network \"layer\" that consists in affinely transforming the input to a matrix, applying its matrix exponential, and applying another affine transformation. A single M-Layer compares favorably to neural ODEs as well as a very simplistic RELU network. Theoretical discussion is generally clear although some places are a bit hard to follow. The robustness result is something, but it follows from a very basic result on matrix exponentials."
    },
    {
        "coherence": 0.08726398488348093,
        "consistency": 0.22319122229642138,
        "fluency": 0.08072371985660108,
        "relevance": 0.015086672190418246,
        "overall": 0.10156639980673042,
        "Generated": "303",
        "Gold": "The authors have built a platform for assisting medical residents and their supervising doctors in keeping track of their progress toward different milestones. The system was designed over multiple iterations with feedback from the residents and supervisors and was finally deployed and monitored for a period of four months to collect in-the-wild usage data. The motivation and context is sound, with references on how information visualization and dashboards support learning analytics or educational data visualization. The proposed methodology of design and development relies on well established practices.\nThe authors have built a platform for assisting medical residents and their supervising doctors in keeping track of their progress toward different milestones. The system was designed over multiple iterations with feedback from the residents and supervisors and was finally deployed and monitored for a period of four months to collect in-the-wild usage data. The motivation and context is sound, with references on how information visualization and dashboards support learning analytics or educational data visualization. The proposed methodology of design and development relies on well established practices."
    },
    {
        "coherence": 0.16612337310147954,
        "consistency": 0.21365309065559174,
        "fluency": 0.1552107160356705,
        "relevance": 0.026367030180998265,
        "overall": 0.14033855249343502,
        "Generated": "304",
        "Gold": "This paper proposes a discrete, structured latent variable model for visual question answering. This paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The authors compare the performance of V- NMN and NMN on SHAPES dataset. The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.Experiments performed on the SHAPes dataset show good performance compared to neural model networks.\nThis paper proposes a discrete, structured latent variable model for visual question answering. This paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The authors compare the performance of V- NMN and NMN on SHAPES dataset. The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.Experiments performed on the SHAPes dataset show good performance compared to neural model networks."
    },
    {
        "coherence": 0.23715879247452262,
        "consistency": 0.3302963693825252,
        "fluency": 0.07732481207975374,
        "relevance": 0.04365597177580853,
        "overall": 0.1721089864281525,
        "Generated": "305",
        "Gold": "This paper provides an empirical study of binary weight networks (BWNs) They find that there exists a subnetwork that stabiles early in training. They propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs. The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWN's and maybe BNNs. However, the paper seems to be incomplete and needs to be further improved.\nThis paper provides an empirical study of binary weight networks (BWNs) They find that there exists a subnetwork that stabiles early in training. They propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs. The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWN's and maybe BNNs. However, the paper seems to be incomplete and needs to be further improved."
    },
    {
        "coherence": 0.2229604121857697,
        "consistency": 0.3783195470019781,
        "fluency": 0.11261984656572895,
        "relevance": 0.029250394712057527,
        "overall": 0.18578755011638357,
        "Generated": "306",
        "Gold": "The paper presents several interesting insights about the behavior of adversarial training algorithms. The proposed methods are shown to out-perform PGD-AT and TRADES. They also discover that robust overfitting can occur with memorization and use temporal ensembling to mitigate it. However, the findings in the paper are disconnected and some of them lack explanations. The authors propose a mitigation algorithm by a regularization term for avoiding the excessive memorization.\nThe paper presents several interesting insights about the behavior of adversarial training algorithms. The proposed methods are shown to out-perform PGD-AT and TRADES. They also discover that robust overfitting can occur with memorization and use temporal ensembling to mitigate it. However, the findings in the paper are disconnected and some of them lack explanations. The authors propose a mitigation algorithm by a regularization term for avoiding the excessive memorization."
    },
    {
        "coherence": 0.19621552507844536,
        "consistency": 0.22353716239751828,
        "fluency": 0.11260272011548048,
        "relevance": 0.026557680468243628,
        "overall": 0.1397282720149219,
        "Generated": "307",
        "Gold": "The paper proposes a method for learning convolutional filters with trainable size. The learnable convolutions are called FlexConv, and a network that deploys them is called FlexNet. Anisotropic Gabor kernels are used within the multiplicative filter networks. The results obtained are higher than those reported by other existing works. The authors are expected to demonstrate the superiority of FlexNet on typical and practical tasks. The reviewer will keep the score.\nThe paper proposes a method for learning convolutional filters with trainable size. The learnable convolutions are called FlexConv, and a network that deploys them is called FlexNet. Anisotropic Gabor kernels are used within the multiplicative filter networks. The results obtained are higher than those reported by other existing works. The authors are expected to demonstrate the superiority of FlexNet on typical and practical tasks. The reviewer will keep the score."
    },
    {
        "coherence": 0.18296374159130935,
        "consistency": 0.2779145710806235,
        "fluency": 0.08475577638350645,
        "relevance": 0.018508082617644615,
        "overall": 0.14103554291827097,
        "Generated": "308",
        "Gold": "The HRL method uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation. The higher level policy is trained as usual by commanding lower level policies. Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out. This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers.\nThe HRL method uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation. The higher level policy is trained as usual by commanding lower level policies. Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out. This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers."
    },
    {
        "coherence": 0.1314992325200322,
        "consistency": 0.27375864381878556,
        "fluency": 0.11309599983644313,
        "relevance": 0.027250922107720888,
        "overall": 0.13640119957074542,
        "Generated": "309",
        "Gold": "This paper improves existing DRL-based CO methods in two terms. It leverage a continuous probabilistic space for the solutions, leading to a REINFORCEMENT-based training method which is more efficient than previous Q-learning or Actor-Critic methods. The key novelties and contributions are an RL-based approach to train the widely-used GCN model to generate probability heatmap, and a meta-learning approach to finetune the solutions at inference.\nThis paper improves existing DRL-based CO methods in two terms. It leverage a continuous probabilistic space for the solutions, leading to a REINFORCEMENT-based training method which is more efficient than previous Q-learning or Actor-Critic methods. The key novelties and contributions are an RL-based approach to train the widely-used GCN model to generate probability heatmap, and a meta-learning approach to finetune the solutions at inference."
    },
    {
        "coherence": 0.2059618746243247,
        "consistency": 0.2115429545910697,
        "fluency": 0.13107500950482776,
        "relevance": 0.03478581701983846,
        "overall": 0.14584141393501515,
        "Generated": "310",
        "Gold": "The paper tackles the important problem of generating realistic phantoms. The presented method shows better results than the other methods (SyN and Voxelmorph) The deformation field looks highly irregular and seems to have quite a lot of foldings. The paper was published in the Journal of Crystallography, a journal of the Crystallographic Society of America. For more information on this paper, visit the journal's website or go to www.crystallography.org.\nThe paper tackles the important problem of generating realistic phantoms. The presented method shows better results than the other methods (SyN and Voxelmorph) The deformation field looks highly irregular and seems to have quite a lot of foldings. The paper was published in the Journal of Crystallography, a journal of the Crystallographic Society of America. For more information on this paper, visit the journal's website or go to www.crystallography.org."
    },
    {
        "coherence": 0.08257224509004343,
        "consistency": 0.20065433180248007,
        "fluency": 0.0385442815778502,
        "relevance": 0.012756077359497216,
        "overall": 0.08363173395746773,
        "Generated": "311",
        "Gold": "This paper proposes a probabilistic model called Attention-Driven Variational Autoencoder (ADVAE) The model is composed of Transformers rather than previous neural architectures such as RNN. The authors aim to disentangle the semantics of latent variables according to some syntactic roles. While the research presented in this study is very exciting, it is not quite convincing enough yet for me to trust that the results are actually due to the modeling decisions presented.\nThis paper proposes a probabilistic model called Attention-Driven Variational Autoencoder (ADVAE) The model is composed of Transformers rather than previous neural architectures such as RNN. The authors aim to disentangle the semantics of latent variables according to some syntactic roles. While the research presented in this study is very exciting, it is not quite convincing enough yet for me to trust that the results are actually due to the modeling decisions presented."
    },
    {
        "coherence": 0.21826080941436468,
        "consistency": 0.2772385876726091,
        "fluency": 0.08488874522380764,
        "relevance": 0.008556570121299317,
        "overall": 0.14723617810802017,
        "Generated": "312",
        "Gold": "P-Adapters is a model that is between the embedding layer and first attention layer of LLMs. It takes LLM embeddings as input and output prompts used to query the LLM. Experimenta results show that P-Ad adapters perform comparably to the more complex MoE models. The authors advocate for the use of natural language prompts only. That is they do not require additional relation(s) that one can incorporate into the prompts.\nP-Adapters is a model that is between the embedding layer and first attention layer of LLMs. It takes LLM embeddings as input and output prompts used to query the LLM. Experimenta results show that P-Ad adapters perform comparably to the more complex MoE models. The authors advocate for the use of natural language prompts only. That is they do not require additional relation(s) that one can incorporate into the prompts."
    },
    {
        "coherence": 0.27915582739235895,
        "consistency": 0.29622057186632095,
        "fluency": 0.07267628213247498,
        "relevance": 0.019309557534230887,
        "overall": 0.16684055973134646,
        "Generated": "313",
        "Gold": "This paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning. The finding that regularization of the learned policy is particularly significant suffers from the lack of a recipe to perform this regularization offline. While the paper presents interesting insights, I am not sure how much these findings can be generalized. The algorithm under study is not described completely. Important hyper-parameters are missing. The basic steps of training the model is also unclear.\nThis paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning. The finding that regularization of the learned policy is particularly significant suffers from the lack of a recipe to perform this regularization offline. While the paper presents interesting insights, I am not sure how much these findings can be generalized. The algorithm under study is not described completely. Important hyper-parameters are missing. The basic steps of training the model is also unclear."
    },
    {
        "coherence": 0.15567133588736268,
        "consistency": 0.20414017430956052,
        "fluency": 0.10604719816210315,
        "relevance": 0.018876714056707638,
        "overall": 0.1211838556039335,
        "Generated": "314",
        "Gold": "The paper studies learning zero-sum Markov Games with kernel function approximation. They propose KernelCCE-VTR algorithm based on value-target regression. The paper provides sound and elegant regret bounds in terms of the effective dimension of the kernel. The work appears to be original and is mostly clear given the technical difficulty. It only requires the transition dynamic belongs to some RKHS. The regret upper bound is provided and it matches the best-known complexity.\nThe paper studies learning zero-sum Markov Games with kernel function approximation. They propose KernelCCE-VTR algorithm based on value-target regression. The paper provides sound and elegant regret bounds in terms of the effective dimension of the kernel. The work appears to be original and is mostly clear given the technical difficulty. It only requires the transition dynamic belongs to some RKHS. The regret upper bound is provided and it matches the best-known complexity."
    },
    {
        "coherence": 0.15568028229731573,
        "consistency": 0.18931063396154302,
        "fluency": 0.13820702462507728,
        "relevance": 0.021466559721560304,
        "overall": 0.12616612515137407,
        "Generated": "315",
        "Gold": "A TIM layer is proposed to replace the Transformer encoder layer, where a mechanism competition is introduced to suppress all but one group. The experiments on BERT pre-training and fine-tuning do not bring much to the table, as the text data is relatively uniform and does not benefit from the introduced structural inductive bias. The main thing that has been missing in this paper is the fine-details of the approaches. It is quite heuristical to use first two and last layers of Transformer to be vanilla.\nA TIM layer is proposed to replace the Transformer encoder layer, where a mechanism competition is introduced to suppress all but one group. The experiments on BERT pre-training and fine-tuning do not bring much to the table, as the text data is relatively uniform and does not benefit from the introduced structural inductive bias. The main thing that has been missing in this paper is the fine-details of the approaches. It is quite heuristical to use first two and last layers of Transformer to be vanilla."
    },
    {
        "coherence": 0.24175365989067865,
        "consistency": 0.35628205845007904,
        "fluency": 0.13569031634322393,
        "relevance": 0.02557275225591859,
        "overall": 0.18982469673497504,
        "Generated": "316",
        "Gold": "The method specifically focuses on the ability to extend molecular scaffolds (predefined subparts of a molecule) with structural motifs and individual atoms. The approach uses a library of motifs, extracted from the training data by breaking down molecules through acyclic bonds adjacent to a cycle. The method is set up in an autoencoder fashion with a GCN encoder. Using a scaffold as the initial seed can make sure the generated molecules contain the specified scaffold.\nThe method specifically focuses on the ability to extend molecular scaffolds (predefined subparts of a molecule) with structural motifs and individual atoms. The approach uses a library of motifs, extracted from the training data by breaking down molecules through acyclic bonds adjacent to a cycle. The method is set up in an autoencoder fashion with a GCN encoder. Using a scaffold as the initial seed can make sure the generated molecules contain the specified scaffold."
    },
    {
        "coherence": 0.16256733425056746,
        "consistency": 0.24209601286807358,
        "fluency": 0.06644344439756582,
        "relevance": 0.021904837667413066,
        "overall": 0.12325290729590499,
        "Generated": "317",
        "Gold": "This paper derives a novel lower bound for the GP posterior contraction rate when the true function has an additive structure. For any GP in such scenario, the contraction rate is worse than the minimax estimation rate of deep GP. The paper is significant enough for publication in this venue in my opinion. At a broad level all work in this area is somewhat abstracted from practical reality. This is a purely theoretical work. The authors have not discussed negative societal impact of their work.\nThis paper derives a novel lower bound for the GP posterior contraction rate when the true function has an additive structure. For any GP in such scenario, the contraction rate is worse than the minimax estimation rate of deep GP. The paper is significant enough for publication in this venue in my opinion. At a broad level all work in this area is somewhat abstracted from practical reality. This is a purely theoretical work. The authors have not discussed negative societal impact of their work."
    },
    {
        "coherence": 0.33618830523707066,
        "consistency": 0.3398651584665188,
        "fluency": 0.16434458320523768,
        "relevance": 0.03954256429828481,
        "overall": 0.21998515280177802,
        "Generated": "318",
        "Gold": "The motivation of this model is that, in a company or a department, workers many need to work in pairs. The motivating example of \"pairing up employees\" is really weak and sounds made-up to fit the problem definition. Unfortunately, the algorithms and analysis is just a rehashing of existing ideas. The authors study MMS and EF1 allocations, and the design of approximation algorithm for optimizing the social welfare. The goal is to allocate (or partially allocate) the nodes to n agents.\nThe motivation of this model is that, in a company or a department, workers many need to work in pairs. The motivating example of \"pairing up employees\" is really weak and sounds made-up to fit the problem definition. Unfortunately, the algorithms and analysis is just a rehashing of existing ideas. The authors study MMS and EF1 allocations, and the design of approximation algorithm for optimizing the social welfare. The goal is to allocate (or partially allocate) the nodes to n agents."
    },
    {
        "coherence": 0.11941619057255425,
        "consistency": 0.2328110498434306,
        "fluency": 0.13593234798319875,
        "relevance": 0.018947038647338245,
        "overall": 0.12677665676163047,
        "Generated": "319",
        "Gold": "This paper attempts to address the semi-supervised learning topic by proposing a method based on an aggregated loss considering both cross-entry and Davies-Bouldin Index. The experiments section of the paper has some shortcomings as well. The datasets used with the exception of MNIST are not standard ones used in recent semi- supervised learning papers. The comparison studies reported in Fig. 4 are not clearly documented and possibly are not fair either. A correct method may not necessarily be effective, which would require an extensive evaluation.\nThis paper attempts to address the semi-supervised learning topic by proposing a method based on an aggregated loss considering both cross-entry and Davies-Bouldin Index. The experiments section of the paper has some shortcomings as well. The datasets used with the exception of MNIST are not standard ones used in recent semi- supervised learning papers. The comparison studies reported in Fig. 4 are not clearly documented and possibly are not fair either. A correct method may not necessarily be effective, which would require an extensive evaluation."
    },
    {
        "coherence": 0.4256709721649623,
        "consistency": 0.47377919137093566,
        "fluency": 0.23383034564708174,
        "relevance": 0.06455194536988196,
        "overall": 0.29945811363821545,
        "Generated": "320",
        "Gold": "The proposed framework of DATASIFTER is interesting and novel with promising results. The method primarily depends on a set selection phase for which no empirical/theoretical evidence is provided. Models considered in the work are small in size as compared to state-of-the-art models. Theoretically shows that Shapley value based methods has small domination ratio. There is no theoretical analysis of DataSifter. The paper was well-structured, easy to follow, and presented ideas clearly.\nThe proposed framework of DATASIFTER is interesting and novel with promising results. The method primarily depends on a set selection phase for which no empirical/theoretical evidence is provided. Models considered in the work are small in size as compared to state-of-the-art models. Theoretically shows that Shapley value based methods has small domination ratio. There is no theoretical analysis of DataSifter. The paper was well-structured, easy to follow, and presented ideas clearly."
    },
    {
        "coherence": 0.11354652110038023,
        "consistency": 0.19419515834027998,
        "fluency": 0.17566584702790883,
        "relevance": 0.011342116879529572,
        "overall": 0.12368741083702466,
        "Generated": "321",
        "Gold": "Paper is on modeling the prediction of ancestor relation between names of science institutions. The proposed approach is set-based models (with neural encodings) where the overlap between two names is measured by set overlap at the unigram level. The paper shows how to infer the organisational structure of an institution. It presents a model for predicting the is-ancestor relationships of institutions based on their string names. The experimental evaluation is on a single dataset only.\nPaper is on modeling the prediction of ancestor relation between names of science institutions. The proposed approach is set-based models (with neural encodings) where the overlap between two names is measured by set overlap at the unigram level. The paper shows how to infer the organisational structure of an institution. It presents a model for predicting the is-ancestor relationships of institutions based on their string names. The experimental evaluation is on a single dataset only."
    },
    {
        "coherence": 0.077643316665879,
        "consistency": 0.13575944288283961,
        "fluency": 0.3090674039773602,
        "relevance": 0.013320873074263597,
        "overall": 0.1339477591500856,
        "Generated": "322",
        "Gold": "This submission focuses on the cold start problem of new entities. It combines the strengths of a new entity and the existing entities, and the content features of the new entity. The proposed method outperforms several intuitive naïve strategies as well as MAML. It uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. It outperforms a wide range of baselines on MovieLens-1M and a medical synthetic dataset.\nThis submission focuses on the cold start problem of new entities. It combines the strengths of a new entity and the existing entities, and the content features of the new entity. The proposed method outperforms several intuitive naïve strategies as well as MAML. It uses a new feature's meta-information and observed samples for the features to predict the network weights for predicting the feature value from other features. It outperforms a wide range of baselines on MovieLens-1M and a medical synthetic dataset."
    },
    {
        "coherence": 0.07562959731159735,
        "consistency": 0.16211316158305544,
        "fluency": 0.1616444252190834,
        "relevance": 0.026527101713518485,
        "overall": 0.10647857145681366,
        "Generated": "323",
        "Gold": "This paper considers the problem of adapting the learning rate and smoothing parameter in the Directional Gaussian Smoothing (DGS) algorithm. The authors claim DGS is particularly sensitive to these parameters and thus attribute the strong experimental results to these changes. The paper is heavily based on [Zhang et al., 2020], where the approach of building a \"truly non-local gradient\" is brought forward. The proposed automated tuning approach is supported by a large set of experiments.\nThis paper considers the problem of adapting the learning rate and smoothing parameter in the Directional Gaussian Smoothing (DGS) algorithm. The authors claim DGS is particularly sensitive to these parameters and thus attribute the strong experimental results to these changes. The paper is heavily based on [Zhang et al., 2020], where the approach of building a \"truly non-local gradient\" is brought forward. The proposed automated tuning approach is supported by a large set of experiments."
    },
    {
        "coherence": 0.20822284878686426,
        "consistency": 0.38453003759904253,
        "fluency": 0.32565402322449544,
        "relevance": 0.026824595234373534,
        "overall": 0.23630787621119395,
        "Generated": "324",
        "Gold": "This paper proposes a new data distillation approach based on neural feature regression that is similar to a truncated backgprop through time using a pool of models. The approach sets a new state-of-the-art results both in terms of accuracy and training efficiency. This work avoids using a surrogate objective like previous works (DSAm DM, MTT) While also circumventing the computational constraints of other methods that optimize the true objective (DD, KIP)\nThis paper proposes a new data distillation approach based on neural feature regression that is similar to a truncated backgprop through time using a pool of models. The approach sets a new state-of-the-art results both in terms of accuracy and training efficiency. This work avoids using a surrogate objective like previous works (DSAm DM, MTT) While also circumventing the computational constraints of other methods that optimize the true objective (DD, KIP)"
    },
    {
        "coherence": 0.29254618054157483,
        "consistency": 0.2316214163611016,
        "fluency": 0.3822900731508511,
        "relevance": 0.023434111381824694,
        "overall": 0.23247294535883808,
        "Generated": "325",
        "Gold": "The paper claims that a lot of previous work in QA-based MRC focused only on entity-aware common sense knowledge. To overcome this, the paper proposes to build a finer-grained local fact, (entity, predicate, entity) triplet, and then connect the triplet nodes with global information such as coreference. In the experiment section, the authors show that the proposed FOCAL REASONER outperforms the previous SOTA models.\nThe paper claims that a lot of previous work in QA-based MRC focused only on entity-aware common sense knowledge. To overcome this, the paper proposes to build a finer-grained local fact, (entity, predicate, entity) triplet, and then connect the triplet nodes with global information such as coreference. In the experiment section, the authors show that the proposed FOCAL REASONER outperforms the previous SOTA models."
    },
    {
        "coherence": 0.0476365021791688,
        "consistency": 0.06157083660171235,
        "fluency": 0.31400554330333197,
        "relevance": 0.01153051902187987,
        "overall": 0.10868585027652325,
        "Generated": "326",
        "Gold": "This paper combines elements of two existing reinforcement learning approaches, namely, Deep Q-learning Networks (DQN) with Prioritised Experience Replay (PER) and Deep Deterministic Policy Gradient (DDPG) The algorithm has a limited novelty with a simple modification of the DDPG algorithm to add the PER component. Empirical evaluations of the proposed algorithm are conducted on Mujoco benchmarks while the results are mixed. The submission is below the novelty threshold for a publication at ICLR.\nThis paper combines elements of two existing reinforcement learning approaches, namely, Deep Q-learning Networks (DQN) with Prioritised Experience Replay (PER) and Deep Deterministic Policy Gradient (DDPG) The algorithm has a limited novelty with a simple modification of the DDPG algorithm to add the PER component. Empirical evaluations of the proposed algorithm are conducted on Mujoco benchmarks while the results are mixed. The submission is below the novelty threshold for a publication at ICLR."
    },
    {
        "coherence": 0.18296728297139286,
        "consistency": 0.3854730445007978,
        "fluency": 0.3046071665896714,
        "relevance": 0.023046543387673393,
        "overall": 0.22402350936238385,
        "Generated": "327",
        "Gold": "This work targets the problem of populating the buffer memory in the Experience Replay (ER)-based paradigm. In ER, we are allowed to store a small subset of the incoming data stream, therefore deciding on a good policy to populate the memory buffer is important. The paper proposes a new criterion to pick informative points and void outliers. The performance may critically depend on balancing how much we want to “surprise” vs “learn”\nThis work targets the problem of populating the buffer memory in the Experience Replay (ER)-based paradigm. In ER, we are allowed to store a small subset of the incoming data stream, therefore deciding on a good policy to populate the memory buffer is important. The paper proposes a new criterion to pick informative points and void outliers. The performance may critically depend on balancing how much we want to “surprise” vs “learn”"
    },
    {
        "coherence": 0.23523835130514054,
        "consistency": 0.5406764456413347,
        "fluency": 0.10793952879385328,
        "relevance": 0.01897633744222624,
        "overall": 0.2257076657956387,
        "Generated": "328",
        "Gold": "This paper proposes an image enhancement method using an unpaired-GAN based translation network (cycleGAN) It combines geometric, lighting consistency, and a contextual loss criterion. The main weakness of the paper is that none of the individual contributions is individually novel. The work is incremental and uses already existing building blocks. The results do not show any significant improvement over those of the existing methods. This paper proposes a low light image Enhancement method. It brings geometric and lighting prior to serve as assistant to the supervision. It also separate their discriminators for color, texture and edges.\nThis paper proposes an image enhancement method using an unpaired-GAN based translation network (cycleGAN) It combines geometric, lighting consistency, and a contextual loss criterion. The main weakness of the paper is that none of the individual contributions is individually novel. The work is incremental and uses already existing building blocks. The results do not show any significant improvement over those of the existing methods. This paper proposes a low light image Enhancement method. It brings geometric and lighting prior to serve as assistant to the supervision. It also separate their discriminators for color, texture and edges."
    },
    {
        "coherence": 0.3098193061738798,
        "consistency": 0.41123008884635637,
        "fluency": 0.17375694361059077,
        "relevance": 0.030981886496235934,
        "overall": 0.2314470562817657,
        "Generated": "329",
        "Gold": "The paper presents an attention-based approach to learning a policy for solving TSP and other routing-type combinatorial optimization problems. The attention model is inspired by the Transformer architecture of Vaswani et al. (2017) The writing is clear and sharp. The experimental results are clearly presented and well-illustrated. The theoretical contributions are not ground-breaking. Although it could be quite interesting, as it is it's not very well motivated.\nThe paper presents an attention-based approach to learning a policy for solving TSP and other routing-type combinatorial optimization problems. The attention model is inspired by the Transformer architecture of Vaswani et al. (2017) The writing is clear and sharp. The experimental results are clearly presented and well-illustrated. The theoretical contributions are not ground-breaking. Although it could be quite interesting, as it is it's not very well motivated."
    },
    {
        "coherence": 0.20442005613779698,
        "consistency": 0.421578033840186,
        "fluency": 0.06882537870730451,
        "relevance": 0.03317669982002601,
        "overall": 0.1820000421263284,
        "Generated": "330",
        "Gold": "The authors propose a new method to do counterfactual inference on time series data in healthcare. This is done by modelling interventions in continuous time with differential equations augmented by auxiliary confounding variables to reduce bias. They demonstrate the proposed method on tumor growth simulation and sepsis patient treatment response. The experiments presented appear promising to the time series inference problem. The results in Table 1 are underwhelming considering the error bars of the results relative to the simpler TSD-RMSN.\nThe authors propose a new method to do counterfactual inference on time series data in healthcare. This is done by modelling interventions in continuous time with differential equations augmented by auxiliary confounding variables to reduce bias. They demonstrate the proposed method on tumor growth simulation and sepsis patient treatment response. The experiments presented appear promising to the time series inference problem. The results in Table 1 are underwhelming considering the error bars of the results relative to the simpler TSD-RMSN."
    },
    {
        "coherence": 0.24786699341121457,
        "consistency": 0.2672552464965389,
        "fluency": 0.10136531407433205,
        "relevance": 0.023921676352368765,
        "overall": 0.16010230758361357,
        "Generated": "331",
        "Gold": "The paper studies the problem of robust machine learning, where the labels of the a fraction of samples are arbitrarily corrupted. The paper proposes an algorithm to tackle this problem and evaluates it on a standard datasets. The experiments suggest that the proposed algorithm is better than the baselines. The theoretical results make sense, but there lack detailed proofs to make this paper self-contain. The novelty of the theorems may be overclaimed. The related work section misses MANY related results on corrupted data and robust mean estimation.\nThe paper studies the problem of robust machine learning, where the labels of the a fraction of samples are arbitrarily corrupted. The paper proposes an algorithm to tackle this problem and evaluates it on a standard datasets. The experiments suggest that the proposed algorithm is better than the baselines. The theoretical results make sense, but there lack detailed proofs to make this paper self-contain. The novelty of the theorems may be overclaimed. The related work section misses MANY related results on corrupted data and robust mean estimation."
    },
    {
        "coherence": 0.21276426498398043,
        "consistency": 0.28358151127831716,
        "fluency": 0.3163445880013535,
        "relevance": 0.060577930243870474,
        "overall": 0.2183170736268804,
        "Generated": "332",
        "Gold": "Simulation Transformer (SiT) is a transformer based approach for particle-based fluid simulations. SiT models are much more light-weight (i.e., far fewer parameters) SiT is evaluated in four simulated environments: FluidFall, FluidShake, BoxBath, RiceGrip. The mechanism does not offer the main advantage of transformers which is not having to require to compute specialized edge level embeddings, which are expensive."
    },
    {
        "coherence": 0.07401139112945211,
        "consistency": 0.19220134061251604,
        "fluency": 0.1522496546908842,
        "relevance": 0.014448409613525295,
        "overall": 0.1082276990115944,
        "Generated": "333",
        "Gold": "The authors propose a learning-based approach for image clustering. The idea is reasonable and the method seems sound, however a similar approach has been proposed in Zhuang et al., ICCV'19 (Local Aggregation for Unsupervised Learning of Visual Embeddings) The writing is in general clear and undestandable. The description of the algorithm is often too vague and fragmented in mini-sections. The paper in its present form is not ready to be published though.\nThe authors propose a learning-based approach for image clustering. The idea is reasonable and the method seems sound, however a similar approach has been proposed in Zhuang et al., ICCV'19 (Local Aggregation for Unsupervised Learning of Visual Embeddings) The writing is in general clear and undestandable. The description of the algorithm is often too vague and fragmented in mini-sections. The paper in its present form is not ready to be published though."
    },
    {
        "coherence": 0.10124966791734988,
        "consistency": 0.19891921962766967,
        "fluency": 0.10288276197163111,
        "relevance": 0.021173742487881207,
        "overall": 0.10605634800113298,
        "Generated": "334",
        "Gold": "Squeezeformer is a novel hybrid attention-convolution architecture for ASR through a series of extensive architectural studies and improvements over the Conformer architecture. Conformer has been the de-facto architecture for E2E speech processing tasks. The authors have open sourced the code and trained model checkpoints which should be supremely helpful to the community. The paper finds experimentally that the subsequent activations are are highly correlated in the higher confomer blocks.\nSqueezeformer is a novel hybrid attention-convolution architecture for ASR through a series of extensive architectural studies and improvements over the Conformer architecture. Conformer has been the de-facto architecture for E2E speech processing tasks. The authors have open sourced the code and trained model checkpoints which should be supremely helpful to the community. The paper finds experimentally that the subsequent activations are are highly correlated in the higher confomer blocks."
    },
    {
        "coherence": 0.14407230801959364,
        "consistency": 0.22027605270706746,
        "fluency": 0.25395640620958476,
        "relevance": 0.01937930308324258,
        "overall": 0.1594210175048721,
        "Generated": "335",
        "Gold": "The paper studies perfect sampling from pairwise comparisons. The algorithm doesn't need to know the distribution Q, but the complexity guarantees will depend on the structure of Q. The major technical innovation is a new variant of the Coupling from the Past (CFTP) algorithm that allows the perfect sampling. The submission was reorganised from a longer paper to fit within NeurIPS's strict page limits. The running time depends on the spectral gap of the Markov chain M which depends on Q.\nThe paper studies perfect sampling from pairwise comparisons. The algorithm doesn't need to know the distribution Q, but the complexity guarantees will depend on the structure of Q. The major technical innovation is a new variant of the Coupling from the Past (CFTP) algorithm that allows the perfect sampling. The submission was reorganised from a longer paper to fit within NeurIPS's strict page limits. The running time depends on the spectral gap of the Markov chain M which depends on Q."
    },
    {
        "coherence": 0.17859282639526042,
        "consistency": 0.2582079180690609,
        "fluency": 0.3229663623701008,
        "relevance": 0.024091595158101796,
        "overall": 0.19596467549813096,
        "Generated": "336",
        "Gold": "This paper tries to adapt the concept of averaging to GAN training. In a simple min-max example the iterates obtained by gradient method do not converge to the equilibrium of the game but their average does. Experiments confirmed high performance of averaging. The theoretical underpinnings in Section 3.1 are quite thin, and focus on describing a bilinear saddle problem. This work first provides intuitions on the potential benefits of exponential moving average (EMA) on a simple illustrative example.\nThis paper tries to adapt the concept of averaging to GAN training. In a simple min-max example the iterates obtained by gradient method do not converge to the equilibrium of the game but their average does. Experiments confirmed high performance of averaging. The theoretical underpinnings in Section 3.1 are quite thin, and focus on describing a bilinear saddle problem. This work first provides intuitions on the potential benefits of exponential moving average (EMA) on a simple illustrative example."
    },
    {
        "coherence": 0.1268792440609132,
        "consistency": 0.1718504670618782,
        "fluency": 0.21507290415728564,
        "relevance": 0.009507430965911095,
        "overall": 0.13082751156149702,
        "Generated": "337",
        "Gold": "The authors present a new dataset of 9,280 expert-authored summaries from large-scale civil rights lawsuits. They experiment with state-of-the-art models for abstractive summarization (BART, Pegasus, LED, and PRIMERA) The results indicate that exiting popular summarization models cannot achieve satisfying performances. The existence of multiple levels of granularity is a very nice feature. The experiments are well done and reasonable in light of the claims made by the paper.\nThe authors present a new dataset of 9,280 expert-authored summaries from large-scale civil rights lawsuits. They experiment with state-of-the-art models for abstractive summarization (BART, Pegasus, LED, and PRIMERA) The results indicate that exiting popular summarization models cannot achieve satisfying performances. The existence of multiple levels of granularity is a very nice feature. The experiments are well done and reasonable in light of the claims made by the paper."
    },
    {
        "coherence": 0.21665741489513907,
        "consistency": 0.32921500757049704,
        "fluency": 0.24731338282509932,
        "relevance": 0.012160016448626574,
        "overall": 0.2013364554348405,
        "Generated": "338",
        "Gold": "Theoretically, it shows that a GAN with over-parameterized 1-layer neural network generator and a linear discriminator can converge to global saddle point via stochastic optimisation. Similar results are obtained for nonlinear generators and discriminators under some conditions. The paper does not provide any bound on the gap between training error and test error. The experimental results cannot fully validate the authors' claims. It is not clear to me why Figure 1 should concern \"overparametrization\" rather than model complexity.\nTheoretically, it shows that a GAN with over-parameterized 1-layer neural network generator and a linear discriminator can converge to global saddle point via stochastic optimisation. Similar results are obtained for nonlinear generators and discriminators under some conditions. The paper does not provide any bound on the gap between training error and test error. The experimental results cannot fully validate the authors' claims. It is not clear to me why Figure 1 should concern \"overparametrization\" rather than model complexity."
    },
    {
        "coherence": 0.1915684971315718,
        "consistency": 0.1809581241683607,
        "fluency": 0.205915173565512,
        "relevance": 0.027540524632848964,
        "overall": 0.15149557987457338,
        "Generated": "339",
        "Gold": "Research topic is interesting. Understanding the tradeoff between privacy, fairness, and accuracy is essential for the development of trustworthy algorithms. Supportive experiments look sound and serve as a good explanation for the theorems. Findings are supported with strong theoretical proofs and empirical results. I think to get more conclusive findings, the authors need to experiment with other popular fairness notions. Theorem 2 and its proof is included in Appendix A. Related work on state of the art regarding the trade-off between accuracy and fairness.\nResearch topic is interesting. Understanding the tradeoff between privacy, fairness, and accuracy is essential for the development of trustworthy algorithms. Supportive experiments look sound and serve as a good explanation for the theorems. Findings are supported with strong theoretical proofs and empirical results. I think to get more conclusive findings, the authors need to experiment with other popular fairness notions. Theorem 2 and its proof is included in Appendix A. Related work on state of the art regarding the trade-off between accuracy and fairness."
    },
    {
        "coherence": 0.23840444894401702,
        "consistency": 0.38826313302012655,
        "fluency": 0.05459602872535905,
        "relevance": 0.05617510912783501,
        "overall": 0.18435967995433442,
        "Generated": "340",
        "Gold": "Conformal prediction is similar to multi-label classification, but with a statistical sound way of thresholding each (class-specific) classifier. The concrete, deep implementation of the approach is rather straightforward and substandard for ICLR. Since a validation set is used to compute the quantiles, substantial 'power' is lost. The authors seem to be the first to attempt multiclass classification. They effectively build an independent classifier for each class that estimates whether an example comes from that class.\nConformal prediction is similar to multi-label classification, but with a statistical sound way of thresholding each (class-specific) classifier. The concrete, deep implementation of the approach is rather straightforward and substandard for ICLR. Since a validation set is used to compute the quantiles, substantial 'power' is lost. The authors seem to be the first to attempt multiclass classification. They effectively build an independent classifier for each class that estimates whether an example comes from that class."
    },
    {
        "coherence": 0.22643233064998286,
        "consistency": 0.24002277580834822,
        "fluency": 0.11707132053314562,
        "relevance": 0.02442610129209685,
        "overall": 0.1519881320708934,
        "Generated": "341",
        "Gold": "The paper studies batch (offline) learning in linear contextual bandits. They analyze a class of pessimistic learning rules indexed by different l_p norms. They show that each learning rule under a norm is minimax optimal under the specific norm. Among them, the algorithm based on $\\ell_\\infty$ confidence set has the smallest suboptimality gap. It also proves lower bounds for classes of linear contextualBandit problems. It provides a novel family of pessimisticlearning rules that generalizes over the Bellman--consistent pessimism and lower confidence bound strategies.\nThe paper studies batch (offline) learning in linear contextual bandits. They analyze a class of pessimistic learning rules indexed by different l_p norms. They show that each learning rule under a norm is minimax optimal under the specific norm. Among them, the algorithm based on $\\ell_\\infty$ confidence set has the smallest suboptimality gap. It also proves lower bounds for classes of linear contextualBandit problems. It provides a novel family of pessimisticlearning rules that generalizes over the Bellman--consistent pessimism and lower confidence bound strategies."
    },
    {
        "coherence": 0.07414496042686762,
        "consistency": 0.18107982113074167,
        "fluency": 0.22536401255068375,
        "relevance": 0.018629105846766438,
        "overall": 0.12480447498876486,
        "Generated": "342",
        "Gold": "This paper presents a method to denoise low-dose CT images. The method operates on the image domain as well as on the spatial frequency domain. The authors show that the combination of networks operating in the image and spatial frequency domains leads to better denoising results. The paper is straight forward, well-structured and the aims, methods and results and discussion are interesting, informative and clearly presented. The proposed work only compared vertically with different compositon of the I and F Unet.\nThis paper presents a method to denoise low-dose CT images. The method operates on the image domain as well as on the spatial frequency domain. The authors show that the combination of networks operating in the image and spatial frequency domains leads to better denoising results. The paper is straight forward, well-structured and the aims, methods and results and discussion are interesting, informative and clearly presented. The proposed work only compared vertically with different compositon of the I and F Unet."
    },
    {
        "coherence": 0.16523497266700976,
        "consistency": 0.21152853622101475,
        "fluency": 0.12827077120196193,
        "relevance": 0.013091666622358495,
        "overall": 0.12953148667808623,
        "Generated": "343",
        "Gold": "The paper targets to demonstrate social perception and human-AI collaboration in common household activities. It shows the development of a multi-agent virtual environment that is used to test an AI agent’s ability to reason. The authors used a combination of existing techniques to solve the problem rather than coming up with new learning techniques from scratch. The work is significant to researchers interested in modelling social intelligence. The paper was published in the journal Autonomous Agents and Multi-Agent Systems (ACMAS)\nThe paper targets to demonstrate social perception and human-AI collaboration in common household activities. It shows the development of a multi-agent virtual environment that is used to test an AI agent’s ability to reason. The authors used a combination of existing techniques to solve the problem rather than coming up with new learning techniques from scratch. The work is significant to researchers interested in modelling social intelligence. The paper was published in the journal Autonomous Agents and Multi-Agent Systems (ACMAS)"
    },
    {
        "coherence": 0.14771858768584187,
        "consistency": 0.3166284361459403,
        "fluency": 0.19600350592974589,
        "relevance": 0.027536882968426202,
        "overall": 0.17197185318248856,
        "Generated": "344",
        "Gold": "In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to “denoise” a recurrent neural network. The idea is that for every normal step of an RNN, one induces an additional \"dimension\" of recurrency in order to create attractor dynamics around that particular hidden state. I think the work is quite novel, and it also connects to bodies of literature (hopfield networks -- attractors based and more mainstream GRU/LSTM nets)\nIn this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to “denoise” a recurrent neural network. The idea is that for every normal step of an RNN, one induces an additional \"dimension\" of recurrency in order to create attractor dynamics around that particular hidden state. I think the work is quite novel, and it also connects to bodies of literature (hopfield networks -- attractors based and more mainstream GRU/LSTM nets)"
    },
    {
        "coherence": 0.18830206208735353,
        "consistency": 0.20994048786166425,
        "fluency": 0.24091324323934474,
        "relevance": 0.009754842496619927,
        "overall": 0.1622276589212456,
        "Generated": "345",
        "Gold": "Anchor-free approach with attention mechanism for polyp detection. Using attention mechanism, the architecture removes the setting of priors, that is, anchors. The architecture bases on the body of ResNet-50 and the head of FCOS. The evaluation did not show clear benefits of the proposed approach. In Table 2 and 4 there is not a clear tendency to better performance. The biggest strength of the paper is clearly the development of a large-scale polyp pathology dataset.\nAnchor-free approach with attention mechanism for polyp detection. Using attention mechanism, the architecture removes the setting of priors, that is, anchors. The architecture bases on the body of ResNet-50 and the head of FCOS. The evaluation did not show clear benefits of the proposed approach. In Table 2 and 4 there is not a clear tendency to better performance. The biggest strength of the paper is clearly the development of a large-scale polyp pathology dataset."
    },
    {
        "coherence": 0.2199236174286068,
        "consistency": 0.32174986633991187,
        "fluency": 0.23947549045195823,
        "relevance": 0.026066943370324086,
        "overall": 0.20180397939770026,
        "Generated": "346",
        "Gold": "In this paper, the authors propose a modification to standard neural networks used for object classification tasks to incorporate “introspective learning’ This consists on training a multi-layer perceptron (MLP) on the neural network introspective features. Experiments show slight improvements in classification accuracy and cuts of calibration errors. Results on CIFAR10-Corrupted are very promising, and the authors further show the method is able to improve in active learning and OOD detection.\nIn this paper, the authors propose a modification to standard neural networks used for object classification tasks to incorporate “introspective learning’ This consists on training a multi-layer perceptron (MLP) on the neural network introspective features. Experiments show slight improvements in classification accuracy and cuts of calibration errors. Results on CIFAR10-Corrupted are very promising, and the authors further show the method is able to improve in active learning and OOD detection."
    },
    {
        "coherence": 0.23361785126893092,
        "consistency": 0.3362679998972384,
        "fluency": 0.2671906681697176,
        "relevance": 0.009339264852245666,
        "overall": 0.21160394604703314,
        "Generated": "347",
        "Gold": "RepB-SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy. The experiments demonstrate the effectiveness of RepB-BDE over almost 10 baselines in the popular offline RL benchmark D4RL. It builds on \"Representation Balancing MDP (RepBM)\" and tries to improve the procedure by \"stationary DIstribution Correction Estimation (DICE)\nRepB-SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy. The experiments demonstrate the effectiveness of RepB-BDE over almost 10 baselines in the popular offline RL benchmark D4RL. It builds on \"Representation Balancing MDP (RepBM)\" and tries to improve the procedure by \"stationary DIstribution Correction Estimation (DICE)"
    },
    {
        "coherence": 0.20321793534829666,
        "consistency": 0.2621679333655924,
        "fluency": 0.3137012471425895,
        "relevance": 0.03168435770530475,
        "overall": 0.20269286839044584,
        "Generated": "348",
        "Gold": "This paper studies the relationship between disentanglement and multi-task learning. The number of learning tasks and also the relevance of different tasks could matter. The authors try to address one of my questions on the number of tasks, but the conclusion is inconclusive. Overall the main claim of the paper is interesting and moderately well supported. The analysis goes through multiple different network architectures and tabulates different metrics, and is reasonably thorough. The visualizations are also clear and fairly convincing.\nThis paper studies the relationship between disentanglement and multi-task learning. The number of learning tasks and also the relevance of different tasks could matter. The authors try to address one of my questions on the number of tasks, but the conclusion is inconclusive. Overall the main claim of the paper is interesting and moderately well supported. The analysis goes through multiple different network architectures and tabulates different metrics, and is reasonably thorough. The visualizations are also clear and fairly convincing."
    },
    {
        "coherence": 0.2492038245153606,
        "consistency": 0.32990775281828805,
        "fluency": 0.1829467569106071,
        "relevance": 0.04073532057967284,
        "overall": 0.20069841370598215,
        "Generated": "349",
        "Gold": "This work proposes a sparsity enforcing structure for GANs by splitting layers into the generator for the sparse vector and for the final image. The proposed method yielded improved performance over conventional methods in FID, IS for generators and PSNR for denoising with DIP. In practice it mostly boils down to encouraging sparsity in the last layer activations. The promised understanding of the network internals is forgotten in practice. The method is relatively simple and it could possibly be widely adopted if the experiments in the paper generalize to a wider range of problems.\nThis work proposes a sparsity enforcing structure for GANs by splitting layers into the generator for the sparse vector and for the final image. The proposed method yielded improved performance over conventional methods in FID, IS for generators and PSNR for denoising with DIP. In practice it mostly boils down to encouraging sparsity in the last layer activations. The promised understanding of the network internals is forgotten in practice. The method is relatively simple and it could possibly be widely adopted if the experiments in the paper generalize to a wider range of problems."
    },
    {
        "coherence": 0.17015520507552376,
        "consistency": 0.1905237051023292,
        "fluency": 0.36991462638380973,
        "relevance": 0.05747400745000964,
        "overall": 0.19701688600291808,
        "Generated": "350",
        "Gold": "The authors prove the (cc-) universality property of group convolutional neural networks (GCNNs) with one hidden layer using the tools from ridglet theory. While the work seems rigorous, I think it misses some motivation and an analysis of the theoretical results. The paper provides a proper amount of details and explanations. However, I found the paper a bit hard to follow. The main results showed the cc-universality of GCNNs in a constructive and unified manner.\nThe authors prove the (cc-) universality property of group convolutional neural networks (GCNNs) with one hidden layer using the tools from ridglet theory. While the work seems rigorous, I think it misses some motivation and an analysis of the theoretical results. The paper provides a proper amount of details and explanations. However, I found the paper a bit hard to follow. The main results showed the cc-universality of GCNNs in a constructive and unified manner."
    },
    {
        "coherence": 0.06396331385434639,
        "consistency": 0.146119082771703,
        "fluency": 0.11250109235938967,
        "relevance": 0.017323684326805745,
        "overall": 0.0849767933280612,
        "Generated": "351",
        "Gold": "The paper proposes a method to correct high-frequencies details in the textures of animated clothes. The main idea is to train a network to learn the 2D offset in the UV space for a given pose and view. Once texture perturbations are recovered from at least two unique camera views, 3D geometry can then be reconstructed to recover high-frequency wrinkles. The paper shows results using a t-shirt, on interpolating to novel views and 3D reconstruction.\nThe paper proposes a method to correct high-frequencies details in the textures of animated clothes. The main idea is to train a network to learn the 2D offset in the UV space for a given pose and view. Once texture perturbations are recovered from at least two unique camera views, 3D geometry can then be reconstructed to recover high-frequency wrinkles. The paper shows results using a t-shirt, on interpolating to novel views and 3D reconstruction."
    },
    {
        "coherence": 0.09654379502883213,
        "consistency": 0.21025342106154118,
        "fluency": 0.0848209472055104,
        "relevance": 0.014377199927657881,
        "overall": 0.1014988408058854,
        "Generated": "352",
        "Gold": "The paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives.\nThe paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives."
    },
    {
        "coherence": 0.07418193390989332,
        "consistency": 0.25519269598178146,
        "fluency": 0.11712966993909188,
        "relevance": 0.010803838009227684,
        "overall": 0.11432703445999859,
        "Generated": "353",
        "Gold": "HW-NAS is an important area of research, in particular for bringing powerful DL models to edge devices and for reducing energy consumption. The proposed HW-NAS-Bench fills an important gap and can prove to be very useful for practitioners and HW- NAS researchers. The paper promises to release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet.\nHW-NAS is an important area of research, in particular for bringing powerful DL models to edge devices and for reducing energy consumption. The proposed HW-NAS-Bench fills an important gap and can prove to be very useful for practitioners and HW- NAS researchers. The paper promises to release inference time and power usage measurements and code for 5-6 different hardware devices on two existing NAS benchmark tasks: NASBench-201 and FBNet."
    },
    {
        "coherence": 0.2750360934944445,
        "consistency": 0.3428274425610097,
        "fluency": 0.15416880649688217,
        "relevance": 0.07673435693652547,
        "overall": 0.21219167487221546,
        "Generated": "354",
        "Gold": "The paper focuses on the behavior of gradient descent in one-hidden-layer neural networks (with fixed second layer weights) In this setup, the labels are generated by an unknown teacher network with the same architecture, and the student aims to recover the teacher weights. The main techniques used in this paper seem to be based on existing approaches in Fu et al, 2020 and Zhong etal, 2017. I think the authors may consider adding a few more relevant references about theoretical aspect of Gaussian mixtures."
    },
    {
        "coherence": 0.10409043709936472,
        "consistency": 0.23275371334461248,
        "fluency": 0.22731373579141256,
        "relevance": 0.026153026536360157,
        "overall": 0.1475777281929375,
        "Generated": "355",
        "Gold": "The authors identify correctly weaknesses in their method, i.e. overfitting of the networks. The approach itself is also potentially interesting. The paper can be largely improved in terms of language and form, e.g. no brackets around Figure references. The authors identify correct weaknesses in the method, but the approach itself could be improved. The method itself is potentially interesting, but it can be improved in the form of the language and language.\nThe authors identify correctly weaknesses in their method, i.e. overfitting of the networks. The approach itself is also potentially interesting. The paper can be largely improved in terms of language and form, e.g. no brackets around Figure references. The authors identify correct weaknesses in the method, but the approach itself could be improved. The method itself is potentially interesting, but it can be improved in the form of the language and language."
    },
    {
        "coherence": 0.13772722890968764,
        "consistency": 0.25958616998163225,
        "fluency": 0.14567733500752786,
        "relevance": 0.031055395918222164,
        "overall": 0.14351153245426748,
        "Generated": "356",
        "Gold": "The paper proposes a simple method to generate logic rules, where rules are generated by a shortest-path heuristic and rule weights by solving a linear program. In the light of the current state-of-the-art, the paper falls short severely. It's also overly limited in the type of rules that can be generated and how they can be combined. Some interesting experimental results are well demonstrated. However, it is still hard to judge whether the algorithm is an improvement on previous work.\nThe paper proposes a simple method to generate logic rules, where rules are generated by a shortest-path heuristic and rule weights by solving a linear program. In the light of the current state-of-the-art, the paper falls short severely. It's also overly limited in the type of rules that can be generated and how they can be combined. Some interesting experimental results are well demonstrated. However, it is still hard to judge whether the algorithm is an improvement on previous work."
    },
    {
        "coherence": 0.17710504152430315,
        "consistency": 0.42051350017385575,
        "fluency": 0.15330443355291418,
        "relevance": 0.044797062335770146,
        "overall": 0.19893000939671082,
        "Generated": "357",
        "Gold": "This work considers an important problem of generating adversarial examples to attack a black-box model. The paper proposes a new approach to consider an adversarial example as a result of a sequence of pixel changes from a benign instance. The adversarial generation problem can be considered as a bandit problem. The proposed approach achieved query efficiency and success rate which are not too far away from the current state of the art. The effectiveness is demonstrated on various classifiers for ImageNet and compared against several baseline methods.\nThis work considers an important problem of generating adversarial examples to attack a black-box model. The paper proposes a new approach to consider an adversarial example as a result of a sequence of pixel changes from a benign instance. The adversarial generation problem can be considered as a bandit problem. The proposed approach achieved query efficiency and success rate which are not too far away from the current state of the art. The effectiveness is demonstrated on various classifiers for ImageNet and compared against several baseline methods."
    },
    {
        "coherence": 0.265201626402543,
        "consistency": 0.27338494490367854,
        "fluency": 0.2338706840151341,
        "relevance": 0.05477740737803081,
        "overall": 0.20680866567484663,
        "Generated": "358",
        "Gold": "The paper addresses the problem of training a GAN to match the distribution of part of the dataset called the 'desired data distribution' The authors propose to use an additional set S of pairwise samples (X_1,X_2) called the set of preferences to guide the training of a WGAN. This set specifies that the user prefers sample X_1 over X_2. A corresponding loss function is added to the WGAN loss to ensure that the model learns samples that have high rank.\nThe paper addresses the problem of training a GAN to match the distribution of part of the dataset called the 'desired data distribution' The authors propose to use an additional set S of pairwise samples (X_1,X_2) called the set of preferences to guide the training of a WGAN. This set specifies that the user prefers sample X_1 over X_2. A corresponding loss function is added to the WGAN loss to ensure that the model learns samples that have high rank."
    },
    {
        "coherence": 0.20747052466305407,
        "consistency": 0.30742427370406195,
        "fluency": 0.2932951767402816,
        "relevance": 0.05792633441894425,
        "overall": 0.21652907738158547,
        "Generated": "359",
        "Gold": "The authors study a stochastic bandit problem when the reward function and constraint function lie in a reproducing kernel Hilbert space (RKHS) with a bounded norm. The paper considers soft constraints that may be violated in any round as long as the cumulative violations are small. It argues that by carefully combining Gaussian processes, UBC, and primal-dual algorithms, a non-trivial algorithm can be designed. This framework can employ general exploration strategies (GP-UCB and GP-TS)\nThe authors study a stochastic bandit problem when the reward function and constraint function lie in a reproducing kernel Hilbert space (RKHS) with a bounded norm. The paper considers soft constraints that may be violated in any round as long as the cumulative violations are small. It argues that by carefully combining Gaussian processes, UBC, and primal-dual algorithms, a non-trivial algorithm can be designed. This framework can employ general exploration strategies (GP-UCB and GP-TS)"
    },
    {
        "coherence": 0.018736984260130634,
        "consistency": 0.07385422027915742,
        "fluency": 0.08895627022087237,
        "relevance": 0.014575631357386477,
        "overall": 0.04903077652938673,
        "Generated": "360",
        "Gold": "The paper focuses on the problem of multi-agent cooperation in social dilemmas. The authors use the bias toward status-quo in human psychology to motivate a new training method. Experiments show that SQL achieve better social welfare than LOLA and standard independent RL. The writing is very clear and easy to follow. The approach uses a status quo loss and a method for converting multi-step games into a matrix game. This paper proposed a loss function that leads to cooperation between two individually optimized agents.\nThe paper focuses on the problem of multi-agent cooperation in social dilemmas. The authors use the bias toward status-quo in human psychology to motivate a new training method. Experiments show that SQL achieve better social welfare than LOLA and standard independent RL. The writing is very clear and easy to follow. The approach uses a status quo loss and a method for converting multi-step games into a matrix game. This paper proposed a loss function that leads to cooperation between two individually optimized agents."
    },
    {
        "coherence": 0.1299888363246885,
        "consistency": 0.17321461120695134,
        "fluency": 0.17149262426172357,
        "relevance": 0.01630959047972902,
        "overall": 0.12275141556827311,
        "Generated": "361",
        "Gold": "The method is based on a recently proposed compression technique relative entropy coding. It then proves that with some small modification (clipping the model updates), the algorithm is inherently differentially private. Empirical evaluation shows the proposed method can achieve much more communication reduction at the cost of accuracy degradation. The method can save communication for ~(4000, 20, 800, 100) times as much as it would cost to do the same thing over a larger number of rounds.\nThe method is based on a recently proposed compression technique relative entropy coding. It then proves that with some small modification (clipping the model updates), the algorithm is inherently differentially private. Empirical evaluation shows the proposed method can achieve much more communication reduction at the cost of accuracy degradation. The method can save communication for ~(4000, 20, 800, 100) times as much as it would cost to do the same thing over a larger number of rounds."
    },
    {
        "coherence": 0.30090104986431543,
        "consistency": 0.31736912676469387,
        "fluency": 0.3062838372272532,
        "relevance": 0.028074379216959287,
        "overall": 0.23815709826830544,
        "Generated": "362",
        "Gold": "This paper presents an approach to distill a model-based planning expert into a policy to enable real-time execution on robotic systems. The policy is learned via imitation learning of trajectories from the expert. This approach is tested on four continuous control tasks and achieves good performance compared to baselines. This paper combines GPS and DAgger to learn a policy network. Their approach uses both the iCEM controller and the learned policy with DAgger-like relabeling. The experiment results show promising results.\nThis paper presents an approach to distill a model-based planning expert into a policy to enable real-time execution on robotic systems. The policy is learned via imitation learning of trajectories from the expert. This approach is tested on four continuous control tasks and achieves good performance compared to baselines. This paper combines GPS and DAgger to learn a policy network. Their approach uses both the iCEM controller and the learned policy with DAgger-like relabeling. The experiment results show promising results."
    },
    {
        "coherence": 0.1922929389885586,
        "consistency": 0.2792247339575426,
        "fluency": 0.17259852040116497,
        "relevance": 0.025009074960444543,
        "overall": 0.16728131707692767,
        "Generated": "363",
        "Gold": "This paper proposes a gradient-based method for solving Offline MBO problems using infinite-width DNN models. The approach facilitates efficiently optimizing designs X via a proposed bidirectional objective. The proposed method achieved the best performance on six out of seven widely studied offline model-based optimization task. The authors claim that the backward mapping can distill more information into high-scoring designs. Experiments show that BDI achieves state-of-the-art performance in both continuous and discrete tasks.\nThis paper proposes a gradient-based method for solving Offline MBO problems using infinite-width DNN models. The approach facilitates efficiently optimizing designs X via a proposed bidirectional objective. The proposed method achieved the best performance on six out of seven widely studied offline model-based optimization task. The authors claim that the backward mapping can distill more information into high-scoring designs. Experiments show that BDI achieves state-of-the-art performance in both continuous and discrete tasks."
    },
    {
        "coherence": 0.1114105452893284,
        "consistency": 0.15083199691478802,
        "fluency": 0.34011449835903407,
        "relevance": 0.01704496595663835,
        "overall": 0.1548505016299472,
        "Generated": "364",
        "Gold": "Active learning is an integer optimization problem that minimises the distance Wasserstein distance between unlabelled pool of data. This is done in a feature space (in this case trained using self-supervised methods all the data) The method outperforms existing active learning methods for very small labelling budgets. I think overall the approach makes sense, but I didn't find significantly novel contributions, either theoretically or empirically. As a result, I'll vote for a weak acceptance.\nActive learning is an integer optimization problem that minimises the distance Wasserstein distance between unlabelled pool of data. This is done in a feature space (in this case trained using self-supervised methods all the data) The method outperforms existing active learning methods for very small labelling budgets. I think overall the approach makes sense, but I didn't find significantly novel contributions, either theoretically or empirically. As a result, I'll vote for a weak acceptance."
    },
    {
        "coherence": 0.05244898010806554,
        "consistency": 0.1135608625782854,
        "fluency": 0.06815776658341614,
        "relevance": 0.008924081351494187,
        "overall": 0.06077292265531532,
        "Generated": "365",
        "Gold": "Theorem 1 seems to be incorrect. The writing quality of the paper can be further improved, especially in Section 4.3. The objective of IL is to obtain policies that can generate or keep being on optimal state trajectories as the true expert does. The method achieves superior performance to standard BC and comparable performance to some prior IL works. The empirical results show that it works very well on classic locomotion tasks with artificial noises. It uses this bound to motivate the construction of an algorithm.\nTheorem 1 seems to be incorrect. The writing quality of the paper can be further improved, especially in Section 4.3. The objective of IL is to obtain policies that can generate or keep being on optimal state trajectories as the true expert does. The method achieves superior performance to standard BC and comparable performance to some prior IL works. The empirical results show that it works very well on classic locomotion tasks with artificial noises. It uses this bound to motivate the construction of an algorithm."
    },
    {
        "coherence": 0.10291578866789731,
        "consistency": 0.10458889110798153,
        "fluency": 0.4685423442182467,
        "relevance": 0.02586213366124552,
        "overall": 0.17547728941384277,
        "Generated": "366",
        "Gold": "The paper advocates Equivariance Self-Supervised Learning (E-SSL) as a more general framework than Invariance SSL. The proposed E-SSL framework boils down to adding an additional equivariance objective (mainly 4-fold rotations) to popular I-SSL methods. The empirical results show encouraging results in CIFAR-10 and ImageNet. The advantage of the proposed method is demonstrated relative to SimCLR and SimSiam on Cifar-10, ImageNet, and a new scientific application domain: learning frequency responses of photonic crystals.\nThe paper advocates Equivariance Self-Supervised Learning (E-SSL) as a more general framework than Invariance SSL. The proposed E-SSL framework boils down to adding an additional equivariance objective (mainly 4-fold rotations) to popular I-SSL methods. The empirical results show encouraging results in CIFAR-10 and ImageNet. The advantage of the proposed method is demonstrated relative to SimCLR and SimSiam on Cifar-10, ImageNet, and a new scientific application domain: learning frequency responses of photonic crystals."
    },
    {
        "coherence": 0.26592508111703544,
        "consistency": 0.3074034813152186,
        "fluency": 0.3636414734457889,
        "relevance": 0.03823771903681088,
        "overall": 0.24380193872871345,
        "Generated": "367",
        "Gold": "The paper proposes a system for using natural language feedback generated by large language models(LLMs) to improve task planning in embodied environments. The system does not require training a specialized model for the task and uses pre-trained models prompted with information from the environment. Three types of environment feedback, i.e. success detection, passive and active scene descriptions, are expressed in language. It is unclear which type of feedback should be active at each time step.\nThe paper proposes a system for using natural language feedback generated by large language models(LLMs) to improve task planning in embodied environments. The system does not require training a specialized model for the task and uses pre-trained models prompted with information from the environment. Three types of environment feedback, i.e. success detection, passive and active scene descriptions, are expressed in language. It is unclear which type of feedback should be active at each time step."
    },
    {
        "coherence": 0.10299651728694292,
        "consistency": 0.1642238047906397,
        "fluency": 0.47159137238708004,
        "relevance": 0.021262919404652303,
        "overall": 0.19001865346732874,
        "Generated": "368",
        "Gold": "The paper introduces a dataset and benchmark for video and image quality of video encoders. The dataset consists of compressed videos and subjective scores, collected via crowd-sourced comparison. The paper is well written and provides clarity of thought. The evaluation scheme used is thorough. The results section could have been written in a more lucid and better way. It is dedicated to video quality assessment via trainable algorithms, focusing on measuring video quality after compression. It considers more video encoding standards and collects subjective scores via crowd sourcing.\nThe paper introduces a dataset and benchmark for video and image quality of video encoders. The dataset consists of compressed videos and subjective scores, collected via crowd-sourced comparison. The paper is well written and provides clarity of thought. The evaluation scheme used is thorough. The results section could have been written in a more lucid and better way. It is dedicated to video quality assessment via trainable algorithms, focusing on measuring video quality after compression. It considers more video encoding standards and collects subjective scores via crowd sourcing."
    },
    {
        "coherence": 0.17688029865717791,
        "consistency": 0.1943303925315066,
        "fluency": 0.36015513621686074,
        "relevance": 0.03539517836634199,
        "overall": 0.1916902514429718,
        "Generated": "369",
        "Gold": "Neural Tangent Kenrel offers a compelling framework to (partially) understand some theoretical aspects of neural networks. This paper builds and extends the theory of infinite-width neural kernel computations. It explicitly computes the NNGP and NTK kernels for a wider range of activation functions. Most theorems proved in this paper are incremental, but still preserves enough significance. The paper is well organized and well written. The authors also observe a 106 x speedup on a toy task compared with the kernel evaluation.\nNeural Tangent Kenrel offers a compelling framework to (partially) understand some theoretical aspects of neural networks. This paper builds and extends the theory of infinite-width neural kernel computations. It explicitly computes the NNGP and NTK kernels for a wider range of activation functions. Most theorems proved in this paper are incremental, but still preserves enough significance. The paper is well organized and well written. The authors also observe a 106 x speedup on a toy task compared with the kernel evaluation."
    },
    {
        "coherence": 0.22601272543629874,
        "consistency": 0.526906931769515,
        "fluency": 0.10530403231770599,
        "relevance": 0.029650417511461313,
        "overall": 0.22196852675874526,
        "Generated": "370",
        "Gold": "The paper proposed a transformer-based generative model, named anoformer, to detect anomaly for time series in an unsupervised manner. Anoformer combined the Transformer model and the GAN model by adopting transformer-blocks as the generator and discriminator module of GAN. The paper is easy to follow and well-organized. The performance gains are significant. The method is evaluated on a few datasets and achieves better results than a set of deep learning approaches.\nThe paper proposed a transformer-based generative model, named anoformer, to detect anomaly for time series in an unsupervised manner. Anoformer combined the Transformer model and the GAN model by adopting transformer-blocks as the generator and discriminator module of GAN. The paper is easy to follow and well-organized. The performance gains are significant. The method is evaluated on a few datasets and achieves better results than a set of deep learning approaches."
    },
    {
        "coherence": 0.2150519285993215,
        "consistency": 0.40614332090793526,
        "fluency": 0.31175387260030507,
        "relevance": 0.02077824874883319,
        "overall": 0.23843184271409876,
        "Generated": "371",
        "Gold": "The authors of this manuscript propose Geo-NeuS, a novel neural implicit 3D reconstruction method. They combine a SDF-based neural implicit surface representation, that is usually only optimized with the reconstruction loss. The proposed method clearly improves quantitatively (Tab 1) as well as qualitatively (Fig 4) over the state-of-the-art. The results demonstrate high quality 3d scene geometry reconstruction. The paper heavily relies on existing structure-from-motion pipelines to compute the camera parameters.\nThe authors of this manuscript propose Geo-NeuS, a novel neural implicit 3D reconstruction method. They combine a SDF-based neural implicit surface representation, that is usually only optimized with the reconstruction loss. The proposed method clearly improves quantitatively (Tab 1) as well as qualitatively (Fig 4) over the state-of-the-art. The results demonstrate high quality 3d scene geometry reconstruction. The paper heavily relies on existing structure-from-motion pipelines to compute the camera parameters."
    },
    {
        "coherence": 0.09616780087251495,
        "consistency": 0.14840298513334357,
        "fluency": 0.47745881992784595,
        "relevance": 0.021470866313003874,
        "overall": 0.18587511806167709,
        "Generated": "372",
        "Gold": "SignCorpus is the largest ever collected sign language pretraining dataset. This is very useful for finetuning neural network models for multilingual sign identification and finger spelling. Overall the paper is very well structured, easy to follow and critical for sign language ML research in my opinion. The key weakness of the submission is the lack of human toplines for native signers for each of the 10 languages. The data gathering procedure is well described and justified, using techniques inspired of other tasks, datasets and models.\nSignCorpus is the largest ever collected sign language pretraining dataset. This is very useful for finetuning neural network models for multilingual sign identification and finger spelling. Overall the paper is very well structured, easy to follow and critical for sign language ML research in my opinion. The key weakness of the submission is the lack of human toplines for native signers for each of the 10 languages. The data gathering procedure is well described and justified, using techniques inspired of other tasks, datasets and models."
    },
    {
        "coherence": 0.09699379809622631,
        "consistency": 0.1719928604920853,
        "fluency": 0.1821546580979601,
        "relevance": 0.026725711935527148,
        "overall": 0.11946675715544972,
        "Generated": "373",
        "Gold": "This paper proposes an approach to tackle the top-k elements in the ranking problem. This could be very helpful in domains like recommender systems, information retrieval, ... The document mainly focuses on the Borda count algorithm and its connection with other approaches. The paper is well-written and I found it to be easy to follow. The time complexity of proposed BC algorithm has been lowered compared with the other methods. Authors showed the conclusion mainly by a concentration bound with some further computations on the parameters.\nThis paper proposes an approach to tackle the top-k elements in the ranking problem. This could be very helpful in domains like recommender systems, information retrieval, ... The document mainly focuses on the Borda count algorithm and its connection with other approaches. The paper is well-written and I found it to be easy to follow. The time complexity of proposed BC algorithm has been lowered compared with the other methods. Authors showed the conclusion mainly by a concentration bound with some further computations on the parameters."
    },
    {
        "coherence": 0.22459247062363477,
        "consistency": 0.3521210277018869,
        "fluency": 0.27171565629652944,
        "relevance": 0.03908025137325941,
        "overall": 0.22187735149882762,
        "Generated": "374",
        "Gold": "The paper proposes combining model-based RL with high-level skill learning and composition through hierarchical RL, into a single reinforcement learning framework. The approach consists of multiple learned components working together. Despite its computational complexity, the paper is original and sufficiently empirically motivated for acceptance. The experiments are done in the locomotion domain and show better or comparables results to DREAM. The authors propose an extension of the Dreamer method to incorporate high- level planning via the cross-entropy method and temporally extended skill policies.\nThe paper proposes combining model-based RL with high-level skill learning and composition through hierarchical RL, into a single reinforcement learning framework. The approach consists of multiple learned components working together. Despite its computational complexity, the paper is original and sufficiently empirically motivated for acceptance. The experiments are done in the locomotion domain and show better or comparables results to DREAM. The authors propose an extension of the Dreamer method to incorporate high- level planning via the cross-entropy method and temporally extended skill policies."
    },
    {
        "coherence": 0.08122842057972755,
        "consistency": 0.11346015679565524,
        "fluency": 0.11177565846433125,
        "relevance": 0.040216698337264996,
        "overall": 0.08667023354424476,
        "Generated": "375",
        "Gold": "BVAE-TTS is yet another non-autoregressive speech synthesis model. The paper is well written and results are strong, although I would prefer if the method itself were explained more clearly. My opinion is that it's a borderline for ICLR, since the importance of the proposed VAE was not well justified. This paper combined fastspeech with a hierarchical VAE to achieve parallel and high quality text-to-mel syntheisis. The key strength of this paper is the architecture is new.\nBVAE-TTS is yet another non-autoregressive speech synthesis model. The paper is well written and results are strong, although I would prefer if the method itself were explained more clearly. My opinion is that it's a borderline for ICLR, since the importance of the proposed VAE was not well justified. This paper combined fastspeech with a hierarchical VAE to achieve parallel and high quality text-to-mel syntheisis. The key strength of this paper is the architecture is new."
    },
    {
        "coherence": 0.13725944722978672,
        "consistency": 0.35287287284661983,
        "fluency": 0.3793316136778653,
        "relevance": 0.03642999866878335,
        "overall": 0.2264734831057638,
        "Generated": "376",
        "Gold": "The results of this paper are promising as it clearly shows how global image labels can be leveraged in radiography. Main weakness of the paper relates to the lack of baselines. The idea of using grad-CAM to generate saliency maps as weak labels is not new as reported by the authors. It builds on top of recent advances in weakly supervised and semi-supervised learning. The paper is mainly an application paper with limited methodological novelty but seems to lack some experimental rigour.\nThe results of this paper are promising as it clearly shows how global image labels can be leveraged in radiography. Main weakness of the paper relates to the lack of baselines. The idea of using grad-CAM to generate saliency maps as weak labels is not new as reported by the authors. It builds on top of recent advances in weakly supervised and semi-supervised learning. The paper is mainly an application paper with limited methodological novelty but seems to lack some experimental rigour."
    },
    {
        "coherence": 0.11418029060333408,
        "consistency": 0.16519665871520042,
        "fluency": 0.3557932096260717,
        "relevance": 0.03851338175434554,
        "overall": 0.1684208851747379,
        "Generated": "377",
        "Gold": "The paper studies a batched reinforcement learning (RL) problem where the interaction is divided into $M$ epochs (‘batches’). The agent chooses the batch lengths before the interaction starts and can only update its policy at the beginning of a new batch. Authors propose an algorithm that achieves near-optimal regret bound $O(\\sqrt{SAH^3K})$ in $K$ episodes. I think that the framework is interesting and the results are nontrivial.\nThe paper studies a batched reinforcement learning (RL) problem where the interaction is divided into $M$ epochs (‘batches’). The agent chooses the batch lengths before the interaction starts and can only update its policy at the beginning of a new batch. Authors propose an algorithm that achieves near-optimal regret bound $O(\\sqrt{SAH^3K})$ in $K$ episodes. I think that the framework is interesting and the results are nontrivial."
    },
    {
        "coherence": 0.20551548466923508,
        "consistency": 0.3932000291715051,
        "fluency": 0.4184758481484431,
        "relevance": 0.04536983361585921,
        "overall": 0.2656402989012606,
        "Generated": "378",
        "Gold": "The authors present  \"augmented\" tensor decomposition. The objective is designed for downstream tasks (by way of \"alignment\") in addition to data reconstruction. The quality of the decomposition is determined not only by reconstruction error, but by correct clustering on features. Better embedding spaces, modeled by latent factors, can be learned by augmenting the training data. This paper propose a novel self-supervised framework for CP Tensor Decomposition.\nThe authors present  \"augmented\" tensor decomposition. The objective is designed for downstream tasks (by way of \"alignment\") in addition to data reconstruction. The quality of the decomposition is determined not only by reconstruction error, but by correct clustering on features. Better embedding spaces, modeled by latent factors, can be learned by augmenting the training data. This paper propose a novel self-supervised framework for CP Tensor Decomposition."
    },
    {
        "coherence": 0.05659528929110479,
        "consistency": 0.05277371057200577,
        "fluency": 0.3140842906579138,
        "relevance": 0.009139449694436086,
        "overall": 0.10814818505386511,
        "Generated": "379",
        "Gold": "The paper proposes a benchmark to measure the quality of the systems in the growing domain of code intelligence. The authors presented three Transformer-based systems (with BERT-style encoder, GPT-style decoder and encoder-decoder) to set up baselines. It consists of 10 tasks across 14 datasets, where 5 datasets are newly introduced. The collection of these tasks cover code-code, text- code, code-text and text-text settings.\nThe paper proposes a benchmark to measure the quality of the systems in the growing domain of code intelligence. The authors presented three Transformer-based systems (with BERT-style encoder, GPT-style decoder and encoder-decoder) to set up baselines. It consists of 10 tasks across 14 datasets, where 5 datasets are newly introduced. The collection of these tasks cover code-code, text- code, code-text and text-text settings."
    },
    {
        "coherence": 0.32267687809458206,
        "consistency": 0.4291352032846246,
        "fluency": 0.17928476961166498,
        "relevance": 0.04544164418124037,
        "overall": 0.244134623793028,
        "Generated": "380",
        "Gold": "The paper explores a very important question in dynamical system identification of how to make recurrent neural networks (RNNs) learn both long-term and short-term dependencies without the gradient vanishing or exploding limitation. They suggest using piece-wise linear RNNs with a novel regularization technique. I think the idea is quite novel and interesting. But the experiment section is confusing by missing some explanations. And the presentation of the neuron model is not sufficient enough to prove that rPLRNN find interesting and interpretable dynamics.\nThe paper explores a very important question in dynamical system identification of how to make recurrent neural networks (RNNs) learn both long-term and short-term dependencies without the gradient vanishing or exploding limitation. They suggest using piece-wise linear RNNs with a novel regularization technique. I think the idea is quite novel and interesting. But the experiment section is confusing by missing some explanations. And the presentation of the neuron model is not sufficient enough to prove that rPLRNN find interesting and interpretable dynamics."
    },
    {
        "coherence": 0.3481502935429609,
        "consistency": 0.3424242747561698,
        "fluency": 0.1993766707853487,
        "relevance": 0.028997079372455287,
        "overall": 0.22973707961423367,
        "Generated": "381",
        "Gold": "This paper proposes to achieve disentanglement of latent factors by simultaneously learning them with the sparse causal graph that relates them. The proposed method is evaluated on two synthetic datasets. While the proposed method outperforms the shown baselines, it is not clear how robust the method is. The paper is written and structured clearly and easy to follow. The only major drawback of this paper is the limited experimental evaluation. It is very well written andeasy to follow, despite several more complex statements.\nThis paper proposes to achieve disentanglement of latent factors by simultaneously learning them with the sparse causal graph that relates them. The proposed method is evaluated on two synthetic datasets. While the proposed method outperforms the shown baselines, it is not clear how robust the method is. The paper is written and structured clearly and easy to follow. The only major drawback of this paper is the limited experimental evaluation. It is very well written andeasy to follow, despite several more complex statements."
    },
    {
        "coherence": 0.24425682448910715,
        "consistency": 0.2506666470362449,
        "fluency": 0.40141692641684,
        "relevance": 0.025961853336741746,
        "overall": 0.23057556281973346,
        "Generated": "382",
        "Gold": "Manuscript presents a method to train neural networks over synthetic data. The work collected a large collection of 21K OpenGL programs for rendering diverse set of synthetic images. Experiments show that the collected programs can be used for pretraining in both supervised and unsupervised fashion. This paper proposed a dataset for learning representations without access to real images. The dataset is composed of twenty-one thousand OpenGL programs. The synthetic data can help reduce the concerns about using real images in training such as human face.\nManuscript presents a method to train neural networks over synthetic data. The work collected a large collection of 21K OpenGL programs for rendering diverse set of synthetic images. Experiments show that the collected programs can be used for pretraining in both supervised and unsupervised fashion. This paper proposed a dataset for learning representations without access to real images. The dataset is composed of twenty-one thousand OpenGL programs. The synthetic data can help reduce the concerns about using real images in training such as human face."
    },
    {
        "coherence": 0.2581577756488907,
        "consistency": 0.1902948734737855,
        "fluency": 0.2852039660560685,
        "relevance": 0.08708426662786709,
        "overall": 0.20518522045165294,
        "Generated": "383",
        "Gold": "This paper considers the challenging problem of generalizing well to new RL tasks. The main approach is a combination of two recent approaches: Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI) The main problem - to me - with described approach is that the Q* value now lives in a much higher dimensional space. Ideas seem to me to be a rather small (epsiilon) improvement over the cited works.\nThis paper considers the challenging problem of generalizing well to new RL tasks. The main approach is a combination of two recent approaches: Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI) The main problem - to me - with described approach is that the Q* value now lives in a much higher dimensional space. Ideas seem to me to be a rather small (epsiilon) improvement over the cited works."
    },
    {
        "coherence": 0.2025436014509788,
        "consistency": 0.2611007581232343,
        "fluency": 0.2700044787458653,
        "relevance": 0.028504723932596655,
        "overall": 0.19053839056316876,
        "Generated": "384",
        "Gold": "The paper presents a method for semi-supervised video salient object detection. It adopts Cross-Frame Global Matching Module (CFGMM) to exploit temporal information. Adversarial learning is adopted to generate reliable pseudo-labels. It uses Uncertainty-Aware Dual Decoder Module (UADDM) to locate uncertain regions. The experimental results show that the method can achieve better performance with fewer gts. The proposed UGPLG generates temporally consistent pseudo labels.\nThe paper presents a method for semi-supervised video salient object detection. It adopts Cross-Frame Global Matching Module (CFGMM) to exploit temporal information. Adversarial learning is adopted to generate reliable pseudo-labels. It uses Uncertainty-Aware Dual Decoder Module (UADDM) to locate uncertain regions. The experimental results show that the method can achieve better performance with fewer gts. The proposed UGPLG generates temporally consistent pseudo labels."
    },
    {
        "coherence": 0.04056864669957047,
        "consistency": 0.07540924743284076,
        "fluency": 0.27715169365179537,
        "relevance": 0.036108032147951725,
        "overall": 0.10730940498303958,
        "Generated": "385",
        "Gold": "The paper describes dynamic residual adapters designed to adaptively account for latent domains. This framework injects adaptivity into networks, preventing them from overfitting to the largest domains in distributions. The approach closes a large amount of the performance gap to domain-supervised solutions. The authors propose a method for latent domain learning. The method consists of two parts: dynamic residual adapter and weighted domain transfer. Empirical results show that the method brings improvement to minority domains.\nThe paper describes dynamic residual adapters designed to adaptively account for latent domains. This framework injects adaptivity into networks, preventing them from overfitting to the largest domains in distributions. The approach closes a large amount of the performance gap to domain-supervised solutions. The authors propose a method for latent domain learning. The method consists of two parts: dynamic residual adapter and weighted domain transfer. Empirical results show that the method brings improvement to minority domains."
    },
    {
        "coherence": 0.16729005778650077,
        "consistency": 0.2135684019060608,
        "fluency": 0.24439826682427326,
        "relevance": 0.03364984772385579,
        "overall": 0.16472664356017266,
        "Generated": "386",
        "Gold": "The authors show a new image processing through DL aplication set-up. They use semi-automatically annotated images and simulations as ground truth. The results show that the network could be able to generalize and be more accurate than the annotations performed to train it. This can find good use in lab settings where actual images are not needed. It is similar to the old ideas where people were trying to detect detection in compressed videos without decompressing videos.\nThe authors show a new image processing through DL aplication set-up. They use semi-automatically annotated images and simulations as ground truth. The results show that the network could be able to generalize and be more accurate than the annotations performed to train it. This can find good use in lab settings where actual images are not needed. It is similar to the old ideas where people were trying to detect detection in compressed videos without decompressing videos."
    },
    {
        "coherence": 0.3497514571204075,
        "consistency": 0.3281522401539659,
        "fluency": 0.3129087013334776,
        "relevance": 0.038386165921366615,
        "overall": 0.2572996411323044,
        "Generated": "387",
        "Gold": "Support Decomposition Variational Inference (SDVI) targets probabilistic programs with stochastic support. Main idea is to perform variational inferences separately on sub-programs with static support. Authors demonstrate their method's performance on a toy program, an infinite mixture model, and learning Gaussian process kernels in a PCFG. Their results appear to clearly set a new state of the art in log-predictive density and ELBO.\nSupport Decomposition Variational Inference (SDVI) targets probabilistic programs with stochastic support. Main idea is to perform variational inferences separately on sub-programs with static support. Authors demonstrate their method's performance on a toy program, an infinite mixture model, and learning Gaussian process kernels in a PCFG. Their results appear to clearly set a new state of the art in log-predictive density and ELBO."
    },
    {
        "coherence": 0.4502983273124059,
        "consistency": 0.47936190054716576,
        "fluency": 0.23445352246691722,
        "relevance": 0.038540263081176945,
        "overall": 0.30066350335191644,
        "Generated": "388",
        "Gold": "The paper revisits a broad range of vision and language tasks (VQA, VLN, SNLI-VE, Image Captioning) The authors found that CLIP's ViT- based visual encoder performed far worse than the resnets. The experiments suggest that the simple change to CLIP offers significant benefits over commonly used encoders such as BottomUp-TopDown. It demonstrates the potentials of the model in generalizing to different downstream applications.\nThe paper revisits a broad range of vision and language tasks (VQA, VLN, SNLI-VE, Image Captioning) The authors found that CLIP's ViT- based visual encoder performed far worse than the resnets. The experiments suggest that the simple change to CLIP offers significant benefits over commonly used encoders such as BottomUp-TopDown. It demonstrates the potentials of the model in generalizing to different downstream applications."
    },
    {
        "coherence": 0.3822479469356605,
        "consistency": 0.45430558267226756,
        "fluency": 0.30836909010420155,
        "relevance": 0.030199381763596322,
        "overall": 0.2937805003689315,
        "Generated": "389",
        "Gold": "New method has interpretation of Frank-Wolfe algorithm. To decrease the variance of the dual gradients, one can use a parallel mini-batching, which provides the method with a provable acceleration given a set of parallel computational units. The significance is unclear, especially given what appears to be a significant gap between the theory and the experiments. The presentation of the parallelized algorithm has the flavor of being dishonest. The experiments are very far away from the theory.\nNew method has interpretation of Frank-Wolfe algorithm. To decrease the variance of the dual gradients, one can use a parallel mini-batching, which provides the method with a provable acceleration given a set of parallel computational units. The significance is unclear, especially given what appears to be a significant gap between the theory and the experiments. The presentation of the parallelized algorithm has the flavor of being dishonest. The experiments are very far away from the theory."
    },
    {
        "coherence": 0.230763726276178,
        "consistency": 0.20817121988071716,
        "fluency": 0.351807607446625,
        "relevance": 0.04434975407409085,
        "overall": 0.20877307691940275,
        "Generated": "390",
        "Gold": "The paper leverages the recent breakthrough of Masked AutoEncoders (MAE) for the domain of robotics. It shows that visual knowledge can be transferred from different domains such as ego4d into specific robotics domains thanks to MAE. The paper finds that MAE is superior to CLIP pretraining. This paper evaluates vision encoders pretrained using masked autoencoders on large scale datasets. The baselines compare to only ViT architectures. These are generally harder to train than simpler conv architectures like ResNet.\nThe paper leverages the recent breakthrough of Masked AutoEncoders (MAE) for the domain of robotics. It shows that visual knowledge can be transferred from different domains such as ego4d into specific robotics domains thanks to MAE. The paper finds that MAE is superior to CLIP pretraining. This paper evaluates vision encoders pretrained using masked autoencoders on large scale datasets. The baselines compare to only ViT architectures. These are generally harder to train than simpler conv architectures like ResNet."
    },
    {
        "coherence": 0.3117259750102978,
        "consistency": 0.45022747106046973,
        "fluency": 0.18404121879181318,
        "relevance": 0.12274265544926587,
        "overall": 0.26718433007796166,
        "Generated": "391",
        "Gold": "The problem of group robustness (in terms of worst-group performance) is particularly challenging when some groups are underrepresented in the training data. Classical approaches use under/over-sampling, reweighting, and two-phase training to deal with this issue. The authors propose to use contrastive adapters without retraining from scratch. The paper shows that neither a naive version of the zero-shot, nor linear probing on the target dataset, is the optimal choice.\nThe problem of group robustness (in terms of worst-group performance) is particularly challenging when some groups are underrepresented in the training data. Classical approaches use under/over-sampling, reweighting, and two-phase training to deal with this issue. The authors propose to use contrastive adapters without retraining from scratch. The paper shows that neither a naive version of the zero-shot, nor linear probing on the target dataset, is the optimal choice."
    },
    {
        "coherence": 0.300089318106964,
        "consistency": 0.19709584820051831,
        "fluency": 0.38728593095132297,
        "relevance": 0.07371085555982158,
        "overall": 0.2395454882046567,
        "Generated": "392",
        "Gold": "The paper presents a neural network model (SYNONYMNET) for automatically discovering synonymous entities from a large free-text corpus with minimal human annotation. The solution is fairly natural in the form of a siamese network, a class of neural network architectures that contain two or more identical subnetworks. It is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. The paper is full of English mistakes.\nThe paper presents a neural network model (SYNONYMNET) for automatically discovering synonymous entities from a large free-text corpus with minimal human annotation. The solution is fairly natural in the form of a siamese network, a class of neural network architectures that contain two or more identical subnetworks. It is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. The paper is full of English mistakes."
    },
    {
        "coherence": 0.02579458157682296,
        "consistency": 0.1129825889040553,
        "fluency": 0.17101246677578125,
        "relevance": 0.019334103082419864,
        "overall": 0.08228093508476984,
        "Generated": "393",
        "Gold": "A combination of U-Net, memory network and Hough voting from three views sounds novel. Paper is well written and easy to follow. Problem aiming to solve is clinically relevant. Some details need to be clarified. Results are encouraging and show the potential of the proposed method. The paper misses a lot of literature review for instance segmentation in medical imaging. The proposed method achieves state-of-the-art results. Experiments on a large-scale cross-sectional MR imaging study shows the effectiveness of the propose method.\nA combination of U-Net, memory network and Hough voting from three views sounds novel. Paper is well written and easy to follow. Problem aiming to solve is clinically relevant. Some details need to be clarified. Results are encouraging and show the potential of the proposed method. The paper misses a lot of literature review for instance segmentation in medical imaging. The proposed method achieves state-of-the-art results. Experiments on a large-scale cross-sectional MR imaging study shows the effectiveness of the propose method."
    },
    {
        "coherence": 0.1648061743900884,
        "consistency": 0.25770126882996647,
        "fluency": 0.1862051538132872,
        "relevance": 0.026985851406077475,
        "overall": 0.15892461210985487,
        "Generated": "394",
        "Gold": "Algorithm works by smoothing the Gaussian parameters w.r.t. the centered Gaussian rv. The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers. It is possible to construct models where the ELBO has a reasonable value but the smoothed objective behaves catastrophically. The authors provide an analytical analysis of bias and variance. Last they train multiple VAEs models, measure the posterior collapse and observe a phase transition behaviour.\nAlgorithm works by smoothing the Gaussian parameters w.r.t. the centered Gaussian rv. The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers. It is possible to construct models where the ELBO has a reasonable value but the smoothed objective behaves catastrophically. The authors provide an analytical analysis of bias and variance. Last they train multiple VAEs models, measure the posterior collapse and observe a phase transition behaviour."
    },
    {
        "coherence": 0.22277467552015887,
        "consistency": 0.2184503015264388,
        "fluency": 0.1904199194149669,
        "relevance": 0.040078389923460946,
        "overall": 0.16793082159625639,
        "Generated": "395",
        "Gold": "This work considers the graph task of learning node representations that are invariant to small edge perturbations. It achieves this through a data augmentation procedure that samples new ‘fake’ edges and regularizes the GNN equivariant representations to be unable to predict these fake edges. The paper falls well within the scope of the conference. Its writing could be improved and would benefit from a thorough language revision. The state-of-the-art is appropriately identified and even though its novelty is somehow incremental, its value seems beyond doubt.\nThis work considers the graph task of learning node representations that are invariant to small edge perturbations. It achieves this through a data augmentation procedure that samples new ‘fake’ edges and regularizes the GNN equivariant representations to be unable to predict these fake edges. The paper falls well within the scope of the conference. Its writing could be improved and would benefit from a thorough language revision. The state-of-the-art is appropriately identified and even though its novelty is somehow incremental, its value seems beyond doubt."
    },
    {
        "coherence": 0.07726075258866756,
        "consistency": 0.09468757195628527,
        "fluency": 0.5050207402893293,
        "relevance": 0.014945189772494783,
        "overall": 0.17297856365169423,
        "Generated": "396",
        "Gold": "Red is a new methodology to produce reliable confidence scores to detect missclassification errors in neural networks. The method is straightforward to implement and performs well against the baselines considered on classification tasks for 125 UCI datasets. It is not demonstrated whether RED would outperform other confidence scoring and OOD detection methods. Unless there is a good justification for the limited set of baselines, I believe the paper's claims to generality are limited. It would be good to apply SVGP directly to some of these datasets and compare the results against NN+SVGP results.\nRed is a new methodology to produce reliable confidence scores to detect missclassification errors in neural networks. The method is straightforward to implement and performs well against the baselines considered on classification tasks for 125 UCI datasets. It is not demonstrated whether RED would outperform other confidence scoring and OOD detection methods. Unless there is a good justification for the limited set of baselines, I believe the paper's claims to generality are limited. It would be good to apply SVGP directly to some of these datasets and compare the results against NN+SVGP results."
    },
    {
        "coherence": 0.33423805248265176,
        "consistency": 0.2144398115769868,
        "fluency": 0.23951640584923464,
        "relevance": 0.05148843663761007,
        "overall": 0.2099206766366208,
        "Generated": "397",
        "Gold": "This paper proposes a bootstrap framework to study the generalization problem of deep learning. It decomposes the traditional test error into an ‘Ideal World’ test error plus the gap between. Empirically, it demonstrates that such gap (soft-error) is small in supervised image classification. I have respect for this paper trying to define some statistical terms in the deep learning society, but I have a very strong concern about the novelty of the framework.\nThis paper proposes a bootstrap framework to study the generalization problem of deep learning. It decomposes the traditional test error into an ‘Ideal World’ test error plus the gap between. Empirically, it demonstrates that such gap (soft-error) is small in supervised image classification. I have respect for this paper trying to define some statistical terms in the deep learning society, but I have a very strong concern about the novelty of the framework."
    },
    {
        "coherence": 0.36554392925538626,
        "consistency": 0.45669670848193455,
        "fluency": 0.3892966147315785,
        "relevance": 0.27723180432553074,
        "overall": 0.3721922641986075,
        "Generated": "398",
        "Gold": "The paper focuses on the monotonic improvement for model-based reinforcement learning. The primary reason for the model bias is a mismatch between the samples in the model learning stage and the policy optimization stage. To tackle the same, they formulate a constrained bi-level optimization framework for the MBRL problem. The intuition behind the event-triggered equation 5.1 is also not very clear. What will the fraction of state coverage give?"
    },
    {
        "coherence": 0.37767065543157613,
        "consistency": 0.2941552972861847,
        "fluency": 0.41257368833255564,
        "relevance": 0.04267750374300443,
        "overall": 0.28176928619833025,
        "Generated": "399",
        "Gold": "The paper presents a new algorithm called Regioned Episodic Reinforcement Learning (RERL) It combines ideas from episodic memory, with automatic sub-goal creation or “goal-oriented’ RL. The method works by dividing the state space into regions, where a different goal identifies each region. Exploitation is leveraged through a region-based memory. The paper could benefit from a clear presentation in order to help properly understand the proposed methods.\nThe paper presents a new algorithm called Regioned Episodic Reinforcement Learning (RERL) It combines ideas from episodic memory, with automatic sub-goal creation or “goal-oriented’ RL. The method works by dividing the state space into regions, where a different goal identifies each region. Exploitation is leveraged through a region-based memory. The paper could benefit from a clear presentation in order to help properly understand the proposed methods."
    },
    {
        "coherence": 0.12294084016519802,
        "consistency": 0.27049228134915015,
        "fluency": 0.21265344009624612,
        "relevance": 0.04308094952379814,
        "overall": 0.1622918777835981,
        "Generated": "400",
        "Gold": "InterPOLE uses Bayesian techniques to estimate decision dynamics as well as decision boundaries. Results on simulated and real-world domains show that their method explains the decisions in behavior data. The application to healthcare is useful and interesting. In terms of alternative approaches to modeling agent/decision maker behavior, I can think of at least one other alternative. ‘Agent Markov Model’ introduced by Unhelkar and Shah (Learning Models of Sequential Decision-Making with Partial Specification of Agent Behavior AAAI 2019) is also modeling agent behavior.\nInterPOLE uses Bayesian techniques to estimate decision dynamics as well as decision boundaries. Results on simulated and real-world domains show that their method explains the decisions in behavior data. The application to healthcare is useful and interesting. In terms of alternative approaches to modeling agent/decision maker behavior, I can think of at least one other alternative. ‘Agent Markov Model’ introduced by Unhelkar and Shah (Learning Models of Sequential Decision-Making with Partial Specification of Agent Behavior AAAI 2019) is also modeling agent behavior."
    },
    {
        "coherence": 0.07806948152950598,
        "consistency": 0.23458252152998738,
        "fluency": 0.06004121195998897,
        "relevance": 0.05165897115151151,
        "overall": 0.10608804654274846,
        "Generated": "401",
        "Gold": "The paper presents a novel solution to an interesting problem. When KBs are automatically expanded user feedback is crucial to identify incorrect and missing entity attributes and relations. The human computation aspects of the paper are lacking sufficient explanation in terms of implementation in real settings. It is unclear about the algorithm implementation, such as what is the implementation of feedback mention. The example for constructing positve/negative feedback is too vague. The proposed hierarchical model looks reasonable and effective.\nThe paper presents a novel solution to an interesting problem. When KBs are automatically expanded user feedback is crucial to identify incorrect and missing entity attributes and relations. The human computation aspects of the paper are lacking sufficient explanation in terms of implementation in real settings. It is unclear about the algorithm implementation, such as what is the implementation of feedback mention. The example for constructing positve/negative feedback is too vague. The proposed hierarchical model looks reasonable and effective."
    },
    {
        "coherence": 0.047204634040955505,
        "consistency": 0.13233882112862233,
        "fluency": 0.2365462608495637,
        "relevance": 0.012960074393086631,
        "overall": 0.10726244760305705,
        "Generated": "402",
        "Gold": "This paper performs theoretical analysis of the relationship between supervised learning (SL) and self-supervised learning (SSL) It aims to quantify the gap in training loss between SL and contrastive SSL on FSL tasks. The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution. The main concern is that Theorem 1 and 2 are quite loose. Is it work for any fk and fq? Can you provide more strict error bound?\nThis paper performs theoretical analysis of the relationship between supervised learning (SL) and self-supervised learning (SSL) It aims to quantify the gap in training loss between SL and contrastive SSL on FSL tasks. The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution. The main concern is that Theorem 1 and 2 are quite loose. Is it work for any fk and fq? Can you provide more strict error bound?"
    },
    {
        "coherence": 0.16079050290214517,
        "consistency": 0.31665279328784773,
        "fluency": 0.1512759949575657,
        "relevance": 0.015074653534562367,
        "overall": 0.16094848617053026,
        "Generated": "403",
        "Gold": "This is an interesting paper on a topic with real-world application: anomaly detection. The introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. Most claims of novelty can be clearly refuted. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. The math in the paper is mostly redundant. The proposed approach of requesting expert feedback for the top ranked anomalies is straightforward and unsurprising.\nThis is an interesting paper on a topic with real-world application: anomaly detection. The introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. Most claims of novelty can be clearly refuted. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. The math in the paper is mostly redundant. The proposed approach of requesting expert feedback for the top ranked anomalies is straightforward and unsurprising."
    },
    {
        "coherence": 0.12416310870929922,
        "consistency": 0.30572325997361127,
        "fluency": 0.26960069821894267,
        "relevance": 0.024313642510421594,
        "overall": 0.18095017735306868,
        "Generated": "404",
        "Gold": "The authors propose a metric learning and clustering method based on the idea of learning the metric from the context. They use the self-attention block module of the multi-head attention based transformer to embed the data and learn a kernel using the ground truth labels. The experiments show favorable results on the toy dataset and are competitive with methods that use a prespecified clustering. It would help to have a real world task where the approach taken (learning the similarity metric followed by off the shelf clustering) is the best.\nThe authors propose a metric learning and clustering method based on the idea of learning the metric from the context. They use the self-attention block module of the multi-head attention based transformer to embed the data and learn a kernel using the ground truth labels. The experiments show favorable results on the toy dataset and are competitive with methods that use a prespecified clustering. It would help to have a real world task where the approach taken (learning the similarity metric followed by off the shelf clustering) is the best."
    },
    {
        "coherence": 0.09001773116768558,
        "consistency": 0.1229059862492682,
        "fluency": 0.05070311475566239,
        "relevance": 0.023801081353864332,
        "overall": 0.07185697838162013,
        "Generated": "405",
        "Gold": "Confident Adaptive Language Modeling (CALM) to early-exit dynamically during the decoding process. The authors provide a theoretical guarantee that local confidence measures could lead to the confident generation of global outputs. CALM is practically useful to speed up the inference. The proposed method is shown to (with some caveats) result in potential speedup of up to 3x in some cases. This paper first analyzes these types of model with an oracle to show what the best case speedups could be. It then proposes a few new ways to do early exit.\nConfident Adaptive Language Modeling (CALM) to early-exit dynamically during the decoding process. The authors provide a theoretical guarantee that local confidence measures could lead to the confident generation of global outputs. CALM is practically useful to speed up the inference. The proposed method is shown to (with some caveats) result in potential speedup of up to 3x in some cases. This paper first analyzes these types of model with an oracle to show what the best case speedups could be. It then proposes a few new ways to do early exit."
    },
    {
        "coherence": 0.12574774238530215,
        "consistency": 0.20886350666571454,
        "fluency": 0.17245751259934144,
        "relevance": 0.025479145228828517,
        "overall": 0.13313697671979668,
        "Generated": "406",
        "Gold": "This paper proposes a novel architecture and regularization technique for RNN. The hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata is much simpler, and (2) forces RNN to operate like an automata. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks.\nThis paper proposes a novel architecture and regularization technique for RNN. The hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata is much simpler, and (2) forces RNN to operate like an automata. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks."
    },
    {
        "coherence": 0.0766704725989454,
        "consistency": 0.20840733087618127,
        "fluency": 0.15383686802190039,
        "relevance": 0.018207813906415734,
        "overall": 0.1142806213508607,
        "Generated": "407",
        "Gold": "The authors proposed to learn the parameters in Schwarz preconditioner that generalize it to unstructured grid. The main difference compared to the prior work is the proposed new loss function inspired by Gelfand's formula. Experiments on the 2D Helmholtz equation have been done to demonstrate the effectiveness of the approach. The major contribution is the application of TAGConv to solving PDEs. The method looks solid, and is well-validated.\nThe authors proposed to learn the parameters in Schwarz preconditioner that generalize it to unstructured grid. The main difference compared to the prior work is the proposed new loss function inspired by Gelfand's formula. Experiments on the 2D Helmholtz equation have been done to demonstrate the effectiveness of the approach. The major contribution is the application of TAGConv to solving PDEs. The method looks solid, and is well-validated."
    },
    {
        "coherence": 0.08071519032876646,
        "consistency": 0.16217517132116147,
        "fluency": 0.07789322494047936,
        "relevance": 0.020158963464778423,
        "overall": 0.08523563751379643,
        "Generated": "408",
        "Gold": "The authors conduct an empirical evaluation on two image datasets, two text datasets and one tabular dataset. They conclude that many of the state-of-the-art metrics do not even satisfy the first requirement. Overall, the methods adopted by the study are technically sound and the paper is well written. The experiment results revealed that the cosine similarity of the gradients of the loss performed the best. The authors find an explanation for those results, based on geometrical properties of the measured objects.\nThe authors conduct an empirical evaluation on two image datasets, two text datasets and one tabular dataset. They conclude that many of the state-of-the-art metrics do not even satisfy the first requirement. Overall, the methods adopted by the study are technically sound and the paper is well written. The experiment results revealed that the cosine similarity of the gradients of the loss performed the best. The authors find an explanation for those results, based on geometrical properties of the measured objects."
    },
    {
        "coherence": 0.22573390970866541,
        "consistency": 0.2456945947504176,
        "fluency": 0.06798785079946315,
        "relevance": 0.021323959512150076,
        "overall": 0.14018507869267405,
        "Generated": "409",
        "Gold": "This paper focuses on the task of 3D mesh stylization according to the text prompt. The impact of the work is incremental, but the general direction of text-guided geometry/ texture generation is a significant one. The paper describes a method for generating a reflectance map (and lighting environment) for a given 3D shape, based on a text prompt, using a CLIP-guided loss. The key idea is to jointly learn three disentangled components, ie, the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition.\nThis paper focuses on the task of 3D mesh stylization according to the text prompt. The impact of the work is incremental, but the general direction of text-guided geometry/ texture generation is a significant one. The paper describes a method for generating a reflectance map (and lighting environment) for a given 3D shape, based on a text prompt, using a CLIP-guided loss. The key idea is to jointly learn three disentangled components, ie, the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition."
    },
    {
        "coherence": 0.26293487783040187,
        "consistency": 0.3930562413993843,
        "fluency": 0.0733212173848929,
        "relevance": 0.05283668691261499,
        "overall": 0.19553725588182352,
        "Generated": "410",
        "Gold": "Distilled Gradient Aggregation (DGA) approach combines the advantages of both global IG and local FG approaches via a new sequential feature distillation algorithm. The DGA approach effectively addresses the limitations of local approach (FullGrad) that uses a single anchor point to compute attribution. DGA is motivated by problems of some related work, i.e. Integrated Gradient and FullGrad. The current way the method is presented to the audience is quite confusing.\nDistilled Gradient Aggregation (DGA) approach combines the advantages of both global IG and local FG approaches via a new sequential feature distillation algorithm. The DGA approach effectively addresses the limitations of local approach (FullGrad) that uses a single anchor point to compute attribution. DGA is motivated by problems of some related work, i.e. Integrated Gradient and FullGrad. The current way the method is presented to the audience is quite confusing."
    },
    {
        "coherence": 0.16005821732083167,
        "consistency": 0.207255474380331,
        "fluency": 0.06918069850049395,
        "relevance": 0.012130428327518787,
        "overall": 0.11215620463229385,
        "Generated": "411",
        "Gold": "This paper discusses the oversmoothing in transformer models such as BERT. It proposes a hierarchical fusion method as a solution. Theoretical analysis is new and provides interesting findings. The proposed fusion methods are not that novel and not well theoretically and empirically proved. The author applied a novel approach to shed light on an insufficiently investigated problem. Overall, I vote for accepting this paper. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks. It is not completely new but applicable to analysis on transformers.\nThis paper discusses the oversmoothing in transformer models such as BERT. It proposes a hierarchical fusion method as a solution. Theoretical analysis is new and provides interesting findings. The proposed fusion methods are not that novel and not well theoretically and empirically proved. The author applied a novel approach to shed light on an insufficiently investigated problem. Overall, I vote for accepting this paper. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks. It is not completely new but applicable to analysis on transformers."
    },
    {
        "coherence": 0.14870806785020804,
        "consistency": 0.19308325243436766,
        "fluency": 0.07225316748968745,
        "relevance": 0.015540730601370102,
        "overall": 0.10739630459390832,
        "Generated": "412",
        "Gold": "The authors present U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. They provide a large quantity of unlabelled data complementing 8 of the existing multidomain labeled datasets. The paper is easy to read and easy to follow, and the experimental evaluations are robust. The major practical concern I have about this endeavor is standardization across new and old instances. Although the dataset itself is very useful, the analysis and bench marking needs some improvement.\nThe authors present U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. They provide a large quantity of unlabelled data complementing 8 of the existing multidomain labeled datasets. The paper is easy to read and easy to follow, and the experimental evaluations are robust. The major practical concern I have about this endeavor is standardization across new and old instances. Although the dataset itself is very useful, the analysis and bench marking needs some improvement."
    },
    {
        "coherence": 0.1443205437724719,
        "consistency": 0.19760935922226566,
        "fluency": 0.06041012000466291,
        "relevance": 0.016946008175498403,
        "overall": 0.10482150779372472,
        "Generated": "413",
        "Gold": "This paper studies the training algorithms for multi-layer over-parameterized neural networks. It starts from gauss-newton-methods and incorporates tensor-based sketching techniques and preconditioning to improve the per-iteration computational complexity. This paper proved that the second-order method can minimize the training loss in linea rate on multi- layer over- Parametrized Neural Networks. The ideas in this paper can inspire new algorithms that enjoy lower per- iteration costs.\nThis paper studies the training algorithms for multi-layer over-parameterized neural networks. It starts from gauss-newton-methods and incorporates tensor-based sketching techniques and preconditioning to improve the per-iteration computational complexity. This paper proved that the second-order method can minimize the training loss in linea rate on multi- layer over- Parametrized Neural Networks. The ideas in this paper can inspire new algorithms that enjoy lower per- iteration costs."
    },
    {
        "coherence": 0.3325083738012046,
        "consistency": 0.4061264034942181,
        "fluency": 0.09267654703976688,
        "relevance": 0.016271689667138624,
        "overall": 0.21189575350058207,
        "Generated": "414",
        "Gold": "This paper proposes to reformulate the QA task in SQUAD as a retrieval task. The basic approach is to embed the question and the paragraph and train a system to put the correct paragraph close to the question. I had a very hard time following the details of the proposed approach. This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residualnetwork on top of the word embeding.\nThis paper proposes to reformulate the QA task in SQUAD as a retrieval task. The basic approach is to embed the question and the paragraph and train a system to put the correct paragraph close to the question. I had a very hard time following the details of the proposed approach. This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residualnetwork on top of the word embeding."
    },
    {
        "coherence": 0.1220153119574791,
        "consistency": 0.17323767262737025,
        "fluency": 0.10274054840621732,
        "relevance": 0.011159883712429123,
        "overall": 0.10228835417587394,
        "Generated": "415",
        "Gold": "MATE is a multi-agent learning environment that models an asymmetric game between two teams of players. The cameras are fixed in their location but can change direction and zoom, with the goal of observing the targets. The environment could serve as a basic simulator for real-world applications. It could provide a guideline for algorithm and application researchers in the field of tracking and cargoes. The HiT-MAC method should be performed as well in the environment.\nMATE is a multi-agent learning environment that models an asymmetric game between two teams of players. The cameras are fixed in their location but can change direction and zoom, with the goal of observing the targets. The environment could serve as a basic simulator for real-world applications. It could provide a guideline for algorithm and application researchers in the field of tracking and cargoes. The HiT-MAC method should be performed as well in the environment."
    },
    {
        "coherence": 0.08942671169029573,
        "consistency": 0.13090352288068238,
        "fluency": 0.14520435756572492,
        "relevance": 0.011423987203060966,
        "overall": 0.094239644834941,
        "Generated": "416",
        "Gold": "The paper proposes the video prediction model that can handle multiple objects. The proposed model is built from attention-based capsule network. It generates video in a hierarchical feature computation. The model learns to discern each concept without any need for labels such as bounding boxes. In general, I think it is an interesting paper. But I still have some concerns about the significance of the new task. There are a few questions I have listed above, but the paper to me seems interesting enough consider for acceptance.\nThe paper proposes the video prediction model that can handle multiple objects. The proposed model is built from attention-based capsule network. It generates video in a hierarchical feature computation. The model learns to discern each concept without any need for labels such as bounding boxes. In general, I think it is an interesting paper. But I still have some concerns about the significance of the new task. There are a few questions I have listed above, but the paper to me seems interesting enough consider for acceptance."
    },
    {
        "coherence": 0.07232004006068053,
        "consistency": 0.1479172476179056,
        "fluency": 0.07469711058077581,
        "relevance": 0.017957243573909272,
        "overall": 0.07822291045831781,
        "Generated": "417",
        "Gold": "This paper presents an evaluation and explanation of the limited adversarial transferability in the ASR system. It first lists 11 known factors, e.g., smoothness of gradients, and then proposes four potential factors that limit the transferability. The method of how the authors come up with the factors is not explained. I believe the authors provide a useful beginning of understanding why ASR systems may be not susceptible to targetedtransferability. It is known that targeted transferability is ineffective against typical ASRsystems.\nThis paper presents an evaluation and explanation of the limited adversarial transferability in the ASR system. It first lists 11 known factors, e.g., smoothness of gradients, and then proposes four potential factors that limit the transferability. The method of how the authors come up with the factors is not explained. I believe the authors provide a useful beginning of understanding why ASR systems may be not susceptible to targetedtransferability. It is known that targeted transferability is ineffective against typical ASRsystems."
    },
    {
        "coherence": 0.223803114218315,
        "consistency": 0.42073457407992715,
        "fluency": 0.17080485327112283,
        "relevance": 0.018384132799211576,
        "overall": 0.20843166859214413,
        "Generated": "418",
        "Gold": "A method for OOD generalization is proposed. To prevent a model from learning \"shortcut features\" (simple features which do not work in OOD setting~spurious correlations), an autoencoder is trained. The explorer model is then trained on the reconstructed image. Some of the shortcut features were hopefully removed by AE to fool the Blocked model. The approach was evaluated on ColorMNIST, CelebA and BAR benchmarks. The results on CMNIST are not directly comparable to prior work due to a different model architecture.\nA method for OOD generalization is proposed. To prevent a model from learning \"shortcut features\" (simple features which do not work in OOD setting~spurious correlations), an autoencoder is trained. The explorer model is then trained on the reconstructed image. Some of the shortcut features were hopefully removed by AE to fool the Blocked model. The approach was evaluated on ColorMNIST, CelebA and BAR benchmarks. The results on CMNIST are not directly comparable to prior work due to a different model architecture."
    },
    {
        "coherence": 0.18247455898876935,
        "consistency": 0.2686001320165692,
        "fluency": 0.11248715767853923,
        "relevance": 0.0265667863418157,
        "overall": 0.1475321587564234,
        "Generated": "419",
        "Gold": "The paper studies one variant of fair multi-armed bandit (MAB) problem. For each arm $k$ played, there is a known fairness parameter $tau_k$ which gives the fraction of times that this arm should be played. The authors propose and analyze an algorithm where the UCB index of an arm $i$   is increased by a specified amount (which is a parameter for the algorithm) $A_i$\nThe paper studies one variant of fair multi-armed bandit (MAB) problem. For each arm $k$ played, there is a known fairness parameter $tau_k$ which gives the fraction of times that this arm should be played. The authors propose and analyze an algorithm where the UCB index of an arm $i$   is increased by a specified amount (which is a parameter for the algorithm) $A_i$"
    },
    {
        "coherence": 0.21768302289946703,
        "consistency": 0.36409190042136136,
        "fluency": 0.04855392441293057,
        "relevance": 0.022214224683130716,
        "overall": 0.16313576810422242,
        "Generated": "420",
        "Gold": "This paper tries to demystify the power of embedding and embedding space. It tries to connect the structure of learned embeddings and knowledge generation process. The authors tested Gaussian model, Preferential Placement (PP) model, and Directional Preferential placement (DPP) model. Since the authors did not give a clear conclusion, the technical contribution of this submission seems to be weak. This paper lacks some key elements, such as problem formulation and rationale discussion.\nThis paper tries to demystify the power of embedding and embedding space. It tries to connect the structure of learned embeddings and knowledge generation process. The authors tested Gaussian model, Preferential Placement (PP) model, and Directional Preferential placement (DPP) model. Since the authors did not give a clear conclusion, the technical contribution of this submission seems to be weak. This paper lacks some key elements, such as problem formulation and rationale discussion."
    },
    {
        "coherence": 0.07656903852979911,
        "consistency": 0.18210967435530134,
        "fluency": 0.18420136747855353,
        "relevance": 0.0169627616241915,
        "overall": 0.11496071049696138,
        "Generated": "421",
        "Gold": "This paper proposes a method of compressing neural radience fields (NeRF's) by learning mappings from latent codes to model parameters. While maintaining the same level of quality, this method is able to compress NeRF models for more efficient sender-to-receiver transmission. The paper proposes to add entropy penalized reparametrization into Nerf. The authors show that the function F could largely compress the original Nerf models while keeping similar PSNR.\nThis paper proposes a method of compressing neural radience fields (NeRF's) by learning mappings from latent codes to model parameters. While maintaining the same level of quality, this method is able to compress NeRF models for more efficient sender-to-receiver transmission. The paper proposes to add entropy penalized reparametrization into Nerf. The authors show that the function F could largely compress the original Nerf models while keeping similar PSNR."
    },
    {
        "coherence": 0.06710448708307629,
        "consistency": 0.11651786628999596,
        "fluency": 0.2805542917562262,
        "relevance": 0.01924123050066871,
        "overall": 0.1208544689074918,
        "Generated": "422",
        "Gold": "Stochastic gradient descent may easily have very poor behavior. It could converge to a maximum, or diverge even in convex settings if the learning rate is too high. In the most general nonconvex case, many counter-intuitive phenomena of SGD may arise. This paper is well written and has a good presentation of the results. It shows that SGD can converge to local maxima with high probability for specific cases.\nStochastic gradient descent may easily have very poor behavior. It could converge to a maximum, or diverge even in convex settings if the learning rate is too high. In the most general nonconvex case, many counter-intuitive phenomena of SGD may arise. This paper is well written and has a good presentation of the results. It shows that SGD can converge to local maxima with high probability for specific cases."
    },
    {
        "coherence": 0.10792455178466542,
        "consistency": 0.2072227650301003,
        "fluency": 0.15276131429468234,
        "relevance": 0.01442326650728297,
        "overall": 0.12058297440418275,
        "Generated": "423",
        "Gold": "The paper studies the effect of visual perspective by using the images coming from a camera installed on the hand of the robot. They show results on six different manipulation tasks adapted from Meta-World and their choice of perspective improves OOD generalisation. It seems to suggest that a zoomed-in view of the object by using a hand perspective almost always helps generalization. The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed.\nThe paper studies the effect of visual perspective by using the images coming from a camera installed on the hand of the robot. They show results on six different manipulation tasks adapted from Meta-World and their choice of perspective improves OOD generalisation. It seems to suggest that a zoomed-in view of the object by using a hand perspective almost always helps generalization. The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed."
    },
    {
        "coherence": 0.11338470121157894,
        "consistency": 0.20704604367800086,
        "fluency": 0.31589022565954733,
        "relevance": 0.018708495862567415,
        "overall": 0.16375736660292362,
        "Generated": "424",
        "Gold": "The paper addresses the problem of predicting links and time-stamps in a temporal knowledge graph. The experiments conducted over GDELT and ICEWS14 datasets show that the proposed GHNN offers significantly better results than Know-Evolve. The paper is well written and the model seems sound. Overall, the paper is nicely written and easy to follow. The proposed method only considers limited information from historical event sequences. The results from different models are very close to each other.\nThe paper addresses the problem of predicting links and time-stamps in a temporal knowledge graph. The experiments conducted over GDELT and ICEWS14 datasets show that the proposed GHNN offers significantly better results than Know-Evolve. The paper is well written and the model seems sound. Overall, the paper is nicely written and easy to follow. The proposed method only considers limited information from historical event sequences. The results from different models are very close to each other."
    },
    {
        "coherence": 0.20411169152937375,
        "consistency": 0.739386629592045,
        "fluency": 0.3154258694624563,
        "relevance": 0.038372845798509816,
        "overall": 0.3243242590955962,
        "Generated": "425",
        "Gold": "The paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks.\nThe paper proposed TAP-Vid dataset, which is a benchmark for tracking any point in a video. Given a video clip and a set of query points, the goal is to predict trajectories over the whole video. The contributed data is data recorded from a pre-existing robotic object-stacking simulator. annotations are tracks of points on the surfaces of objects across four datasets. The quality of crowdsourced annotations is measured by crowdsourcing annotations of this synthetic data and comparing point tracks."
    },
    {
        "coherence": 0.06890546332888386,
        "consistency": 0.08702148367736029,
        "fluency": 0.31468198262080643,
        "relevance": 0.010746612776189395,
        "overall": 0.12033888560080999,
        "Generated": "426",
        "Gold": "The paper proposes CoLES that uses contrastive learning to learn representations of event sequence related to user behavior. The method trains in a self-supervised manner by randomly slicing event sequences to generate sub-sequences. CoLES algorithm randomly selects subsequences from the full event sequence from each user. The results show 1~2% improvements which do not match the statements. The paper was published in the Journal of Machine Learning and Machine Intelligence.\nThe paper proposes CoLES that uses contrastive learning to learn representations of event sequence related to user behavior. The method trains in a self-supervised manner by randomly slicing event sequences to generate sub-sequences. CoLES algorithm randomly selects subsequences from the full event sequence from each user. The results show 1~2% improvements which do not match the statements. The paper was published in the Journal of Machine Learning and Machine Intelligence."
    },
    {
        "coherence": 0.1148576252790223,
        "consistency": 0.16837993555547287,
        "fluency": 0.3467774833802348,
        "relevance": 0.024208871517020194,
        "overall": 0.16355597893293752,
        "Generated": "427",
        "Gold": "The authors address a very difficult problem, the problem of dealing with unobserved populations in spiking neural networks. They use mean field approximations to abstract parts of the SNN, resulting in the latent dynamics. After pre-defining a single or multiple neural clusters they can recover the connectivity value(s) from snippets of 10 sec activity of only some observed neurons. The proposed model, neurLVM, is fit via the approximate “hard EM” algorithm.\nThe authors address a very difficult problem, the problem of dealing with unobserved populations in spiking neural networks. They use mean field approximations to abstract parts of the SNN, resulting in the latent dynamics. After pre-defining a single or multiple neural clusters they can recover the connectivity value(s) from snippets of 10 sec activity of only some observed neurons. The proposed model, neurLVM, is fit via the approximate “hard EM” algorithm."
    },
    {
        "coherence": 0.04799697029187719,
        "consistency": 0.12074169919775479,
        "fluency": 0.27511857520035676,
        "relevance": 0.027767733889029936,
        "overall": 0.11790624464475467,
        "Generated": "428",
        "Gold": "This work proposes Distribution Embedding Network (DEN), a meta-learning model for classification. DEN is designed for the setting where data distribution and the number of features can vary across tasks. It classifies examples based on an embedding of data distribution. A novel method for task generation is leveraged using binary classifiers. The proposed network is applied to a classifier aggregation task. The motivation for this method is not very well explained. The experiments are hard to interpret.\nThis work proposes Distribution Embedding Network (DEN), a meta-learning model for classification. DEN is designed for the setting where data distribution and the number of features can vary across tasks. It classifies examples based on an embedding of data distribution. A novel method for task generation is leveraged using binary classifiers. The proposed network is applied to a classifier aggregation task. The motivation for this method is not very well explained. The experiments are hard to interpret."
    },
    {
        "coherence": 0.19518102286568462,
        "consistency": 0.35221580078610587,
        "fluency": 0.21685649010611438,
        "relevance": 0.01852033096219341,
        "overall": 0.1956934111800246,
        "Generated": "429",
        "Gold": "This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations. By using these networks within a CFR framework, the authors manage to avoid huge memory requirements. CFR is the leading method for finding equilibria in imperfect information games. It is typically employed with a tabular policy, limiting its applicability to large games. The authors propose \"Double Neural CFR\", which uses neural network function approximation in place of the tabular update.\nThis paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations. By using these networks within a CFR framework, the authors manage to avoid huge memory requirements. CFR is the leading method for finding equilibria in imperfect information games. It is typically employed with a tabular policy, limiting its applicability to large games. The authors propose \"Double Neural CFR\", which uses neural network function approximation in place of the tabular update."
    },
    {
        "coherence": 0.12496025442887763,
        "consistency": 0.28734760424922445,
        "fluency": 0.05783452428009029,
        "relevance": 0.027998206893565292,
        "overall": 0.12453514746293942,
        "Generated": "430",
        "Gold": "AdaSpeech is a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. The strengths of this paper are entirely through the strong numerical results. It focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder.\nAdaSpeech is a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. The strengths of this paper are entirely through the strong numerical results. It focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder."
    },
    {
        "coherence": 0.05516755211640911,
        "consistency": 0.21654669572547836,
        "fluency": 0.11228026112009185,
        "relevance": 0.05259002206130905,
        "overall": 0.10914613275582208,
        "Generated": "431",
        "Gold": "The authors introduce a simple extension of PPO that uses a single expert demonstration and a modified sampling algorithm to show better performance than vanilla PPO in a difficult 3D setting. This is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios. The method is tested on four environment variations, as constructed in the Animal-AI Olympics challenge env.\nThe authors introduce a simple extension of PPO that uses a single expert demonstration and a modified sampling algorithm to show better performance than vanilla PPO in a difficult 3D setting. This is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios. The method is tested on four environment variations, as constructed in the Animal-AI Olympics challenge env."
    },
    {
        "coherence": 0.11181879641972886,
        "consistency": 0.14799760239445833,
        "fluency": 0.3227361546044029,
        "relevance": 0.018988818278296925,
        "overall": 0.15038534292422173,
        "Generated": "432",
        "Gold": "This paper studies the model-based reinforcement learning. They propose a posterior sampling algorithm and provide a Bayesian regret guarantee. Under the assumption that the reward and transition functions are Gaussian processes with linear kernels, the regret bound is in the order of H^1.5 d sqrt{T} The definition of $BayesRegret$ seems incorrect as it takes $M^*$ as input argument. There is no study on fundamental limit.\nThis paper studies the model-based reinforcement learning. They propose a posterior sampling algorithm and provide a Bayesian regret guarantee. Under the assumption that the reward and transition functions are Gaussian processes with linear kernels, the regret bound is in the order of H^1.5 d sqrt{T} The definition of $BayesRegret$ seems incorrect as it takes $M^*$ as input argument. There is no study on fundamental limit."
    },
    {
        "coherence": 0.1033730831137996,
        "consistency": 0.15755858373416676,
        "fluency": 0.1609214236195076,
        "relevance": 0.013679926047014226,
        "overall": 0.10888325412862206,
        "Generated": "433",
        "Gold": "The proposed method uses CNN for image classification on Chest X-rays and CNN-RNN structure is then applied to generate reports. Different strategy is applied for normal/abnormal cases. The paper reads well and the methodology is relatively clear. The authors however did not made clear what motivated their technical choices compared with Li et.al. (2018) and Xue et al. ( 2018) This work proposed an approach that generates radiology reports from chest x-ray images by splitting pathology-related sentences and the others.\nThe proposed method uses CNN for image classification on Chest X-rays and CNN-RNN structure is then applied to generate reports. Different strategy is applied for normal/abnormal cases. The paper reads well and the methodology is relatively clear. The authors however did not made clear what motivated their technical choices compared with Li et.al. (2018) and Xue et al. ( 2018) This work proposed an approach that generates radiology reports from chest x-ray images by splitting pathology-related sentences and the others."
    },
    {
        "coherence": 0.11062151225387623,
        "consistency": 0.30944360518983605,
        "fluency": 0.14542024534117898,
        "relevance": 0.026893016842880903,
        "overall": 0.14809459490694304,
        "Generated": "434",
        "Gold": "The paper could use some more discussion and careful definitions. The literature section on exact (homotopy) methods for the elastic net and the lasso is lacking. It is not clear that the bound on the number of problems needed for a certain accuracy would be a useful addition. The paper seems to be theoretically sound and follows a logical progression. The authors show that, given a set of validation losses and a distribution over this set, a possibly small number of draws from this distribution are needed to minimize the average validation loss.\nThe paper could use some more discussion and careful definitions. The literature section on exact (homotopy) methods for the elastic net and the lasso is lacking. It is not clear that the bound on the number of problems needed for a certain accuracy would be a useful addition. The paper seems to be theoretically sound and follows a logical progression. The authors show that, given a set of validation losses and a distribution over this set, a possibly small number of draws from this distribution are needed to minimize the average validation loss."
    },
    {
        "coherence": 0.08253221146829147,
        "consistency": 0.12279330153804234,
        "fluency": 0.20063001673167152,
        "relevance": 0.016012174902606747,
        "overall": 0.10549192616015302,
        "Generated": "435",
        "Gold": "The paper addresses learning the ratio between two densities from their samples. The paper is well written, although quite difficult to follow without some theoretical background. It is unclear to me how strong the theoretical claims are. I am also concerned about the reasoning behind several claims and the potential gap between the derived theory and experiments. The authors propose a new risk estimator for DRE, providing a non-negative Bregman divergence estimator, with the non- negative correction.\nThe paper addresses learning the ratio between two densities from their samples. The paper is well written, although quite difficult to follow without some theoretical background. It is unclear to me how strong the theoretical claims are. I am also concerned about the reasoning behind several claims and the potential gap between the derived theory and experiments. The authors propose a new risk estimator for DRE, providing a non-negative Bregman divergence estimator, with the non- negative correction."
    },
    {
        "coherence": 0.0887332902126715,
        "consistency": 0.1367787943601966,
        "fluency": 0.31194150467205156,
        "relevance": 0.016467595751896973,
        "overall": 0.13848029624920416,
        "Generated": "436",
        "Gold": "The paper perfectly match the EC workshop, and I am looking forward to a discussion with the authors. The models also highlight some limitation of classic optimization of the lewis game (i.e. the lack of feedback). The paper is published in the open-source journal, Theoretical Computer Science. It is published by Oxford University Press on behalf of the European Council on Computer Science (ECCS), a member of the Royal Society of London.\nThe paper perfectly match the EC workshop, and I am looking forward to a discussion with the authors. The models also highlight some limitation of classic optimization of the lewis game (i.e. the lack of feedback). The paper is published in the open-source journal, Theoretical Computer Science. It is published by Oxford University Press on behalf of the European Council on Computer Science (ECCS), a member of the Royal Society of London."
    },
    {
        "coherence": 0.08060207726187853,
        "consistency": 0.23863040572067415,
        "fluency": 0.2264650993636017,
        "relevance": 0.014451730001933717,
        "overall": 0.14003732808702202,
        "Generated": "437",
        "Gold": "The paper introduces an Active-Passive SimStereo dataset. This is a synthetic dataset created with physically-based rendering engine Blender. The noise introduce in the paper does not affect negatively most models. It is the first rendered based dataset for stereo vision. The paper is fairly well written and easy to follow. The mathematical notation of the paper and naming convention has some weaknesses. The reported metrics show that existing methods can generalize to active stereo if trained on passive stereo.\nThe paper introduces an Active-Passive SimStereo dataset. This is a synthetic dataset created with physically-based rendering engine Blender. The noise introduce in the paper does not affect negatively most models. It is the first rendered based dataset for stereo vision. The paper is fairly well written and easy to follow. The mathematical notation of the paper and naming convention has some weaknesses. The reported metrics show that existing methods can generalize to active stereo if trained on passive stereo."
    },
    {
        "coherence": 0.10077046478897554,
        "consistency": 0.3054460361454139,
        "fluency": 0.28137662518259,
        "relevance": 0.012041679177428656,
        "overall": 0.17490870132360203,
        "Generated": "438",
        "Gold": "The paper proposes an ensemble approach, called SRR (Self-supervise, Refine, Repeat), for robust unsupervised anomaly detection. The idea is to filter out potential anomaly samples (data refinement) by ensemble model. The effectiveness of the proposed framework is validated on top of contrastive learning-based models. SRR brings the advantages of making the anomaly decision boundaries more robust and giving better data representations. The paper is ready for acceptance.\nThe paper proposes an ensemble approach, called SRR (Self-supervise, Refine, Repeat), for robust unsupervised anomaly detection. The idea is to filter out potential anomaly samples (data refinement) by ensemble model. The effectiveness of the proposed framework is validated on top of contrastive learning-based models. SRR brings the advantages of making the anomaly decision boundaries more robust and giving better data representations. The paper is ready for acceptance."
    },
    {
        "coherence": 0.14770073033761863,
        "consistency": 0.24131939544492997,
        "fluency": 0.24513313390973296,
        "relevance": 0.017438954049447418,
        "overall": 0.16289805343543226,
        "Generated": "439",
        "Gold": "This paper studies the Personalized PageRank (PPR) problem. PPR is a very commonly used metric, and differentially private algorithms are not known. While there is no private algorithm for PPR, randomized response seems like too weak a baseline. The authors propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. The paper is organized well. Paper topic is interesting. theoretical bounds are not studied (NP-Complete etc)\nThis paper studies the Personalized PageRank (PPR) problem. PPR is a very commonly used metric, and differentially private algorithms are not known. While there is no private algorithm for PPR, randomized response seems like too weak a baseline. The authors propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. The paper is organized well. Paper topic is interesting. theoretical bounds are not studied (NP-Complete etc)"
    },
    {
        "coherence": 0.13212728473658308,
        "consistency": 0.39423439601851884,
        "fluency": 0.10212512495502037,
        "relevance": 0.035183342361886395,
        "overall": 0.16591753701800216,
        "Generated": "440",
        "Gold": "Generalization bounds are an extremely central topic of ML. While the extension to stochastic gradient dynamics is very useful, many typos and formulations are difficult to understand. The bound on Cif-10 looks fairly loose even though part of it is non-vacuous. While computing two gradients in a row with one of them to be a full batch could be expensive, this is not the case in this case. The bounds themselves give nice insights into the performance of various algorithms.\nGeneralization bounds are an extremely central topic of ML. While the extension to stochastic gradient dynamics is very useful, many typos and formulations are difficult to understand. The bound on Cif-10 looks fairly loose even though part of it is non-vacuous. While computing two gradients in a row with one of them to be a full batch could be expensive, this is not the case in this case. The bounds themselves give nice insights into the performance of various algorithms."
    },
    {
        "coherence": 0.05320135402477311,
        "consistency": 0.1366218427019951,
        "fluency": 0.10951929389560881,
        "relevance": 0.015907460289110543,
        "overall": 0.07881248772787189,
        "Generated": "441",
        "Gold": "The paper proposed a computationally efficient search procedure for partially observable environments. The key idea behind the proposed method, LBS, is to obtain Monte Carlo estimates of the expected return for every possible action in a given action-observation history of an agent. This work provides a novel extension to the state-of-art approach (SPARTA) to solving Dec-POMDPs, a co-operative variant of partially observable stochastic games.\nThe paper proposed a computationally efficient search procedure for partially observable environments. The key idea behind the proposed method, LBS, is to obtain Monte Carlo estimates of the expected return for every possible action in a given action-observation history of an agent. This work provides a novel extension to the state-of-art approach (SPARTA) to solving Dec-POMDPs, a co-operative variant of partially observable stochastic games."
    },
    {
        "coherence": 0.06312892972283489,
        "consistency": 0.1555403922993342,
        "fluency": 0.2702470846070369,
        "relevance": 0.009944288503333674,
        "overall": 0.12471517378313492,
        "Generated": "442",
        "Gold": "The authors propose a novel and effective method to improve the transferability of adversarial attacks. They increase the logit margins between targeted and non-targeted classes. Comprehensive experiments are presented and the combining logit calibrations have significantly better performance than previous methods. This work is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods. The method uses logit calibration in the CE loss function so that it can improve the targeted adversarial attack.\nThe authors propose a novel and effective method to improve the transferability of adversarial attacks. They increase the logit margins between targeted and non-targeted classes. Comprehensive experiments are presented and the combining logit calibrations have significantly better performance than previous methods. This work is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods. The method uses logit calibration in the CE loss function so that it can improve the targeted adversarial attack."
    },
    {
        "coherence": 0.03283517751085328,
        "consistency": 0.10968121355447952,
        "fluency": 0.1516001233710284,
        "relevance": 0.010458974442130179,
        "overall": 0.07614387221962285,
        "Generated": "443",
        "Gold": "This paper considers generative learning by discretizing a Wasserstein gradient with Euler methods. The proposed method is obtained by minimizing the f-divergence between the initial distribution and the target distribution. The authors have addressed the concerns to some extent in the updated paper. However, some typos/approximative statements make the reading experience difficult. This paper tackles generative modeling via finding the push forward functions T that iteratively moves particles from a reference distribution toward the target data distribution.\nThis paper considers generative learning by discretizing a Wasserstein gradient with Euler methods. The proposed method is obtained by minimizing the f-divergence between the initial distribution and the target distribution. The authors have addressed the concerns to some extent in the updated paper. However, some typos/approximative statements make the reading experience difficult. This paper tackles generative modeling via finding the push forward functions T that iteratively moves particles from a reference distribution toward the target data distribution."
    },
    {
        "coherence": 0.029039999251053952,
        "consistency": 0.07611134741134344,
        "fluency": 0.2705608406785246,
        "relevance": 0.01120316705225309,
        "overall": 0.09672883859829377,
        "Generated": "444",
        "Gold": "This work analyzes the training dynamics of two-layer ReLU networks applied to separable problem data. It is based on the equivalent convex formulation by Pilanci, Ergen 2020. The main result states that the gradient-flow training dynamics provably converges to a \"maximum-margin classifier\" The corresponding non-convex max-margin problem with the following dual reformulation is clear (maybe mathematically correct)\nThis work analyzes the training dynamics of two-layer ReLU networks applied to separable problem data. It is based on the equivalent convex formulation by Pilanci, Ergen 2020. The main result states that the gradient-flow training dynamics provably converges to a \"maximum-margin classifier\" The corresponding non-convex max-margin problem with the following dual reformulation is clear (maybe mathematically correct)"
    },
    {
        "coherence": 0.08872968654269164,
        "consistency": 0.19206701222975756,
        "fluency": 0.23675604767856215,
        "relevance": 0.016126832330296197,
        "overall": 0.1334198946953269,
        "Generated": "445",
        "Gold": "The goal is to continuously learn a model with new few-shot tasks without forgetting the previous ones. The paper introduces a prompt tuning-based framework that augments the pre-trained language model (T5) with continuous prompts. Prompts are simultaneously optimized for task solving and data generation. Using the T5 model, the paper demonstrated the effectiveness of the proposed methods using text classification, named entity recognition (NER), and summarization tasks.\nThe goal is to continuously learn a model with new few-shot tasks without forgetting the previous ones. The paper introduces a prompt tuning-based framework that augments the pre-trained language model (T5) with continuous prompts. Prompts are simultaneously optimized for task solving and data generation. Using the T5 model, the paper demonstrated the effectiveness of the proposed methods using text classification, named entity recognition (NER), and summarization tasks."
    },
    {
        "coherence": 0.06411800223805385,
        "consistency": 0.13614086982888476,
        "fluency": 0.2945493971731235,
        "relevance": 0.00900870965772533,
        "overall": 0.12595424472444686,
        "Generated": "446",
        "Gold": "GLUE is a benchmark consisting of multiple natural language understanding tasks. It functions via uploading to a website and receiving a score based onprivately held test set labels. tasks include acceptability judgement, sentiment prediction, semantic equivalencedetection. GLUE aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. The work also collects an expert evaluated diagnostic evaluation dataset for further examination.\nGLUE is a benchmark consisting of multiple natural language understanding tasks. It functions via uploading to a website and receiving a score based onprivately held test set labels. tasks include acceptability judgement, sentiment prediction, semantic equivalencedetection. GLUE aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. The work also collects an expert evaluated diagnostic evaluation dataset for further examination."
    },
    {
        "coherence": 0.04650431330547194,
        "consistency": 0.08707764056474514,
        "fluency": 0.20889036723448778,
        "relevance": 0.01126524573266077,
        "overall": 0.08843439170934139,
        "Generated": "447",
        "Gold": "The paper is entitled \"Deep Learning for Model Correction in Cardiac Electrophysiological Imaging\" However since all data is simulated, there is really no \"imaging\" involved. Authors should indicate the simulation nature of the paper in the title to set fair expectations. The significance and motivation of the proposed method need to be better explained. The design of the data-driven model is somewhat arbitrary. There is a lack of comparison with existing methods.\nThe paper is entitled \"Deep Learning for Model Correction in Cardiac Electrophysiological Imaging\" However since all data is simulated, there is really no \"imaging\" involved. Authors should indicate the simulation nature of the paper in the title to set fair expectations. The significance and motivation of the proposed method need to be better explained. The design of the data-driven model is somewhat arbitrary. There is a lack of comparison with existing methods."
    },
    {
        "coherence": 0.1198721024243989,
        "consistency": 0.16394421099033726,
        "fluency": 0.33334908902263477,
        "relevance": 0.020493493817496974,
        "overall": 0.15941472406371698,
        "Generated": "448",
        "Gold": "The paper looks at the task of event prediction within communities of Continuous Temporal Dynamic Graph (CTDG) It aims at jointly predicting the event time and the two nodes involved in the event with the CEP3 method. The event prediction is clearly introduced and well formalized. My major concerns are about the model presentation and the experimental setup. The authors propose a united model composed of graph neural networks and marked temporal point process. Experiments are conducted to show the improved performance.\nThe paper looks at the task of event prediction within communities of Continuous Temporal Dynamic Graph (CTDG) It aims at jointly predicting the event time and the two nodes involved in the event with the CEP3 method. The event prediction is clearly introduced and well formalized. My major concerns are about the model presentation and the experimental setup. The authors propose a united model composed of graph neural networks and marked temporal point process. Experiments are conducted to show the improved performance."
    },
    {
        "coherence": 0.1411369743104201,
        "consistency": 0.29797711403247107,
        "fluency": 0.22747368881111976,
        "relevance": 0.022302421125433108,
        "overall": 0.17222254956986102,
        "Generated": "449",
        "Gold": "This work proposes a modification of the original Transformer architecture by replacing attention layers and layers in its Feed-Forward Networks with learned shared dictionaries. The proposed model, called DictFormer, has a smaller number of parameters and uses a smaller amount of computational operations. When evaluated against these models on popular machine translation, summarization, and language modeling benchmarks, Dict former achieves comparable or better performance. Potentially a great paper, but if so it deserves to be much better explained.\nThis work proposes a modification of the original Transformer architecture by replacing attention layers and layers in its Feed-Forward Networks with learned shared dictionaries. The proposed model, called DictFormer, has a smaller number of parameters and uses a smaller amount of computational operations. When evaluated against these models on popular machine translation, summarization, and language modeling benchmarks, Dict former achieves comparable or better performance. Potentially a great paper, but if so it deserves to be much better explained."
    },
    {
        "coherence": 0.1305979860940259,
        "consistency": 0.23735554816336799,
        "fluency": 0.11321737588467536,
        "relevance": 0.04714423960970011,
        "overall": 0.13207878743794232,
        "Generated": "450",
        "Gold": "The main issue with this framework is the motivation for data poisoning. We probably also need to see if we can identify the perturbation introduced by using anomaly detection techniques. If they can be easily identified, decision-makers can run these techniques to check if there are any anomalies. The bi-level optimization approach is principled, well-justified and is applicable to a number of methods for off-policy policy evaluation. The paper introduces an important problem, namely data poisoning of off-Policy evaluation.\nThe main issue with this framework is the motivation for data poisoning. We probably also need to see if we can identify the perturbation introduced by using anomaly detection techniques. If they can be easily identified, decision-makers can run these techniques to check if there are any anomalies. The bi-level optimization approach is principled, well-justified and is applicable to a number of methods for off-policy policy evaluation. The paper introduces an important problem, namely data poisoning of off-Policy evaluation."
    },
    {
        "coherence": 0.18457200110129146,
        "consistency": 0.257627359999423,
        "fluency": 0.10841823811033915,
        "relevance": 0.01245703027152085,
        "overall": 0.14076865737064362,
        "Generated": "451",
        "Gold": "The network is memory and runtime efficient, thus appropriate for certain clinical applications. Paper is very similar (especially all maths) to paper from same 1st author in Brudfors-2019. But clear novelty claimed to now use the MRF-NN model with a UNet instead of a probabilistic segmentation model. Unfortunately, visual results are not very convincing. With larger UNet architecture (j=4,5), the difference of performance between MRf-UNet and UNet gets less significant.\nThe network is memory and runtime efficient, thus appropriate for certain clinical applications. Paper is very similar (especially all maths) to paper from same 1st author in Brudfors-2019. But clear novelty claimed to now use the MRF-NN model with a UNet instead of a probabilistic segmentation model. Unfortunately, visual results are not very convincing. With larger UNet architecture (j=4,5), the difference of performance between MRf-UNet and UNet gets less significant."
    },
    {
        "coherence": 0.047943943973776944,
        "consistency": 0.0756475829685759,
        "fluency": 0.0740456923320055,
        "relevance": 0.017501028746770113,
        "overall": 0.053784562005282116,
        "Generated": "452",
        "Gold": "The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks. In its current form, it's more like a sketchy note rather than a serious academic paper. The experiments appear to show better training at early epochs, but none of the models appear to have been trained to convergence. The authors introduce concept of q-calculus into neural networks along with its advantages. They define a family of Stochastic activation functions based on standard functions together with q-Calculus.\nThe authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks. In its current form, it's more like a sketchy note rather than a serious academic paper. The experiments appear to show better training at early epochs, but none of the models appear to have been trained to convergence. The authors introduce concept of q-calculus into neural networks along with its advantages. They define a family of Stochastic activation functions based on standard functions together with q-Calculus."
    },
    {
        "coherence": 0.12846495595238794,
        "consistency": 0.27213731713788775,
        "fluency": 0.11928574447685672,
        "relevance": 0.014272552243779804,
        "overall": 0.13354014245272805,
        "Generated": "453",
        "Gold": "The paper proposed a Topological Graph Layer (TOGL) plugin for Graph Neural Networks to boost the ability of topological structure detection. The paper is well motivated and mostly clear and easy to follow. Authors propose a topology-aware layer that is compatible with GNNs and can encode connected components and cycles. It differs from the closely related work *Graph Filtration Learning (GFL) The paper provides new insights into topological aware GNN and I summarize the pros as follows.\nThe paper proposed a Topological Graph Layer (TOGL) plugin for Graph Neural Networks to boost the ability of topological structure detection. The paper is well motivated and mostly clear and easy to follow. Authors propose a topology-aware layer that is compatible with GNNs and can encode connected components and cycles. It differs from the closely related work *Graph Filtration Learning (GFL) The paper provides new insights into topological aware GNN and I summarize the pros as follows."
    },
    {
        "coherence": 0.07384219981716218,
        "consistency": 0.1299590319209771,
        "fluency": 0.13406277053335497,
        "relevance": 0.015847205096870882,
        "overall": 0.08842780184209129,
        "Generated": "454",
        "Gold": "The paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations. It incorporates a denoising component based on graph wavelet transforms. The proposed method can outperform baseline methods on graph classification and social recommendation. The experimental results are very limited. The baseline methods are not well established. There are far better results on these datasets such as GIN. The writing and organization of this paper is good.\nThe paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations. It incorporates a denoising component based on graph wavelet transforms. The proposed method can outperform baseline methods on graph classification and social recommendation. The experimental results are very limited. The baseline methods are not well established. There are far better results on these datasets such as GIN. The writing and organization of this paper is good."
    },
    {
        "coherence": 0.1145094898992509,
        "consistency": 0.18912159841326964,
        "fluency": 0.20081740401298723,
        "relevance": 0.015111540129459547,
        "overall": 0.12989000811374185,
        "Generated": "455",
        "Gold": "The authors propose a novel method, called Consensus, to study the interpretability of models, when ground truth of interpretations is not available. The proposed method consists of three stages: forming a committee of deep models, aggregating the results in “quasi ground-truth”, and ranking models based on the similarity with the quasi ground- truth. The approach is evaluated for image classification on ImageNet and CUB-200-2011.\nThe authors propose a novel method, called Consensus, to study the interpretability of models, when ground truth of interpretations is not available. The proposed method consists of three stages: forming a committee of deep models, aggregating the results in “quasi ground-truth”, and ranking models based on the similarity with the quasi ground- truth. The approach is evaluated for image classification on ImageNet and CUB-200-2011."
    },
    {
        "coherence": 0.11378560799057982,
        "consistency": 0.24214526551910384,
        "fluency": 0.17757878302551322,
        "relevance": 0.02081180695896632,
        "overall": 0.1385803658735408,
        "Generated": "456",
        "Gold": "Paper addresses few-shot learning in a combination of episodic meta-learning-based and pre-train finetune-based approach. It proposes meta-dropout, that consists in using dropout on training the base classes during meta-training, but not when tuning on novel classes. Extensive results on several benchmark like PascalVOC, COCO, CUB and mini-ImageNet validate the proposed approach. Although the experiments are widely validated on several benchmarks, the comparison of the experiments miss several State-of-the-art paper results.\nPaper addresses few-shot learning in a combination of episodic meta-learning-based and pre-train finetune-based approach. It proposes meta-dropout, that consists in using dropout on training the base classes during meta-training, but not when tuning on novel classes. Extensive results on several benchmark like PascalVOC, COCO, CUB and mini-ImageNet validate the proposed approach. Although the experiments are widely validated on several benchmarks, the comparison of the experiments miss several State-of-the-art paper results."
    },
    {
        "coherence": 0.30754972620711957,
        "consistency": 0.2330742184605831,
        "fluency": 0.12664542691080763,
        "relevance": 0.03062320449768963,
        "overall": 0.17447314401904998,
        "Generated": "457",
        "Gold": "The paper introduces Variational Predictive Routing (VPR), a spatiotemporal deep generative model for hierarchical video representation learning. VPR automatically detects expected and unexpected changes at each level of the latent hierarchy, which it uses to “route” bottom-up and top-down signals. Compared to a baseline method (VTA), VPR detects changes in synthetic datasets with high accuracy. The paper includes some interesting experiments to support the effectiveness of the proposed method.\nThe paper introduces Variational Predictive Routing (VPR), a spatiotemporal deep generative model for hierarchical video representation learning. VPR automatically detects expected and unexpected changes at each level of the latent hierarchy, which it uses to “route” bottom-up and top-down signals. Compared to a baseline method (VTA), VPR detects changes in synthetic datasets with high accuracy. The paper includes some interesting experiments to support the effectiveness of the proposed method."
    },
    {
        "coherence": 0.14795912345828102,
        "consistency": 0.24180997507145613,
        "fluency": 0.25395614744026507,
        "relevance": 0.0248714171432664,
        "overall": 0.16714916577831715,
        "Generated": "458",
        "Gold": "In this work, the authors investigate the generative and denoising capabilities of the Diffusion-based Deep Generative Models (DDGMs) They claim that a DDGM is a combination of two parts. The first one generates noisy samples from the pure noise by inputting more signal from a learned data distribution. The second one removes the remaining noise from the signal. The proposed DAED seems to have performance on par with DDG Ms for most setups.\nIn this work, the authors investigate the generative and denoising capabilities of the Diffusion-based Deep Generative Models (DDGMs) They claim that a DDGM is a combination of two parts. The first one generates noisy samples from the pure noise by inputting more signal from a learned data distribution. The second one removes the remaining noise from the signal. The proposed DAED seems to have performance on par with DDG Ms for most setups."
    },
    {
        "coherence": 0.29347260697160366,
        "consistency": 0.41676060136886073,
        "fluency": 0.29503145749665355,
        "relevance": 0.08984918064680872,
        "overall": 0.27377846162098163,
        "Generated": "459",
        "Gold": "The paper presents Symbolic Reward Machines (SRM) which helps in achieving interpretable and explainable reward functions. The paper also proposes a hierarchical Bayesian framework to concretize SRM using expert demonstrations. The authors adopt a Bayesian deep learning approach to simultaneously learn a reward approximator, a distribution over the parameters of the reward machine, and the policy to optimize the most-likely reward instance. They demonstrate that the inferred RM outperforms standard reward definition using standard RL approaches."
    },
    {
        "coherence": 0.07349268807141451,
        "consistency": 0.2298604470848005,
        "fluency": 0.1245087672688352,
        "relevance": 0.032621309988008565,
        "overall": 0.11512080310326468,
        "Generated": "460",
        "Gold": "The paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA-NAS), using importance sampling. GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks. The proposed method could be broadly applied to micro/macro, constrained/unconstrained search problems. While there are some promising experimental results, I lean slightly against acceptance. The details regarding training and the generator architecture are relegated to the appendix.\nThe paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA-NAS), using importance sampling. GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks. The proposed method could be broadly applied to micro/macro, constrained/unconstrained search problems. While there are some promising experimental results, I lean slightly against acceptance. The details regarding training and the generator architecture are relegated to the appendix."
    },
    {
        "coherence": 0.15745388068529995,
        "consistency": 0.22686079909149592,
        "fluency": 0.16379277022357874,
        "relevance": 0.016712537169308102,
        "overall": 0.14120499679242068,
        "Generated": "461",
        "Gold": "The paper proposes a decision-tree based method to cluster a set of plan traces in an unsupervised fashion. The splits at each node in the tree are done based on LTL formulae that separate the set of plans at that node into two subsets while optimizing an information-gain measure. Having a method that generates LTL formulas that distinguish one plan from another would be beneficial. The paper was published in the Journal of Computer Science.\nThe paper proposes a decision-tree based method to cluster a set of plan traces in an unsupervised fashion. The splits at each node in the tree are done based on LTL formulae that separate the set of plans at that node into two subsets while optimizing an information-gain measure. Having a method that generates LTL formulas that distinguish one plan from another would be beneficial. The paper was published in the Journal of Computer Science."
    },
    {
        "coherence": 0.04077609123906975,
        "consistency": 0.105890778763119,
        "fluency": 0.24788997856383507,
        "relevance": 0.02628565040573613,
        "overall": 0.10521062474293998,
        "Generated": "462",
        "Gold": "The authors present \"an accelerated first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions\" The analysis aims to establish 'an accelerated rate' up to multiplicative factors induced by the manifold's sectional curvature. It is proven in this paper that this Riemannian algorithm enjoys the same convergence rate as the Euclidean AGD algorithm without convex constraints. The literature review is comprehensive and the background section is well-written.\nThe authors present \"an accelerated first-order method for the optimization of smooth and (strongly or not) geodesically-convex functions\" The analysis aims to establish 'an accelerated rate' up to multiplicative factors induced by the manifold's sectional curvature. It is proven in this paper that this Riemannian algorithm enjoys the same convergence rate as the Euclidean AGD algorithm without convex constraints. The literature review is comprehensive and the background section is well-written."
    },
    {
        "coherence": 0.12802959814805784,
        "consistency": 0.2202346500383739,
        "fluency": 0.1829634905586461,
        "relevance": 0.024570259028664516,
        "overall": 0.1389494994434356,
        "Generated": "463",
        "Gold": "The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function. This approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b)\nThe paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function. This approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b)"
    },
    {
        "coherence": 0.07814629166382636,
        "consistency": 0.25669303438272006,
        "fluency": 0.2993014367402622,
        "relevance": 0.012628587789243997,
        "overall": 0.16169233764401317,
        "Generated": "464",
        "Gold": "CyclesGym is a new benchmark environment for reinforcement learning (RL) applications to agriculture and farming. The benchmark allows for different crops to be produced over a span of multiple years. For the agronomist, this environment could provide a more realistic simulation of the real world than current benchmarks. A key challenge all of these agronomic simulator-RL approaches have is that the simulators are not reliable. Until that is addressed, it's unclear that these simulators have any real value.\nCyclesGym is a new benchmark environment for reinforcement learning (RL) applications to agriculture and farming. The benchmark allows for different crops to be produced over a span of multiple years. For the agronomist, this environment could provide a more realistic simulation of the real world than current benchmarks. A key challenge all of these agronomic simulator-RL approaches have is that the simulators are not reliable. Until that is addressed, it's unclear that these simulators have any real value."
    },
    {
        "coherence": 0.18417105938403913,
        "consistency": 0.22244946921779102,
        "fluency": 0.16603866479267473,
        "relevance": 0.030023805326273904,
        "overall": 0.1506707496801947,
        "Generated": "465",
        "Gold": "This paper studies the problem of answering \"first-order\" questions that correspond to a single fact in a knowledge graph. The key contribution is a way of generating synthetic questions for the relations in the unseen domain for data augmentation. The paper is narrow and focused in its contribution, but the problem is significant enough to merit its attention. It proposes a data generation technique for domain adaptation. The evaluation appears to be well designed and shows some interesting and solid results.\nThis paper studies the problem of answering \"first-order\" questions that correspond to a single fact in a knowledge graph. The key contribution is a way of generating synthetic questions for the relations in the unseen domain for data augmentation. The paper is narrow and focused in its contribution, but the problem is significant enough to merit its attention. It proposes a data generation technique for domain adaptation. The evaluation appears to be well designed and shows some interesting and solid results."
    },
    {
        "coherence": 0.1796837646711841,
        "consistency": 0.2574508588528216,
        "fluency": 0.39027595559185124,
        "relevance": 0.019433137962255945,
        "overall": 0.21171092926952823,
        "Generated": "466",
        "Gold": "An interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain. This paper describes experiments that inject linguistic information (for example dependency structures) into BERT, then measure improvements in correlation with FMRI measurements. The results show improvements in decoding relative to baseline models. The single biggest concern is the extraordinarily high word perplexity scores in Table 2. The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure can improve brain decoding performance.\nAn interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain. This paper describes experiments that inject linguistic information (for example dependency structures) into BERT, then measure improvements in correlation with FMRI measurements. The results show improvements in decoding relative to baseline models. The single biggest concern is the extraordinarily high word perplexity scores in Table 2. The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure can improve brain decoding performance."
    },
    {
        "coherence": 0.3894361535080709,
        "consistency": 0.4705752271336147,
        "fluency": 0.2791530909192238,
        "relevance": 0.022342823857487013,
        "overall": 0.2903768238545991,
        "Generated": "467",
        "Gold": "The paper provides theoretical analysis of the regularized backup for MCTS. It also provides some empirical gains on certain toy domains and some atari games. The paper seems to miss some highly related literature, in particular. The main problem is Proposition 1 and the definition of the convex conjugate. The setting is not clearly written and some key definitions are not introduced properly (see specific comments below). Given that it is hard to understand the main results. The authors consider planning for Markov Decision Process.\nThe paper provides theoretical analysis of the regularized backup for MCTS. It also provides some empirical gains on certain toy domains and some atari games. The paper seems to miss some highly related literature, in particular. The main problem is Proposition 1 and the definition of the convex conjugate. The setting is not clearly written and some key definitions are not introduced properly (see specific comments below). Given that it is hard to understand the main results. The authors consider planning for Markov Decision Process."
    },
    {
        "coherence": 0.07088893703256047,
        "consistency": 0.15230283230991357,
        "fluency": 0.3282101304732281,
        "relevance": 0.017635191947057018,
        "overall": 0.1422592729406898,
        "Generated": "468",
        "Gold": "The paper elegantly formulates it as an extension of the CTC framework and applies it to two tasks (ASR and OCR) The method shows robust training behaviors compared with the original CTC training. This paper would have a broad impact on the machine learning community. The problem addressed by this paper is interesting and relevant. The proposed algorithm was evaluated with simulated masked data and it was shown that the proposed algorithm can handle incomplete labels more effectively.\nThe paper elegantly formulates it as an extension of the CTC framework and applies it to two tasks (ASR and OCR) The method shows robust training behaviors compared with the original CTC training. This paper would have a broad impact on the machine learning community. The problem addressed by this paper is interesting and relevant. The proposed algorithm was evaluated with simulated masked data and it was shown that the proposed algorithm can handle incomplete labels more effectively."
    },
    {
        "coherence": 0.16545126665380752,
        "consistency": 0.2383463966148591,
        "fluency": 0.280892211983436,
        "relevance": 0.022637200918222192,
        "overall": 0.17683176904258122,
        "Generated": "469",
        "Gold": "This paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian. It further proposes an extension with back propagation through the Lanczos algorithm, in order to train end to end models. The main advantage of the proposed method is its ability to generalize well in the presence of a small amount of training data. The authors discuss related work in a thorough and meaningful manner.\nThis paper proposes to use a Lanczos alogrithm, to get approximate decompositions of the graph Laplacian. It further proposes an extension with back propagation through the Lanczos algorithm, in order to train end to end models. The main advantage of the proposed method is its ability to generalize well in the presence of a small amount of training data. The authors discuss related work in a thorough and meaningful manner."
    },
    {
        "coherence": 0.1892513188155388,
        "consistency": 0.2792460790456108,
        "fluency": 0.22982478973465778,
        "relevance": 0.036244533199482155,
        "overall": 0.18364168019882238,
        "Generated": "470",
        "Gold": "AdaAug is an Automated Data Augmentation (AutoDA) method to learn a class/instance-dependent augmentation policy efficiently. The key ideas of AdaAug are two-fold. It uses a hidden feature of the original input to adapt the augmentation for each instance. It could be the first trial to automatically learn class-dependent or instance-dependent augmentations. This paper proposes a search algorithm for instance conditional data augmentation. The paper is well-written and easy to follow.\nAdaAug is an Automated Data Augmentation (AutoDA) method to learn a class/instance-dependent augmentation policy efficiently. The key ideas of AdaAug are two-fold. It uses a hidden feature of the original input to adapt the augmentation for each instance. It could be the first trial to automatically learn class-dependent or instance-dependent augmentations. This paper proposes a search algorithm for instance conditional data augmentation. The paper is well-written and easy to follow."
    },
    {
        "coherence": 0.06678953000229149,
        "consistency": 0.20597008386433555,
        "fluency": 0.24222261233940653,
        "relevance": 0.02769985004100545,
        "overall": 0.13567051906175978,
        "Generated": "471",
        "Gold": "The authors design a distributional word embedding method inspired by Gibsonian theories of perception. They use matrix factorization techniques to derive low-rank object representations in what they call an \"affordance space\" They argue that the learned representations are interpretable, and that this affordance space \"underlies the mental representation of objects\" The main problem I have with the paper as it stands is that it's not clear what the overall goal of the work is. A related sub-field of computational linguistics has been investigating a similar problem.\nThe authors design a distributional word embedding method inspired by Gibsonian theories of perception. They use matrix factorization techniques to derive low-rank object representations in what they call an \"affordance space\" They argue that the learned representations are interpretable, and that this affordance space \"underlies the mental representation of objects\" The main problem I have with the paper as it stands is that it's not clear what the overall goal of the work is. A related sub-field of computational linguistics has been investigating a similar problem."
    },
    {
        "coherence": 0.2124814048378626,
        "consistency": 0.15628744746653414,
        "fluency": 0.36221368891756306,
        "relevance": 0.01954560202901535,
        "overall": 0.18763203581274376,
        "Generated": "472",
        "Gold": "The authors combine a variational auto-encoder and a GAN to generate synthetic cine MR images. They test their method on two well-known datasets showing good results. I have some concerns regarding the results of this work. For instance, the segmentation results are quite similar or even slightly worse than the data augmentation. The novelty of the study is quite limited. Given the limitations of the short paper I think that this is an interesting and reasonably well described work.\nThe authors combine a variational auto-encoder and a GAN to generate synthetic cine MR images. They test their method on two well-known datasets showing good results. I have some concerns regarding the results of this work. For instance, the segmentation results are quite similar or even slightly worse than the data augmentation. The novelty of the study is quite limited. Given the limitations of the short paper I think that this is an interesting and reasonably well described work."
    },
    {
        "coherence": 0.13251775671451438,
        "consistency": 0.24462410933440565,
        "fluency": 0.10118134293033212,
        "relevance": 0.01734151848977224,
        "overall": 0.1239161818672561,
        "Generated": "473",
        "Gold": "The authors studied contraction properties of continuous-time recurrent neural networks. They showed that a network of provably stable RNNs (net of nets) can be trained to reach competitive performance on several benchmarks, including sequential CIFAR10. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices. The presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, the paper needs improvement to meet the acceptance threshold.\nThe authors studied contraction properties of continuous-time recurrent neural networks. They showed that a network of provably stable RNNs (net of nets) can be trained to reach competitive performance on several benchmarks, including sequential CIFAR10. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices. The presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, the paper needs improvement to meet the acceptance threshold."
    },
    {
        "coherence": 0.19416641000956153,
        "consistency": 0.2896851836552858,
        "fluency": 0.1660480384515993,
        "relevance": 0.030931283492979498,
        "overall": 0.17020772890235653,
        "Generated": "474",
        "Gold": "Concept bottleneck models implicitly learn to explain the downstream tasks in addition to learning how to perform them. Authors propose Concept Embedding Models (CEMs) that address this issue. CEM contains an embedding generator layer that considers two embedding representations (one for activate and one for inactivate) and then produces a representation for one concept. Results show that the model produces high task accuracy and interpretable at the same time compared to CBMs.\nConcept bottleneck models implicitly learn to explain the downstream tasks in addition to learning how to perform them. Authors propose Concept Embedding Models (CEMs) that address this issue. CEM contains an embedding generator layer that considers two embedding representations (one for activate and one for inactivate) and then produces a representation for one concept. Results show that the model produces high task accuracy and interpretable at the same time compared to CBMs."
    },
    {
        "coherence": 0.1681348095902607,
        "consistency": 0.2472052716425983,
        "fluency": 0.27602497170603485,
        "relevance": 0.09113408038428679,
        "overall": 0.1956247833307952,
        "Generated": "475",
        "Gold": "The paper proposes the novel idea of using contingency awareness to aid exploration in sparse-reward reinforcement learning tasks. The core idea is to identify controllable (learned) features of the state, which in an Atari game typically corresponds to the position of the player-controlled character. Once this position is known, one can use existing count-based exploration mechanisms to encourage the agent to visit new positions. This approach has clear advantages to earlier proposed count based techniques. The choice of extra loss functions is not very well motivated."
    },
    {
        "coherence": 0.3266287653997491,
        "consistency": 0.30389463075777023,
        "fluency": 0.2997792960271944,
        "relevance": 0.02665217462231703,
        "overall": 0.2392387167017577,
        "Generated": "476",
        "Gold": "Paper introduces a new algorithm within the paradigm of Centralised Training and Decentralised Execution for multi-agent games. EMIX improves upon QMIX by adding a global surprise minimisation objective to the centralised-value function training. Paper also demonstrates improved performance than baselines on most games within Starcraft II. The formulation of the energy-based model is theoretically akin to the minimum conjugate entropy objective. With practical function approximation, the proposed algorithm according to the theory was shown achieving superior performance.\nPaper introduces a new algorithm within the paradigm of Centralised Training and Decentralised Execution for multi-agent games. EMIX improves upon QMIX by adding a global surprise minimisation objective to the centralised-value function training. Paper also demonstrates improved performance than baselines on most games within Starcraft II. The formulation of the energy-based model is theoretically akin to the minimum conjugate entropy objective. With practical function approximation, the proposed algorithm according to the theory was shown achieving superior performance."
    },
    {
        "coherence": 0.15696452952535891,
        "consistency": 0.22135054215169042,
        "fluency": 0.20499406091956324,
        "relevance": 0.024464857181020053,
        "overall": 0.1519434974444082,
        "Generated": "477",
        "Gold": "This paper proposes a method to find the optimal approximation of the target distribution, which has the smallest KL divergence. The theoretical results rely on the assumption of log concave distribution, an assumption that is also made in the study of Langevin type algorithms. This paper produces an algorithm for computing variational posteriors that are normals or mixtures of normals. This is achieved via Wasserstein gradient flows. Remarkably, the system of the Langevin-type stochastic process this gives rise to was proposed before.\nThis paper proposes a method to find the optimal approximation of the target distribution, which has the smallest KL divergence. The theoretical results rely on the assumption of log concave distribution, an assumption that is also made in the study of Langevin type algorithms. This paper produces an algorithm for computing variational posteriors that are normals or mixtures of normals. This is achieved via Wasserstein gradient flows. Remarkably, the system of the Langevin-type stochastic process this gives rise to was proposed before."
    },
    {
        "coherence": 0.2633335579541536,
        "consistency": 0.33240968119844655,
        "fluency": 0.3120887562575986,
        "relevance": 0.036478159237326425,
        "overall": 0.23607753866188128,
        "Generated": "478",
        "Gold": "The paper proposes a GNN-based method for objective space decomposition (ODA) in IP problems. The method is strictly designed for the ODA approach to be solved, which allows it to gains the performance improvements, but does perhaps not leave much room for further generalization. The proposed GNN model takes the instance information (decision variables, constraints, and objectives) as input, and then adaptively discards local upper bounds (to update the search region) without calling an IP solver.\nThe paper proposes a GNN-based method for objective space decomposition (ODA) in IP problems. The method is strictly designed for the ODA approach to be solved, which allows it to gains the performance improvements, but does perhaps not leave much room for further generalization. The proposed GNN model takes the instance information (decision variables, constraints, and objectives) as input, and then adaptively discards local upper bounds (to update the search region) without calling an IP solver."
    },
    {
        "coherence": 0.2635256503602312,
        "consistency": 0.28880298540086097,
        "fluency": 0.19223527008266444,
        "relevance": 0.02266929947146927,
        "overall": 0.19180830132880644,
        "Generated": "479",
        "Gold": "The paper presents a VAE variant that disentangles the features of the inference network at every layer. The approach is implemented as \"recursive disentanglement network\" based on a switch network. The results in dSprites and 3DShapes dataset suggest this variant performs better than well-known VAE networks. Overall, the idea is technically sound and the results look promising. I hope the readers could clarify these questions and I will adjust the score accordingly.\nThe paper presents a VAE variant that disentangles the features of the inference network at every layer. The approach is implemented as \"recursive disentanglement network\" based on a switch network. The results in dSprites and 3DShapes dataset suggest this variant performs better than well-known VAE networks. Overall, the idea is technically sound and the results look promising. I hope the readers could clarify these questions and I will adjust the score accordingly."
    },
    {
        "coherence": 0.1958729599090642,
        "consistency": 0.2022339922588847,
        "fluency": 0.07401613607601713,
        "relevance": 0.05263871084537823,
        "overall": 0.13119044977233607,
        "Generated": "480",
        "Gold": "This paper proposes a method for mean-variance trade-off optimization in RL. The method, named EQUMRL, tries to optimize the Pareto efficiency by maximizing the quadratic utility function. The experimental results show that the algorithm has better performance compared to baselines. The paper is well written and easy to follow in general. The survey in the appendix is helpful for understanding concepts and motivations from a finance and economics perspective.\nThis paper proposes a method for mean-variance trade-off optimization in RL. The method, named EQUMRL, tries to optimize the Pareto efficiency by maximizing the quadratic utility function. The experimental results show that the algorithm has better performance compared to baselines. The paper is well written and easy to follow in general. The survey in the appendix is helpful for understanding concepts and motivations from a finance and economics perspective."
    },
    {
        "coherence": 0.19153771159476235,
        "consistency": 0.3081040145892946,
        "fluency": 0.12250302833863042,
        "relevance": 0.04105362036095803,
        "overall": 0.16579959372091135,
        "Generated": "481",
        "Gold": "The authors propose a novel segmentation scheme that combines the block motion vectors for feature warping, bi-directional propagation, and feature fusion. The experiments show that the model outperforms one recent, closely related work wrt inference time while preserving accuracy. Authors compare against similar pipelines for static processing and show gains in terms of computation time. The authors are expected to conduct more comprehensive experiments. The paper advances a method for accelerating semantic segmentation on video content at higher resolutions.\nThe authors propose a novel segmentation scheme that combines the block motion vectors for feature warping, bi-directional propagation, and feature fusion. The experiments show that the model outperforms one recent, closely related work wrt inference time while preserving accuracy. Authors compare against similar pipelines for static processing and show gains in terms of computation time. The authors are expected to conduct more comprehensive experiments. The paper advances a method for accelerating semantic segmentation on video content at higher resolutions."
    },
    {
        "coherence": 0.353481805105749,
        "consistency": 0.2900003808851903,
        "fluency": 0.274481935491139,
        "relevance": 0.03243093856892384,
        "overall": 0.23759876501275054,
        "Generated": "482",
        "Gold": "coreset WDRO approach is shown to outperform some benchmark methods on well-known machine learning datasets. I do not believe there is an immediate risk of the negative social impact of this work. The numerical experiments look very limited compared to the theoretical richness of the paper. It would be great to see more modern computational experiments with more visualizations. The sound theoretical analysis, clarity of writing, and significance of the contribution all sum up to a strong submission. The authors adequately addressed the limitations and potential negative societal impact of their work.\n coreset WDRO approach is shown to outperform some benchmark methods on well-known machine learning datasets. I do not believe there is an immediate risk of the negative social impact of this work. The numerical experiments look very limited compared to the theoretical richness of the paper. It would be great to see more modern computational experiments with more visualizations. The sound theoretical analysis, clarity of writing, and significance of the contribution all sum up to a strong submission. The authors adequately addressed the limitations and potential negative societal impact of their work."
    },
    {
        "coherence": 0.3096322714987731,
        "consistency": 0.3390900977097987,
        "fluency": 0.15030966203946491,
        "relevance": 0.034030154063304036,
        "overall": 0.20826554632783517,
        "Generated": "483",
        "Gold": "Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? The paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. Instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix.\nAuthors proposes new method against adversarial attacks. Paper is organized well and easy to follow. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? The paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. Instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix."
    },
    {
        "coherence": 0.09845351291194522,
        "consistency": 0.18708172266149523,
        "fluency": 0.16161369567326575,
        "relevance": 0.016297936124275488,
        "overall": 0.11586171684274542,
        "Generated": "484",
        "Gold": "This paper proposes a siamese network for visual tracking, namely SiamCAN. It utilizes cross channel attention, spatial attention, and anchor-free regression head. The method achives state-of-the-art performance on four visual tracking benchmarks. The novelty of the paper is deficient, the proposed method such as cross-attention mechanism and anchored regression have been previously exploited in existing models. The claims, method, and empirical methodology are correct.\nThis paper proposes a siamese network for visual tracking, namely SiamCAN. It utilizes cross channel attention, spatial attention, and anchor-free regression head. The method achives state-of-the-art performance on four visual tracking benchmarks. The novelty of the paper is deficient, the proposed method such as cross-attention mechanism and anchored regression have been previously exploited in existing models. The claims, method, and empirical methodology are correct."
    },
    {
        "coherence": 0.03874482366480276,
        "consistency": 0.06878389948524316,
        "fluency": 0.10873752246122942,
        "relevance": 0.02622607687230218,
        "overall": 0.06062308062089438,
        "Generated": "485",
        "Gold": "The paper provides a reformulation of the fundamental operations in Euclidean space that are used in neural networks for the Poincaré ball model of hyperbolic space. The paper was well-written, and the authors performed regular sanity checks. Some details could be improved, for example, the BLEU score is difficult to interpret, the architecture and task details in Section 4.2 are sparse. Overall, the model shows general improvement over the baseline hyperbolics neural network models.\nThe paper provides a reformulation of the fundamental operations in Euclidean space that are used in neural networks for the Poincaré ball model of hyperbolic space. The paper was well-written, and the authors performed regular sanity checks. Some details could be improved, for example, the BLEU score is difficult to interpret, the architecture and task details in Section 4.2 are sparse. Overall, the model shows general improvement over the baseline hyperbolics neural network models."
    },
    {
        "coherence": 0.4079600769746847,
        "consistency": 0.4733137075831976,
        "fluency": 0.11673355023491777,
        "relevance": 0.035728664382267336,
        "overall": 0.25843399979376686,
        "Generated": "486",
        "Gold": "The authors find that the popular triplet loss will force all same-class instances to a single center in a noisy scenario. After some analyses, the authors propose a simple sampling strategy, EPS. The method achieves good visualization results on MNIST and gets promising performance on benchmarks. I don't think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. I would suggest using three classes to derive theorems, rather than using two.\nThe authors find that the popular triplet loss will force all same-class instances to a single center in a noisy scenario. After some analyses, the authors propose a simple sampling strategy, EPS. The method achieves good visualization results on MNIST and gets promising performance on benchmarks. I don't think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. I would suggest using three classes to derive theorems, rather than using two."
    },
    {
        "coherence": 0.18697959965373254,
        "consistency": 0.2127050732904185,
        "fluency": 0.18715505344583072,
        "relevance": 0.02484545965281011,
        "overall": 0.15292129651069797,
        "Generated": "487",
        "Gold": "This paper extends the notion of \"value equivalence\" among MDPs w.r.t. sets of policies and value functions. It is then possible, for any two classes of models, to find an epsilon for which the models satisfy the definition. Bounds that relate performance of policies across such approximate value equivalent models are also obtained. The paper supports these results by providing intuitions and discussions of their implications. The submission looks technically sound and the core ideas are well-explained.\nThis paper extends the notion of \"value equivalence\" among MDPs w.r.t. sets of policies and value functions. It is then possible, for any two classes of models, to find an epsilon for which the models satisfy the definition. Bounds that relate performance of policies across such approximate value equivalent models are also obtained. The paper supports these results by providing intuitions and discussions of their implications. The submission looks technically sound and the core ideas are well-explained."
    },
    {
        "coherence": 0.12246956982163544,
        "consistency": 0.19087866093199043,
        "fluency": 0.1448609758753269,
        "relevance": 0.036869528146524284,
        "overall": 0.12376968369386927,
        "Generated": "488",
        "Gold": "The work proposes a task that requires several aspects of compositional reasoning. The task of the model is to verify logical statements about an image. The statements are generated based on a grammar over a controlled set of relations/attributes/terms. This work proposes the CURI dataset to measure productive concept learning under uncertainty. The dataset is designed using a concept space defined by a language. The authors also design several out-of-generalization data splits that test models' ood generalization performance.\nThe work proposes a task that requires several aspects of compositional reasoning. The task of the model is to verify logical statements about an image. The statements are generated based on a grammar over a controlled set of relations/attributes/terms. This work proposes the CURI dataset to measure productive concept learning under uncertainty. The dataset is designed using a concept space defined by a language. The authors also design several out-of-generalization data splits that test models' ood generalization performance."
    },
    {
        "coherence": 0.08546965013835131,
        "consistency": 0.11972618772019128,
        "fluency": 0.16012298360869243,
        "relevance": 0.023902501718128747,
        "overall": 0.09730533079634095,
        "Generated": "489",
        "Gold": "This paper proposes a new framework that computes the task-specific representations to modulate the model parameters during the multi-task learning (MTL) This framework uses a single model with shared representations for learning multiple tasks together. The paper argues that providing OHV text labels is not always possible for tasks involving non-text modalities such as speech and images. While the results in Table 2 and Table 3 look promising, the presentation would benefit from more analyses experiments.\nThis paper proposes a new framework that computes the task-specific representations to modulate the model parameters during the multi-task learning (MTL) This framework uses a single model with shared representations for learning multiple tasks together. The paper argues that providing OHV text labels is not always possible for tasks involving non-text modalities such as speech and images. While the results in Table 2 and Table 3 look promising, the presentation would benefit from more analyses experiments."
    },
    {
        "coherence": 0.12001730256225507,
        "consistency": 0.16966880318540903,
        "fluency": 0.18340951579770048,
        "relevance": 0.015454169353949339,
        "overall": 0.12213744772482847,
        "Generated": "490",
        "Gold": "This work defines a search space over 13 regularisation techniques and employs one flavour of Bayesian Optimisation + Hyperband approach. It concludes by substantiating three claims with corresponding experiments. The paper is easy to follow with a thorough literature survey (to my knowledge) and contains relevant experiments. Using 40 tabular datasets, they show mixtures nearly always outperform tuning a single regularizer. The study is formulated as hyper-parameter optimization task, heavily using Auto-Pytorch library.\nThis work defines a search space over 13 regularisation techniques and employs one flavour of Bayesian Optimisation + Hyperband approach. It concludes by substantiating three claims with corresponding experiments. The paper is easy to follow with a thorough literature survey (to my knowledge) and contains relevant experiments. Using 40 tabular datasets, they show mixtures nearly always outperform tuning a single regularizer. The study is formulated as hyper-parameter optimization task, heavily using Auto-Pytorch library."
    },
    {
        "coherence": 0.08542558186239449,
        "consistency": 0.15336952512714036,
        "fluency": 0.10712485715320504,
        "relevance": 0.02586193562403989,
        "overall": 0.09294547494169494,
        "Generated": "491",
        "Gold": "This paper introduces an additional objective to GAN training called “difference departure from normality” that is meant to encourage correlation between real and generated samples projected into some subspace. This is intended to prevent instability during training. The technique is applied to audio spectrogram generation. This paper proposes a conditioning method to train the GANs. It is claimed that this conditioning will not limit the exploration of all modes of real data distribution. The experimental results also support the authors' argument about the improved stability.\nThis paper introduces an additional objective to GAN training called “difference departure from normality” that is meant to encourage correlation between real and generated samples projected into some subspace. This is intended to prevent instability during training. The technique is applied to audio spectrogram generation. This paper proposes a conditioning method to train the GANs. It is claimed that this conditioning will not limit the exploration of all modes of real data distribution. The experimental results also support the authors' argument about the improved stability."
    },
    {
        "coherence": 0.2418570640698817,
        "consistency": 0.2415454860488399,
        "fluency": 0.24459412423538548,
        "relevance": 0.028157840454602706,
        "overall": 0.18903862870217744,
        "Generated": "492",
        "Gold": "The authors propose a measure of compositionality in representations. Given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation of the input x. The framework assumes the presence of an oracle that can give us the true compositional structure. The experiments seem to be fairly similar kinds of composition with very few attributes (mostly bigrams) missing.\nThe authors propose a measure of compositionality in representations. Given instances of data x annotated with semantic primitives, the authors learn a vector for each of the primitive such that the addition of the vectors of the primitives is very close (in terms of cosine) to the latent representation of the input x. The framework assumes the presence of an oracle that can give us the true compositional structure. The experiments seem to be fairly similar kinds of composition with very few attributes (mostly bigrams) missing."
    },
    {
        "coherence": 0.14537411506981135,
        "consistency": 0.17289598317140897,
        "fluency": 0.14080975090724526,
        "relevance": 0.027959984193546193,
        "overall": 0.12175995833550293,
        "Generated": "493",
        "Gold": "This paper presents a general MCMC algorithm for approximating the joint posterior distribution of model parameters and true/denoised data, given a corrupted data set. The paper also states the $O(n)$ time complexity of this step and presents a derivation of MCMC's ergodicity in this context. In this paper, authors propose a method for noise-aware statistical inference under DP. The aim is to enable Bayesian inference that can capture the DP induced noise. The proposed Metropolis within Gibbs sampler targets asymptotically the correct distribution.\nThis paper presents a general MCMC algorithm for approximating the joint posterior distribution of model parameters and true/denoised data, given a corrupted data set. The paper also states the $O(n)$ time complexity of this step and presents a derivation of MCMC's ergodicity in this context. In this paper, authors propose a method for noise-aware statistical inference under DP. The aim is to enable Bayesian inference that can capture the DP induced noise. The proposed Metropolis within Gibbs sampler targets asymptotically the correct distribution."
    },
    {
        "coherence": 0.12896552087943214,
        "consistency": 0.1384791718179487,
        "fluency": 0.09560744768797343,
        "relevance": 0.01842450348268758,
        "overall": 0.09536916096701045,
        "Generated": "494",
        "Gold": "This paper studies how the preprocessed data can be reused as auxiliary tasks in primary multi-task learning (MTL) for the multimodal emotion detection task. Two different hierarchical-level models, FLAT-MTL hierarchical attention model and HAN-Rock model, are proposed. The paper is well written and easy to follow. It is motivated by the analysis of video-conferencing videos which is indeed a crucial and topical issue.\nThis paper studies how the preprocessed data can be reused as auxiliary tasks in primary multi-task learning (MTL) for the multimodal emotion detection task. Two different hierarchical-level models, FLAT-MTL hierarchical attention model and HAN-Rock model, are proposed. The paper is well written and easy to follow. It is motivated by the analysis of video-conferencing videos which is indeed a crucial and topical issue."
    },
    {
        "coherence": 0.17968438590015276,
        "consistency": 0.20730947642836936,
        "fluency": 0.10732344090406659,
        "relevance": 0.029502662979376763,
        "overall": 0.13095499155299137,
        "Generated": "495",
        "Gold": "MorphTE is a new word embedding compression method with morphological augmentation. The authors conduct experiments on machine translation, question answering, and natural language inference tasks. This is important research that mixes pre-existing knowledge and corpus-derived knowledge. The paper fails to be clear with respect to the use of the tensor product. It is unclear how this model generalizes. The method is well motivated and performs well on machine translations.\nMorphTE is a new word embedding compression method with morphological augmentation. The authors conduct experiments on machine translation, question answering, and natural language inference tasks. This is important research that mixes pre-existing knowledge and corpus-derived knowledge. The paper fails to be clear with respect to the use of the tensor product. It is unclear how this model generalizes. The method is well motivated and performs well on machine translations."
    },
    {
        "coherence": 0.41283302806803546,
        "consistency": 0.3801559214429164,
        "fluency": 0.37233292285023883,
        "relevance": 0.014477648448501938,
        "overall": 0.29494988020242313,
        "Generated": "496",
        "Gold": "The paper presents some interesting ideas and is well written, but the content is not quite sufficient for publication. The task of telegraphic sentence compression, whose usefulness is not a priori obvious, is barely motivated. The authors are encouraged to apply there methods to full lengths documents, which would make for a more substantial contribution. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference.\nThe paper presents some interesting ideas and is well written, but the content is not quite sufficient for publication. The task of telegraphic sentence compression, whose usefulness is not a priori obvious, is barely motivated. The authors are encouraged to apply there methods to full lengths documents, which would make for a more substantial contribution. The paper is in general well structured and is easy to follow. However, I think the submission does not have enough content to be accepted to the conference."
    },
    {
        "coherence": 0.22451963707037892,
        "consistency": 0.30713925243368534,
        "fluency": 0.1262555046280868,
        "relevance": 0.04061057139459084,
        "overall": 0.17463124138168545,
        "Generated": "497",
        "Gold": "The paper proposes combining two orthogonal algorithms -- the lazily aggregated gradient(LAQ)method and adaptive quantization (AdaQuantFL)-- to reduce communication complexity in federated learning. At the heart of the combining strategy is Eq. 7, which allocates the bits used in each iteration. The idea of adaptation of quantization and frequency of communications is very nice; however the broadcasting procedure seems to be very time-consuming.\nThe paper proposes combining two orthogonal algorithms -- the lazily aggregated gradient(LAQ)method and adaptive quantization (AdaQuantFL)-- to reduce communication complexity in federated learning. At the heart of the combining strategy is Eq. 7, which allocates the bits used in each iteration. The idea of adaptation of quantization and frequency of communications is very nice; however the broadcasting procedure seems to be very time-consuming."
    },
    {
        "coherence": 0.26058687325382796,
        "consistency": 0.38977523291888444,
        "fluency": 0.2120751922048242,
        "relevance": 0.051094875228774504,
        "overall": 0.22838304340157778,
        "Generated": "498",
        "Gold": "The authors report on a technique to address learning rate hyperparameter tuning for deep learning referred to as optimizer grafting. The paper proposes a meta-algorithm that blends the steps of two optimizers by combining the step magnitude of one (M) with the direction of the other (D) The experiments performed within the paper are of high quality, satisfactorily demonstrating the main claims of the paper. My main qualm with the experiments is the lack of motivation for the specific experimental configurations.\nThe authors report on a technique to address learning rate hyperparameter tuning for deep learning referred to as optimizer grafting. The paper proposes a meta-algorithm that blends the steps of two optimizers by combining the step magnitude of one (M) with the direction of the other (D) The experiments performed within the paper are of high quality, satisfactorily demonstrating the main claims of the paper. My main qualm with the experiments is the lack of motivation for the specific experimental configurations."
    },
    {
        "coherence": 0.09072868176854401,
        "consistency": 0.22335535236685003,
        "fluency": 0.2695309618680855,
        "relevance": 0.028195967204314394,
        "overall": 0.15295274080194848,
        "Generated": "499",
        "Gold": "This is a well executed paper with an interesting application of reinforcement learning to liver tumor ablation planning. The approach is 10x faster than the state-of-the-art and yields significantly better results in terms of placement accuracy. Some aspects of the experiment are confusing. The results are a bit too short, e.g., there is no visualization of resulting paths. It also appears to be a modified re-implementation without much detail.\nThis is a well executed paper with an interesting application of reinforcement learning to liver tumor ablation planning. The approach is 10x faster than the state-of-the-art and yields significantly better results in terms of placement accuracy. Some aspects of the experiment are confusing. The results are a bit too short, e.g., there is no visualization of resulting paths. It also appears to be a modified re-implementation without much detail."
    },
    {
        "coherence": 0.3315237343342486,
        "consistency": 0.3269774932963235,
        "fluency": 0.2458858937473548,
        "relevance": 0.0321308569586289,
        "overall": 0.23412949458413895,
        "Generated": "500",
        "Gold": "Distance Map Layers (DML) can be used as the one just before the final layer in a neural network for classification. DML is mainly used to improve the diversity of predictions from the ensemble members. The experiments are extensive and really promising. The authors derive a new methodology to promote \"orthogonality\" between different ensembles. They randomly choose the centers to vary over classifiers, and impose the I-covariance matrices to be dissimilar.\nDistance Map Layers (DML) can be used as the one just before the final layer in a neural network for classification. DML is mainly used to improve the diversity of predictions from the ensemble members. The experiments are extensive and really promising. The authors derive a new methodology to promote \"orthogonality\" between different ensembles. They randomly choose the centers to vary over classifiers, and impose the I-covariance matrices to be dissimilar."
    },
    {
        "coherence": 0.11758099146969048,
        "consistency": 0.22312532971688986,
        "fluency": 0.08211663981085673,
        "relevance": 0.05767278583204789,
        "overall": 0.12012393670737125,
        "Generated": "501",
        "Gold": "The theory of g-identifiability is a complex one. It is not clear if the new algorithm can be used to solve the problem of how to identify certain types of effects. There are many problems with the way the theory is written. The positivity assumption could be explained in more practical terms, i.e. what does it mean when one has to actually estimate the causal quantity of interest. This paper highlights an issue with a high-impact paper in the literature.\nThe theory of g-identifiability is a complex one. It is not clear if the new algorithm can be used to solve the problem of how to identify certain types of effects. There are many problems with the way the theory is written. The positivity assumption could be explained in more practical terms, i.e. what does it mean when one has to actually estimate the causal quantity of interest. This paper highlights an issue with a high-impact paper in the literature."
    },
    {
        "coherence": 0.06461333710222295,
        "consistency": 0.18234227693668548,
        "fluency": 0.17216859292449313,
        "relevance": 0.026640572478300973,
        "overall": 0.11144119486042564,
        "Generated": "502",
        "Gold": "This paper considers distributed (centralized and decentralized) smooth nonconvex minimization problems in the general case and in the special (quadratic) overparameterized case. The authors give lower bounds on the amount of information transferred in terms of dimension, number of agents, amount of data, and solution accuracy. I like the paper but have some questions/concerns, due to which I have currently weakly accepted the paper."
    },
    {
        "coherence": 0.1098703742498807,
        "consistency": 0.30136836714354787,
        "fluency": 0.08490382491590168,
        "relevance": 0.012913919898411568,
        "overall": 0.12726412155193548,
        "Generated": "503",
        "Gold": "Theorem 3.1, their main theoretical result, upper bounds the fairness violation and lower bounds the utility of the ranking solution found using these constraints. They provide a feasible relaxation of these constraints such that existing methods for optimization can be applied. They demonstrate empirical results on two ‘real’ and one synthetic dataset. In experiments, the proposed approach is the most fair of all baselines and achieves the best tradeoff between utility and fairness.\nTheorem 3.1, their main theoretical result, upper bounds the fairness violation and lower bounds the utility of the ranking solution found using these constraints. They provide a feasible relaxation of these constraints such that existing methods for optimization can be applied. They demonstrate empirical results on two ‘real’ and one synthetic dataset. In experiments, the proposed approach is the most fair of all baselines and achieves the best tradeoff between utility and fairness."
    },
    {
        "coherence": 0.10309461854845847,
        "consistency": 0.20534833829453594,
        "fluency": 0.1762919141995952,
        "relevance": 0.014553499479963672,
        "overall": 0.12482209263063831,
        "Generated": "504",
        "Gold": "The authors propose a 3-phase heuristic algorithm to learn a causal graph from interventional data using continuous optimization. The proposed method is not a systematic approach and accordingly it's hard to reason about its use. Data is discrete-valued, there are no hidden confounders, each intervention affects only one variable, but the location of it may be unknown. The approach suggested fits the network before interventions, then again assigns a likelihood score to the network parameters.\nThe authors propose a 3-phase heuristic algorithm to learn a causal graph from interventional data using continuous optimization. The proposed method is not a systematic approach and accordingly it's hard to reason about its use. Data is discrete-valued, there are no hidden confounders, each intervention affects only one variable, but the location of it may be unknown. The approach suggested fits the network before interventions, then again assigns a likelihood score to the network parameters."
    },
    {
        "coherence": 0.037752908749745274,
        "consistency": 0.10077698289756651,
        "fluency": 0.24274952877487788,
        "relevance": 0.014401199816254779,
        "overall": 0.0989201550596111,
        "Generated": "505",
        "Gold": "Transformers are quite new to the community, and although the authors do not present positive results on the ability of transformers to detect OOD, it could lead to fruitful discussions for further research on the subject. I would have liked to see one other method for OOD detection presented, e.g., ensembling several models. The conclusions of the paper seem very preliminary and are hard to judge without information about the respective models predictive performance.\nTransformers are quite new to the community, and although the authors do not present positive results on the ability of transformers to detect OOD, it could lead to fruitful discussions for further research on the subject. I would have liked to see one other method for OOD detection presented, e.g., ensembling several models. The conclusions of the paper seem very preliminary and are hard to judge without information about the respective models predictive performance."
    },
    {
        "coherence": 0.12134996994158306,
        "consistency": 0.2462192007820611,
        "fluency": 0.11366839030782694,
        "relevance": 0.017427413740874673,
        "overall": 0.12466624369308645,
        "Generated": "506",
        "Gold": "Object-centric models are known to be brittle in training and with regard to hyperparameters. In their experiments, the authors show that the training of SLATE becomes more stable when truncating the gradients after the last refinement step instead of propagating it through the unrolled refinement procedure. The paper combines existing techniques from two groups of works. First, unsupervised object-centric learning with sets representation. Second, implicit differentiation techniques.\nObject-centric models are known to be brittle in training and with regard to hyperparameters. In their experiments, the authors show that the training of SLATE becomes more stable when truncating the gradients after the last refinement step instead of propagating it through the unrolled refinement procedure. The paper combines existing techniques from two groups of works. First, unsupervised object-centric learning with sets representation. Second, implicit differentiation techniques."
    },
    {
        "coherence": 0.1685433995112953,
        "consistency": 0.32211213922136817,
        "fluency": 0.11405216901633859,
        "relevance": 0.015893832732463673,
        "overall": 0.15515038512036644,
        "Generated": "507",
        "Gold": "The paper presents aims to present a method for decomposing a CAD sketch into a set of of _modular-concepts. The modular concepts are compostions of lower-level primitives connected via constraints. The whole pipeline is trained end-to-end under a (permutation-equivariant) reconstruction loss. The proposed method also is capable of auto-completion for CAD modeling. The authors perform experiments on a large scale CAD sketch dataset and mainly demonstrates its applications for design intent interpretation.\nThe paper presents aims to present a method for decomposing a CAD sketch into a set of of _modular-concepts. The modular concepts are compostions of lower-level primitives connected via constraints. The whole pipeline is trained end-to-end under a (permutation-equivariant) reconstruction loss. The proposed method also is capable of auto-completion for CAD modeling. The authors perform experiments on a large scale CAD sketch dataset and mainly demonstrates its applications for design intent interpretation."
    },
    {
        "coherence": 0.056574499958994724,
        "consistency": 0.14421711051615466,
        "fluency": 0.06380143293476012,
        "relevance": 0.009227928367175555,
        "overall": 0.06845524294427127,
        "Generated": "508",
        "Gold": "The paper proposes an approach called VoiceFixer which is aimed at restoring degraded speech signals. It considers a variety of speech degradations - additive noise, reverberations, clipping and limited bandwidth. The paper describes a two stage approach in which the first stage aims to produce restored mel-spectrogram and then a vocoder is used to synthesize the speech. Overall, I believe that the paper has great potential if some concerns are addressed.\nThe paper proposes an approach called VoiceFixer which is aimed at restoring degraded speech signals. It considers a variety of speech degradations - additive noise, reverberations, clipping and limited bandwidth. The paper describes a two stage approach in which the first stage aims to produce restored mel-spectrogram and then a vocoder is used to synthesize the speech. Overall, I believe that the paper has great potential if some concerns are addressed."
    },
    {
        "coherence": 0.06660663643602587,
        "consistency": 0.13805052256272404,
        "fluency": 0.04151226075908664,
        "relevance": 0.015007380515653812,
        "overall": 0.0652942000683726,
        "Generated": "509",
        "Gold": "The paper presents a promising idea to build interpretable models by combining discriminative and generative approach. The proposed model uses an invertible neural network to model the data distribution. It is not clear which attributes such as smiling; gender are important for the classifier's positive /negative attractive decision. The authors propose an attribution map based on those counterfactUALs and evaluate them in a qualitative manner, based on their own observations on 3 datasets.\nThe paper presents a promising idea to build interpretable models by combining discriminative and generative approach. The proposed model uses an invertible neural network to model the data distribution. It is not clear which attributes such as smiling; gender are important for the classifier's positive /negative attractive decision. The authors propose an attribution map based on those counterfactUALs and evaluate them in a qualitative manner, based on their own observations on 3 datasets."
    },
    {
        "coherence": 0.16869771032557904,
        "consistency": 0.21412572724587048,
        "fluency": 0.07434811292115366,
        "relevance": 0.05946093115508374,
        "overall": 0.12915812041192173,
        "Generated": "510",
        "Gold": "This work fills a necessary gap in the risk based decision making literature. It also highlights the limits of current approaches. The author of the paper should be asked to explain his reasons for using a distorted utility function.Experiments show that additive risk leads to worse risk-sensitive performance than composite risk in RL problems. How to confirm that it is a particular case of the composite risk when the epistemic risk measure is replaced with expectation as announced in the abstract?\nThis work fills a necessary gap in the risk based decision making literature. It also highlights the limits of current approaches. The author of the paper should be asked to explain his reasons for using a distorted utility function.Experiments show that additive risk leads to worse risk-sensitive performance than composite risk in RL problems. How to confirm that it is a particular case of the composite risk when the epistemic risk measure is replaced with expectation as announced in the abstract?"
    },
    {
        "coherence": 0.11864791442889712,
        "consistency": 0.2486311655331942,
        "fluency": 0.016637540093148523,
        "relevance": 0.0291942015936283,
        "overall": 0.10327770541221704,
        "Generated": "511",
        "Gold": "The paper lifts the goal recognition design framework to planning domains with incomplete information. It presents a challenging domain for existing combinations of heuristics and search algorithms. The paper would greatly benefit from restructuring the sections such that the k-planner translation is introduced after the definition of the GRD-APK problem. The BFS can be enhanced through causal graph-pruning, which is shown to be safe. This paper deals with the goalrecognition design problem. It extendspreviously used definitions to consider a partially-informed actor."
    },
    {
        "coherence": 0.17291794994737333,
        "consistency": 0.19786784423429854,
        "fluency": 0.09631640756492545,
        "relevance": 0.047771769016001095,
        "overall": 0.1287184926906496,
        "Generated": "512",
        "Gold": "This paper defines a new \"safety\" measure for a ML model given a good reference model. The safety metric is the maximum of outputs deviation between the target model and the reference model for some input sets. Such a measure can be efficiently computed for decision trees, generalized linear and additive models. However, the proposed approach is somewhat too restrictive to generalize to all ML models. This paper proposes to use maximum deviation, defined by the difference between a reference model and a trained model over a superset of training data.\nThis paper defines a new \"safety\" measure for a ML model given a good reference model. The safety metric is the maximum of outputs deviation between the target model and the reference model for some input sets. Such a measure can be efficiently computed for decision trees, generalized linear and additive models. However, the proposed approach is somewhat too restrictive to generalize to all ML models. This paper proposes to use maximum deviation, defined by the difference between a reference model and a trained model over a superset of training data."
    },
    {
        "coherence": 0.03983364073685948,
        "consistency": 0.12764180462779645,
        "fluency": 0.047720648708137726,
        "relevance": 0.01641762438993702,
        "overall": 0.057903429615682664,
        "Generated": "513",
        "Gold": "The hypothesis of converging feature spaces is interesting, but the conclusion from the current experiment results is overstretching. Investigating potential theoretical models or more models and datasets could be fruitful directions for future work. The structure of the paper is well-organized, just some sentences are hard to parse. There are few discussions to provide the readers with some insights. It seems the angle typically converges to 10 to 20 degrees. I hope the authors carefully consider how to enhance this paper.\nThe hypothesis of converging feature spaces is interesting, but the conclusion from the current experiment results is overstretching. Investigating potential theoretical models or more models and datasets could be fruitful directions for future work. The structure of the paper is well-organized, just some sentences are hard to parse. There are few discussions to provide the readers with some insights. It seems the angle typically converges to 10 to 20 degrees. I hope the authors carefully consider how to enhance this paper."
    },
    {
        "coherence": 0.09464945675921231,
        "consistency": 0.13963470509339712,
        "fluency": 0.06195555594232635,
        "relevance": 0.010204858855509027,
        "overall": 0.0766111441626112,
        "Generated": "514",
        "Gold": "The paper claims that a *non-uniform compression* improves the final accuracy of the pruned models on the original data and adversarially perturbed data. The strategy found is implemented on two state-of-the-art robust pruning method, HYDRA and Robust-ADMM. The empirical studies are thorough and convincing. This paper may bring insights and exhibit impact on the network pruning community. The equation between L157 and L158 is informal.\nThe paper claims that a *non-uniform compression* improves the final accuracy of the pruned models on the original data and adversarially perturbed data. The strategy found is implemented on two state-of-the-art robust pruning method, HYDRA and Robust-ADMM. The empirical studies are thorough and convincing. This paper may bring insights and exhibit impact on the network pruning community. The equation between L157 and L158 is informal."
    },
    {
        "coherence": 0.11848404129883755,
        "consistency": 0.20598323015127507,
        "fluency": 0.08892494952763526,
        "relevance": 0.015890216666122364,
        "overall": 0.10732060941096755,
        "Generated": "515",
        "Gold": "This work studies four task-agnostic metrics for evaluating reinforcement learning agents: human similarity, curiosity, empowerment and information gain. Results show that a combination of task reward and curiosity better explain human behavior. The paper as it stands, provide useful, but expected insights. The curiosity and exploration is an important topic for RL research and we need more in-depth analysis of this paper. The difficulty with the paper is that it's not clear what exactly you're after here.\nThis work studies four task-agnostic metrics for evaluating reinforcement learning agents: human similarity, curiosity, empowerment and information gain. Results show that a combination of task reward and curiosity better explain human behavior. The paper as it stands, provide useful, but expected insights. The curiosity and exploration is an important topic for RL research and we need more in-depth analysis of this paper. The difficulty with the paper is that it's not clear what exactly you're after here."
    },
    {
        "coherence": 0.21779663704297827,
        "consistency": 0.3449209677842848,
        "fluency": 0.10881605165138475,
        "relevance": 0.017151218694985343,
        "overall": 0.1721712187934083,
        "Generated": "516",
        "Gold": "PROSPECT is a database for proteomics research based on a massive collection of LC-MS/MS datasets. The paper is well written, and the package adds some functionality to the original database. The primary weakness of the paper is the relatively limited original content. The ultimate goal here is to provide an efficient resource dataset for training new ML models. The number of benchmarks presented here are few to none. This work is relevant in the field of proteomics analysis with Mass Spectrometry.\nPROSPECT is a database for proteomics research based on a massive collection of LC-MS/MS datasets. The paper is well written, and the package adds some functionality to the original database. The primary weakness of the paper is the relatively limited original content. The ultimate goal here is to provide an efficient resource dataset for training new ML models. The number of benchmarks presented here are few to none. This work is relevant in the field of proteomics analysis with Mass Spectrometry."
    },
    {
        "coherence": 0.0987565423292942,
        "consistency": 0.1300039908049208,
        "fluency": 0.04513619438630645,
        "relevance": 0.015545976679078722,
        "overall": 0.07236067604990004,
        "Generated": "517",
        "Gold": "Anchor-changing Regularized Natural Policy Gradient (ARNPG) is a novel framework for designing policy- gradient based methods for multi-objective MDPs. It builds on first-order methods for convex optimization problems. In contrast to prior work, it allows convergence without artificial or impractical assumptions. Some empirical experiments corroborate theoretical results. Theoretically, the designed algorithms based on the ARNPG framework achieve $O(\\frac{1}{T})$ global convergence with exact gradients.\nAnchor-changing Regularized Natural Policy Gradient (ARNPG) is a novel framework for designing policy- gradient based methods for multi-objective MDPs. It builds on first-order methods for convex optimization problems. In contrast to prior work, it allows convergence without artificial or impractical assumptions. Some empirical experiments corroborate theoretical results. Theoretically, the designed algorithms based on the ARNPG framework achieve $O(\\frac{1}{T})$ global convergence with exact gradients."
    },
    {
        "coherence": 0.11808170388208596,
        "consistency": 0.23081967403660578,
        "fluency": 0.09244282937480794,
        "relevance": 0.009015124405131544,
        "overall": 0.1125898329246578,
        "Generated": "518",
        "Gold": "The paper introduces automatically generated, variance-aware test sets for machine translation. 70 test sets covering 35 translation directions based on the datasets from WMT16 to WMT20. The authors promise to release the code for automatic test set generation which will be a valuable asset for future research. While the technique is simple, I think it is an interesting approach, and correlation with human evaluations shows that the resulting test sets meet their purpose. The paper presents a technique to filter machine translation test sets in order to keep a set of instances that allow to better evaluate and compare systems.\nThe paper introduces automatically generated, variance-aware test sets for machine translation. 70 test sets covering 35 translation directions based on the datasets from WMT16 to WMT20. The authors promise to release the code for automatic test set generation which will be a valuable asset for future research. While the technique is simple, I think it is an interesting approach, and correlation with human evaluations shows that the resulting test sets meet their purpose. The paper presents a technique to filter machine translation test sets in order to keep a set of instances that allow to better evaluate and compare systems."
    },
    {
        "coherence": 0.03940740604972193,
        "consistency": 0.08351465204287203,
        "fluency": 0.06197184989584418,
        "relevance": 0.014873749675438282,
        "overall": 0.0499419144159691,
        "Generated": "519",
        "Gold": "This paper presents a label-invariant augmentation for graph-structured data. It conducts the augmentation in the representation space and augments the most difficult sample. Experimental results show that the developed metho outperforms classical GNN-based methods and recent graph contrastive learning. They present experiments on  8 different benchmarks, and show competitive results. The experimental results demonstrate the effectiveness of the proposed augmentation strategy. Different from the conventional node/edge modification and subgraph extraction.\nThis paper presents a label-invariant augmentation for graph-structured data. It conducts the augmentation in the representation space and augments the most difficult sample. Experimental results show that the developed metho outperforms classical GNN-based methods and recent graph contrastive learning. They present experiments on  8 different benchmarks, and show competitive results. The experimental results demonstrate the effectiveness of the proposed augmentation strategy. Different from the conventional node/edge modification and subgraph extraction."
    },
    {
        "coherence": 0.16242949778745733,
        "consistency": 0.3513975850658017,
        "fluency": 0.1697727773515238,
        "relevance": 0.09984316099222391,
        "overall": 0.19586075529925168,
        "Generated": "520",
        "Gold": "This work has revealed a serious issue of adversarial defenses on GNN. Existing defense GNN models can be easily broken by applying customized adaptive attacks. Authors also propose a new metric for measuring the quality of attacks/defenses. The proposed method can also offer a black-box attack to assess a model’s robustness. The paper suggests that non-adaptive attacks lead to an overstate on adversarial robustness, and thus the authors recommend using adaptive attacks as a gold-standard.\nThis work has revealed a serious issue of adversarial defenses on GNN. Existing defense GNN models can be easily broken by applying customized adaptive attacks. Authors also propose a new metric for measuring the quality of attacks/defenses. The proposed method can also offer a black-box attack to assess a model’s robustness. The paper suggests that non-adaptive attacks lead to an overstate on adversarial robustness, and thus the authors recommend using adaptive attacks as a gold-standard."
    },
    {
        "coherence": 0.0445805932562986,
        "consistency": 0.07049489244323719,
        "fluency": 0.17050600078279696,
        "relevance": 0.017484146192709306,
        "overall": 0.07576640816876051,
        "Generated": "521",
        "Gold": "This paper explores how different strategies for fine-tuning affect in- and out-of-distribution performance. The paper theoretically analyzes the tradeoffs in a simplified scenario with two-layer networks. Their experiments on a number of datasets including CIFAR, WILDS-FMoW confirm the intuitions from their theory. This paper suggests that this occurs because fine- tuning distorts features in conjunction with the final linear layer. It is known that fine- Tuning (FT) outperforms linear probing (LP) ID.\nThis paper explores how different strategies for fine-tuning affect in- and out-of-distribution performance. The paper theoretically analyzes the tradeoffs in a simplified scenario with two-layer networks. Their experiments on a number of datasets including CIFAR, WILDS-FMoW confirm the intuitions from their theory. This paper suggests that this occurs because fine- tuning distorts features in conjunction with the final linear layer. It is known that fine- Tuning (FT) outperforms linear probing (LP) ID."
    },
    {
        "coherence": 0.17601246201085755,
        "consistency": 0.2931489687496899,
        "fluency": 0.2040964351474386,
        "relevance": 0.019413219773751992,
        "overall": 0.1731677714204345,
        "Generated": "522",
        "Gold": "The authors consider the problems of prediction with expert advice and online linear regression with bounded, differentiable and strongly convex losses. Using curvature of the strongly-convex loss functions and properly tuning the learning rate, they make appear a negative term in the respective regret bounds of both the algorithm. The paper is a bit hard to follow because of the amount of information that is provided in the text. The writing and clarity of the paper can be improved.\nThe authors consider the problems of prediction with expert advice and online linear regression with bounded, differentiable and strongly convex losses. Using curvature of the strongly-convex loss functions and properly tuning the learning rate, they make appear a negative term in the respective regret bounds of both the algorithm. The paper is a bit hard to follow because of the amount of information that is provided in the text. The writing and clarity of the paper can be improved."
    },
    {
        "coherence": 0.09848221101466316,
        "consistency": 0.1706211504868161,
        "fluency": 0.1256982903903345,
        "relevance": 0.019287735305668248,
        "overall": 0.10352234679937049,
        "Generated": "523",
        "Gold": "DeepSIM is capable of generating the plausible results by manipulating its contents in both a low and a high-level manner. The editing effects of edge maps are not distinct from those of segmentation maps. The authors need to clarify why the VGGNet-based perceptual loss encourages the model to maintain the fidelity. This paper provides an augmentation method to enable single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself.\nDeepSIM is capable of generating the plausible results by manipulating its contents in both a low and a high-level manner. The editing effects of edge maps are not distinct from those of segmentation maps. The authors need to clarify why the VGGNet-based perceptual loss encourages the model to maintain the fidelity. This paper provides an augmentation method to enable single image training. The network learns to map between a primitive representation of the image (e.g. edges and segmentation) to the image itself."
    },
    {
        "coherence": 0.0721408446273632,
        "consistency": 0.12278287046056283,
        "fluency": 0.2009200566865022,
        "relevance": 0.015198803895276971,
        "overall": 0.10276064391742629,
        "Generated": "524",
        "Gold": "This paper proposes ILQR-VAE, a novel method that allows to simultaneously learn latent dynamics and infer unobserved control inputs. The method relies on IQLR solver and recent advances allowing for implicit differentiation to maximize an Evidence Lower Bound on log-likelihood of observation. Authors show comparisons to other models on toy datasets and benchmark datasets for neural data analysis methods. This paper presents a new approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and - importantly - external inputs.\nThis paper proposes ILQR-VAE, a novel method that allows to simultaneously learn latent dynamics and infer unobserved control inputs. The method relies on IQLR solver and recent advances allowing for implicit differentiation to maximize an Evidence Lower Bound on log-likelihood of observation. Authors show comparisons to other models on toy datasets and benchmark datasets for neural data analysis methods. This paper presents a new approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and - importantly - external inputs."
    },
    {
        "coherence": 0.06787180337311287,
        "consistency": 0.19840838794217666,
        "fluency": 0.2042161885158112,
        "relevance": 0.011768572243933345,
        "overall": 0.12056623801875851,
        "Generated": "525",
        "Gold": "GOL is a method for rank estimation using the order constraint and the metric constraint. The algorithm performs best in 80% of benchmark datasets for facial age estimation, HCI classification, and aesthetic score regression. A GOL embedding space represents the direction and distance between objects represent order and metric relations between their ranks. The paper is easy to read. However, parts of the introduction and related work section are repetitive. There are multiple summaries of a paper's reviews.\nGOL is a method for rank estimation using the order constraint and the metric constraint. The algorithm performs best in 80% of benchmark datasets for facial age estimation, HCI classification, and aesthetic score regression. A GOL embedding space represents the direction and distance between objects represent order and metric relations between their ranks. The paper is easy to read. However, parts of the introduction and related work section are repetitive. There are multiple summaries of a paper's reviews."
    },
    {
        "coherence": 0.10268915689538138,
        "consistency": 0.14041945813870815,
        "fluency": 0.19753395907742874,
        "relevance": 0.017173921741322228,
        "overall": 0.11445412396321013,
        "Generated": "526",
        "Gold": "The paper addresses how to learn policies for tasks in which constraints are specified in natural language. They propose a model that encodes the different types of natural language constraints into intermediate representations. Then, they use this as input along with the observation to produce an action at each time step for a safe trajectory. The proposed system is made of two parts: a constraint interpreter that is (mostly) trained in a supervised way with Amazon Mechanical Turk. And a policy that is trained through PCPO, a TRPO-like constraint-aware policy optimization algorithm.\nThe paper addresses how to learn policies for tasks in which constraints are specified in natural language. They propose a model that encodes the different types of natural language constraints into intermediate representations. Then, they use this as input along with the observation to produce an action at each time step for a safe trajectory. The proposed system is made of two parts: a constraint interpreter that is (mostly) trained in a supervised way with Amazon Mechanical Turk. And a policy that is trained through PCPO, a TRPO-like constraint-aware policy optimization algorithm."
    },
    {
        "coherence": 0.25389193027707124,
        "consistency": 0.42196417678356396,
        "fluency": 0.2525109731253388,
        "relevance": 0.029782860042708893,
        "overall": 0.23953748505717073,
        "Generated": "527",
        "Gold": "This paper argues that one reason evaluating the strong Lottery Ticket Hypothesis is difficult is the lack of ground-truth tickets. The authors note a distinction between kinds of sparse networks in the literature. \"Weak tickets\" require training to perform comparably to the original network, while \"strong tickets\" do not. The motivation and the correctness of the lottery ticket planting are not clear to me. The proposed framework is driven by theoretical analysis. The paper is not well-written and the logical flow can be improved.\nThis paper argues that one reason evaluating the strong Lottery Ticket Hypothesis is difficult is the lack of ground-truth tickets. The authors note a distinction between kinds of sparse networks in the literature. \"Weak tickets\" require training to perform comparably to the original network, while \"strong tickets\" do not. The motivation and the correctness of the lottery ticket planting are not clear to me. The proposed framework is driven by theoretical analysis. The paper is not well-written and the logical flow can be improved."
    },
    {
        "coherence": 0.1825027751370372,
        "consistency": 0.3511720290224374,
        "fluency": 0.18097869614242135,
        "relevance": 0.03195807954969865,
        "overall": 0.18665289496289864,
        "Generated": "528",
        "Gold": "Physic-informed neural networks (PINNs) are a new paradigm for injecting the knowledge of governing equations into neural networks. This paper tackles a critical problem of PINNs since training for boundary conditions frequently collides with training for governing equations. The authors test on 3 PDEs: the heat equation in a battery pack, the Navier-Stokes equation over an airfoil, and a 10 dimensional heat equation. Thorough experiments have been done to demonstrate the proposed approach's effectiveness in terms of accuracy.\nPhysic-informed neural networks (PINNs) are a new paradigm for injecting the knowledge of governing equations into neural networks. This paper tackles a critical problem of PINNs since training for boundary conditions frequently collides with training for governing equations. The authors test on 3 PDEs: the heat equation in a battery pack, the Navier-Stokes equation over an airfoil, and a 10 dimensional heat equation. Thorough experiments have been done to demonstrate the proposed approach's effectiveness in terms of accuracy."
    },
    {
        "coherence": 0.19121333521333236,
        "consistency": 0.22760904115005892,
        "fluency": 0.07360786450423507,
        "relevance": 0.018095900090393705,
        "overall": 0.12763153523950502,
        "Generated": "529",
        "Gold": "The paper proposes an unsupervised learning algorithm to learn a goal conditioned policy and the corresponding reward function. The paper is very well written and easy to understand. Original UVFA [1] paper should be cited while citing goal conditioned policies. The problem is important in order to push the field forward to learning representations of the environment without predicting value functions from scalar rewards. In this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully un supervised way.\nThe paper proposes an unsupervised learning algorithm to learn a goal conditioned policy and the corresponding reward function. The paper is very well written and easy to understand. Original UVFA [1] paper should be cited while citing goal conditioned policies. The problem is important in order to push the field forward to learning representations of the environment without predicting value functions from scalar rewards. In this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully un supervised way."
    },
    {
        "coherence": 0.2613671061003617,
        "consistency": 0.4894734056294349,
        "fluency": 0.0535065066854453,
        "relevance": 0.03552294875794646,
        "overall": 0.2099674917932971,
        "Generated": "530",
        "Gold": "The work is well-written and clearly presented. It builds on similar ideas as outlined in several previous works. The experiments has primarily focussed on one aspect while studying on several datasets with varying hetero/homophily. The primary value added explored in this work is to the analysis (and empirics) of the ability to handle heterophilious graphs. The authors present structural constraints on common GNN architectures, allowing them to be interpreted as gradient flows.\nThe work is well-written and clearly presented. It builds on similar ideas as outlined in several previous works. The experiments has primarily focussed on one aspect while studying on several datasets with varying hetero/homophily. The primary value added explored in this work is to the analysis (and empirics) of the ability to handle heterophilious graphs. The authors present structural constraints on common GNN architectures, allowing them to be interpreted as gradient flows."
    },
    {
        "coherence": 0.10445002142467415,
        "consistency": 0.26619218721551996,
        "fluency": 0.0808570453984387,
        "relevance": 0.022645757905075004,
        "overall": 0.11853625298592695,
        "Generated": "531",
        "Gold": "The proposed algorithm addresses the important problem of over-fitting that arises from data driven approaches. The Bayesian aspect is emphasized well with good theoretical background explanations. The proposed approach seems to require more steps and calculations than previous works. The structure is ok albeit not great. The technique appears to work relatively well, although figs 2,3 are quite small. The authors should add a pseudo code in the Appendix section to understand the overall flow of the posterior weight estimation.\nThe proposed algorithm addresses the important problem of over-fitting that arises from data driven approaches. The Bayesian aspect is emphasized well with good theoretical background explanations. The proposed approach seems to require more steps and calculations than previous works. The structure is ok albeit not great. The technique appears to work relatively well, although figs 2,3 are quite small. The authors should add a pseudo code in the Appendix section to understand the overall flow of the posterior weight estimation."
    },
    {
        "coherence": 0.04505443536063376,
        "consistency": 0.10412383766474154,
        "fluency": 0.167489823694146,
        "relevance": 0.018283935929937854,
        "overall": 0.08373800816236479,
        "Generated": "532",
        "Gold": "The authors propose to use NNs to approximate the spectral coefficients of a spectral expansion. I suggest further investigations and analysis on the accuracy/efficiency of the proposed technique. Although the authors didn't discuss the choice of the approximation in detail, the paper is well written. The paper presents an extension of PINN to stochastic PDEs which contain partially unknown parameters. I found the paper well-organized and clearly conveyed what the authors try to express.\nThe authors propose to use NNs to approximate the spectral coefficients of a spectral expansion. I suggest further investigations and analysis on the accuracy/efficiency of the proposed technique. Although the authors didn't discuss the choice of the approximation in detail, the paper is well written. The paper presents an extension of PINN to stochastic PDEs which contain partially unknown parameters. I found the paper well-organized and clearly conveyed what the authors try to express."
    },
    {
        "coherence": 0.16090993527030076,
        "consistency": 0.280301110686908,
        "fluency": 0.1400234665878288,
        "relevance": 0.02615222496686107,
        "overall": 0.15184668437797466,
        "Generated": "533",
        "Gold": "The paper considers the task of quantile regression in a survival analysis context, i.e. one where some examples are right-censored. As in almost all recent works, the encoder backbone is a neural network. The authors use neural networks to replace the linear models. They further improve the sequential grid algorithm with bootstrap weights. A new method is proposed for simultaneous quantile regressions, which is interpreted as a form of expectation maximization. This approach requires the sequential optimization of a new NN for each quantile level.\nThe paper considers the task of quantile regression in a survival analysis context, i.e. one where some examples are right-censored. As in almost all recent works, the encoder backbone is a neural network. The authors use neural networks to replace the linear models. They further improve the sequential grid algorithm with bootstrap weights. A new method is proposed for simultaneous quantile regressions, which is interpreted as a form of expectation maximization. This approach requires the sequential optimization of a new NN for each quantile level."
    },
    {
        "coherence": 0.14340701762025015,
        "consistency": 0.25567474889223674,
        "fluency": 0.09444097165739607,
        "relevance": 0.020497217630250093,
        "overall": 0.12850498895003326,
        "Generated": "534",
        "Gold": "Max-Simes-Fisher (MaSF) is designed for deep neural networks and combines sample features extracted from multiple layers in the network. The method returns a $p$-value for every test sample at a specified significance level. Compared with existing methods for OOD detection, the proposed method achieves similar or better True Positive Rates. As the framework is novel and results shows good performance both in accuracy and computational complexity, I feel that the study deserves to be published."
    },
    {
        "coherence": 0.04941814793381833,
        "consistency": 0.12084788064596412,
        "fluency": 0.1174270776821638,
        "relevance": 0.021443793824525968,
        "overall": 0.07728422502161807,
        "Generated": "535",
        "Gold": "The paper investigates the over-parameterization of attention heads in Transformer’s multi-head attention. Authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. They propose a reparameterized approach allowing the parameters of queries and keys to be shared between heads. This is called “collaborative attention” The paper is well-written and organized, the experiments are thorough.\nThe paper investigates the over-parameterization of attention heads in Transformer’s multi-head attention. Authors show that query-key projections are redundant because trained concatenated heads tend to compute their attention patterns on common features. They propose a reparameterized approach allowing the parameters of queries and keys to be shared between heads. This is called “collaborative attention” The paper is well-written and organized, the experiments are thorough."
    },
    {
        "coherence": 0.13661041281616806,
        "consistency": 0.17514503938099324,
        "fluency": 0.23059263361437893,
        "relevance": 0.010533245714472258,
        "overall": 0.1382203328815031,
        "Generated": "536",
        "Gold": "FedGLOMO is communication-efficient that integrates variance-reduction, both at the server and at local client updates. Assumption 4 is not standard and the authors claimed that it holds for alpha = n. I'm not very familiar with federated learning and it is hard to judge for me how restrictive it is. I find the paper well written. There are places where the statement could be improved. The algorithm can be applied to many ML problems.\nFedGLOMO is communication-efficient that integrates variance-reduction, both at the server and at local client updates. Assumption 4 is not standard and the authors claimed that it holds for alpha = n. I'm not very familiar with federated learning and it is hard to judge for me how restrictive it is. I find the paper well written. There are places where the statement could be improved. The algorithm can be applied to many ML problems."
    },
    {
        "coherence": 0.04774934763713125,
        "consistency": 0.13458496937609296,
        "fluency": 0.13201060307744833,
        "relevance": 0.014658543546437848,
        "overall": 0.0822508659092776,
        "Generated": "537",
        "Gold": "The paper proposed a TUNIT framework with a guiding network, which could encode style codes for the generator and predict the domain labels for the discriminator. The generator given a source image produces an output that follows the structure of the source yet preserves the style and \"domain\" details of the reference image. The proposed method additionally adds a pseudo label prediction branch to separate domains based on maximizing the mutual information. The setting is new, it might not make sense in practice.\nThe paper proposed a TUNIT framework with a guiding network, which could encode style codes for the generator and predict the domain labels for the discriminator. The generator given a source image produces an output that follows the structure of the source yet preserves the style and \"domain\" details of the reference image. The proposed method additionally adds a pseudo label prediction branch to separate domains based on maximizing the mutual information. The setting is new, it might not make sense in practice."
    },
    {
        "coherence": 0.16846305360859526,
        "consistency": 0.18851066333900623,
        "fluency": 0.1766585766256506,
        "relevance": 0.02304732168403981,
        "overall": 0.13916990381432295,
        "Generated": "538",
        "Gold": "The authors propose MetODS: Meta Optimized Dynamical Synapses. The algorithm learns by updating its weights through interaction with the environment. The paper is well motivated by biological and existing artificial neural methods. The authors have improved the paper and addressed some of my concerns. While I still find the assessment to be problematic in some areas, I think the present results, together with the interesting idea, make for a paper that could be a useful contribution to the field.\nThe authors propose MetODS: Meta Optimized Dynamical Synapses. The algorithm learns by updating its weights through interaction with the environment. The paper is well motivated by biological and existing artificial neural methods. The authors have improved the paper and addressed some of my concerns. While I still find the assessment to be problematic in some areas, I think the present results, together with the interesting idea, make for a paper that could be a useful contribution to the field."
    },
    {
        "coherence": 0.09031559852942174,
        "consistency": 0.24291641936190977,
        "fluency": 0.12127077391232646,
        "relevance": 0.015112257424202946,
        "overall": 0.11740376230696523,
        "Generated": "539",
        "Gold": "This paper proposes GT-GAN, a framework for synthesizing regular and irregular time series. It uses various techniques such as generative adversarial networks, auto-encoders, neural ordinary differential equations, and continuous time-flow processes. Experiments show that this method can outperform baselines. The authors evaluate their approach against standard baselines and datasets. In particular, their method outperforms baselines in the irregular data setting with missing observations.\nThis paper proposes GT-GAN, a framework for synthesizing regular and irregular time series. It uses various techniques such as generative adversarial networks, auto-encoders, neural ordinary differential equations, and continuous time-flow processes. Experiments show that this method can outperform baselines. The authors evaluate their approach against standard baselines and datasets. In particular, their method outperforms baselines in the irregular data setting with missing observations."
    },
    {
        "coherence": 0.2468957823040774,
        "consistency": 0.379776984830715,
        "fluency": 0.10077176002363887,
        "relevance": 0.02611990839279683,
        "overall": 0.18839110888780702,
        "Generated": "540",
        "Gold": "This paper introduces dynamic anchor boxes as a query formulation for detection transformer (DETR) The basic idea is to use explicit positional priors and scale priors from anchor boxes. Anchor boxes are also adjusted layer-by-layer to learn the optimal anchor setting. The proposed query formulation introduces marginal computational overhead. The slow convergence of DETR is likely caused by the multiple mode property of the queries. To solve the problem, explicit positionalpriors are used to improve query-to-feature similarity and accelerate model convergence.\nThis paper introduces dynamic anchor boxes as a query formulation for detection transformer (DETR) The basic idea is to use explicit positional priors and scale priors from anchor boxes. Anchor boxes are also adjusted layer-by-layer to learn the optimal anchor setting. The proposed query formulation introduces marginal computational overhead. The slow convergence of DETR is likely caused by the multiple mode property of the queries. To solve the problem, explicit positionalpriors are used to improve query-to-feature similarity and accelerate model convergence."
    },
    {
        "coherence": 0.08515317146896584,
        "consistency": 0.22308817954446292,
        "fluency": 0.07029877038695992,
        "relevance": 0.014071431292576852,
        "overall": 0.09815288817324139,
        "Generated": "541",
        "Gold": "This paper proposes an algorithm to alter the structure of a graph by adding/deleting edges. The main idea is to use the idea of meta-gradients from meta-learning to solve the bilevel optimization problem. Experiments on a few data sets prove the effectiveness of the proposed approach. The experimental results on three graph datasets show that the proposed model could improve the misclassification rate. This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters.\nThis paper proposes an algorithm to alter the structure of a graph by adding/deleting edges. The main idea is to use the idea of meta-gradients from meta-learning to solve the bilevel optimization problem. Experiments on a few data sets prove the effectiveness of the proposed approach. The experimental results on three graph datasets show that the proposed model could improve the misclassification rate. This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters."
    },
    {
        "coherence": 0.12853731897268042,
        "consistency": 0.27720199370692256,
        "fluency": 0.17576351858681075,
        "relevance": 0.010170482952757593,
        "overall": 0.1479183285547928,
        "Generated": "542",
        "Gold": "The goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner. The model is constructed on the basis of self-organizing maps (SOM) and involves reconstruction error in the training. This is one of the best papers I have reviewed in a while. The only question I have is in terms of the medical data. The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients.\nThe goal is to learn a discrete two-dimensional representation of the time series data in an interpretable manner. The model is constructed on the basis of self-organizing maps (SOM) and involves reconstruction error in the training. This is one of the best papers I have reviewed in a while. The only question I have is in terms of the medical data. The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients."
    },
    {
        "coherence": 0.06914284657550704,
        "consistency": 0.10522941986951526,
        "fluency": 0.09298353899438837,
        "relevance": 0.011931931655982131,
        "overall": 0.0698219342738482,
        "Generated": "543",
        "Gold": "SuNCTt is a new loss designed to speed up the convergence of semi-supervised training. The loss involves the computation of similarity between anchor and other images with the same class. Despite the paper has some interesting results, it lacks of real contributions to show its real contributions. The improvement over SimCLR does not seem to be significant under the same training epochs. Using a small amount of labeled data can reduce computation by half.\nSuNCTt is a new loss designed to speed up the convergence of semi-supervised training. The loss involves the computation of similarity between anchor and other images with the same class. Despite the paper has some interesting results, it lacks of real contributions to show its real contributions. The improvement over SimCLR does not seem to be significant under the same training epochs. Using a small amount of labeled data can reduce computation by half."
    },
    {
        "coherence": 0.06462433763295379,
        "consistency": 0.16848030182036172,
        "fluency": 0.08480658198749824,
        "relevance": 0.01590467426317048,
        "overall": 0.08345397392599606,
        "Generated": "544",
        "Gold": "The paper proposes a trainable way to re-order or recover the ordering of features from sets of examples. This could be used to build a common feature space (or embedding) for a neural net. The paper is overall really hard to follow, statements are often confusing or misleading. The ground-truth feature alignment matrix is almost impractical to collect. The empirical results are also not convincing. The major challenge lies in the supervision needed to train the alignment matrix or function.\nThe paper proposes a trainable way to re-order or recover the ordering of features from sets of examples. This could be used to build a common feature space (or embedding) for a neural net. The paper is overall really hard to follow, statements are often confusing or misleading. The ground-truth feature alignment matrix is almost impractical to collect. The empirical results are also not convincing. The major challenge lies in the supervision needed to train the alignment matrix or function."
    },
    {
        "coherence": 0.08552251435111972,
        "consistency": 0.16796210304315512,
        "fluency": 0.12487948743391768,
        "relevance": 0.01783692305025543,
        "overall": 0.09905025696961199,
        "Generated": "545",
        "Gold": "The paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives.\nThe paper introduces OpenSRH, a public dataset including 1300+ clinical SRH images from 300+ brain tumor patients. Contrastive learning on a (frozen) computer vision model (head) significantly improves the classification accuracy of patch-level inference to (almost) patient-level accuracy. The proposed workflow demonstrated in the benchmarking aims to accelerate the development of fast, reliable, and accessible intraoperative diagnosis practices. The reviewer is not convinced that this paper suits the NeurIPS conference's objectives."
    },
    {
        "coherence": 0.11325302396960137,
        "consistency": 0.1317981815659087,
        "fluency": 0.1675445399087781,
        "relevance": 0.01855267569249499,
        "overall": 0.10778710528419579,
        "Generated": "546",
        "Gold": "The paper presents a method to learn a variable error bound to use when building piecewise linear segments for learned indices for databases. It is based on an estimation function that uses a sample to estimate the mean and the variance of the remaining data which will be covered by the next linear segment. The major contribution of this paper is proposing an efficient, pluggable d learned index framework. This paper is trying to solve a problem with practical significance.\nThe paper presents a method to learn a variable error bound to use when building piecewise linear segments for learned indices for databases. It is based on an estimation function that uses a sample to estimate the mean and the variance of the remaining data which will be covered by the next linear segment. The major contribution of this paper is proposing an efficient, pluggable d learned index framework. This paper is trying to solve a problem with practical significance."
    },
    {
        "coherence": 0.029706313336956645,
        "consistency": 0.05945829303761247,
        "fluency": 0.125931148010253,
        "relevance": 0.012727788446270264,
        "overall": 0.0569558857077731,
        "Generated": "547",
        "Gold": "Greedy-GQ is an RL algorithm for a control problem that extends on GTD, which is a prediction algorithm. While Greedy- GQ asymptotically converges to a stationary point, it does so with high sample complexity. The authors reduce the variance of Greedy GQ by incorporating SVRG. The paper seems fairly incremental in terms of the proposed method, applying an existing method for variance reduction to an existing algorithm.\nGreedy-GQ is an RL algorithm for a control problem that extends on GTD, which is a prediction algorithm. While Greedy- GQ asymptotically converges to a stationary point, it does so with high sample complexity. The authors reduce the variance of Greedy GQ by incorporating SVRG. The paper seems fairly incremental in terms of the proposed method, applying an existing method for variance reduction to an existing algorithm."
    },
    {
        "coherence": 0.1339087345736614,
        "consistency": 0.17010863483059085,
        "fluency": 0.18943035580093767,
        "relevance": 0.023705299342196822,
        "overall": 0.1292882561368467,
        "Generated": "548",
        "Gold": "This paper proposes an algorithm to verify whether or not there exists an adversarial example in an Lp ball of size espilon around a given training sample. Unlike other approximate verification techniques, the method is able to show the existence of adversarial examples (in some cases) The proposed verification method is incomplete and returns, given an input either one of 2 certificates (robust or not_robust) or abstains from certification. The scalability to large networks seems to be an issue, although the proposed method significantly outperforms prior work.\nThis paper proposes an algorithm to verify whether or not there exists an adversarial example in an Lp ball of size espilon around a given training sample. Unlike other approximate verification techniques, the method is able to show the existence of adversarial examples (in some cases) The proposed verification method is incomplete and returns, given an input either one of 2 certificates (robust or not_robust) or abstains from certification. The scalability to large networks seems to be an issue, although the proposed method significantly outperforms prior work."
    },
    {
        "coherence": 0.3656611740694733,
        "consistency": 0.4577813907350059,
        "fluency": 0.09050925639972654,
        "relevance": 0.010833092588132497,
        "overall": 0.23119622844808455,
        "Generated": "549",
        "Gold": "Two types of inductive bias: feature-level bias and exemplar-vs-rule bias. Figure shows how two types of bias generalize to different regions of a data set. The results show that the rule-based bias is more dominant than the feature- level bias. The result is a more generalization of the rule based bias to other regions of the data set, rather than just to the whole data set as a result of the feature based bias.\nTwo types of inductive bias: feature-level bias and exemplar-vs-rule bias. Figure shows how two types of bias generalize to different regions of a data set. The results show that the rule-based bias is more dominant than the feature- level bias. The result is a more generalization of the rule based bias to other regions of the data set, rather than just to the whole data set as a result of the feature based bias."
    }
]