1	This paper considers the problem of (biological) sequence design and optimization. The authors made an interesting yet important case that in certain sequence design tasks, a simple evolutionary greedy algorithm could be competitive with the increasingly complex contemporary black-box optimization models.  Most reviewers appreciate the design of the open-source simulation environment in benchmarking AdaLead (and other competing algorithms) in a number of biological sequence design tasks (e.g. TF binding, RNA, and protein). However, there is a common concern that the experimental results are not fine-grained enough to explain the outperforming results of the proposed algorithm. There are also unresolved comments on missing important BO baselines in the empirical study. As a purely empirical work, these appear to be important concerns. While these results appear to be useful for the domain of biological sequence design, the reviewers are unconvinced that the proposed algorithm is significantly novel, or the results are sufficiently compelling to merit an acceptance to this venue.
1	The paper addresses the problem of domain generalization for learning spatio-temporal dynamics. It proposes a solution where an encoder captures some characteristics of a given environment, and a forecaster autoregressively predicts future dynamics conditioned on the characteristics learned by the encoder. Said otherwise, the forecaster learns the general form of dynamics parameterized by an environment representation extracted by the encoder. The conditioning is implemented via an adaptive instance normalization mechanism. A form of padding is also introduced in order to take into account boundary conditions. The two components encoder and forecaster are trained sequentially. This approach is casted in a meta-learning framework. Theoretical results inspired by multi-task learning and domain adaptation are also demonstrated. The model is evaluated and compared to different baselines on three problems, and for two different settings: varying initial conditions with a given dynamics, and dynamics with varying parameters.  This is a borderline paper. It targets a timely and important problem of domain generalization for dynamic environments. The proposed solution is original and compares well experimentally to several baselines. It allows for better generalization performance for the two test settings considered. In the current version, the paper however suffers from different weaknesses. First there is the imprecision of the arguments and the description of the experiments. Some of the arguments and claims are vague and sometimes abusive, not backed up by evidence. For example, a central claim is that the encoder learns time invariant quantities characterizing the environment when the learned representations indeed change with a time shift in the input for any environment. The same goes for the argument developed for the padding construction. It is claimed to model boundary conditions, but this is not supported by any theoretical or empirical evidence. As noted by the reviewers, the theoretical analysis is disconnected from the algorithmic and experimental developments and does not bring much additional value to the paper. What is more embarrassing is that some of the claims in this section are overstated and induce incorrect conclusions.  From Theorem 3.1 and proposition 3.3, the authors suggest that multitask learning leads to better generalization than learning independently, while this is not formally guaranteed by the results (this is acknowledged by the authors in a later comment). Besides, the conditions of validity are not discussed while they seem to only cover situations for which the train and the test distributions are the same. The same holds for the second theoretical results (theorem 3.4). It is claimed that this result supports the authors’ idea of training encoder and forecaster sequentially, while it does not. Besides, the bounds in this result cannot be controlled as noted by the reviewers and are not useful in practice.  Overall, the paper addresses an important topic and proposes new solutions. The results are promising and it is indeed an interesting contribution. However, inaccuracies and incorrect or exaggerated claims make it difficult to accept the current version of the article. The article would make a strong and innovative contribution if it were written as a purely experimental article with a detailed description of the experiments and comparisons.
1	After careful review, I believe that this paper is a useful contribution to the study of animal pose estimation and tracking. The paper received positive reviews from all the reviewers and the authors have successfully addressed the concerns regarding the lack of sufficient novelty and insufficient benchmarking. The authors also added additional low shot experiments to demonstrate the usefulness of the dataset. Based on this, I think the paper meets the bar for the track and should be accepted.
1	This paper proposes a latent point diffusion model, LION, for 3D shape generation. The model builds two denoising diffusion models in the latent spaces of a variational autoencoder. The latent spaces combine a global shape latent representation with a point-structured latent space.  Comprehensive experiments are conducted to evaluate the performance of the proposed method.  The authors address the major concerns of the reviewers and strengthen the paper by providing additional empirical results. After the rebuttal, all four reviewers reach an agreement on accepting the paper because of the novelty and the state-of-the-art performance. The AC agrees with the reviewers and recommends accepting the paper.
1	The paper introduces a new method to probe contextualized word embeddings for syntax and sentiment properties using hyperbolic geometry. The paper is written well and relevant to the ICLR community. Reviewers highlight that the proposed Poincaré probe offers solid results, extensive experiments that support the benefits of the approach, and proposes a new approach to analyze the geometry of BERT models. The revised version clarified various concerns of the initial reviews and improved the manuscript (comparison to Euclidean probes, low dimensional examples, new results on edge length distributions etc.). Overall, the paper makes valuable contributions to probing contextualized word embeddings and the majority of reviewers and the AC support acceptance for its contributions. Please revise your paper to take feedback from reviewers after rebuttal into account (especially to further improve clarity and discussion of the method).
1	Pros: - Provides a practical technique which can dramatically speed up PDE solving -- this is an important and widely applicable contribution. - Paper is simultaneously clearly written and mathematically sophisticated. - The experimental results as impressive.  Cons: - There were concerns that the paper lacks novelty compared to Li et al 2020b, where the underlying theoretical framework was developed. The primary novelty would seem to be: - - using Fourier transforms as the specific neural operator - - the strength of the experimental results  Overall, I recommend acceptance. I believe the techniques in this paper will be practically useful for future research.
1	The authors present a matrix factorization for the social behavior of honey bees in a hive. All the reviewers appreciated the interesting application.  However, substantial concerns were raised about the model motivation and the interpretation of the learned factors. To quote one reviewer, "Some of these bells and whistles may not even be needed, so simplifying the model and streamlining the text would go a long way for me." Another said, "The paper requires more principled motivation for the choices the authors made as well as cleaning up the notation."  The authors did address some of these concerns in discussion, but there are too many lingering concerns to recommend acceptance.  Given the unique application of this paper, the authors might also consider a journal that specializes in computational biology instead.
1	The paper proposes a semi-supervised segmentation technique which uses consistency regularization via different morphological feature perturbations to increase performance with unlabelled data.  It consists of an encoder and two-head decoders. One decoder learns positive attention to the foreground regions generating dilated features using atrous convolutions. The other decoder learns negative attention to the foreground on the same image thereby generating eroded features using skip connections. A consistency regularization is then applied on these paired predictions of an unlabeled image to improve performance. The method is evaluated on two different datasets, and the authors compared the proposed method with other methods from the literature, and reported superior results.   During the rebuttal phase the authors made significant efforts to revise their manuscript especially to improve clarity of their presentation and after rebuttal, all reviewers agreed for weak acceptance. I also agree with this decision and support publication of this paper.
1	The paper presents a construction for deep learning on point clouds that evolve over time. The key characteristics of the data are irregular sampling in the spatial domain and regular sampling in the temporal domain. The presented construction addresses both these aspects of the data. The review by R3 was negative but was addressed by the authors and R3 did not participate in the discussion. The AC supports acceptance.
1	The paper develops a theoretical framework (in the context of learning theory) for training overparameterized networks with label noise, and shows that, in the context of ensemble distillation, teacher networks need not be good classifiers as long as they are good conditional samplers (in the sense defined in the paper).  This is a primarily theoretical paper, with limited empirical results. All reviewers are positive about the paper: one reviewer recommends acceptance (7), the other three recommend weak acceptance (6). The reviewers are positive about the theoretical aspects of the paper, describing them as interesting and technically sound, but are less convinced about the practicality of the theory or the significance of the empirical results.  Given that the theoretical contribution of the paper seems significant and no concerns have been raised about the soundness of the theoretical arguments, I'm happy to recommend acceptance.
1	Three experts reviewed this paper and all recommended rejection. The rebuttal did not change the reviewers' recommendations. The reviewers was not excited by the proposed probabilistic framework and raised many concerns regarding the comparison with baselines and competing methods, limited size of datasets, and limited scope of one dataset for one task. Considering the reviewers' concerns, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere.
1	This paper studied imitation learning in the classification setting. The paper shows that using proper online learning algorithms is not sufficient to obtain sublinear regret, and devises an improper learning framework that relies on online linear optimization resulting in provably efficient algorithms.   All the reviewers appreciated the theoretical novelty and are unanimous in their decision to accept the paper. Please incorporate the reviewers' feedback and the resulting discussion. Adding in some basic experimental results (outlined in the "Experimental plan" comment) would strengthen the paper.
1	The paper makes a novel contribution to methods for generating novel molecules from scratch. The core idea is to generate a shape that fits the molecular pocket without looking at the protein structure.  Two out of three reviewers recommended acceptance. Reviewers emphasize that the method is innovative and interesting, and the empirical performance appealing (especially given that only the shape information is provided to the model). Strong performance is enabled by good design choices made across the paper, such as including the pretraining stage.  The reviewer that recommend rejection raised issues related to the novelty and clarity of the paper. However, I believe the paper is sufficiently clear and novel to meet the bar for acceptance.  Overall, it is my pleasure to recommend acceptance of the paper.
1	This paper had mixed reviews.   One very positive expert reviewer (8) pointed out this paper used a rigorous approach to showing than FP8 can outperform INT8 in inference, which I agree is very interesting and useful.   Another reviewer gave borderline acceptance (5), and I did not find any remaining concerns following the authors' response.  One reviewer gave borderline reject (4), but I did not find any remaining coherent major concerns following the authors' rebuttal. Also, this reviewer seemed less experienced, so I down-weighted this reviewer's score.   Another reviewer gave borderline reject (4) with the following remaining concerns:  (1) "I think this scheme does not show advantages over the existing work. " But I don't think this is true, since as far as I know previous work did not show such an advantage of FP over INT.  (2) "The practical application of the algorithm in this paper will bring extra overhead. " I agree the authors should give more details here (especially for flexible), but at least for the flex bias method, the extra overhead seems quite reasonable (as this is similar to the standard method used for INT), so I'm not sure what is the issue.    (3) "it is a very intuitive view that we should adopt a format with more exponent bits on the data with a large distribution range".  But the authors' response correctly said this is not true (as a uniform distribution would be better represented using INT, no matter what it's range), and is not what they are saying.   Therefore, I think the reviewer had some errors in understanding here, and so I down-weighted this reviewer's score.  Also, the following paper seems relevant: A Block Minifloat Representation for Training Deep Neural Networks, ICLR 2022
1	There is a clear consensus to accept this manuscript.  The results are impressive, and have a nice theoretical orientation that will allow the results to have continued impact as the field advances.  There are some minor errors by the authors in the discussions, which are worth the authors being aware of before submitting their final version.  In particular, they state that:  "The authors of [2,3] considered a bounded activation function in the infinite-width limit with large depth, in which case their variance  V ℓ α α  always converged to a finite fixed point when depth is large [2, eq. 3]. Their chaotic and ordered phases are then defined by the behaviour of the correlation fixed point [2, eq. 5], which in turn determines the behaviour of the gradient [2, eq. 16].  In our case, shaping the activation leads to an unbounded function, and consequently the variance  V ℓ α α  is not always bounded - even if we take the same limit as [2] (but shaping depends on depth instead like the DKS/TAT papers), in which case we get an ODE with finite time explosion. Intuitively, if we drop the Brownian motion from eq. 18 and consider  d X t = b X t ( X t − 1 ) , d t , ,  which is the logistic ODE, and has a finite time explosion if  X 0 > 1  and  b > 0 . At the same time, due to shaping, our correlation  ρ t α β  will actually be able to avoid the fixed point (i.e., non-degenerate). So the gradient will be well behaved from the perspective of correlations (we are in the critical regime defined by [2]), but it may still explode due to variances exploding.  References Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolafia, D.A., Pennington, J. and Sohl-Dickstein, J., 2018. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148. https://arxiv.org/pdf/1810.05148.pdf Schoenholz, S.S., Gilmer, J., Ganguli, S. and Sohl-Dickstein, J., 2016. Deep information propagation. arXiv preprint arXiv:1611.01232. https://arxiv.org/pdf/1611.01232.pdf Yang, G. and Schoenholz, S., 2017. Mean field residual networks: On the edge of chaos. Advances in neural information processing systems, 30. https://arxiv.org/pdf/1712.08969.pdf "  And while [2] states they consider bounded activations, it is not used or necessary and is not used in [3] or subsequent more recent work that discusses the edge of chaos further; see for instance: Activation function design for deep networks: linearity and effective initialisation by Murray et al. and On the impact of the activation function on deep neural networks training by Hayou et al.
1	Summary of discussion: Three reviewers rated the paper Good (7) while Reviewer2 disagreed. R2's criticism was focussed on how this work is placed within existing/related literature, and no technical problem was identified. The authors have addressed some of R2's comments/concerns, R2 has not participated in the discussion.  Novelty and contributions: Overall the reviews seem consistent with an incremental paper which is technically valid, improves the state of the art on a reasonably difficult task. However, it does not appear from the reviews that the paper substantially advances our understanding of machine learning more broadly beyond this specific application.  Experiments: There is some disagreement among reviewers on the adequacy of the experiments, with at least two reviewers calling for experiments involving 'natural photos'. I believe the author's responses adequately address these concerns: they pointed out that the key selling point of their paper is the ability to model structured noise which is less relevant in natural photos.  On the balance of things, I think this paper should be accepted, but I wouldn't argue if it did not make the cut due to its narrow scope. For this reason, I recommended poster presentation.
1	All four reviewers enjoyed this paper and were particularly impressed by the videos provided in the supplementary material. The results are very impressive indeed. The reviewers also agreed that using a multi stage approach was interesting and effective. The two new datasets were deemed useful to the generation community and the proposed metrics and human evaluations were appreciated by the reviewers. A few smaller concerns included a missing failure analysis and some clarifications questions which were addressed in the rebuttal. Given the above, I recommend acceptance.
1	The reviewers were generally split on this paper. On the one hand, reviewers generally appreciated the clear presentation, discussion, and explanations, and the experiments. On the other hand, most reviewers commented on the lack of comparative evaluation to other works, including works that are related conceptually. While the authors have a potentially reasonable argument for omitting such comparisons, in the balance I do not believe that the reviewers were actually convinced by this. Particularly when the novelty of the contribution is not crystal clear, such comparisons are important, so I am inclined to not recommend acceptance at this point (though I acknowledge that the paper is clear borderline and could be accepted).
1	Three out of four reviewers are positive about the paper after the author response and during the discussion.  Strengths include * The proposed method for parameter reduction in transformers allows end-2-end learning cross-modal representations especially on long videos, which has not been possible before * Good performance on audio and video understanding * Extensive set of ablations  Concerns include a somewhat incremental nature of the paper and the still large computational resources to run the experiments. I think, both, the ideas and results are interesting to the community and recommend accept.
1	The reviews are generally positive (though somewhat short), and a large pre-training corpus for legal text will likely be useful for NLP research. One reviewer gave a reject score (5) with the following two weaknesses:  A) The dataset comes from a wide spectrum of law-related data sources, which may differ substantially and hence limit the usefulness of the dataset.  B) Privacy   I am not too worried about Point A because large language models seem to be able to learn from diverse data sources.  Regarding Point B, the separate ethics review mentions the privacy concerns as well but finds that the submission sufficiently discusses this concern and sees no serious ethical issues.  Hence overall I recommend accepting the paper.
1	The paper presents a new problem: open-set single domain generalization, where only one source domain is available and unknown classes and unseen target domains increase the difficulty of the task. To tackle this challenging problem, this paper designs a CrossMatch approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. Then, the paper proposes a cross-classifier consistency regularization that minimizes the multi-binary classifier's output and one-vs-all multi-class classifier's output.   The proposed OS-SDG is an interesting and realistic problem. However, since it is way more challenging, the optimal solution to it remains elusive. Some reviewers think the method might be heuristic and lack theoretical guarantees. Nevertheless, the results are promising and the paper makes a first step toward the challenging OS-SDG problem. Another concern is that the CCR loss needs more ablation studies to further analyze its role. Though the authors have added more explanation of this part, I suggest the authors put more ablation studies in the final supplementary document.   Overall, the paper is novel and interesting.  I would recommend acceptance of this paper given its novelty and impressive performance, but I highly suggest the authors add more ablation studies in the final supplementary, as suggested by the reviewers.
1	The authors study a linear bandit problem with biased feedback, develop an algorithm and bound the corresponding regret. The bandit problem they study is meaningful and highly relevant. I therefore recommend to accept the paper.
0	This is an empirical paper that proposed a few different settings for applying GNNs on temporal data, including what context window to use, code-start vs warm-start, incremental training vs static.  This paper also proposed and released a few more temporal graph datasets, which could be useful.  The consensus assessment of the reviewers is that the contributions of this paper are incremental, and the results are expected and not exciting enough.  I want to in particular point out that the results highlighted in the paper, that a GNN with window size 1 is sufficient to recover 90% of the performance of the model on full graph, is probably not the correct message to communicate.  This either indicates that the data and task used in the benchmarks do not require sophisticated long-horizon temporal information (which makes the comparison between any methods uninteresting), or it indicates that the metric is not sensitive enough to sufficiently distinguish models trained with different settings.  I would recommend rejection and encourage the authors to improve this paper.
1	The Reviewers appreciated the novelty factor of the contrastive meta-learning algorithm proposed in the paper, the theoretical analysis establishing a formal connection with equilibrium propagation, and the appealing features of the resulting meta-learning procedure, which include memory and computation efficiency, as well as the fact that the algorithm affords a biologically-plausible implementation that only requires locally available information for parameter updates. To concretely showcase these properties the paper demonstrates two instantiations of the proposed algorithm that are mechanistically realized through synaptic consolidation and top-down neuronal modulation, respectively. Finally, the paper validates the algorithm on standard few-shot learning benchmarks. The main weaknesses of the paper identified by the Reviewers are the empirical evaluation, which would benefit from a more extensive comparison between methods and more experiments on more challenging meta-learning datasets, and the discussion on the candidate neurobiological substrate for a brain implementation of contrastive meta-learning, which would benefit from a more detailed and systematic description. These limitations however do not substantially detract from the overall quality, relevance and interest of the paper, which Reviewers unanimously recommend for acceptance.
1	The reviews are a bit divergent. While all the reviewers appreciate the clarity of the paper and the theory-inspired proposed algorithm, they raised some concerns e.g., on the assumption employed for obtaining a theoretical result, as well as on marginal (or worse) EO fairness performance in some cases. Although concerns on the fairness performance improvement in light of the employed metrics are still unresolved, many of the concerns are properly addressed, and with regard to the writing quality and insights, I believe that the paper is worth being published. Hence, I recommend the acceptance of this paper.
1	This paper proposes a simple, theoretically motivated approach for post-training quantization. The authors justify its effectiveness with both a sound theoretical analysis, and strong empirical results across many tasks and models, including a state-of-the-art result for 2-bit quantized weights/activations. All reviewers agreed the paper is worth accepting, with 3/4 rating it as a clear accept following the discussion period, and the fourth reviewer not giving strong reasons not to accept.
1	This paper introduces a method to increase diversity/individuality of agents in a MARL setup, based on intrinsic rewards coming from a classifier over behaviours.  Reviewers tend to agree that this is an important/interesting problem, which is related to exploration, a central problem in reinforcement learning. Several reviewers point out that the paper is well written. I appreciate that the authors have been responsive to reviews and have answered and/or addressed several points of concern of the reviewers. The proposed method performs well on the experiments carried out.  Reviews still point out several things that could be improved. The experiments mostly report reward curved, and only few results are actually clearly pointing out the individuality between agents. The fact that this method outperforms the baselines is good, but does not prove individuality and may simply be due to the authors spending more time on the tasks, or other undiscovered phenomenon. A reviewer is concerned that this extra reward could encourage trivial behaviours, and it seems clear that it will if the relative weight of the intrinsic reward is too high. This should be discussed more. Finally, a reviewer points out that classifier-based intrinsic reward for diversity already exists in published works and that this paper is incremental work.  The average score for this paper is very close to the acceptance threshold, but based on the reviews I recommend to reject this paper for ICLR 2021. I am confident that when the authors address further the reviewers concerns and improve the experimental results, this paper will be published in a future venue.
1	The paper introduces a new method for encoding dynamics of temporal networks.  The approach, while not ground-breaking, is interesting and the results are fairly convincing.  The submission raised a number of concerns from the reviewers. They questioned the complexity of the proposed approach (R3 and R4), the clarity/readability (R2 and R1), and appropriateness of the link sampling strategy (R2), as well as raised several more minor (from my perspective) issues. I believe that the authors adequately addressed most of these concerns in their rebuttal and the revision.  R2 has confirmed that they read the rebuttal and raised their score to strong accept. Unfortunately, the other reviewers have not engaged during the discussion period, and it is unclear if they are satisfied with the clarifications and changes. Nevertheless, after reading the authors' responses and skimming through the manuscript, I believe that most concerns have been addressed, and this is a good paper that deserves to be accepted. That being said, the issue of readability has been raised by the reviewers, and, while I do not think the paper is unreadable, I do agree that there is much room for improvement. I would encourage the authors to polish the manuscript for the camera-ready version, as well as try to address the remaining concerns raised by the reviewers.
1	Executive summary:  The problem considered in this paper is as follows: There is a distribution over items X \subseteq [0,\bar{x}]^n where x_i denotes the value of the item to recipient i. There are also matching constraints {p_i}_{i \in N}, which require that each agent should be matched a p_i fraction pf the times. The goals is to maximize the sum of recipient utilities subject to the matching propability constraints, and also satisfying that no recipient i envies another recipient by more than a factor \gamma_i.  It is shown that this problem can be solved as a semi-discrete optimal transport problem. They also give a stochastic optimization algorithm which converges at rate O(1/sqrt(T)), and a PAC-style sample complexity result (showing that with O(n/eps^2) samples an eps-approximate solution can be found with high probability).  Discussion and recommendation:  This paper is a bit out of my comfort zone, so I am mostly relying on the reviews, which are rather positive and supportive of the paper. The connection to optimal transport is appreciated, and the approximation results (while rather standard) seem to find their audience as well.  Weak accept.
1	The paper analyses theoretically the 'Matthew effect' (disparate impact) in the setting  of the semi-supervised learning and its effect on fairness and performance.   All reviewers agree that the paper deals with a very interesting topic and important problem.  The paper discusses and presents a thorough and convincing analysis of the effect. There were multiple concerns raised mainly around the lack of clarity at parts of the paper. The authors did a very good job at resolving those and bringing their submission to a good standard.  In the rebuttal I was glad to see a great dialog evolving among the authors and reviewers.  I congraultate both sides.   Happy to recommend acceptance.
1	This paper proposes to learn a latent space representation such that some linear equivariance and symmetry constraints are respected in the latent space, with the goal to improve sample efficiency. One core idea is that the latent space is also the same as the space of linear transformation used in the constraints, which is shown to simplify some of the mathematical derivations. Experiments on the Atari 100K benchmark demonstrate a statistical improvement over the SPR baseline when using the SE(2) group of linear transformations as latent space.  Following the discussion period, most reviewers were in favor of acceptance. However, one reviewer remained unconvinced, and after carefully reading the paper, I actually share the same concerns, i.e., that it is unclear under which conditions the proposed approach actually works, and what makes it work. I believe that, as a research community, we should value understanding over moving the needle on benchmarks, especially when proposing such a complex method as this one (see Fig. 5).  More specifically:  1. The method is only evaluated on Atari games, showing some improvements when using SE(2), and arguing that there are corresponding symmetries in such games. There is however no analysis demonstrating (or even hinting at the fact) that the proposed technique is actually learning to take advantage of such symmetries (NB: I had a quick look at the animation added by the authors in the supplementary material, but I do not see if/how they help on this point). Even if analyzing representations on Atari may be tricky, I believe that given the motivation of this new algorithm, it *must* be evaluated on some toy example (e.g., the pendulum mentioned throughout the paper) to validate that it is learning what we want it to learn (although I also agree with the authors that experimenting on a more complex benchmark like Atari is equally important).  2. The idea of embedding states into the same space as transformations is interesting, and brings some advantages when writing down equations, as demonstrated by the authors. However, there is no justification besides mathematical convenience, and it doesn't seem intuitive to me at all that why this should be a good idea, considering that it ties the state representation to the mathematical representation of group transformations. For instance, what does the spcial group element $e$ mean for a state? And this coupling makes it difficult to interpret the effect of using a different group of transformations: for instance when moving from GL(2) to SE(2), is the observed benefit because we are using only specific transformations, or simply because we are reducing the dimensionality of the state embedding? (note that in Fig. 4(c) the MLP variant has similar performance to GL(2), and based on my understanding they use the same embedding dimensionality  ==> I believe it would be important to check what would happen with an MLP variant using the same dimensionality as SE(2))  3. The effect of the $L_{GET}$ loss is not convincing, as pointed out by several reviewers. I think it would have been an opportunity for the authors to investigate why, especially since it seems to work in some games and not others. But just focusing on "here are the 17/26 games where it works better" doesn't really bring added value here. Do these games have some specific properties that make them better candidates to take advantage of $L_{GET}$? This could have been a very interesting insight if that was the case, but as it is now, I am not sure what we can learn from that.  4. There are several implementation "details", some moving the final algorithm farther from its theoretical justification, that are not ablated, making it difficult to understand their impact (ex: using target networks, the choice of the value of M, using projections onto the unit sphere of some arbitrary dimensionality, how the $s'$ state is chosen in $L_{GET}$)  As a result, we have here an algorithm with some interesting theoretical background, but with a lot of moving components which -- when properly tweaked -- can lead to a statistically meaningful improvement on Atari 100K -- without really understanding why. I believe this is not quite enough for publication at ICLR, and I would encourage the authors to delve deeper into the understanding of their algorithm, which I hope will bring useful insights to the research community working on representation learning.
1	This work introduce a new benchmark to evaluate machine learning techniques for physical simulation. Based on two real-world applications (power grid and pneumatic simulation), the authors clarified the research questions and define evaluation metrics. Existing ML-based methods do not perform well on the proposed datasets. Overall all reviewer agreed that this work proposes a promising benchmark and I think the proposed benchmark can be useful in real-world application. Even though one reviewer has some concerns on details and setups, the authors provided detailed response to address them. For this reason, I'd like to recommend accept.
0	This paper proposed a personalized federated learning algorithm which takes into account the similarity of gradient of different users to update the model. Although the ideas presented are intuitive, the algorithms have fundamental limitations, for example, they may cause large overhead of memory, communication and computation, and are unsuitable for privacy-preserving machine learning. In addition, there are no rigorous analysis and the experiments are not convincing. This is a clear rejection.
1	The paper proposes to add a regularisation term H to RL algorithms in order to work around issues caused by the multiple fixed points of the Bellman’s optimality equation. The added H term is inspired by quantum field theory, specifically the K-spin Ising model. All reviewers thought this was an interesting idea, but by the end of the review period, there remained some problems with this paper. Indeed, this paper is not a theory paper, and there is no mathematical proof that the added H term does accomplish the stated goal of variance reduction. This leaves us with empirical evidence. Unfortunately, as was pointed out by reviewers, "Experiment is limited to the 6 MuJoCo tasks", which is not enough to convince that the algorithm should generally work. Finally, many reviewers were confused by the claim that PPO solves the Bellman Optimality Equation. By the end of the review, not all reviewers were convinced this problem had been resolved. This point should be clarified, and it would be better for the paper to go through a new round of reviews before being accepted for publication.
1	Four knowledgeable referees reviewed this submission. The reviews raised concerns about the novelty of the proposed approach (rY1T, S4w5), the motivation of the model design and properties (mij9, r2Bq), and the empirical evidence to support some of the effectiveness and efficiency claims (rY1T, r2Bq, S4w5). The rebuttal addresses the reviewers' concerns by (1) highlighting the differences of the proposed approach with ConvNext, (2) providing additional comparisons with state-of-the-art methods as suggested by the reviewers, (3) performing ablations of the MSCA design which empirically emphasize its advantages, and (4) partially clarifying the motivation. The authors engage in discussion with the reviewers and provide additional clarifications (e.g. what will be introduced in the main body of the paper, and whether the code will be released). During the discussion phase, the reviewers show some hesitations wrt novelty of the proposed approach which is perceived as incremental wrt ConvNext. However, the reviewers agree that the paper is well written, the approach is simple and appears effective, and the experimental evidence is extensive and supports the claims made in the manuscript. The reviewers appreciate the benchmarking efforts of this work and lean towards acceptance. The AC agrees with the reviewers' assessment that the strength of this paper lies in its extensive experimental validation, and recommends to accept.
1	The paper shows that using final fully-connected layers helps the generalization of convolutional neural networks in low-data regimes. The addition of these layers significantly improves model quality resulting in a network with the same number of parameters and better generalization performance.  Initially reviewers had mixed evaluation of the paper. All the reviewers saw that the proposed method is simple and easy to follow, at the same time providing clear improvements over baselines. Also agreed that the results are "significant" and "surprising" effect. There were some concerns raised by the reviewers but the author's rebuttal mostly addressed and improved the paper with sufficiently more experiments and analysis supporting the main claim. Reviewer `DX6o` mentioned that there are few updates promised by the authors which can't be validated until camera ready but it does not seem to warrant block publication.   The Author-Reviewer discussion period was active and the authors did a great job clearing various concerns and questions and all reviewers agreed to support acceptance of the paper. The paper demonstrates a simple yet effective method for small data regime which would be interesting to the broad NeurIPS audience both for practitioners as well as researchers.
1	Meta Review: This paper applies reinforcement learning (RL) to learning a robust policy in restless bandits, which performs well in the worst case. This is a major departure from the traditional approaches to (restless) bandits, where the policy is designed manually to have low regret in theory. While the authors analyzed an idealized variant of their approach, the strengths are generality and being data-adaptive. None of the reviewers had major concerns. I also looked at the paper. My comment is that the authors should present their work better in the context of other attempts to learn bandit policies from data, such as  Algorithms for the multi-armed bandit problem: https://arxiv.org/pdf/1402.6028.pdf  Differentiable meta-learning of bandit policies: https://proceedings.neurips.cc/paper/2020/file/171ae1bbb81475eb96287dd78565b38b-Paper.pdf  Policy gradient optimization of Thompson sampling policies: https://arxiv.org/pdf/2006.16507.pdf  I support acceptance of this paper.
1	Meta Review: AC read the paper, reviews, and responses. AC appreciates the simple and effective StackMix method that surpasses all existing baselines.  Though the average rating is below the acceptance bar, AC still recommends acceptance due to the comprehensive experimental results that may shed light on future research in the community. However, AC suggests that the authors do follow the negative comments, especially from Reviewer Z7zb, to improve the quality of the paper for publication.
1	The paper tackles the problem of missing data in centralized training multi-agent RL approaches. The authors propose 1) using generative adversarial imputation networks for imputing missing data and 2) discarding training data where data from multiple consecutive timesteps is missing.  Reviewers agreed that the problem of missing data in multi-agent RL is interesting. At the same time, several reviewers shared two main concerns about the experimental evaluation: * The lack of comparisons to baselines other than MADDPG, especially decentralized critic approaches. * The lack of experiments on non-toy domains such as SMAC.  The author response did not sufficiently address these concerns leaving the reviewers in agreement that the paper should not be accepted without these additional experiments.
1	After the rebuttal stage, three of four reviewers recommend acceptance, and one gives a borderline score but argues they lean positive. Concerns seem well addressed; the method is simple yet effective.
1	This paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction.  The main theoretical result is that individual ensemble members should not be individually calibrated in order to have a well-calibrated ensemble prediction.  While other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results.   Pros: * Theoretical study of ensemble calibration with meaningful insights  Cons: * Contributions limited to theoretical study of known observation and dynamic temperature scaling. * Dynamic temperature scaling is not shown to outperform baseline methods. * Limited experimental validation: CIFAR-10/CIFAR-100.  The authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the SKCE calibration measure.  Overall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular: 1. limited experimental evaluation (one type of task, one/two datasets only), and 2. given known literature the benefit of the derived theoretical results to practioners is not clear.  The discussions have been unable to resolve this disagreement.
0	This paper was a difficult decision. Overall it seems to be a quality paper, well written and with many experiments, in particular evaluating learned representations across various tasks and datasets. The authors were also quite courteous in their replies which is appreciated. I really like the point the paper makes about video as a natural augmentation and I find that novel amid the recent NCE surge, where most papers rely critically on augmentation. R4 was also very positive about the paper overall concept.  In terms of paper weaknesses two of the reviewers voted for rejection because the paper ignores existing work on contrastive learning from videos. The authors rebuttal is that they are the first evaluating on images, not on videos. All reviewers also point out limited technical novelty, which the authors acknowledge. Finally, R1 is not very confident about the experiments.  Overall, and after calibration, the appropriate recommendation seems to be rejection.
1	This paper presents a model-accelerator co-design framework to enable on-device Multi-task Learning (MTL). At the model level, customized mixture-of-expert (MOE) layers are introduced for MTL, which alleviate gradient conflict at training time and improve the efficiency at inference time via sparse activation. At the accelerator level, the paper proposes computation reordering which allows zero-overhead switching between tasks. The algorithm is verified the on popular multi-task datasets, and the accelerator is implemented on commercial FPGAs, demonstrating improved efficiency.  The paper is very well written, the details on the algorithm and hardware implementation are clearly explained. The author chose a particular setting of MTL, then design the model and tailor the parameters to enable efficient on-device MTL. The work is complete, covering from algorithm design to hardware implementation with sufficient innovations.   Reviewers have raised concerns such as 1). Evaluation on small datasets. During rebuttal period, the authors provide more experimental results from the large-scale Taskonomy dataset. 2). Overclaiming. For example, double buffering is a well-known technique for dataflow optimization. The technique itself is by no means novel. However, I think using it to solve a practical problem still has value.  Overall, it is a solid paper and is recommended for acceptance.
1	Reviewers recommended borderline accept, borderline reject, and accept. Reviewers found the article studies one of the main issues of GNNs and proposes a simple but effective solution method supported by extensive experimental evaluation. There were some reservations about the comparison with other methods and the theoretical analysis. While some of these items could be addressed during the discussion period, leading to updated more favorable ratings, some reservations about the theoretical part persisted. There were also persisting disagreements about the novelty and about the issues that are solved by the proposed method compared with previous methods. All together I found that the merits outweighed the shortcomings and hence am recommending accept. However, I strongly encourage the authors to carefully consider the reviewers comments when preparing the final manuscript, particularly that they work on the discussion and clarification of the novelty of the method, particularly the issues between over smoothing and overfitting and the corresponding presentation in the work, and the reservations on the theoretical part.
1	The paper swaps characteristics of an object in one image onto those of another object in another image--for example, adding fur to a car.  The authors give some examples where the task could be useful.  Further, they successfully argue  that this task is an illustration that the disentanglement task has been done well.  Two reviewers argued for acceptance, two for just-below-the-bar rejection.  The 2nd of those in favor of rejection engaged thoughtfully with the authors and raised the score by 1 after that engagement.  We have decided to accept the submission as a poster.
1	This paper investigates an improvement to the direct feedback alignment (DFA) algorithm where the "backward weights" are learned instead of being fixed random matrices. The proposed approach essentially applies the technique of DFA to Kolen-Pollack learning. While reviewers found the paper reasonably clear and thought the experiments were acceptable, there were significant concerns about the novelty of the approach and the fact that the proposed approach was a straightforward combination of existing ideas. Further, the paper could have done a better job situating (and applying) the proposed method to DFA variants that have been proposed since the original DFA paper came out.
1	This paper proposes DOMINO, an optimization framework, for contextual meta reinforcement learning. The reviewers generally agree that the paper is well written, the idea is novel and interesting, the evaluation is comprehensive and the results are impressive. Reviewers also raised a few concerns in the initial reviews, such as the proof of Lemma 1 and Theorem 1, and the mathematical definitions. Throughout the discussion phase, most of these concerns were sufficiently addressed, and the review scores were increased accordingly. Overall, the quality of the revised paper has improved significantly during the rebuttal. Thus, I recommend accepting this paper. Please incorporate the remaining reviewers' suggestions in the future version of this paper.
1	The paper presents a method for future trajectory generation. The main contribution is in proposing a technique for data augmentation in the latent space which encourages prediction of trajectories that are both plausible, but also different from the training set. The results clearly show superior performance on standard benchmarks. The evaluation is thorough and ablations show that the proposed innovation matters.   R2, R3, R4 recommend that the paper be accepted with scores 6, 8, and 6 respectively. R1 recommends the paper be rejected with a score of 5. The main concern of reviewers are:   R1: " In summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results." The authors addressed this concern in their rebuttal.    R2: "Some other points remain still open such as the limited focus on Trajectron in evaluations." Since trajectron is a recent SOTA, I think this is not a big concern. Authors compare against other baseline methods too.   R4: Comparison to Mercat, Jean, et al., ICRA 2020 is missing. The authors mention that their code is unavailable and therefore cannot compare.    R4: "underlying reasons for the success of different components (classification of latent intent and hallucinative latent intent) are hard to explain". I agree with this and this is also my major concern which I detail below.   The paper proposes to find diverse trajectories by generating two latent vectors: z, z'. The first h time steps are generated by latent vector z and the remainder using z'. The generated trajectory is evaluated by a discriminator that ensures plausibility. The latent vectors are chosen to be discrete and a classifier is trained to recognize z from ground truth trajectories. To encourage diverse trajectories, authors use a loss that encourages mis-classification of the latent variable inferred from the generated trajectory. Since the generated trajectory cannot be classified well, it is assumed to be different from the training set.   This formulation is rather adhoc. If the trajectory is indeed different from the training distribution, then it will also fool the discriminator. If it doesnot, then it's not very different. The mis-classification, is akin to encouraging high entropy in the z space inferred from predicted trajectories. With this view, it is possible that there is no need to generate two latent vectors z, z', but simply generate one and use the entropy penalty. I would love to see this experiment and see the authors demystify their method. It would also lead to significant changes in writing. Even now, writing needs improvement. Due to the proposed method being a adhoc trick, that is not well justified, I would normally not recommend acceptance. However, the empirical results are strong, tilting the recommendation to acceptance.
1	The paper improves the state-of-the-art for the minimum enclosing ball problem under differential privacy constraints. Moreover, the algorithm and its analysis are simple and intuitive.   The reviewers agreed that the paper is a concrete advance in the area, and the ideas may lead to a practical implementation. The authors carefully responded to all the issues raised by the reviewers, clearing the way to acceptance.
1	This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy-tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings.  Reviewers have conflicting views on this paper, that have not been reconcilied after the author's response and the discussion. On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in Bayesian neural networks, which could lead to further developments.  On the negative side, the claims made in the introduction are not fully supported by the experiments (the claims have been slightly amended in the revised version), and the take-home message is not so clear. In particular, Bayesian approaches with the proposed priors still underperform compared to SGD without tempering. The authors could also have considered a broader sets of experiments.  Overall, I think the contributions outweight the limitations of this paper, and I would recommend acceptance.
1	The paper claims to present actionable visual representations for manipulating 3D articulated objects. Specifically, the approach learns to estimate the spatial affordance map as well as the trajectories and their scores. After checking the rebuttal from the authors, all reviewers agree that the paper adds value to the research area. In the end, it got three borderline accept ratings. The initial criticism included lacking (experimental) comparison to baselines, and the authors successfully corresponded to the request from the reviewer. One reviewer commented that the proposed approach is a combination of Where2Act and curiosity guidance for RL Policy for Interactive Trajectory Exploration, which we believe is a valid point. Still, the paper extends the previous Where2Act and successfully demonstrates its success on difficult tasks.  We recommend accepting the paper.
1	While this paper has 4 accept recommendations among the four reviewers, I have serious misgivings about the content of this paper. The main experimental insight, that a less trained teacher sometimes performs better, is already known and unsurprising. In fact it's the very point of KD that makes that result interesting --- why should it be better to aim for a noisy target than the true target? On any dataset, if we train the teacher for long enough, we will eventually recover the exact labels to arbitrary precision. In that sense, all KD is with an "intermediate" trained model, and the only question is (has always been) just how early to stop. The next issue with the paper is that the authors claim to "explain" theoretically why KD works from the perspective of information bottleneck theory, however what they offer falls short. The “theory” is more like a story, with significant gaps. Most significantly, there is no logic to carry the leaps from stories of how mutual information evolves to why knowledge distillation should work. Moreover what the authors call "mutual information" in their experiments is not actually mutual information and the surrogates they use seem odd choices that are not consistent. For I(F;Y) the authors look at the output of the teacher model but for I(X;F) the authors look at an intermediate layer of F, training a decoder to predict X from the last convolutional layer of F. Why should the information contained in this middle layer of the teacher model matter when the student only accesses the teacher's output? My ambivalence with this paper is two-fold: (i) that the experimental findings are the main contribution and they are by themselves not sufficient for publication and (ii) that the IB component of this paper is misrepresented as a theoretical explanation of the efficacy of KD but actually it falls short.   Unfortunately I’m discovering these concerns and expressing them after the discussion, hence my recommendation to accept the paper on the basis of the reviewer's initial recommendations. If the work is accepted, I expect the authors to edit it responsibly to remove all misleading claims that suggest that the paper provides a propert theoretical account for why KD works (they certainly have not), versus a speculative intuition, and to be much more careful to disambiguate the quantities that they track from actual mutual information.
1	This paper was reviewed by four experts in the field and received mixed scores (1 borderline accept, 3 borderline reject). The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper. AC feels that this work has great potential, but needs more work to better clarify the contribution and include additional ablated study. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere.
1	Three out of four reviewers provided positive reviews and scores for this submission. They agreed that SAVI++ makes meaningful improvements over a previously proposed SAVI model. Importantly, while most past approaches evaluate on synthetic data, this submission evaluates the proposed model on a real world dataset. The proposed model clearly improves over the baseline and a clear ablation analysis shows where the improvements come from.   One reviewer had concerns about the evaluation using just one real world dataset. This was also brought up by other reviewers, who mentioned that the Waymo dataset has less diversity and fewer videos than others. While a more thorough evaluation would make this a stronger submission, the leap from synthetic evaluations to real world evaluations in this line of research is notable and sets the bar for future work. I also note, based on the discussion, that the employed dataset is not trivial and has several challenges for the model.   Another concern by the reviewer was about missing baselines. The authors did provide additional baselines in their response. While these baselines do not exactly match the ones requested by the reviewer, I think they provide good evidence that the proposed method is able to employ the depth signal effectively.   Overall, this paper makes solid progress on the problem, provides value to the readers and provides strong results on a real world dataset. Given these reasons, I recommend acceptance.
1	This work proposes a channel shuffling as a way to distinguish between backdoor and clean examples, based on the hypothesis that trigger features are sparsely encoded and activated in only a few channels. Reviewers all agreed that is a pretty intuitive, yet effective method and that it had solid evaluations after the rebuttal.  Reviewer oFCu had the concern that this paper is entirely empirical and has no supporting theory. I think this is ok given the precedence of papers in this field and also the framing of security and privacy is a more practically oriented one anyway.  The most critical reviewer (PjHg) pointed out that the benchmarks and related work contextualization were severely lacking. After the rebuttal period, these concerns were mostly alleviated.  Given the strength of the evaluations and the novelty of the idea, I believe this paper should be accepted.  That said, please address the following for the camera ready: Please improve the writing as this was brought up by several reviewers. There are a lot of grammatical errors. Please improve the discussion of the limitations section of this detection method as suggested by reviewer PjHg.
0	This paper provides a unified view of some known methods for monotone operator inclusion problems like Forward-Backward-Forward (FBF) and OGDA, and provides new convergence results for the stochastic version of a variant of FBF called FBFp. All reviewers initially recommended rejection. The rebuttal and the manuscript update addressed several concerns from the reviewers, though the general consensus after rebuttal was still that the paper lacked in significance for the ICLR community. The AC thinks that the paper could make an interesting overview paper in a more optimization / theoretically minded venue.
1	This paper generated significant discussion and division amongst the reviewers. On the positive side, some reviewers enjoyed both contributions, feeling the further empirical investigation of existing attacks to be interesting, and the creation of a benchmark to be very useful. On the negative side, no new positive results were proposed, criticism of previous attacks were considered to be unjust, the focus was somewhat narrow, and a benchmark could plausibly be misleading and detrimental.  Given the highly competitive nature of ICLR and the many other excellent submissions, the committee was unable to accept the paper at this time. Below are some suggestions for future submissions.  The content of the paper is generally fine, as long as the caveats and the "tone" are appropriate: we would hope to not mislead potential readers. Here are some (strong) recommendations: - Previous works were proof of concept attacks, and the authors should be careful to not frame them as being "broken" -- they perhaps were not meant to be robust to these modifications. - The scope is somewhat narrow. There should be some explicitly statement and justification of the scope, and what in particular is *not* covered by the investigation. - Importantly, a single benchmark can't be a unique gold standard, for many reasons discussed by reviewers. Please state these caveats clearly and prominently in the paper and/or code release, as otherwise the presence of a benchmark could do more harm than good. In particular, Reviewer 4 brought up the following philosophical concern with a benchmark, which I believe is quite reasonable, and I reproduce verbatim. The authors should try to address this in the next version: "This kind of benchmark can push the research to a wrong direction. In my view, the point of attacks are to create an alarm for using machine learning in critical applications. Developing these benchmarks would push the competition in the direction of making existing attacks "better" (whatever "better" means in the benchmark) instead of focusing on designing defense techniques or showing the severity of attacks in other situations. This benchmark could also have a bad effect on future attacks (attacks that want to show a new threat, not the one that try to improve the performance of clean label targeted poisoning attacks on deep neural nets) to gain attention from community as they probably will not pass all the criteria of this benchmark."  As another comment (I believe mentioned by other reviewers), it would be nice if all the terms and settings were defined clearly and precisely. For a benchmarking paper, it is important that the reader can clearly understand the threat model, and what does and does not count as a valid attack.  Finally, many of the reviewers gave detailed comments and concerns. The authors should please note and discuss these concerns in future versions (or at least in a supplement or an arXiv version).
1	This paper proposes a differentiable trust region based on closed-form projects for deep reinforcement learning. The update is derived for three types of trust regions: KL divergence, Wasserstein L2 distance, and Frobenius norm, applied to PPO and PAPI, and shown to perform comparably to the original algorithms.  While empirically the proposed solutions does not bring clear benefits in terms of performance, as correctly acknowledged by the authors, it is rigorously derived and carefully described, bringing valuable insights and new tools to the deep RL toolbox. The authors improved the initial submission substantially based on the reviews during the discussion period, and the reviewers generally agree that the work is of sufficient quality that merits publication. To improve the paper and its impact, I would recommend applying the method to also off-policy algorithms for completeness. Overall, I recommend accepting this submission.
1	The paper propose a Fully Online Meta-Learning (FOML) method which extend MAML for continual learning in a fully online learning  without requiring the knowledge of the task boundaries. Experiments show that FOML was able to learn new tasks faster than several existing online learning methods on Rainbow-MNIST, and CIFAR100 datasets.   There are a few major concerns from reviewers. One concern is about the lack of clarity on the problem statement: The authors cast the problem as meta-learning that must be done in a fully online setting, but it requires to store all the training data in a buffer storing all the training data seen so far, which contradicts to the principle of “online learning”. Another major weakness is the poorly written literature survey, which missed to cite a large body of related work in continual learning and online-meta-learning (such as Online Continual Learning, task-free continual learning, continual learning without task boundaries, etc). These should at least be discussed carefully if not fully compared in the empirical studies. Also experiments are quite weak in both settings, datasets and rather out-of-date baselines. Finally, there also lacks of theoretical justification or analysis.   Therefore, the paper is not recommended for acceptance in its current form. I hope authors found the review comments informative and can improve their paper by addressing these review comments carefully in future submissions.
1	Authors introduce 3 modifications to ViT architecture to introduce additional inductive biases to improve performance in low-data scenarios: - SOPE: Sequential Overlapping Patch Embedding -- essentially convolutions before partitioning the image into patches. - DAFF: Dynamic Aggregation Feed Forward -- a DWCONV operation is applied to tokens after a FC layer increases the channel dimension. The new tokens are average pooled, input to additional FC layers, and then are used to scale the CLS token. - HI-MHSA: Head-Interacted Multi-Head Self-Attention -- this approach's name is confusing. This does not change the heads in MHSA. Rather a new mechanism is introduced prior to MHSA to introduce new tokens where each new token is derived from a different partition of the original channel dimensions.  AC recommends authors use a different name. For example, "Intra-Channel Modeling (ICM) MHSA" or something would be more clear.     Performance is evaluated on CIFAR-100, DomainNet subsets, and ImageNet 1K  Pros: - [R/AC] The topic is important to the community. - [R/AC] The paper is well written and clear.  - [R/AC] The authors present improved performance versus other recent SOTA hybrid model designs (during rebuttal phase, though missing from original work -- should be added to paper).   Cons: - [R/AC] The evaluation could be significantly improved. For example, more training experiments on undersampled version of more datasets, with comparisons to other SOTA methods.  - [R/AC] The design is complicated and the motivation isn't always clear.  - [R] Novelty of the components implemented is low.  - [R] Concerns over use of BN as opposed to LN. Authors have provided ablation experiments to demonstrate that BN improves performance of their model over LN. These ablations should be included in the manuscript. - [R/AC] Concerns over lack of comparison to other SOTA methods that mix convolutions with transformers, such as CvT. Authors have provided additional experiment tables that compare against CvT. These tables should be included in the manuscript in a consistent manner (showing number of parameters and FLOPS). - [R/AC] Authors do not include FLOPS in their experiment tables. Please ensure all tables report number of parameters and FLOPS for all models explored. There are python packages to help with computing this, such as "flopth".  - [AC] Some spelling and grammatical mistakes. Please spell check the manuscript.  Overall Recommendation: Reviews lean toward acceptance, but marginally so. Given that the authors have provided more comparisons against recent relevant SOTA methods, and that the reviewers (including expert in the field) lean toward accept, the AC opinion is that this manuscript can be published and provides some valuable knowledge to the community. There are ways in which the paper can still be improved before publication, such as inclusion of additional evaluation datasets.  AC Rating: Borderline Accept
1	This paper studies the transferability of adversarial examples. In general, the reviewers found the paper is well motivated, and the proposed method is simple and effective. Most initial concerns were about missing comparisons and ablations.   All these concerns are well addressed in the rebuttal. As a result, all reviewers unanimously agree to accept this submission.
1	## Summary Offline RL (RL) algorithms aim to learn policies without interacting with an environment purely from the state and actions covered in the offline datasets. However, in real-world datasets, the coverage can be insufficient to learn good policies. Thus it is an important research direction to improve the sample efficiency of those methods. This paper approaches offline RL from a sequence modeling perspective. The paper adopts a variant of trajectory transformers for data generation, and they investigate two of the main design decisions in those models: * Sampling methods (autoregressive vs. teacher-forcing based). * Reuse of model-generated data  The authors validate their idea on two D4RL tasks: * adroit * locomotion  ## Decision  Overall the paper is well-written and easy to understand. The results and experiments are thorough, and the paper goes for more depth in the experiments rather than breadth. The ideas in the paper are not novel, but the paper does not overclaim its contributions, and results are interesting. As a result, I think both NeurIPS and the broader offline RL communities would benefit from the findings of this paper. I am nominating this paper for acceptance.  The reviewers were very positive about this paper during the rebuttal and discussion paper. They all agreed that the paper is valuable and interesting contribution to the community. The main criticism of this paper that came up during the discussion period was that the idea is just a straightforward combination of the existing techniques. However, the idea presented in the paper is coherent, reasonable, and executed well.   The authors provided a very detailed rebuttal with clarifications to the points that reviewers raised. As a result of the rebuttal, some of the reviewers increased their scores. I would recommend that the authors incorporate some of those clarifications into the camera-ready paper version. Some of those are:  * In response to reviewer 9mfT’s question on the results with CQL on additional data generated by Boot is very interesting. I think the authors should include it in the camera-ready version of the paper.  * Additional experiments on other datasets from the D4RL gym environment as asked by reviewer 9mfT. * The experiments asked by the reviewer *hqzJ*.
1	The paper proposes a modification of the well-known FILM model for VQA which targets counting problems in particular, which have been a known weakness of existing models. The improvements have also been tested beyond counting. The experimental results are convincing, in particular a scientific competition has been won. The reviewers also appreciated convincing ablation studies.  The idea bas been perceived as interesting enough for publication, and in combination with the experimental results, this compensated several perceived weaknesses (limited novelty w.r.t. the modified FILM model; justifications of some design choices).  All reviewers agreed that this paper is of interest to the community and proposed acceptance. The AC concurs.
1	The work discusses some of the problems related to fairness that occur when a machine  learning model is applied to data with missing values in graphs. The authors propose a methodology to  compensate for the discrimination across groups.    All the reviewers and the AC agree that the paper overall idea of the paper is strong and very interesting. The paper is extremely well-written, easy to follow and contirbutions and limitation well described.   The main concern with the paper is its practicality as results on real-world datasets are not providing  any promising lift in terms of fairness. This is also acknowledged as a feature direciton by the authors in the rebuttal.  The AC believes that this is a promsing and impactful line of work and will ignite interesting discussions in the NeurIPS community.  As another reviewer pointed out, there is not much work connecting the imputation and fairness in the graph context. Acceptance is recommended.
0	This paper empirically studies the impact of different types of negatives used in recent contrastive self-supervised learning methods. Results were initially shown on Mocov2, though after rebuttal simCLR was also added, and several interesting findings were found including that only hardest 5% of the negatives are necessary and sufficient. While the reviewers saw the benefit of rigorously studying this aspect of recent advances in self-supervised learning, a number of issues were raised including: 1) The limited scope of the conclusions, given that only two (after rebuttal) algorithms were used on one datasets, 2) Limited connections drawn to existing works on hard negative mining (which is very common across machine learning including metric learning and object detection), and 3) Limited discussion of some of the methodological issues such as use of measures that are intrinsically tied to the model's weights (hence being less reliable early in the training) and WordNet as a measure for semantic similarity. Though the authors provided lengthy rebuttals, the reviewers still felt some of these issues were not addressed. As a result, I recommend rejection in this cycle, and that the authors bolster some of these aspects for a submission to future venues.   I would like to emphasize that this type of work, which provides rigorous empirical investigation of various phenomena in machine learning, is indeed important and worth doing. Hence, the lack of a new method (e.g. to address the selection of negatives) was not the basis of the decision. While the paper clearly does a thorough job at investigating these issues for a limited scope (e.g. in terms of datasets), a larger contribution is expected for empirical papers such that 1) we can ensure the generality of the conclusions (across methods and datasets), 2) we have a conceptual framework for understanding the empirical results especially with respect to what is already known in adjacent areas (e.g. metric learning and object detection), and 3) we understand some of the methodological choices that were made and why they are sufficiently justified.
1	Meta Review: This paper on spatial and temporal super-resolution of turbulent flows. The novelty is acknowledged by Reviewer 1 & 2, while Reviewer 3 pointed out that the paper lacks theoretical analysis and formulation. Reviewer 1&2 also raise conern on the risk of overfiiting, and suggest a clear description of experiment settings and full comparison with more recent baselines. In all, the meta-reviewer considers the pros to outweigh the cons, and recommends acceptance. The authors need to incorporate the comments when preparing the final version
1	TAP-Vid presents a benchmark that will be highly useful for tracking research for tracking arbitrary physical points on surfaces over long video clips. The reviews are all positive, with one reviewer raising ethical aspects in preparing the benchmark.  I find the rebuttal sufficient and adequate and hence recommend acceptance of the paper.
0	This proposal introduces CylesGym, the first Reinforcement Learning (RL) benchmark targeted at long horizon decision making in agriculture. Crucially, while prior work addresses single-year decision making, CylesGym captures the long term effects that one year's crop has on future generations.   The benchmark is clearly highly relevant and opens up a new frontier for RL researchers, making it a valuable contribution to the field. Furthermore, the benchmark does a good job of highlighting interesting opportunities for RL method development, such as costly information gathering, and evaluating current algorithms compared to baselines.   There was an active discussion between the reviewers and authors of the benchmark which resolved the majority of issues raised in the initial reviews. As a result there is broad support across the reviewers for the paper. A lingering concern is the sim-to-real gap which is mentioned at a number of places in the paper but could be emphasised more.   Lastly, the paper is well written and the evaluation sound. I believe this benchmark will be welcome by the community but I recommend that the authors address the concerns regarding the writing raised by  Reviewer MEYG, in particular regarding the utility for the agriculture community. In general I do not believe that issues which can be addressed in writing should be a reason to reject but those concerns should be addressed for the final version.
0	This paper proposes a knowledge distillation strategy to enable the use of a large server-side model in federated learning while satisfying the computation constraints of resource-limited clients. The problem is relevant and well-motivated, and the paper presents compelling experimental results to support the proposed strategy. However, reviewers had the following major comments suggestions/: 1) The theoretical analysis section needs improvement in terms of the technical depth and rigor 2) Better explanation of how the proposed strategy compares with previous works/baselines 3) Considering the privacy and scalability properties of the proposed strategy.  The paper generated lots of constructive post-rebuttal discussions between the authors and the reviewers, and I believe the authors received several ideas to improve the work and appreciated the reviews. One of the reviewers increased their score. However, based on the current scores, I still recommend rejection. I do think the paper has promise, and with improvements, the revised version will make an excellent contribution.
1	This paper proposes a new method to perform knowledge distillation (KD) for transformer compression, where two types of contextual knowledge, namely, word relations and layer-transforming relations, are considered for KD. Both pair-wise and triple-wise relations are modeled.   This paper receives two weak reject and two weak accept recommendations. On one hand, the reviewers appreciate that the authors have added more results into the paper to solve their concerns. On the other hand, several concerns still exist. (i) With regards to the compute-performance trade-off, the gains of the method does not seem too great. One reviewer feels that the authors tried to downplay the cost of their method too much. Though we care more about the inference time, the development time in practice should also not be underestimated. (ii) Compared with TinyBERT, the performance gain looks marginal on the GLUE benchmark (Table 1). (iii) It will make the paper more convincing if pre-training experiments can be performed.   Overall, after reading the paper, the AC thinks that the novelty of the proposed method is somewhat limited. The AC is also hesitant about whether modeling word relations and layer-transforming relations simultaneously are needed. The choices for ablation study are also not totally clear.   For example, in Figure 2, it is not clear why the authors choose SST-2 to plot the figure; in Table 5, it is unclear why SST-2, MRPC and QNLI are selected, but not others. When looking at Table 5, it is not totally convincing it is needed to model both WR and LTR, or it is needed to introduce both pair-wise and triple-wise relations. More careful ablation studies are needed. It also remains unclear what kind of word relations or layer-transforming relations are learned.   In summary, this is a borderline paper, and the rebuttal unfortunately did not fully address the reviewers' main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere.
1	There are quite a few problems raised by the reviewers worth paying attention to:   - Thoroughness of evaluations: Performance metrics are difficult to understand and insufficiently justified and described. There's also mention of non-performance related metrics like bias not being adequately considered. There's some debate over the appropriateness of the included baselines, and some skepticism about the justification for some experiments. Furthermore, Reviewer SxsG notes, "It would be great to see standard deviations/any measures of statistical deviations across randomly seeded experimental repetitions in any of the experimental result figures" and I agree. I also think authors could do a better job in the main text or supplement justifying the use of these specific baselines. However, in the revised version of the paper, many of these issues are addressed with the re-write of Section 3.  - Restricted scope: Many mention the limitation of focusing on images and a small handful of datasets. Reviewer aF85 notes, "The datasets (CIFAR10/100 and TinyImageNet) and architectures (MLP/ConvNets/ResNets) seem a little bit limited in scope for understanding transferability across architectures" and I'd agree, though I recognize that a goal of the tool is to allow for others to also contribute datasets, and send their models to be tested. Also, the authors are correct in noting that many of these issues are a byproduct of the fact that much of the data condensation work so far has been focused on image datasets.  - Usability: Reviewer Qbsw, Reviewer SxsG and especially Reviewer 5Lwd all mention issues with documentation, and not how difficult it is to follow the provided instructions in order to assess a model, add a new dataset, etc. However, the authors seem to have addressed many of the concerns, improving documentation significantly following this feedback.   Overall, it seems the authors paid attention to reviewer critiques and responded respectfully and meaningfully to the provided feedback. Given the importance of the topic and the current lack of testing infrastructure in this area, I recommend we accept this paper as a poster. I hope authors continue to take in feedback at the conference to continue to make further improvements to their benchmarking platform.
0	The idea of having two policies with opposing strategies, one aiming to maximize a notion of surprise whereas the other tries to minimize it, is an interesting one. However, even after the author rebuttal, all reviewers have lingering concerns about the evaluation protocol. In addition, there are remaining questions about the bonuses used; there are concerns that these only work for very specific domains. For these reasons, I'm recommending rejection. I encourage the authors to carefully read the concerns of the reviewers about evaluation and consider using a different evaluation protocol for a future version of this work.
1	The paper addresses an exciting problem statement--generating theorems directly in natural language--and shows how to adapt large language models to this task, both for autocompletion, proof reference generation, and wholecloth proof generation. While previous works have considered various auxiliary mathematical tasks posed in natural language, this work takes an important step by making progress toward doing proofs directly in natural language. This is a hard problem, and the authors support their work with experiments showcasing and analyzing different kinds of successes and failures. The reviews are unanimous in recommending acceptance.
1	This work proposes a framework to search for the topology of an artificial neural network jointly with the network training, via a genetic algorithm that can decide structural actions, such as addition or removal of neurons and layers. An extra heuristic based on Bayesian information criterion helps the optimization process decide on its decisions about the topology. They demonstrate improvements over baseline fully-connected networks on SVHN and (augmented) CIFAR-10.  Reviewers and myself agree that this is an interesting idea, and that the paper is easy to follow. While I may not agree that we need to achieve SOTA on these datasets, or see large scale ImageNet-type experiments for novel ideas, I agree with the reviewers, esp R1's point that the current experiments are not satisfactory to meet the bar for acceptance at ICLR.  CIFAR-10 and SVHN are well-established tasks, and showing baseline accuracy of 75%/48% on them respectively doesn't seem to do them justice, especially when most methods (even with low compute requirements) can get > 95% on both, for the past few years. For this work to be of interest to the broader community, it needs to be improved to incorporate at least respectable baselines on these small datasets, and perhaps be improved to work beyond fully connected networks.  At this stage, we need to see a revision of the method and see improvements before an acceptance decision can be made.
1	This is an interesting paper that proposes a novel unsupervised approach for object segmentation from point clouds, for rigid objects. The strong results demonstrated can be impactful both for 3d as well as potentially 2d vision. After rebuttal, all 4 expert reviewers are convinced that the paper should be accepted, so the decision to accept the paper was easy.
1	The paper looks a the Augmented Permanental point process as a model of (spatial) point phenomena.   One reviewer was unconvinced that the method was needed at all, which the authors refuted. There was a long exchange but I'm on the side of the authors here - the method is clearly distinct from a point process defined on the covariate space. I'm happy that the reviewer was able to make their point and that the discussion was enabled, and I applaud the authors for their patient responses. I'm disregarding that reviewer's score.   The reviewers suggest that an important and interesting contribution is the representer theorem for squared processes. I also appreciated the authors discussion on the approximation error on the integral operator. These should be highlighted in the manuscript.   There was the occasional confusion from the reviewers on notations: for example, the reviewers failed to spot the the performance of the method was presented in the paper using the $\tau$ column in the tables. Please, double check all the reviewer feedback for clarifications.   Overall, I think that there are a couple of interesting ideas in the paper that people working on Point Process data will be impacted by, and am recommending that this is just above the acceptance threshold.
1	This paper show that in several different neural network  architectures, recurrent   networks that share parameters over iterations have comparable  performance and similar features to feed-forward networks of the same "effective depth".    Reviewers initially had some reservations about novelty and  generalizability to deeper SOTA networks.  These were successfully addressed by the authors and all reviewers feel the paper is above the bar due to the importance of the area, and that this paper brings together many important insights that, while many may have been known  before, had not previously been all brought together before.  The maze  task was also considered a useful task for the field.  I agree that the paper makes a worthwhile contribution and am in favor of  acceptance.
1	Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.
1	This paper is on a highly-important topic, and makes solid contributions. Anomaly detection for multi-class datasets without class information is an underexplored area. Reviewers have appreciated the strong experimental results (especially on the important MVtech benchmark), high quality paper writing, and explainability results besides accuracy, via a novel attention mechanism. On the flip side, there were concerns on lack of deep analyses of the constituents of the method and novelty (given that there are some recent papers with similar ideas). The scores were borderline and the authors have put significant effort to address the concerns of the reviewers. Especially extra ablation studies and comparisons with other relevant papers are quite helpful in regards to convincingness of the ideas. I support the acceptance of the paper given all. Please update your paper with the additional content you have provided in the responses below.
1	I thank the authors and reviewers for their discussions about this paper. The proposed AT-GAN is a GAN-based method to generate adversarial examples. Similar methods (e.g. Song et al) have been proposed to use GANs to generate adv. examples more efficiently. Authors show their method has some numerical benefits. However, more experiments are needed to further justify it. Also, creating "unrestrictive" adv. examples can cause a risk of generating samples where the true label is flipped. Authors need to clarify it. Given all, I think the paper needs a bit of more work to be accepted. I recommend authors to address the aforementioned concerns in the updated draft.     -AC
1	Even though several concerns have been raised by multiple reviewers, the reviewers largely agreed on the significance and novelty of the contributions. The authors provided quite detailed responses and given all the data, overall I believe the strengths of the paper outweigh its weaknesses. Hence I am recommending an acceptance.
1	Meta Review: The reviewers reach a consensus on the acceptance. The authors are encouraged to take all the comments into consideration and further improve the paper in the camera ready.
1	This paper proposes a new knowledge distillation (KD) method for adversarial training. The key observation is inspiring: soft-labels provided by the teacher gradually becomes less and less reliable during the adversarial training of student model. Based on that,  they propose to partially trust the soft labels provided by the teacher in adversarial distillation.   Reviewers unanimously agree that this paper has clear motivation, well-sorted logic, and neat writing. While some reviewers initially posed concerns on evaluation completeness and detail clarification, they were well addressed during the rebuttal. AC reads the paper/discussion thread and agrees this is a worthy work to get accepted.
0	This paper tackles a small-batch online unsupervised learning problem, specifically proposing an online unsupervised prototypical network architecture that leverages an online mixture-based clustering algorithm and corresponding EM algorithm. Special features are added to deal specifically with the non-stationary distributions that are induced. Results are shown on more realistic streams of data, namely from the RoamingRooms dataset, and compared to existing self-supervised learning algorithms including ones based on clustering principles e.g. SWaV.   Overall, the reviewers were positive about the problem setting and method, but had some concerns about hyper-parameters (hYzM, cvrN, LjvY) and motivation for the specific setting where the method excels compared to other methods not designed for such a setting (hYzM, cvrN), i.e. small-batch setting, where it is not clear where the line should be drawn in terms of batch size and memory requirements with respect to performance differences between the proposed approach and existing self-supervised methods. Importantly, all reviewers had significant confusions about all aspects of the work ranging from low-level details of the proposed method to the empirical setting and evaluation (including for competing methods). After a long discussion, the authors provided a large amount of details about their work, which the reviewers and AC highly appreciate. However, in the end incorporating all of the feedback requires a major revision of the entire paper. Even the reviewers that were more on the positive side (cvrN and LjvY) mentioned it would be extremely beneficial for this paper to be significantly revised and go through another review. Since so many aspects were confusing, it is not clear to the AC that the underlying method, technical contributions, and other aspects of the works had a sufficient chance to be evaluated fairly, given that much of the review period was spent on clearing up such confusion.   In summary, while the paper is definitely promising and tackles an important area for the community, it requires a major revision and should go through the review process when it is more clearly presented. As a result, I recommend rejection at this point, since it is not ready for publication in its current form.
0	This paper aims for detecting not only clean OOD data, but also their adversarially manipulated ones. The authors propose a method for this goal, with no/marginal loss in clean test accuracy (say, Acc) and clean OOD detection accuracy (say, AUC), while existing methods for targeting the same goal suffers from low Acc and AUC. 3 reviewers are positive and 2 reviewers are negative. Reviewers and AC think that the proposed idea of merging a certified binary classifier for in-versus out-distribution with a classifier for the in-distribution task is interesting. However, AC thinks that experimental results are arguable as pointed out by reviewers. For example, in CIFAR-10, the proposed method outperforms the baseline (GOOD) with respect to Acc and AUC, but often significantly underperforms it with respect to GAUC (guaranteed AUC) or AAUC (adversarial AUC). Then, the question is which metric is more important? It is arguable to say whether Acc is more important than GAUC or AAUC. But, at least, AC thinks that AUC and AAUC (or GAUC) are equally important as adversarially manipulated OOD data is nothing but another OOD data made from the original clean OOD data. Hence, the superiority of the proposed method over the baseline is arguable in the experiments, and AC tends to suggest rejection.   ps ... AC is also a bit skeptical on the motivation of this paper. What is the value of obtaining "guaranteed AUC"? It is not the "real/true" worst case OOD performance, as it varies with respect to the tested clean OOD data. Namely, it is the worst case OOD performance just in a certain "subset" of OOD data, i.e., adversarially manipulated OOD data made from a certain clean OOD data. Hence, AC is curious about what is the value of establishing such a "partial" lower bound (rather than "true" lower bound considering all possible OOD data). AC thinks that the problem setup studied in this paper (and some previous papers) looks interesting/reasonable at the first glance, but feels somewhat artificial after a deeper look.
1	The paper works on domain generalization of 3D point cloud classification, and proposes a part-based domain generalization network for the purpose, whose key idea is to build a common feature space of part template and align the part-level features wherein. Three reviewers appreciate the contributions, including the clear motivation, the implicit domain alignment by part-template features, and the proposed part feature aggregation module. They also suggest to improve the paper by clearer definitions of parts, better organization of contrastive learning in the paper, a more complete citation of closely related works, etc.   After discussions between the authors and reviewers, consensus is reached on accepting the paper.  Congratulations!
1	This paper has been independently assessed by three expert reviewers. The results place it at the borderline of acceptance decision: while one of the reviewers gave it a straight accept evaluation, two others assessed it as marginally rejectable, even after discussion with the authors. All of the reviewers agreed that the theoretical results provided should help promote the use of MLE estimators over perhaps more prevalently used in current practice TMO, and that is the main contribution of this work. The reviewers were concerned with the clarity of the presentation and with a confusing notation used. Some of these issues have been addressed in the authors' responses. All things considered, I conclude that this work can be of some interest to the ICLR audience, and as such it can be assessed as marginally acceptable for this conference: "accept if needed". I will recommend it as such for consideration by the Senior Area Chair and the Program Committee.
1	Meta Review: This paper studies the instability of neural network-driven mutual information estimators. It identifies two reasons for instability including the non-convergence of neural networks and the saturating neural network outputs. Then a practical regularization is proposed to mitigate the instability issues. Both theoretical and experimental studies are conducted to demonstrate the effectiveness of the regularizer.  All the reviewers agree that the proposed method is novel, simple, and effective. The theoretical analysis is insightful and the empirical results are good. Nevertheless, there are some concerns on the small scale of datasets. Given that the novelty and technical contribution of this paper outweighs the concerns, I recommend acceptance of this paper. But I highly suggest the authors add results of larger scale experiments (for example Imagenet) in the final version.
0	This paper observes that a fully-convolutional model in the style of recent MLP-Mixer and ViT variants can have surprisingly good initial performance. As this paper attracts certain amount of attentions, three expert reviewers have provided very detailed and serious comments, and two actively engaged with author discussions. AC also carefully read the paper as well as all discussion threads.  AC agrees the authors should not be penalized by not achieving the best performance, nor not comparing with very recent work. The main legitimate critiques, however, focus on three aspects: (1) over-claimed contribution; (2) experiment solidness/competitiveness; and (3) writing completeness/clarity.  First, this paper established an interesting ablation experiment that a very simple model, that uses only standard convolutions to achieve the mixing steps, can roughly "do the work". However, AC disagrees this is a very "surprisingly new" result, on top of MLP-mixer: given convolutions are increasingly re-injected into ViTs to gain the vision inductive bias, their similar role in MLP-mixer should be expected too. Moreover, as in general agreement by reviewers, the paper title might have over-claimed - the authors cannot directly prove this concept "patch is the most critical component" yet. The authors later also agreed and changed some confusing wording, which is a good move (but also, making their contribution now even less obvious).   Second, this method does not achieve noteworthy competitive results compared to others, in order to justify its merit (simplicity alone is good to have, but insufficient to justify a strong work). Importantly, it has been pointed out by two reviewers that the model throughput is much worse than the competitors. AC also noticed that the comparison was not very rigorous, e.g., comparing ConvMixer patch size 7 with DeiT-B 16 patch size 16 doesn't help draw much fair informative conclusion. The cifar-10 results alone did not provide strong support and were later de-emphaszied by authors too.   Third, while NOT being the main reason of rejection, AC personally suggests the authors to responsibly enrich their main text, and to remove the  “A note on paper length” paragraph. The authors intentionally kept the paper length unusually short. Reviewers generally dislike this idea. Being an innovative writer is good, but very relevant details and discussions were left in the supplemental as a result. Especially, AC agrees the whole section A and part of section B of the supplemental should have been in the main paper at very least.  In summary, the authors strive to tell an interesting story, but it is not yet a well settled story. The experiments are not solid enough to support their bold claims. The authors are suggested to improve their work further by taking into account reviewer comments.
1	The reviewers found the paper well written and were satisfied with the experimental setting, which shows clear improvements. The authors made a thorough rebuttal and carefully answered the reviewer's questions and I recommend for acceptance as I believe this will be useful to the community.   I recommend the authors to carefully go over the reviewers’ comments and incorporate them into the final manuscript, along with the additional experiments from the rebuttal.
1	This paper tackles a very timely problem.  Scores of 5,6,6,8 put it in the borderline region, but in the private discussion the more negative reviewer noted that they would also be OK with the paper being accepted. I therefore recommend acceptance.  Going through the paper I missed any mention of available source code. I strongly recommend that the authors make code available; this would greatly increase the paper's impact.
1	All reviewers except one recommend acceptance. The reviewer recommending rejecting the paper (score 5) raises valid concerns, e.g., the lack of semi-supervised baselines and the comparison to a prior benchmark for weak supervision (WRENCH). The authors addressed some of the concerns (e.g., by adding a semi-supervised learning baseline). Regarding the comparison to WRENCH: It is true that WRENCH is similar, but the exact problem benchmarked by WRENCH and AutoWS-Bench-101 is different: in the former, the labeling functions are assumed to be given, in the latter, the labeling functions are learned. I find this difference sufficient for a separate benchmark.  Overall I recommend accepting the paper. I strongly encourage the authors to take all the reviewer comments into account, especially the comparison to baselines form semi-supervised learning. As I understand, automatic weak supervision fits the same abstract problem statement as semi-supervised learning. Hence a comprehensive comparison to semi-supervised learning is essential.
0	The reviewers have provided very detailed, argumented and constructive criticism of the paper.  Although they acknowledge some interest to the paper, they all agree on the fact that this paper had major flaws to be adressed (missing baselines, lack of clarity, code availability).   The authors did not provide a rebuttal.  I follow the reviewer’s rating and recommend rejection of this paper.
1	The authors consider the problem of using expert data with unobserved confounders for both imitation and reinforcement learning settings. They showed how latent confounders  negatively affect the learning process and proposed a sampling algorithm that mitigates the  impact and delivers good empirical results.  I agree with the reviewers, this is a borderline paper but with a preference to accept.  The most salient concern was the lack of clear contribution. While the algorithm is interesting  with good experimental results that attract interest, it lacks actual theoretical backbone.   That being said, the authors put in solid effort and addressed concerns sufficiently in the rebuttal stage. Thus I would prefer to see it accepted. The proposed research direction should be explored in the future.
0	This paper introduces Transformer-QL, a new variant of transformer networks that can process long sequences more efficiently. This is an important research problem, which has been widely studied recently. Unfortunately, this paper does not compare to such previous works (eg. see "Efficient transformers: A survey"), the only considered baselines being Transformer-XL and Compressive transformer. Moreover, the reviewers found the experimental section to be lacking, as the results are weak compared to existing work, and important ablation studies are missing. The authors did not provide a rebuttal. For these reasons, I recommend to reject the paper.
1	The paper offers a more systematic treatment of various symmetry-related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes.   The simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non-trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions.   Overall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry-based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance.
1	This paper proposes to utilize point cloud completion tools to densify sparse point clouds which could subsequently improve the performance of point cloud detection methods. After rebuttal, reviewers agree on the novelty of the method and its effectiveness on the Waymo open dataset. AC recommends this paper for acceptance following the unanimous opinion.
1	The paper provides a new learning technique for problems that require learning embeddings. In particular, the authors analyze a technique that takes into account the frequency of items in an embedding layer to modify the learning rate for each embedding. The paper provides a theoretical analysis of this approached and contrasts it to that of SGD. It also provides experiments validating this approach empirically.  The reviewers agree that the paper provides a simple yet effective method, based on realistic assumptions (non-uniform frequencies). In addition, the paper seems to be well written and easy to follow. One issue raised in the reviews was about the focus of the paper, and the fact that the experiments are limited to recommendation systems even though the method is claimed to be generic for any model requiring embeddings. During the rebuttal the authors provided experiments for an NLP task that show favorable results to the new technique in another regime. Given the overall positive feedback and this new evidence validating the proposed method, I recommend accepting the paper.
1	The submitted paper contains interesting theoretical insights into common approaches for exploration and proposes a new way for deriving intrinsic rewards for exploration which is evaluated in several benchmark environments. While all reviewers appreciate these aspects, there are concerns about whether the paper is ready for publication. In particular, the authors’ response did not clarify all open questions and concerns (although the authors already improved the paper a lot by updating the submitted paper according to recommendations/questions of the reviewers). After discussions and author feedback, 3 knowledgable reviewers suggest (weak) rejection of the paper and 1 reviewer suggested acceptance of the paper. Considering this, I recommend to reject the paper but I would like to encourage the authors to consider the comments of the reviewers to revise their paper accordingly, as I expect the paper to then turn into a strong and impactful one.
1	The paper gives a gradient-free method for generating adversarial examples for the code2seq model of source code.  While the reviewers found the high-level objectives interesting, the experimental evaluation leaves quite a bit to be desired. (Please see the reviews for more details.) As a result, the paper cannot be accepted in the current form. We urge the authors to improve the paper along the lines that the reviews suggest and resubmit to a different venue.
1	This easy-to-follow paper has been thoroughly evaluated by five competent reviewers. Four of them rated the work as acceptable (two full and two weak accepts), while one recommended a rejection. In my opinion, the reviewer with the negative assessment has not raised fundamental issues that would disqualify this paper from being considered for NeurIPS. The authors provided extensive clarifications to all the reviewers, including the one with a negative opinion. That reviewer did not engage in discussion with the authors. I recommend accepting this paper without reservations
1	This paper introduces Back2Future, a deep learning approach for refining predictions when  backfill dynamics are present.    All reviewers agree on that the authors successfully motivate their work and  introduce a topic of great interest, i.e. that of dealing with the effect of revising previously recorded data and its effect  timeseries predictions. The reviewers also underline the strong and thorough experimental section. Among the reviews is also underlined the potential impact of the work for the research domain.   Many thanks to the authors for replying to the minor concerns raised.  I concur with the reviews and find this submission very interesting, convincing and thus  recommend for accept.   Thank you for submitting the paper to ICLR.
1	The paper presents an analysis of the benefit of unsupervised contrastive learning for downstream classification tasks using the cross-entropy loss. Building on prior work, the authors show that the contrastive loss can be bounded in terms of the cross=entropy term and an “intercept” term which depends logarithmically on the number of negative samples per positive sample (for contrastive learning) rather than polynomially as in the prior work.   There are several differences between the setting here and that of the prior work by Arora et al. (2019). First, the work here focuses only on cross-entropy loss and leverages the similarity of the loss structure between the contrastive loss and the cross-entropy loss. Second, the assumptions here are different, e.g., boundedness of the representation. Finally, the assumption that latent classes are the same as the label classes (which is not the case in the prior work) is significantly restrictive.   The writing is poor and the presentation is not clear. Despite the title and various references to learning bounds in the abstract and the main text, there are no learning bounds in the paper. The main result is to bound the contrastive loss in terms of the cross-entropy loss under the assumption that the latent classes and the label classes coincide. Authors state that getting generalization bounds is routine and, therefore, they chose not to give them — I do not see how generalization bounds follow in a straightforward manner here, and even if they do, it is important to write them for completeness.   The main contribution here is that the bounds depend logarithmically in K — the number of negative samples per positive sample — compared to sqrt{K} in the previous work. The previous bound however holds for Lipschitz losses as well, for e.g., hinge loss. So the question remains whether this improvement is only for the cross-entropy loss. Regardless, K is typically small in practical applications. Even the experiments in the paper (Figure 7) suggest that the performance degrades for larger K even on simple tasks. So, the improvement is really somewhat insignificant.    The reviewers were generally positive and appreciated the paper. However, in the light of comments above (of which I am quite certain), unfortunately, I am unable to accept the paper at this point. I believe the comments above (and from the other reviewers) will help improve the overall quality of the paper. I encourage the authors to incorporate the feedback and work towards a stronger submission.
1	This paper proposed Q-value-weighted regression approach for improving the sample efficiency of DRL. It is related to recent papers on advantage-weighted regression methods for RL. The approach is interesting, intuitive, and bears merits. Developing a simple yet sample-efficient algorithm using weighted regression would be a critical contribution to the field. The work has the potential to make an impact, if it has all the necessary ingredients of a strong paper.  However, reviewers raised a few issues that have to be addressed before the paper can be accepted. As some reviewers pointed out, there seem to be unaddressed major issues from previous submissions. Novelty appears limited, especially because the proposed approach is very similar to recent works (e.g., AWR). The experiment section lacks comparison to recent similar algorithms, and the available comparisons appear to be not strong enough to justify merits of the proposed algorithm. Theorem 1 requires an unrealistic state-determines-action assumption for the replay buffer. Although the authors made an effort to justify this assumption, it remains very problematic and rules out most randomized/exploration algorithms.
0	In this paper, the authors consider two algorithms for solving (strongly) monotone variational inequalities with compressed communication guarantees, MASHA1 and MASHA2. MASHA1 is a variant of a recent algorithm proposed by Alacaoglu and Malitsky, while MASHA2 is a variant of MASHA1 that relies on contractive compressors (by contrast, MASHA1 only involves unbiased compressors). The authors then show that - MASHA1 converges at a linear rate (in terms of distance to a solution squared), and at a $1/k$ rate when taking its ergodic averge (in terms of the standard VI gap function). - MASHA2 converges at a linear rate (in terms of distance to a solution squared).  Even though the paper's premise is interesting, the reviewers raised several concerns which were only partially addressed by the authors' rebuttal. One such concern is that the improvement over existing methods is a multiplicative factor of the order of $\mathcal{O}(\sqrt{1/q + 1/M})$ in terms of communication complexity (number of transmitted bits) for the RandK compressor, which was not deemed sufficiently substantive in a VI setting (relative to e.g., wall-clock time, which is not discussed).  After the discussion with the reviewers during the rebuttal phase, the paper was not championed and it was decided to make a borderline "reject" recommendation. At the same time, I would strongly urge the authors to resubmit a properly revised version of their paper at the next opportunity (describing in more detail the innovations from the template method of Alacaoglu and Malitsky, as well as including a more comprehensive cost-benefit discussion of the stated improvements for the RandK/TopK compressors).
1	The authors present an improved method to convert ANNs to spiking neural networks (SNNs). First, a network with quantized activations is constructed, then it is converted. They analyze the conversion errors theoretically. In addition to previously considered errors [Li et al. 2021] they also consider an error they call "unevenness error" and propose a way to compensate for that. They test the method on data sets such as CIFAR-100 and show good improvements over previous methods with respect to classification accuracy and inference time. The reviewers agree that the manuscript presents interesting and valuable work with a significant novel contribution.The manuscript is well written.  Weak points according to the first reviews were: - Lack of ImageNet conversion experiments. - Analysis of energy consumption was missing. - More related work needs to be compared. The revision addressed all these points, This was acknowledged by the reviewers with increased ratings. All reviewers propose acceptance.
1	This paper proposes a simpler alternative to S4 that achieves comparable performance. The method makes sense and the experiments are thorough. All reviewers agreed this is a good paper. I recommend acceptance.
1	The authors introduce a neural network approach for solving the fixed point equations arising in deep equilibrium models. This consists of a tiny network that provides an initial guess for the fixed point, as well as a small network that computes coefficients inside an algorithm inspired by Anderson iteration.  Overall, there is consensus among the reviewers that the paper is well written and is a strong empirical study.  I recommend acceptance as a poster.  Additional remarks:  - The authors argue the DEQs / implicit deep learning models allow a decoupling between representational capacity and inference-time efficiency. Yet, in the "Regularizing Implicit Models" paragraph, they write "Implicit models are known to be slow during training and inference. To address this, recent works have developed certain regularization methods that encourage these models to be more stable and thus easier to solve.", which seems like a contradiction to me. So while in theory I agree with this decoupling, in practice, it seems not completely true.  - Section 3 should include some discussion on conditions on f_theta for the existence of a fixed point.  - Since the initialization and HyperAnderson networks are trained using unrolling, there is some memory overhead compared to vanilla DEQs, that are differentiated purely using implicit differentiation. It would be great to clarify the amount of extra memory needed by these networks. It is necessary to justify that the initialization and HyperAnderson networks are smaller than usual neural networks.
1	Three out of the four reviews rated this paper well below the acceptance threshold. Although the review scores show a relatively large spread, I think that the review contents are more or less coherent across the four reviewers. The equivalence of the state equations (SEs; a set of equations that macroscopically characterizes optimal solutions of certain high-dimensional regression problems) derived from three different approaches (AMP, CGMT, and LOO) is well expected to hold, as the optimal solutions should be independent of how their macroscopic characterization in the form of an SE is derived, and this paper concretely showed such equivalence to hold for three problems. More concretely, Theorem 1 states the equivalence of the SEs for M-estimator derived from the three approaches, Theorem 2 states the equivalence of the SEs for LASSO derived from AMP and CGMT, and Theorem 4 states the equivalence of the SEs for logistic regression derived from LOO and CGMT. The main concern raised by all the reviewers is that this paper does not provide novel and significant insights as to why and how the equivalence arises. Some reviewers also pointed out that this paper lacks citation to the relevant statistical-mechanics literature, as well as that this paper contains so many typos, grammatical errors, and inappropriate typesetting styles. The authors responses were not instrumental in persuading the reviewers with negative evaluation. On the basis of these I would not be able to recommend acceptance of this paper for presentation at ICLR 2021.
1	The reviewers initially disagree on whether this paper has sufficiently advanced the research topic of graph pooling and have concerns on missing the comparison with Wang and Ji (2020), which have also introduced higher-order pooling for graph neural networks (GNNs). After extensive discussions with the authors, the reviewers have reached a consensus towards acceptance: While the performance improvement is marginal, and in some cases, not statistically significant, the proposed tensor decomposition-based pooling algorithm is new and provides a theoretically-sound valuable addition to advanced neighborhood aggregation methods for GNNs.
1	The reviews are generally positive (though somewhat short), and a large pre-training corpus for legal text will likely be useful for NLP research. One reviewer gave a reject score (5) with the following two weaknesses:  A) The dataset comes from a wide spectrum of law-related data sources, which may differ substantially and hence limit the usefulness of the dataset.  B) Privacy   I am not too worried about Point A because large language models seem to be able to learn from diverse data sources.  Regarding Point B, the separate ethics review mentions the privacy concerns as well but finds that the submission sufficiently discusses this concern and sees no serious ethical issues.  Hence overall I recommend accepting the paper.
0	This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. This would the hopefully leads to escaping sharp valleys and better generalization. Authors further provide some related theoretical results and several experiments to show effectiveness of their models.  All reviewers find the proposed method well-motivated, novel and interesting. The paper is well-written and easy to follow. However, both theoretical results and empirical evaluations could be improved significantly:  1- The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. See for eg. R1's comments about this.  2- Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. In particular, there are two main areas to improve:  a) Based on the Appendix D, the choice of hyper-parameters seem to be made in an arbitrary way and all models are forced to use the same hyper-parameters. This way, the choice of hyper-parameters could potentially favor one method over the other. A more principled approach is to tune hyper-parameters separately for each method.  b) It looks like the choice of #epochs has been made in an arbitrary way. For all experiments, it would be much more informative to have a figure similar to the left panel of Fig. 4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not.  c) Based on the current results, SALR's performance  is on par with that of Entropy-SGD on CIFAR-100 and WP and there is a very small gap between them on CIFAR-10 and PTB. I highly recommend adding ImageNet results to make the empirical section stronger. The other option is to compare against other methods in fine-tuning tasks. That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine-tuning tasks.  Given the above issues, my final recommendation is to reject the paper. I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. I hope authors would address the above issues as well and resubmit their work.
1	This paper analyses grid worlds, more precisely multiple objects moving in grid worlds, using the mathematical idea of state complexes.  The state complex represents all possible configurations as a single space, from which domain properties can be ascertained by group-theoretic, combinatorial, or geometric analysis.  In particular, the paper develops a theory around "Gromov's Link Condition" to analyze conditions under which collisions can be prevented in such domains.  The reviewers had a mixed initial response to this paper.  On the positive side, the reviewers appreciated the theoretical development (txoD) and novelty (2SSW).  On the negative side, the reviewers struggled to see the significance or relevance of the work to learning or AI (2SSW, 5vs3).  The reviewers understood the work as a mechanism for collision checking (2SSW), a means to support learning (5vs3), and a computational mechanism for analyzing gridworld dynamics (txoD).  The author response clarified several aspects of the reviews that were misunderstood.  The author response did not sway the reviewers.  Primarily, the concern is that the paper failed to communicate the relevance of the mathematical analysis of gridworlds to an AI audience.  The sole positive reviewer ultimately concurred with the arguments made by the negative reviewers.    Two reviewers indicate to reject, and one indicates a weak accept.  Based on the failure of the paper to clearly communicate the relevance of its ideas to any reviewer, the paper is rejected.  One suggestion for a future revision would be to present these ideas in the general setting of an MDP (instead of a specific domain of a gridworld).  The local combinatorial analysis on a generic MDP could potentially be more useful to the MDP community when considering planning for AI safety or problems of mechanism design.  The evidence needed to validate the ideas for those communities might again be different from the evidence provided in this paper.  As a separate comment, the analysis of the transition dynamics of actions may have related work stemming from predictive state representations.  In the paper's current form, the reviewers were unable to see a clear contribution.
1	Meta Review: This paper receives generally positive ratings. After rebuttal, all reviewers recommend acceptance. One reviewer has concerns that authors should also include domain generalization settings and baselines. AC agrees with this but also understands that it is impossible for one paper to incorporate every piece of OOD such as: Cross-domain/Domain Transfer/Domain Adaptation/Domain Generalization/Few-shot/Zero-Shot/Open-set, etc. As well as the most recent WILDS benchmarks and DomainBed settings, So, AC suggests the authors position this paper topic more clearly in the introduction and experiments. AC recommends acceptance.
1	This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
1	All reviewers are positive or very positive about this work. The authors successfully addressed all questions. I believe this paper should be accepted.
1	The paper attempts at providing a general benchmark for evaluating/analysis of long range transformer models, consisting of a 6 evaluation tasks. The main goal of the paper is to remove conflating factors such as pretraining from model performance and keeping the benchmark accessible. All reviewers agreed that these are important positive aspects of the paper and the presented analysis/results are useful.    While reviewers generally feel positive about the work, there are some critical concerns on how useful this benchmark is in practice, how generalizable are the results, and whether the benchmark is good at what is intended for. For example, the vanilla Transformer model performs very well on all the proposed tasks, making me question on what we can actually learn about long range dependencies through this benchmark. In addition, most tasks are synthetic and all models fail on 1 of the 6 proposed tasks.   Therefore, I think LRA should be viewed more as a tool for analysis, or as authors nicely put in their response, it should be viewed as a means to "encourage hypothesis driven research instead of hillclimbing or SOTA chasing.".  During discussion period with reviewers, while acknowledging the above-mentioned issues, this strength was highlighted as a valuable contribution. Therefore, given the general positive sentiment about the work, I'd recommend accept.
1	The paper studies real world ML APIs' performance shifts due to API updates/retraining and proposes a framework to efficiently estimate those shifts.  The problem is very important and the presented approach definitely novel. My concern is about limited novelty of the theoretical analysis and weak experimental evaluation (just two dates, limited number of systems tested, small number of ablations). As of now the paper looks like an interesting but unfinished proposal. Looking forward to the discussion between the authors and the reviewers to address the concerns.  In the rebuttal, the authors have addressed reviewers' comments, in particular by adding additional experiments that strengthen the paper. All the reviewers recommend the paper to be accepted. It is suggested that in the camera-ready version the authors will add additional details regarding the experiments, as some of the reviewers mentioned.
1	Two reviewers give a weak accept rating while the other one gives a borderline reject rating. Considering the low confidence of the negative comment and the contrary comments in paper writing (confident "easy to follow" vs. unconfident "hard to understand"), the AC would lean to accept this paper.
1	Although the reviewers gave a wide range of ratings to this paper, they all agreed that it is well-written and presents two novel, sound algorithms that perform well in practice. The main concern was that the algorithms merely combine existing techniques, which is true. However, given that the resulting algorithms are elegant, well-motivated, and highly performant, and that achieving the combinations is not trivial, I believe this paper makes a significant contribution to an active field of research and deserves acceptance.  The authors should take the reviewers' comments into account as they prepare their final revision. In particular, I would encourage them to more explicitly describe their reasons for presenting two different algorithms (cardinality constrained/unconstrained). Currently, it feels like the two parts of the paper are somewhat disconnected.
1	The authors new notation for the statistical algorithmic stability. It will be useful for studying algorithmic stability, and will open up new discussion in the field. Overall, the review team provide positive feedback and I would recommend accepting this work.
0	This paper proposes a new framework for molecular representation learning (MRL) using both 2D and 3D molecular data. This framework is general and applied to various problems (e.g., protein-ligand binding pose prediction and molecular conformation prediction). I believe this paper is potentially quite impactful and able to reshape how research is conducted for MRL research.   However, the contribution of this paper is unclear in its current form.  - The proposed methodology is not very novel and uses a combination of existing methods.  - While the main contribution of this paper is to propose a new framework for MRL, the experiments focus on evaluating a single algorithm (i.e., SE(3)-equivariant model + 3 self-supervised tasks) compared to existing algorithms under different frameworks. In other words, the experiments do not deliver new information since (1) existing works demonstrated how combining 2D & 3D data improves downstream task performance and (2) pretraining is useful for the considered downstream tasks.  Overall, I recommend rejection for this paper. However, I believe this paper can be a very strong submission for the next conference if the authors clearly demonstrate their contribution. For example, I think the proposed idea would be pleasantly presented as an important "benchmark" paper, rather than a framework with superior performance.
1	This paper presents a new unsupervised learning method by making full use of pre-trained diffusion probabilistic models. Extensive experiments show that the proposed method can obtain an improvement in performance and learning time. Four reviewers voted for accepting the paper after the rebuttal and the discussion. All concerns raised by the reviewers have been well addressed by the authors. The AC agrees with the reviewers and recommends accepting the paper. Also, AC urges the authors to improve their paper by taking into account all the suggestions from reviewers.
1	Unanimous accept from 3 experienced reviewers with good confidences  Important topic (lifelong RL), clearly explained, well-formulated with a variational and hierarchical Bayesian model, evaluated on a range of relevant experiments in discrete and continuous settings in MuJoCo, and also MetaWorld MT10 & MT50 domains in response to reliever osbM and thFd questions, adapted to the lifelong setting of rolling out tasks sequentially, to include like opening a boxes/closing drawers/opening windows.
1	The paper considers the global convergence and stability of SGD for non-convex setting. The main contribution of the work seems to be to remove uniform bounded assumption on the noise, and to relax the global Holder assumption typically made. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails.  The authors establish that SGD’s iterates will either globally converge to a stationary point or diverge  and hence tehir result exclude limit cycle or oscillation. Under a more restrictive assumption on the joint behavior of the non-convexity and noise model they also show that the objective function cannot diverge, even if the iterates diverge.  The reviewers are on the fence with this paper. While they agree that the paper is interesting, they only give it a score of weak accept (subsequent to rebuttal as well). One of the qualms is that while the authors claim the result helps show success of SGD in more natural non-convex problems, they don’t provide realistic examples supporting their claim. Further, while the extension to holder smoothness assumption while is indeed interesting, unless practical significance is shown via examples, the result is not that exciting.  From my point of view and reading, while the reviews are not extensive, i do not disagree with reviewers sentiment. Technically the paper is strong but there is a unanimous lack of strong excitement for the paper amongst reviewers. While there is this lack of more enthusiasm, given the number of strong submissions this year, I am tending towards a reject.
1	All reviewers agree that the paper is strong enough for acceptance. They highlight the interesting methodological developments, the well though-out experiments, and the easy-to-follow description. The results look promising as well.
1	This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research. All reviewers agree that this is a strong paper and clearly recommend acceptance. I recommend acceptance as an oral since the paper opens the door for a lot of interesting follow-ups.
0	The paper studies mixture of expert policies for reinforcement learning agents, focusing on the problem of policy gradient estimation. The paper proposes a new way to compute the gradient, apply it to two reinforcement learning algorithms, PPO and SAC, and demonstrate it in continuous MuJoCo environments, showing results that are comparable to or slightly exceeds unimodal policies. The main issue raised by multiple reviewers is novelty. Mixture of expert models have been widely studied in the context of reinforcement learning, and while the paper proposes a new method for the gradient computation, a more suitable format, as pointed out by Reviewer 2, could be to ground the paper around the proposed gradient estimator, and compare, both analytically and empirically, it to existing alternatives. Therefore, I recommend rejecting this submission.
1	### Summary of paper This work addresses the problem of grouped feature selection in the supervised learning setting. A new method based on ensemble of features is proposed as well as a new metric to evaluate the results on synthetic, semi-synthetic and real data.   ### Rebuttal Authors were engaged and addressed all the reviewers' concerns and questions.  While reviewers did not engage in the rebuttal discussion nor post-rebuttal discussion, they significantly raised their scores, bringing the average score from 4.75 to 5.75, indicating that their main concerns were addressed.  ### Acceptance There are no major concerns that remain to be addressed and the paper is ready to be published. I hence recommend acceptance of the paper.
1	This paper proposes a framework to train a discriminative model robust against (i) label noise, (ii) out-of-distribution input, and (iii) input corruption. To tackle these problems, a complex model is proposed that combines several existing models including InfoNCE-style contrastive learning, prototypical contrastive loss, Mixup, and reconstruction loss. Noisy training labels are cleaned using a temporally consistent label smoothing mechanism, combined with a curriculum learning algorithm.   Originally, the reviewers raised concerns regarding the limited ablation experiments and the lack of studies on real-world noisy labels. The additional experiments in the revised version addressed some of these concerns. Thus, the reviewers increased their rating slightly.  However, the reviewers in the discussion phase agree that the proposed method has a limited novelty, is complex, and involves many moving parts that require a careful design and hyperparameter tuning, and they do not recommend accepting the submission. I agree with the reviewers and recommend rejection.
0	Authors extend the probabilistic PCA framework to multinomial-distributed data. Scalable estimation of principal components in the model is achieved using a multinomial variational autoencoder in combination with an isometric log-ratio (ILR) transform. The reviewers did not agree on the degree of novelty of the paper to PC estimation. The presentation of the paper can be improved. The reviewers criticise that large changes have been made to the paper during the rebuttal phase. Overall, the paper is borderline and due to the mentioned large changes I recommend a rejection (and re-review at a different venue).
1	This paper studies synthetic data generation for graphs under the constraint of edge differential privacy. There were a number of concerns/topics of discussions, which we consider separately: 1. Theoretical contributions. There are not that many theoretical contributions in this paper. I think this is OK, if the other components are compelling enough. On the theory, the authors mention that accounting for the constants is important in the analysis of DPSGD. On the contrary, I would say that these constants are not very important: if one requires specific constants, numerical procedures can determine values, otherwise for the sake of theory, no one generally needs these constant factors.  2. Empirical/experimental contributions. This was the primary axis for evaluation for this paper. None of the authors were especially compelled by the results. The methods are essentially combinations of known tools from the literature, and it is not clear why these are the right ones to solve this problem in particular. If the results were very exciting, that might be sufficient to warrant acceptance, but it is still not clear how significant the cost of privacy is in this setting. The experiments are not thorough enough to give serious insight here. It is a significant oversight to not provide results on DPGGAN without the privacy constraint, as this is the best performing model with privacy. The omission of something as important as this (and lack of inclusion in the response, with only a promise to include later) is indication that the experiments are not sufficiently mature to warrant publication at this time. The decision of rejection is primarily based on concerns related to the empirical and experimental contributions.  3. Privacy versus link reconstruction. Reviewer 4 had concerns about the notion of privacy, claiming that it does not correspond to the probability of a link being irrecoverable. This is differential privacy "working as intended", which is not intended to make each link be irrecoverable: it is simply to make sure the answer would be similar whether or not the edge were actually present, so it may be possible to predict the presence of an edge even if we are differentially private with respect to it (e.g., the presence of many other short paths between two nodes are likely to imply presence of an edge). Some discussion of this apparent contradiction might be warranted, as this might mislead reader who are specifically trying to prevent edge recovery. It might also be worthwhile to have discussion of node DP in the final paper. The authors comment "we focus on edge privacy because it is essential for the protection of object interactions unique for network data compared with other types of data" -- the stronger notion of node differential privacy might also be applicable here. It would indeed be interesting to know whether it can preserve the relevant statistics (some of which seem more "global" and thus preservable via node DP).
1	This paper approximates the Whittle index in restless bandits using a neural network. Finding the Whittle index is a difficult problem and all reviewers agreed on this. Nevertheless, the scores of this paper are split between 2x 4 and 2x 7, essentially along the line of whether this paper is too preliminary to be accepted. Therefore, I read the paper and propose a rejection.  The reason is that the paper lacks rigor, which was brought up by the two reviewers who suggested rejections. For instance, in the last line of Algorithm 1, it is not clear what kind of a gradient is computed. The reason is that \bar{G}_b is not a proper baseline, as it depends on the future actions of the bandit policy in any given round. I suggest that the authors look at recent papers on meta-learning of bandit policies by policy gradients,  https://papers.nips.cc/paper/2020/hash/171ae1bbb81475eb96287dd78565b38b-Abstract.html  https://arxiv.org/abs/2006.16507  This is the level of rigor that I would expect from this paper, to make sure that the gradients are correct.
1	All four reviewers were against accepting the paper. A major point shared by everyone was lack of clarity: this included its overall writing, its discussion toward prior work, and imprecise math to explain the ideas. The paper did improve quite a bit over its revisions. Whether this clarified all of the reviewers' understanding of the paper remains unclear. The work may ultimately need another cycle of reviews to assess its quality.  Another shared point are a number of recommended ablations in the experiments, as well as going through more comprehensively in the set of studied datasets (R3), effect of AE choices (R2), and alternatives to the geodesic (R1, R2).
0	The paper presents a simple and intuitive method to prune the missing value in the learning and inference steps of the neural networks, leading to similar prediction performance as other methods to impute missing value. It has some really useful insights, but could benefit from one more round of revision for a strong publication:  1. improving the writing so that its sets up the right expectations on the contributions of the paper;  2. providing discussions on its connections (and differences) with zero-imputation and missing-indicator methods;  3. thoroughly investigating the experiment results to illustrate the advantages of the proposed method.    The recommendation of reject is made based on the technical aspect of the paper. ----------------------------- During the rebuttal phase, the authors misused the interactive and transparent (for the better or worse) openreview system by writing inappropriate comments with personal accusations to the reviewers who write negative reviews. We would like to extend the apologies to the reviewers for this unpleasant experience and thank the reviewers for their engagement and work, as well as their fair assessment of the paper.
1	While there was a certain lack of enthusiasm in the scores of the reviewers, the author's answers cleared the concerns of the reviewers participating in the discussion and overall the recommendation leans towards acceptance. This paper is, in the reviewers' opinions, sound and adds to the literature on unsupervised learning of symmetry. The formulation (of learning symmetry by only modelling linear transitions) is nicely simple. Experiments and evaluations generally were considered of adequate quality.
1	This is a valuable benchmark on backdoors. Most reviewers argue for acceptance, some strongly so (with scores 7,7,8,9), while only one reviewer gave a rejecting score. That reviewer mostly questioned the relationship to TrojanZoo, and the authors discussed this comprehensively now. That reviewer, RPfC, also was inactive during the rebuttal and decision process, and thus I do not weigh their (apparently answered) concerns highly. I thus recommend acceptance.
0	The reviewers enjoyed reading about an interesting take on lifelong learning, encapsulating an EM methodology for selecting a transfer configuration and then optimizing the parameters. R3 made valid concerns regarding comparison with previous, recent work. R2 also would prefer to see more thorough experiments (ideally in settings where multiple tasks exist, as also commented by R4). During the rebuttal phase the authors made a good effort to run additional experiments which cover the related work aspect better. These experiments and the overall paper were discussed extensively among reviewers after the rebuttal phase.  In the discussions, the reviewers agreed that an interesting idea can be publishable even if it does not achieve SOTA results in all scenarios, as long as it brings new perspectives and shows at least comparable results. However, in the particular case of this paper, there exist remaining concerns regarding the usefulness and applicability of the method. Specifically, the paper could benefit from a more convincing demonstration about how the method can scale (e.g. R3 and R4’s comments), especially since training time and model capacity are important factors to consider for practical continual learning scenarios. Furthermore, it is not clear how the proposed method can be used in combination with other machine learning tools within a continual learning application, for example by leveraging modern deep architectures or by complementing existing adaptive knowledge approaches (as discussed by R3).   Although the opinions of the reviewers are not fully aligned, this borderline paper seemed to lack an enthusiastic endorsement by a reviewer to compensate for the concerns discussed above and the relatively weak experimental results. Therefore I recommend rejection.
1	The reviewers unanimously recommend accepting the paper - congratulations!  My only concern is in the related work: The submission mentions  > The recent “Model soups” by Wortsman et al. [28] developed a WA algorithm similar to Algorithm 1. However the task, the theoretical analysis and most importantly the goals of these two works are different.  This is not an accurate characterization because Wortsman et al. [28] were also interested in out-of-distribution generalization - their paper mentions "robustness" and "distribution shift" several times and contains results on multiple OOD test sets. The results in this submission and in Wortsman et al. [28] reinforce each other since the two papers evaluate on different OOD benchmarks and find that weight averaging helps in both. I encourage the authors to clarify this in their related work section so that the reader can correctly put the results in context.
1	This paper proposes a new RL benchmark called MineDojo, which consists of thousands of diverse tasks on Minecraft game. The main challenge in designing such a benchmark is to define a good reward function to specify the desired task. This is very tricky since Minecraft is open-ended environment and there are various tasks, where human can't easily define the reward. To address this issue, the authors propose a very interesting idea: utilizing multi-modal (text-video) encoder as a reward. Specifically, the authors pre-trained the multi-modal encoder using contrastive learning (similar to CLIP) using multimodal data collected from the Internet and utilized the similarity between agent's behavior (video) and text as a reward function. Throughout human evaluation, the authors showed that their reward function can induce desired behavior (described by text). Since the authors open-sourced simulator, pre-trained reward model and agents, Minedojo can be a good starting point for making a progress on developing open-ended, multi-task RL agents.  Overall, all reviewers agreed that this is very solid submission and authors also handled concerns from reviewers during discussion period. I recommend acceptance.
1	The paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional. Although the method is appealing, I believe the paper falls a bit short of acceptance at the conference. Too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating low-dimensional distributions living in high dimensional spaces. We encourage the authors to use the feedback contained in this round of reviews to improve their work.
1	This paper is right on the border. I'm going to mark this as accept as the only reviewer marking reject is due to limited evaluation, but i believe the evaluation is ok (as do the other two reviewers). The idea is interesting and novel, and the paper is well written.
1	This work studies the use of EEG-based gaze estimation. Reviewers indicated the promising experimental findings of being able to capture eye movements with a smaller number of electrodes in the EEG cap, and have also provided additional feedback for improvements (see Reviewer dm5p list of comments). An immediate suggestion for improvement would be to incorporate related work from the HCI literature, as pointed out by Reviewer nhaM. Overall, the paper has received positive feedback that suggests acceptance.
1	This paper presents a new NeRF method based on tensor decomposition. The method supports both compression and composability, while achieving similar results compared to standard NeRF models. The method does not use a neural network. Several reviewers found the paper easy to follow, the method novel & sound, and the comparisons comprehensive. Two reviewers mentioned the similarity between the proposed work and TensoRF. The rebuttal addressed most concerns and highlighted the differences between the two works. As TensoRF is a concurrent ECCV submission, the existence of TensoRF should not be used against the proposed work. The AC agreed with most of the reviewers and recommended accepting the paper.
1	Executive summary:  The paper considers the design of greedy online contention resolution schemes (OCRS) for the single-item setting and certain matroids (partition matroids, transversal matroids). The main result is that there is a 1/e-selectable greedy OCRS (which improves over the best known bound of 1/4 for greedy OCRS), and that this is best possible.  Discussion and recommendation:  This is a nice little result. Not tremendously difficult, but fundamental. A plus is that the question is resolved tightly. All but one reviewer felt positively about the paper. A major concern raised in the reviews was that it's unclear why we care about greedy OCRS. In the rebuttal, the authors emphasized that greedy OCRS yield guarantees against an almighty adversary and that they recently found application in a delegation variant of the Pandora's Box problem (Bechtel, Dughmi, and Patel [EC'22]).  (Weak) accept.  ---  Additional comments:  I always thought of OCRS as one of the two main techniques that have emerged for proving prophet inequalities and guarantees for posted-price mechanisms; the other being the "balanced prices" framework. I would encourage to extend the discussion in the related work accordingly, and cite the most relevant works on the "balanced prices" framework. Or, at least, cite the most relevant papers in that direction (see list below).  Citations to add:  Kleinberg and Weinberg. Matroid Prophet Inequalities. STOC'12.  Feldman, Gravin, Lucier. Combinatorial Auctions via Posted Prices. SODA'15.  D\"utting, Feldman, Kesselheim, Lucier. Prophet Inequalities made Easy: Stochastic Optimization by Pricing Non-Stochastic Inputs. FOCS'17.  D\"utting, Kesselheim, Lucier. An O(log log m) Prophet Inequality for Subadditive Combinatorial Auctions. FOCS'20.
1	The authors propose a domain-specific extension of neural operators that is appropriate for photonics applications. This is an interesting application of neural operators which demonstrates the usefulness of building in physical priors. Some reviewers expressed concern about the topic being too far outside the usual focus of NeurIPS, but there is also an upside to introducing novel application areas to the NeurIPS community. All reviewers agreed the work was of high quality and worth accepting, so I recommend acceptance.
1	The authors propose to use implicit policies (similar to a conditional GAN) with a GAN-inspired regularizer. Theoretically, they show an equivalence between policy-matching and state-action-visitation matching. Finally, they evaluate their approach on D4RL and showed improved performance as well as ablations.  Reviewers did not find the theoretical contribution to be significant.  While the exact form may be novel, the general result has been shown in previous work and they only use the general result as a loose motivation for their approach. All reviewers acknowledge their empirical improvements as the primary strength of the paper. While a central component of their story is joint state-action regularization, Reviewer Ht1b identified that their proposed approach does not appear to directly regularize the joint state-action distribution, but rather behaves more similarly to existing policy constraint methods. I agree with Reviewer Ht1b and after much back-and-forth discussion (both Reviewer Ht1b and myself) with the authors, I have not been persuaded otherwise.  The paper has a lot of potential - strong empirical results, but the justification and explanation of the method needs to be rewritten in light of the policy constraint regularization or a stronger argument needs to be put forth in support of joint state-action regularization. I don't think this diminishes the results though, but without this substantial revision, I cannot accept the paper at this time.
1	PAC-Bayes bounds provides a control of the risk of aggregation of predictors. In these bounds, the Kullback-Leibler divergence between the aggregation distribution and a prior appears in the upper bound on the risk. In this paper, the authors prove variants where the KL is replaced by an IPM (Integral Probability Metrics), including the total variation distance and Wasserstein. The important point is that these bounds are close to "uniform" (Vapnik-type) bounds in some unfavorable settings, but also can improve on them is more favorable scenarios.  The reviewers agreed that the results are novel and that the paper is technically sound. These bounds really extend the framework of PAC-Bayes bounds (for example, they are not necessarily vacuous when the posterior is not absolutely continuous with respect to the prior), and the fact that they recover uniform bounds in the worst case is also nice. All the reviewers recommended to accept the paper, and I agree with them.  The reviewers pointed out a few missing references, I will ask the authors to include them in the paper as promised during the discussion. I will add the following references that were not mentioned by the reviewers, and thus leave the authors decide to include them or not: - Alquier and Guedj (2018) actually provided PAC-Bayes bounds based on f-divergences, that were then improved by Ohnishi and Honorio (2021) (especially the dependence with respect to the confidence level). - there were a few attempts to replace the KL by the Wasserstein distance. The benefit were not as clear as in the present paper, but the authors might want to comment on the paper https://hal.archives-ouvertes.fr/hal-03262687/ or Lopez & Jog (2018) on MI bounds...
1	strong paper with many positive remarks, especially after rebuttal. In addition the authors released their code. Recommendation for accept as oral.
1	This paper studies the problem of building world models that can decouple controllable and uncontrollable factors in the environment. The paper received reviews that generally tended towards acceptance. However, the reviewers had difficulty understanding some details and had concerns that the setup might not be the same across environments. The authors provided a rebuttal that addressed most of the reviewers' concerns. The paper was discussed and all the reviewers updated their reviews in the post-rebuttal phase. Reviewers generally agree that the paper should be accepted. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers' feedback and incorporate their comments into the camera-ready.
1	Reward-Biased Maximum Likelihood Estimate (RBMLE) is an approach to balance exploration-exploitation that is based on biasing models with smaller cost functions. The paper considers an augmented version of it (ARBMLE) that confines the search of the model to the confidence set used by an Upper Confidence Bound (UCB)-like algorithm. The paper considers the Linear Quadratic Regulation (LQR) with unknown dynamics, provides a regret bound of ARBMLE, which is comparable to that of UCB-based approach. The paper empirically shows that both ARBMLE and RBMLE outperform many other methods, particularly UCB-based ones.  We have both strong support in favour of acceptance of this paper and some less enthusiastic negative reviews. After reading the paper, the reviews, and the discussions, I am inclined to accept the paper. The main reason is that the paper considers a relatively less-known approach to exploration-exploitation problem, provides reasonable analysis (even though the tools might be standard), and shows promising empirical results. However, my recommendation should not be considered as dismissing the concerns of reviewers. I believe many of them are valid. I merely put less weight on them in my evaluation compared to the negative reviewers.  Let me emphasize a few points brought by reviewers and my own reading of this work. I hope the authors consider them in the revision of their paper.  - The writing quality varies a lot. The first two sections are written clearly and have some nice insights and intuitions, but then the writing quality deteriorates. For example, Section 3.2 becomes confusing (we have E_t, E_1, E_2 with different meanings), and Section 5 becomes a series of lemmas without much insight. Sections 6 and 7 are of better quality again.  - The series the sequence of lemmas in Section 5 is not very insightful. The authors have added a paragraph at the beginning of that section, but I believe that is not enough. My suggestion is that authors either provide better intuition behind each of these lemma, or move them to an appendix.  - Be clear about the dependence of the regret bound on the dimension of the system.  - Assumption 3 requires more discussion.  - The issue of tractability of solving the required optimization problem should be discussed explicitly.  - Given that [31] (Mete et al., "Reward biased maximum likelihood estimation for reinforcement learning", 2021) solves an arguably more general problem (RL instead of LQR), a detailed comparison is needed. What are the differences in insight, proof techniques, etc.?
1	The paper addresses vision-based and proprioception-based policies for learning quadrupedal locomotion, using simulation and real-robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real-world results. Given that there are also real-robot evaluations, and an interesting sim-to-real transfer, the paper appears to be an important acceptance to ICLR.
1	This paper empirically studies various design choices in offline model-based RL algorithms, with a focus on MOPO (Model-based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble.  The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work might be of interest to many researchers working on model-based RL, I also recommend a spotlight presentation for this work.  I have some additional comments:  (1) The paper studies the correlation of uncertainty measures with the next-state MSE, with the aim of showing which one has a higher correlation. The underlying assumption is that the next-state MSE is the gold standard that we should aim for.  If we go back to the MOPO paper, we see that to define an uncertainty-penalized reward, we need an upper bound on the absolute value of G(s, a), which is the difference between the expected value of the value function at the next-state according to the true model and the estimated model.  If we assume that the value function belongs to the Lipschitz function class w.r.t. a metric d, this upper bound is proportional to the 1-Wasserstein distance between the true next-state distributions and the model's distribution. If the dynamics is deterministic, 1-Wasserstein distance becomes the $d( T(s, a), \hat{T}(s,a) )$. If the distance d is the Euclidean distance, this becomes the squared error.  Therefore, the squared error makes sense for deterministic dynamics, and it only provides an upper bound of $|G(s, a)|$. If the environment is not deterministic, the squared error may not be a reasonable gold standard anymore to compare the correlation of various uncertainty measures with.  The paper introduces a generic MDP framework, but does not mention anything about its focus on MBRL for deterministic environments until the last sentence of its conclusion. Please clarify this in your camera ready paper.  (2) The experiments are conducted using 3 or 4 seeds. Although this is the common practice in the deep RL community, it is too small. Standard deviations in Tables 1, 2, ... are computed with 3 seeds, which would be cringeworthy to statisticians and empirical scientists. I encourage the authors to increase the number of independent random experiments to make their results more powerful.
0	This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters.   The reviewers finds the paper technically solid but difficult to read and with a limited contribution.  The AC carefully reads the paper and discussions. Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019). \ Therefore, the AC recommends rejection.
0	This paper proposed an augmentation construction to mitigate the double descent. For any pairs of data points, the constructed input is simply concatenation of two inputs and the constructed label is the average of their corresponding labels. The authors further empirically show that this would mitigate double descent.  Reviewers unanimously like the main idea of the paper but they have other major concerns about this work. The main concern is that we already know double descent is not a practical issue since it can be mitigated by early stopping or proper regularization (Nakkiran 20'). Therefore, the main benefit from this paper could come from a better understanding of double descent using the observations from this construction. However, the paper does not provide us with insightful theoretical or empirical findings beyond the main observation. There are a couple of other concerns as well about discussions around #samples and the fact that the proposed construction is not i.i.d. and also lack of proper discussion about the relationship between the proposed construction and regularization techniques.  Unfortunately, authors did not responded to reviewers concerns. Nonetheless, I encourage authors to read reviewers' specific feedbacks, incorporate them and resubmit their work.  Given the above concerns, I recommend rejecting the paper.
1	The paper studies a high-order discretization of the ODE corresponding to Nesterov's accelerated method, as introduced by Su-Boyd-Candes. The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster. However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach. Thus, the reviews could not recommend acceptance, and I concur. The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue.
1	The paper considers the saddle point problem of finding non-convex/non-concave minimax solutions. Building onEG+ of Diakonikolas et al., 2021 that works under weak MVI conditions, the work presents a new algorithm CurvatureEG+ that works for a larger range of weak MVI condition compared to previous work and also works for the constrained and composite cases. The authors show cases where this algorithm converges while the previous algorithms can be shown to reach limit cycles. Overall, this theoretical work seems strong. Most reviewers seem to agree that the contribution is good enough for publication. Compared to EG+ the additional contribution is to expand the range of weak MVI condition. While this seems like a slight improvement, looking beyond just the final convergence rate, the paper has some nice insights that provides a unifying view that captures past algorithms (like EG+ as special case). I recommend acceptance.
0	The authors study the problem of augmenting embedding-based entity alignment in knowledge graphs (KG) through the use of joint alignment with deduced neural ontologies (more specifically, alignment of the KG 'neural' axioms). Motivated by the observation that the representation between two potentially aligned entities must be bound by a minimal margin, which can be problematic when there are many potential alignments, they propose aligning neural axioms by Wasserstein distance-based loss between learned entity embeddings conditioned on the relation embeddings. Experiments are conducted on OpenEA against multiple strong baselines -- showing that adding the ontology alignment to these baselines improves the results.  == Pros == + The addition of aligning (conditional) ontologies is ostensibly novel. + For KGs with sufficient entity/relation overlap, the proposed NeoEA method is applicable.  + NeoEA has been shown empirically to improve many SoTA methods.  == Cons ==  - While the theoretical justification is a welcome motivation, the reviewers did not find the theoretical arguments significant nor convincing. - Overall, the narrative needs work to make the paper more self-contained and approachable for a broader range of readers. The reviewers (and myself) found many concepts and statements somewhat confusing and needing clearly context and contrast with existing works.  Evaluating along the requested dimensions: - Quality: Conceptually, the core idea is interesting, well-motivated, original, and ostensibly effective. Empirically, NeoEA is shown able to improve upon several strong baseline (underlying) methods.  I believe that all of the reviewers find the work is interesting and promising. However, there were continuing concerns the strength/value of the described theory; it isn't clear if stronger theory isn't possible or if this just hasn't been fleshed out.  - Clarity: Most of the reviewers (and myself) found the paper difficult to follow as a self-contained work in terms of concepts, clear definitions (e.g., \mathcal T isn't defined early on) and the actual applicability of the theory. The figures help, but even these need some work. A related work section (or more structured presentation of related work) might be clarifying along with running examples and a more unifying math presentation that captures existing and proposed work. After thinking about this more, it is actually a relative simple (in a good way) and clever idea. However, it took several readings and readings of related work to get there. Additionally, the fact that all of the reviewers were concerned about different limitations is concerning wrt clarity. Appendix B helps a bit and I believe can also be put into the main paper. - Originality: As best as the reviewers and I can tell, we haven't seen this method applied to entity alignment despite this being a relatively mature subfield. - Significance: The consensus seems to be that the approach could be a notable contribution to an important area. However, it also appears that most of the reviewers don't feel the paper is ready for publication at a top-tier venue yet.  As stated throughout this meta-review, there are several aspects to like about this work including the originality of the idea, strong motivation, and good empirical results. However, we all agreed that the paper isn't quite ready in its current form -- thus, I presently recommend reject for this submission.
1	This work is focused on the estimation of optical flow from a neuromorphic camera that produces Poisson spiking at each pixel with a rate governed by overall intensity. The authors use local space-time aggregation of spike-time differentials to identify features that are then corresponded via a convGRU decoder.   The reviewers found the application interesting, and noted the good performance of the method. There were however a number of concerns about innovation and novelty of the method. Specifically the aggregating spikes to operate on point process data is a standard approach and the assessment of the spiking source of the data was not analyzed. Regardless of the similarity to past methods, overall the reviewers felt that the strengths of the paper, specifically the combination of methods brought together to solve a unique problem, outweighed the weaknesses. Thus I recommend that this work be accepted.
1	# Summary of the Paper  This paper makes two contributions as far as I can tell: 1. The paper attempts to characterize training instability in large language models. 2. The paper presents Sequence Length Warmup (SLW), a technique that the authors claim to reduce training instability and that empirically makes training LLMs much more efficient.  # Metareview  The paper isn't perfect, but it's an obvious accept. Even in the most pessimistic assessment of the paper, the SLW technique is a clear win for efficient training regardless of how harshly you judge the analysis of training instability.  The biggest weaknesses of the paper are (1) all of the methodological and scientific questions around trying to grapple with the phenomena around training stability and (2) claims of a connection between SLW and training instability.  On the topic of instability in large model training (1), there has been plenty of griping on Twitter but very little scientific analysis of the phenomenon. This paper's analysis will hardly be the last word on that topic, but it's a reasonable first attempt at something that other researchers will doubtlessly build on and hone over the coming years. I share Reviewer c8jb's concerns that correlation with gradient norms is insufficient to make claims about training stability or SLW and that more rigorous definitions of training stability are necessary. However, even if, in the very worst-case scenario, this paper is one of the first attempts at characterizing a vexing phenomenon and inspires future researchers to tear apart the modes of analysis and findings in this paper to improve upon them, this paper will have been a worthwhile contribution to the scholarly literature.  On the topic of claims of a connection between SLW and training instability (with the concerns most poignantly expressed by Reviewer S5Qt), this is really a byproduct of the scientific analysis of training instability. This connection (or lack thereof) will become clearer as time goes on and the science improves, and I simply ask that the authors acknowledge the uncertainty here.  **If the above scenarios are the very worst possible outcome for this paper, it's still a major contribution to both science and practice. I therefore advocate for accepting this paper, and the reviewers seem to agree with that assessment (both the good and the bad). I urge the authors to prepare the camera-ready version of the paper by carefully incorporating the feedback of the reviewers and, in particular, Reviewer c8jb, who had some very thoughtful comments about ways to clarify the scientific aspects of the paper. To satisfy the reviewers and future readers, I highly recommend openly and frequently acknowledging the vast uncertainty we have about the scientific aspects of this paper as we strive to make sense of this strange training instability phenomenon.**  The reviewers were enthusiastic and engaged, and the discussion was lively, which suggests to me that this is an exciting paper that deserves to be featured to the community via publication at NeurIPS.  There were some other common comments that I urge the authors to address in order to produce the best and most influential possible version of this paper: * "The reason why instabilities lead to worse performance when they do not cause divergence was not discussed much.", "The reason why using longer sequences creates instability was not discussed much." The authors discussed this a bit during the discussion period, but it's worth emphasizing these as open questions so that other researchers know it's important to follow up on. * It's worth being crystal clear about truncation vs. just focusing on shorter sequences. It's an important experimental detail that may seem surprising or counterintuitive if it's not clearly laid out.
1	A novel second order nonlinear oscillator RNN architecture is proposed, analyzed, and evaluated in this paper. The results are solid and impactful. Authors and expert reviewers showed exemplary interactions with each other, improving the manuscript in significant ways. All four reviewers overwhelmingly recommended accept. I recommend that this paper be selected as an oral presentation.
0	The paper presents a novel deep symbolic regression approach that is a hybridization of existing methods, showing state-of-the-art performance on the SRBench. Let me stress that being a hybrid is no reason to reject a paper as creating hybrids can be a very creative contribution. And for me this is the case here. the hybrid is not just a "mixture" but actually a very creative rewiring of components of the underlying approaches. This is also supported by the ablation study. Moreover, several of the  issued raised were clarified well in the rebuttal.  BTW, the authors may also want to cite other approaches for equation discovery, see e.g.  Jure Brence, Ljupco Todorovski, Saso Dzeroski: Probabilistic grammars for equation discovery.  Knowl. Based Syst. 224: 107077 (2021)  Will Bridewell, Pat Langley, Ljupco Todorovski, Saso Dzeroski: Inductive process modeling. Mach. Learn. 71(1): 1-32 (2008)  But this only for making the paper more self-complete.
1	Three reviewers agree that this work meets the bar for acceptance, rating it weak accept, weak accept, and accept. The work provides bounds for approximating continuous piecewise linear functions by ReLU networks and an algorithm. Reviewers praised the novelty and significance, and were positive about clarifications offered during the discussion period, particularly about the time complexity of the algorithm. Hence I am recommending accept. I encourage the authors to still work on the items of the discussion and the promised additions such as the open source implementation of their algorithm for the final version of the manuscript.
1	This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.
1	This paper provides a global convergence guarantee for feedforward three-layer networks trained with SGD in the MF regime. By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large-width networks  are shown to be well approximated by the MF limit, a continuous-time infinite-width limit (Theorem 3). Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when $y=y(x)$ is a deterministic function of input $x$ (Theorem 8, case 2). The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies. All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs. Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell. The authors response adequately addressed minor concerns raised by the reviewers. I am thus glad to recommend acceptance of this paper.  Pros: - Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large-width three-layer networks and its MF limit in a quantitative way with a less restrictive setting. - Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum. - Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.  In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the  global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost-sure vanishing gradient), which is a quite original contribution of this paper.
1	The authors address the problem of learning environment-invariant representations in the case where environments are observed sequentially. This is done by using a variational Bayesian and bilevel framework.  The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection.  R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer's suggestions to improve the paper.  R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper.  The authors should improve the work taking into account the reviewrs' comments.
1	In this paper, authors provide an analytical theory of curriculum learning for an online teacher-student setting where a subset of features are relevant and are used by the teacher while a student might get distracted and use irrelevant features. In such a setting, the difficulty of examples can be captured by the variance in the irrelevant features. The insights from this analysis help explain existing empirical observations reported in prior work for effectiveness of curriculum learning as opposed to random-ordering and anti-curriculum learning in compute-limited regime. Further interesting connections to the literature on cognitive science are discussed in the paper.  Given the lack of theoretical basis for curriculum learning, reviewers are in agreement that this paper is in-time and impactful for the ML community and they all recommended accepting the paper. However, during the discussion period it became clear that this recommendation was based on the promise of authors to revise the paper which never happened during the rebuttal period.  My final recommendation is to accept the paper based on the authors' promise that the final version will include all changes promised by authors in their response to reviewers. Although there is no formal conditional acceptance mechanism, the authors should view this as a conditional acceptance based on taking their word that they will revise accordingly (see the list of changes below). I will check the camera-ready version and call-out anything less than that.  **List of changes promised by authors (quoted from their response)**  **Authors' response to reviewer Cgtb:**  1- "Unfortunately, implicit curricula cannot be directly studied within our framework, since we always assume the hardness information is completely disclosed. Considering a pseudo-labeling step before the actual learning stage would likely make the (already involved) computation unfeasible. Empirically this procedure has been investigated in [e.g. https://arxiv.org/abs/1812.05159, https://arxiv.org/abs/2012.03107] and, limiting our interest in the generalization aspect, this heuristic does not seem to induce a sizeable improvement. We will include this discussion in the revision." 2- "We mean an algorithm that explicitly depends on the curriculum, for instance by changing its objective function when example difficulty changes. That is, a curriculum aware algorithm would adapt the learning process in order to account for different levels of difficulties in the data. A simple way of implementing this is to modify the training loss, as proposed in this paper. Other approaches may involve adapting the optimization algorithm as proposed in https://proceedings.mlr.press/v139/ruiz-garcia21a.html, or possibly modifying the architecture https://www.sciencedirect.com/science/article/pii/0010027793900584. A key message emerging from our work is that standard algorithms do not dramatically benefit from curriculum, and we believe curriculum-aware algorithms may be the way forward. We will include a clear definition and discussion in the revision."  **Authors' response to reviewer S1ip:**  1- "The first point raised by the reviewer is the object of current investigations (see answer below). We will add more details on the CIFAR experiments in the revised version."  2- "We agree that this appears counter-intuitive. We also have hoped for greater intuition on this point, but we do not have a simple explanation for this phenomenon. This result is what appears from solving the equations and it was checked in the numerical simulations. A possible intuition could be that, in some settings, the large amount of noise contained in the hard data will always be too disruptive for effective learning. Thus, leaving the “clean” data for last could allow the model to better exploit the easy data. We will add this possibility to the revision. Even without a clear intuition, our contribution here is to show, in an identical setting and without finite size effects, that both anticurriculum and curriculum can indeed outperform the baseline."  **Authors' response to reviewer S1ip:**  1- "Overall, the analytical solution is between 2 and 6 orders of magnitude faster. We will add this approximate speed-up factor and discussion to the supplement (or if space allows, the revision)."  2- "The large input limit means that the input size N and the dataset size M go to infinity with finite ratio \alpha=M/N. This is an important point that must have gotten lost during the iterations, thank you for catching this. We will reintroduce it in the revised version."  3- "Figure 1 is based on notations and definitions introduced in section 3 and is not easily understood at the point it is presented. Reply: We will replace the image with explicit notation."  4- "Line 143: "starting from a large initialisation", I assume it means the random initialization scale of the student or teacher network. It is not entirely clear. Please include more details about this initialization. Does it use a normal random distribution? Reply: Yes, we use a normal distribution with a fixed variance. When we refer to large/small initialization we mean large/small initial variance. We will clarify this in the revised version."  5- "Figure 1: "The curriculum boundary lies at α = 1/2". What is the curriculum boundary? It is never defined. The abstract talks about "curriculum boundary consolidation", but it is not further elaborated. Reply: We mean the switching point between the two levels of difficulty. We will clarify this."  6- "Half-way through the paper section numbers stop being used. What I assume are sections 5 and 6 do not have numbers. And I am not sure if the subsections of Section 4 belong there. Reply: We will add 5 and 6 to the last two sections."  7- "Figure 3c "Accuracy hard samples." is not an accurate title. It is accuracy of all samples, easy and hard. It's just that anti-curriculum performs best, which is influenced by hard samples. Reply: We will replace it with just accuracy."  8- "The figure captions are inconsistent in the uses of (a), (b), (left, center, right), and (top, bottom). For example Figure 3 says (a), (b) and then "the right panel". Reply: We will use only the a,b,c notation."  9- "As stated in the paper, the answer does depend on the setup and in principle we would expect different behaviours for non-convex models. In particular, the presence of different basins of attraction towards different minima would suggest that initializing close to a good one would produce a performance improvement. However, note that empirical results in the ML field are not showing clear signals in this direction. A possible explanation is that relying on memory effects in the learning dynamics would require one to hit a sweet spot in the learning rate value and in the number of training epochs, and this seems hard to be achieved consistently. For this reason, we speculate that explicitly enforcing this memory by altering the loss with the curriculum information could be useful even in these settings. We will further emphasize that this statement about memorylessness applies only to our setting."  10- "We thank the reviewer for pointing out several elements that were missing in this version: we will make sure to include them in the revised version of this paper."
1	The paper received two borderline accept recommendations and one accept recommendation from three reviewers with low confidence and a reject recommendation from an expert reviewer.   Although all reviewers found that the paper addresses an important and challenging problem of semantically constraining adversarial attacks as opposed to constraining them artificially by an artificial norm ball. However, during the discussion phase it has been pointed out that there were some important weaknesses indicating that the paper may need one more evaluation round.  The meta reviewer recommends rejection based on the following observations.   In terms of evaluation, while it is understandable the authors were unable to compare to Gowal et al. due to the lack of publicly available implementation, showing Song et al.'s adversarials hurt performance and and are farther than the image manifold has been found puzzling, as this was done by Song et al. only to keep human prediction the same while changing model prediction. Furthermore, the paper did not contain a user study similar to Song et al. for a fair comparison Finally, the discussion revealed that the comparison to "norm-bounded adversarial inputs" may not have clarified whether this experiment faithfully demonstrates an advantage for the contribution as the norm could be contained to a point where accuracy is not reduced, and the discussion on the certified defense being "broken" was inconclusive.
0	This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committe members, who provided a binding decision. The decision is "Significant concerns (Do not publish)". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee's decision .  The technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:  This paper studies the problem of evaluating optimiser's performance, which is important to show whether real progress in research has been made. It proposes several evaluation protocols, and used Hyperband (Li et al. 2017) to automate the tuning of each optimiser in the bench-marking study. Evaluations have been conducted on a wide range of deep learning tasks, and the paper reaches to a conclusion that none of the recently proposed optimisers in evaluation can uniformly out-perform Adam in all the tasks in consideration.  Reviewers agreed that the evaluations are extensive, however there are some shared concerns among reviewers. The paper argues that manual hyper-parameter tuning by humans is the right behavior to target for, which is the motivation to use Hyperband as an automating tool, and there is a human study to demonstrate that Hyperband tuning resembles human tuning behaviour. Some reviewers questioned about this desiderata choice that favours human tuning behaviour, also concerns on how the human study is conducted (and to what extend the human study itself is reflective enough for the human tuning behaviour in general).  Personally I welcome any empirical study that aims at understanding the real progress of a research topic, and I agree it is important to make rigorous automation tools in order to enable such a large scale study. Therefore, while the presented results are extensive, I would encourage the authors to incorporate the feedback from the reviewers to better examine their assumptions.
1	Three reviewers provided positive reviews which were further strengthened post discussion. They agreed that that the motivation was strong, the model was novel and the paper was well written. They appreciated the ablations provided by the authors and found the results compelling. The main concern by the reviewers was a missing experiment which was provided by the authors in their rebuttal, and was appreciated by multiple reviewers. In summary, the reviewers are unanimous in their support of this paper. I agree with their reviews and I recommend acceptance.
1	This paper shows how "road rules" (e.g., implicit designation of fast lanes on a highway) naturally emerge in a multi-agent MDP. The paper shows that interesting traffic rules do emerge, and it presents a detailed analysis of the factors that lead to this emergence. The paper is complemented by documented source code, with the aim to encourage the community to further work on the topic.  The reviewers agreed that this is original work, and appreciated its simplicity. Two concerns that were recurrently voiced were that 1) there is no algorithmic innovation and 2) there is no comparison to baseline models, or more generally a better placement in the context of existing literature.  The authors provided a detailed and, to my eyes, convincing response. With respect to the two concerns above, I would go as far as saying that 1) (no algorithmic innovation) is a feature, not a bug. The paper is interesting exactly because it studies emergent phenomena after framing multi-agent driving as a standard RL problem. Concerning 2) (lack of baselines), it seems to me somewhat besides the point: The paper is not claiming state of the art on some benchmark for a new algorithm, but studying how certain implicit rules emerge in a given setup. In this sense, as the authors point out, rather than looking at alternative baselines, it is informative to look at which aspects of the setup contribute to rule emergence, which is what the paper does.  Although I realize that in proposing this I am going beyond the reviewers' ratings, I found this to be an original and exciting paper, that I would strongly like to see accepted at the conference.
1	This work for the first time proposes to study the task of Open Set 3D Learning for 3D point cloud data. The authors have conducted extensive experiments under different settings with varied category semantic shifts and provided comprehensive experiments benchmarking the popular methods from 2D open set learning which leads to some important conclusions about the transferability of the 2D methods to the 3D settings. The contribution of the work is clear and novel. The AC believes the paper provides important findings for the community to be aware of.  During the rebuttal, reviewers raised up concerns regarding the lack of real-to-real setting and the missing of some evaluations/metrics. The authors have added the requested materials in the revised paper, which addressed most of the raised issues. While there are some minor questions asked by the reviewers, the authors have carefully addressed them and the AC does not think they are major issues preventing me from accepting this paper. The final scores are 3 accepts and 1 reject, and two reviewers confirmed their final decisions after rebuttal. The reviewer who gave the reject review didn't come back for responding to the authors' rebuttal and the AC think the authors have addressed his/her questions well.  Therefore, the AC is very confident in recommending an acceptance of this work to the track. But, please carefully revise the final paper for the camera-ready submission, based on the reviewers' suggestions. Congratulations!
0	This paper proposes to address the problem of domain adaption using Knothe-Rosenblatt transport withe the method denoted as KRDA . The main idea is to perform density estimation of the different distributions with mixture of Gaussians and then estimate a  an explicit mapping between the distribution using  Knothe-Rosenblatt. Experiments show that the proposed method works well on toy and real life datasets.   The paper had low score during the reviews (3,3,3,3). While the reviewers appreciated the idea, they felt that the originality of the method is not well justified compared to a number of existing UDA approaches using OT. Also the reviewers noted several important references missing and that should also be compared during the numerical experiments. A discussion about the limits of the method in high dimension would also be very interesting.  The authors did not provide a reply to the reviewers' comments so their opinion stayed t same during the discussion. The paper is then rejected and the AC strongly suggests that the authors take into account the numerous comments from the reviewers before re-submitting ton a new venue.
1	I agree that it is an interesting idea and shows promise. However, given the current exposition and investigation done in the paper about the approach, I feel that a 'weak accept' is the right decision for this manuscript. I hope this doesn't deter the authors from working on this in the future, and I hope to see a more polished version of this manuscript out soon :)   Thanks!
1	This paper presents a language-guided auxiliary reward mechanism based on generating Q&A pairs based on agent trajectories and rewarding the agent for producing trajectories that yield correct answers from an answering model. The reviewers broadly found the paper compelling and convincing, and thus I am happy to follow their general consensus in recommending acceptance.
1	This paper proposes a method to dynamically group agents with similar representations and assign subtasks to each group so that they can effectively share parameters among agents within the same group while specializing across groups. The results on StarCraft Micromanagement benchmark and Google Research Football domain show that the proposed method outperforms relevant baselines including QMIX, ROMA, and RODE.  The reviewers found that the idea is interesting and technically sound, and the paper is very well-written. Although there were several concerns about the lack of baselines (CDC) and the lack of challenging benchmarks, the authors addressed most of them during the rebuttal period by updating the results with additional baselines, an additional benchmark (Google Research Football), and additional ablation studies. As a result, all of the reviewers agreed that the result is significant enough to be presented at NeurIPS. Thus, I recommend accepting this paper.
1	This paper has divergent views in the sense two reviewers have given positive assessments (6 and 7) while the other reviewer has given a negative assessment (score of 3). This paper also had very 'heavy' discussions between the reviewer with negative opinion and the authors.  First of all I would like to thank the reviewer involved in patiently discussing with the authors dedicating valuable personal time.  Let me start with the aspects all reviewers *more or less agree* on :  a) The main technical piece is an efficient algorithm and provable guarantees for identifying definite non-descendants and definite descendants from an MPDAG - maximum partially directed acyclic graph - the equivalence class of Causal DAGs one obtains after incorporating any arbitrary side information. Previous such results were known for CPDAGs and they don't carry over to MPDAGs. Therefore it is a non trivial result (specifically Lemma 4.4 ). So all reviewers agree that finding definite non descendants in Equivalence classes that also include side information is a very solid contribution.    b) The aspect in which reviewers had divergent opinion is this:  the paper's claim to be able to train counterfactual fair classifiers leveraging the result from [Kusner et. al 2017] that any function of non-descendants is counterfactually fair.  One of the reviewer's strong contention is that in most fairness datasets, most variables that are highly predictive of outcomes will also be downstream of sensitive attributes like race etc.. and therefore relying only on non-descendants is not exactly a realistic application. Authors cited their empirical structure learning results that show very few descendants and comments from Kusner et. al 2017 paper to bolster their case. Reviewer responded by citing alternate statements from the same paper etc..   *My opinion* is that in a specific context when fairness with respect to a specific sensitive attribute is desired, there are also often other features that has no causal relationship with the sensitive attribute but has a *correlation* (Examples include age and race, race and gender etc.. ).  To cite a recent reference please see Example 15 in https://arxiv.org/pdf/2207.11385.pdf (this reference is recent and I am *not* expecting authors or anyone else to have known this - it is just to demonstrate the point). The example shows *testable* correlations between sensitive attributes and non-descendants in COMPAS and Adult datasets.   This shows that a) neither causal sufficiency  nor b) the non-existence of non-descendants are realistic . In fact, spuriously related non-descendants give rise to spurious bias which may not be an object of correction for fairness (broadly speaking). This shows that causal sufficiency is a strong assumption (as authors have assumed) and also non-descendants do exist.  c) Another point to be noted is that Kusner et. al. 2017 do consider confounded models unlike the authors. Once you view exogenous, endogenous (observed) variables and sensitive attribute as one full deterministic system, their point is ALL exogenous + non descendant endogenous variables are "non-descendants" topologically and therefore could be used. They did not imply non-descendants endogenous 'only' as the authors contend in their discussions.  In fact, the algorithm section in Kusner et. al. 2017 - advocates for sampling Exogenous from some side information (level 2 and 3 information) and forming a predictor as a function of exogenous *and* non descendant endogenous variables.  Therefore, reviewer has a valid point on the discussed aspect as well. Authors may want to pay attention to this.   *In summary*: Authors' contention that Kusner et al 2017 paper advocates for non-descendant endogenous as their main sufficient criterion appears to be not exactly correct. However, non-descendants and their confounding with sensitive attribute is a more realistic model. However, authors core technical structure learning contribution is also noteworthy.   If this line of work is to be pursued where one could find non descendants even under limited confounding (between sensitive attributes and non- descendants - a mild violation of causal sufficiency) - it would be a step towards obtaining counterfactually fair classifiers (although even such a classifier would have to sacrifice a lot on accuracy depending on how many descendants one observes).  However, even positive reviewers have opined that main strength of the paper is a solid structure learning result that identifies non-descendants in a fully observational setting.  *Recommendation*: In the spirit of not blocking valid ideas that are fundamental and also the fact that one cannot always make the weakest set of assumptions to make progress, I tend to favor acceptance. A *very strong* suggestion to authors - I would place structure learning as the centerpiece and motivate it by a need to learn non-descendants (in the general sense) motivated by Kusner et al 2017. Authors also need to highlight Fair relax - a relaxation that they have proposed that uses possible descendants and definite non descendants to predict - it seems to be closer than other approaches to counterfactually fair one and therefore removing the singular focus on (as the discussions would have one believe) only observed definite non-descendants.
1	The paper studies the number of linear regions cut out by a randomly initialized deep network, for data with low-dimensional structure (manifold structured data). The main results pertain to the density of linear regions and the average distance to the boundary of a linear region: these results take the same form as in the Euclidean case (with distance inversely proportional to the number of neurons), but depend on geometric properties of the data manifold — in particular, its dimension and curvature. Reviewers generally appreciated the relevance of the paper’s setting: data arising in applications often have low-dimensional structure, and understanding how deep networks interact with the structure of data is an important research direction. At a technical level, the paper builds on techniques of [Hanin and Ronik 2019], but extends these results to manifold structured data. Questions raised by the reviewers include the role of curvature and input dimension in the results and the interpretation of real data experiments. After interacting with the authors, the reviewers considered their main concerns about the paper to be well-addressed. The AC concurs, and recommends acceptance.
1	This paper presents a method, called Zest, to measure the similarity between two supervised machine learning models based on their model explanations computed by the LIME feature attribution method.  The technical novelty and significant are high, and results are strong.  Reviewers had clarifying questions regarding experiments and suggestions to add experiments, which involve additional domains (text and audio) and different families of classifiers, and more contexts based on prior literatures. These were adequately addressed by the authors. Overall, this paper deserves borderline acceptance.
1	Meta Review: This is an interesting and well-written paper. Perhaps it it less obvious however, how it contributes to the existing literature. It clearly builds heavily on earlier work and only the rebuttal made it a bit clear (at least to me) what is innovative about the proposed approach. There was a consensus among the reviewers that the paper could be accepted if there is space. I ask the authors to incorporate the clarifying comments they sent to the referees - I think they are really crucial to improve the paper. I also realized that none of the referees pointed out that the "exponential family embeddings" are really just special generalized linear models. I think it is important to keep track of the original ideas and it would be good if the authors should mention generalized linear models explicitly (as the paper proposing "exponential family embeddings" does).
1	This work studies minimax optimization for convex-concave objective. It studies the population loss version of this question and shows linear time differentially private algorithms for this problem that achieve the optimal privacy utility trade-off. The algorithm is based on the phased ERM approach. The reviewers were in agreement that this problem is of interest and the paper makes a significant improvement on previous work to be interesting.  I would recommend acceptance.
1	A recent line of work on the role of stochasticity in ML suggests that variants of GD which use non-traditional step size schedules (a) may perform relatively well in certain settings (b) due to an implicit chaotic behavior. The present paper studies a variant of GD (MPGD) augmented with an explicit chaotic component, implemented by means of an external deterministic dynamical system, as a theoretical model for investigating these hypotheses. Recent results are shown to imply generalization bounds for the limiting stochastic process. Numerical results are provided for comparing the performance of MPGD to existing methods.  The reviewers have generally found the use of a GD variant with an explicit chaotic term, as well as the proposed analytic framework, interesting and appreciated the clarity and rigor of the results given in the paper. In later discussions, concerns regarding the relevance of the theoretical model to (a) and (b) above were raised by the reviewers, questioning more broadly the significance of MPGD and the respective limiting SDE to the general understanding of SGD/GD. All in all, I think this is a reasonable paper to accept if there is room. The authors are encouraged to revise the paper according to the important feedback given by the reviewers.
1	This paper proposed a simple framework by combining CNN and Transformer by cross teaching for semi-supervised medical image segmentation. Experiments on cardiac application demonstrate better performance over other methods. Overall, this paper is well organized and the results are convincing. The majority of reviewers (3/4) has recommended weak accept. Based on own reading, I also recommend accept, although comparison results with other SOTA methods such as 3D models can be added in the final version.
0	The paper proposes a method to perform self-supervised model ensembling by learning representations directly through gradient descent at inference. The effectiveness is evaluated by k-nearest neighbors accuracy.  The reviewers agreed that the paper studies an important and interesting problem of leveraging model ensembling for self-supervised learning, which could improve both the performance and robustness of the learned representations. However, the reviewers also agreed that there were issues with the soundness of the empirical evaluation, which was a key reason for rejection.
1	This paper is concerned with the ongoing research program of mapping the approximation power of different GNN architectures. It provides significant advances in the study of equivariant GNNs and nice extensions in the invariant case by closing existing gaps between distinct GNN families.  All reviewers agreed that this is a strong submission with substantial new theoretical results. The AC recommends a strong acceptance.
1	This is a very high-quality dataset, which is especially noteworthy since legal NLP benchmarks are very difficult to build. The authors approached every aspect of the process with extreme care. It will likely have an impact on law practice and start interesting discussions about the use of ML in these settings. The paper is also very well written and enjoyable to read.  Most reviewers are heavily in favor of acceptance. Reviewer Aeqk brought up some weaknesses, but at least from the AC's perspective, these seem to be answered well by the authors.
1	This work presents dreamgrader that aims to provide feedback to student-authored interactive programs. Reviewers all agreed that this paper presents a novel and original idea, solid experiments on real-world programs, as well as potential impact on MOOCs. There were some minor concerns and most got resolved during the discussion stage. Thus we recommend acceptance.
1	The paper receives overall positive reviews and rebuttal has resolved the reviewer's concerns. The paper proposes a new framework that directly takes raw event clouds as inputs for object tracking. Reviewers agree that this innovation is inspiring. AC agrees and recommends accepting the paper.
1	This manuscript enjoyed universal recommendation of acceptance from the reviewers after the initial review phase. The reviewers did note several minor issues in these initial reviews, many of which were resolved by insightful responses from the authors. I encourage the authors to edit the manuscript to reflect the insights gained from this interaction when preparing an updated version.
1	The paper offers novel insights about memorization, the process by which deep neural networks are able to learn examples with incorrect labels. The core insight is that late layers are responsible for memorization. The paper presents a thorough examination of this claim from different angles. The experiments involving rewinding late layers are especially innovative.  The reviewers found the insights valuable and voted unanimously for accepting the paper. The sentiment is well summarized by R2: "The findings of the paper are interesting. It shows the heterogeneity in layers and training stage of the neural net".  I would like to bring to your attention the Coherent Gradients paper (see also R1 comment). This and other related papers already discusses the effect of label permutation on the gradient norm. Please make sure you discuss this related work. As a minor comment, please improve the resolution of all figures in the paper.   In summary, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera-ready version.
1	Reviewers appreciated the novelty of the proposed activation function, the theoretical motivation and its connection to the SwisH activation. In terms of presentation and soundness of the results, Reviewers pointed out some weaknesses in the initial reviews for this paper. In particular, the reviews voiced some concerns with the clarity and formatting of some figures, the lack of clarity of a mathematical derivation, and most of all, issues in the presentation of the empirical results that didn't report confidence intervals allowing for an assessment of the statistical significance of accuracy differences. These weaknesses were however addressed in ways that satisfied the Reviewers in the rebuttals and subsequent versions of the paper. Thanks to these welcome changes the paper has now garnered unanimous consensus among Reviewers that it should be accepted.
1	This paper proposed a fully decentralized algorithm for bilevel optimization. Although the techniques are a combination of existing ones from bilevel literature and the decentralized optimization literature, but the setting considered is considerably sophisticated (i.e., both levels are distributed). The algorithm is a single-timescale, and the rates are good for both nonconvex and convex settings. The reviewers all appreciate the contribution of this work. Therefore I recommend acceptance of the paper.
1	The submission proposes a method to make a pre-existing model equivariant to desired symmetries: frame averaging. The strategy relies on a significant reduction of the number of symmetries to average over (with respect to the Reynolds operator) and uniform subsampling. The paper also demonstrates the usefulness of this method theoretically (universal approximation result) and practically (competitive performance). The contributions are clear and the core idea is simple. I recommend this paper for acceptance with spotlight.
1	This paper aims to augment efficient CNNs with self-attention. However, since the naive approach to self-attention is computationally expensive and would contradict the point of efficient CNNs, the authors introduce a new attention mechanism which captures long-range information without substantially added computation cost. The paper demonstrates that GhostNetV2 exhibits markedly better performance at various compute limits as compared to previously proposed efficient networks. Three of the reviewers were quite positive on this paper, noting the novelty of the approach and the strength of the empirical results. One reviewer had several concerns, primarily regarding comparison to NAS based approaches and the novelty of the approach. I agree with the other reviewers that it is not reasonable to compare NAS approaches to non-NAS approaches, and agree that there are marked differences between this work and the previous work cited. I therefore recommend acceptance. I think this will be a valuable contribution to the efficient network community.
1	The paper presents a dataset of 1M videos frames with tracked animal poses and behavior annotations. All reviewers agreed that the paper is well-written and that the dataset is useful -- as evidenced by the fact that it has already been used to organize a challenge. Reviewers raised some minor concerns including more details about annotators and the relationship between the proposed benchmark and prior work on human action recognition. The author response was satisfactory in addressing these concerns, and in the end all reviewers voted to accept the paper. Congratulations on having your paper accepted to the NeurIPS 2021 Track on Datasets and Benchmarks! The authors are encouraged to take the feedback from reviewers into account when preparing the camera-ready version of the paper.
1	Unanimous accept from 3 reviewers.  I'm uncertain about "accept" given reviewer's XJeV and  Jba5 reviews are on the short and vague side. Reviewer vqqM never responded, even though they would have been a great reviewer for this work (reminded them once and they confirmed, but forgot to follow up again). Reviewer TKTA's review was the most useful, a borderline accept. There is reviewer consensus on novelty, in addition to being well written, with convincing results on MuJoCo and a highway environment.  I myself am unfamiliar with Riemannian metrics/manifolds, however after reading up on the subject, I worried this paper might have been too close to "Latent Space Oddity: on the Curvature of Deep Generative Models" to learn (or compute) latent space metrics, however this works differs by (1) using a variational *forwards* model to consider dynamics, (2) using an ensemble of model to consider epistemic uncertainty, and (3) tying both these aleatoric and epistemic forms of uncertainty into an offline-RL setting, where rewards are pessimistically estimated under uncertainty. While it seems a little ad hoc to suggest this particular method _for_ a particular application (offline RL), which confuses the narrative and motivation a bit, it does seem to give better RL performance in these settings than L2 and ensembling/bootstrapping in Figure 4. This is the most borderline paper I've seen as AC this NeurIPS, but if forced to make a decision, I lean accept.
1	This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis.  3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the main contribution interesting and significant enough for acceptance. Although I think this is a reasonable objection, it is not shared by the other 3 reviewers. Since the negative reviewer does not point out any critical flaws in the paper, I think the positive opinions should outweight the negative one in this case. I therefore recommend accepting the paper.
1	This paper proposes new benchmarks for probing video-text models' robustness, that include a multitude of visual/textual perturbations. All of the reviewers have acknowledged the usefulness and effort put into the benchmark construction and presented analysis. Some of their concerns centered around: lacking metric definitions, limited scope (video-text), detachment from real-world scenarios, issues with figure/table presentation, etc. After rather extensive discussions, many of the concerns have been resolved. Presently, 4 out of 6 reviewers argue for acceptance (3 of them strongly). The remaining 2 reviewers maintain their opinion on the limitations of the presented work, most importantly somewhat inconclusive takeaways, limited number of models compared (initially 5, the authors have added one more), mostly one task (video-text retrieval). I believe having 6 models is acceptable for the proposed study. Upon carefully examining the claims/arguments, I encourage the authors to "scale down" their claims/narrative (and perhaps even rename the paper) to **more explicitly acknowledge the emphasis on video-text retrieval**. But I still think the analysis as such is valuable and thus recommend acceptance.
1	This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows. The paper provides a hardness result for arbitrary conditional queries. Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the *conditioning* is relaxed.  There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced. After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done. Novel computational complexity results *as such* are not really in the scope of ICLR. There's nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed.  Like R4, I do not follow how this hardness result is meant to motivate the smoothing that's applied. The paper is unambiguous that the goal is to do conditional inference. A hardness result is presented for conditional inference, and so a relaxed surrogate is presented. This has a minor problem that it's not clear the relaxed problem avoids the complexity boundary of the original one. There's a larger problem, though. The hardness result has not been sidestepped! The goal is still to solve conditional inference. The algorithm that's presented is still an approximate algorithm for conditional inference. R4 suggests that other approximation algorithms should be compared to. The authors responded to this point, but I am not able to understand the response. For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (e.g. without smoothing)  None of the above is to say that the smoothing approach is bad. It may very well be. However, I think that either the existing argument should be clarified or a different argument should be given.  Finally here are two minor points (These weren't raised by reviewers and aren't significant for acceptance of the paper. I'm just bringing them up in case they are useful.)  Is Eq. 3 (proof in Appendix B.1) not just an example of the invariance of the KL-divergence under diffeomorphisms?  Proof in appendix B.2 appears to just a special case of the standard chain rule of KL-divergence (e.g. as covered in Cover and Thomas)
1	The reviewers are unanimous in their strong positive opinion on this paper.  The authors have given the first efficient algorithms for learning noisy linear sorting functions with theoretical guarantees a relevant and useful problems setup for the NeuRIPS community.  The reviewers consider the paper clear and well-presented and thus this is a natural accept.
1	This is a nice application-motivated paper that introduces and tests a novel stochastic variant of a transformer architecture.  All three reviewers recommend acceptance (albeit one is borderline). The borderline review focuses on the closeness of this to existing work, and the relatively incremental nature of the contribution. While I do see the potential concern, I think all in all the consensus is clearly to accept.  Interaction between reviewers and authors led to a number of beneficial changes to the paper during the review process.
1	This paper proposed a flow-based approach FCause to Bayesian causal discovery that is scalable, flexible, and adaptive to missing data. Reviewers were split on this paper and could not reach a consensus during the discussion, and no reviewer pushed for acceptance. After taking a closer look myself, I agree with several of the reviewers that while the core ideas here are interesting and novel, there remain too many unresolved issues that require another round of revision.  I encourage the authors to carefully take in account the reviewers' comments and re-submit this promising work to another ML venue.
1	This submission receives reviews from 6 different reviews. Most reviewers (5/6) appreciate the contribution of the new dataset. They acknowledge that the problem setup is interesting and the dataset may be useful for different research communities: computer vision, time series analysis, medical. On the other hand, reviewer 38gN concerns about the novelty of the proposed dataset. AC reads all reviews and comments, and discussions, and is convinced that the proposed dataset will provide a useful benchmark for research, thus AC recommends to accept this submission as a poster. AC recommends the authors to incorporate all suggestions from reviewers for the final camera version.
1	The paper proposes a self-supervised deep-learning framework for image-to-image translation tasks, such as segmentation, that accommodates and fully exploits longitudinal data. Specifically the method provides a mechanism to impose consistency in output across multiple points from the same individual and simple regularisation terms to avoid some problems common with other methods, such as mode collapse. The authors compare the method against baselines in two distinct neuroimaging segmentation tasks, which nicely demonstrate the additional power afforded by imposing longitudinal consistency.  The reviews overall reported that the submission tackles an important problem, presents well formulated experiments, and shows a significant advance over the SOTA.   The reviews raised concerns raised included non-specialist accessibility, adding more related work, and questions w.r.t. the claims, presentation, and relevance of the results, and particularly about the mode collapse problem. The authors have addressed these concerns, in particular by adding a new section (Section E/Need for regularization) to the Supplementary Material which contains several new visualizations and quantifications of the mode collapse problem (from ablation experiments). The paper has been updated to reflect some  clarifications required by reviewer de4n. Some citations suggested by Reviewer r9Zh are now discussed in the submission.  As it is, the paper meets all conditions for acceptance at NeurIPS 2022.
1	The paper received mixed scores from the reviewers (7, 7, 3). I believe that a survey paper is a valid contribution type for GI's HCI track, as it helps further our knowledge about designs that have been explored and highlights what can be done in the future. While the paper has shortcomings (addressed in the paper) and offers limited results (as acknowledged by all reviewers), the results are timely (R3) and interesting (R1, R3). Based on this, I recommend that the paper be accepted.     Below I summarize the key issues identified by the reviewers and encourage the authors to read through the individual reviews carefully to address other concerns.   - Address the methodological limitations early in the paper (R2, R3)  - More directly define the scope of this work (e.g., what does CV-systems encapsulate (R1), acknowledge that risks not identified in the analysis may still exist (R3))  - Provide more details about background literature (R1, R3)  - Clearly highlight the surprising and new results (R1)    Recommendation: Accept
1	The paper studies PAC reinforcement learning in tabular episodic MDPs with deterministic transitions and provides upper and lower bounds on the sample-complexity that match up to horizon and log-factors. Overall, all reviewers rate this paper positively (after the authors' responses and discussion). They view the contribution of a fine-grained instance-dependent guarantees in this setting as significant and particularly appreciated the novel insights, e.g., relating the MaxCoverage function in Algorithm 1, to the StaticMaxCoverage in Algorithm 3 or the inclusion of graph-theoretical concepts in the lower bound analysis. There were also several limitations raised, in particular the deterministic transition assumption and the reward range assumption used in the lower bound. However, some of these can be addressed by clarification and more detailed discussion in the camera ready. All in all, this is a solid paper and is recommended to be accepted.
1	This paper analyzes local SGD under the random reshuffling data selection setting. As is the case for standard random reshuffling, better rates are shown for local SGD when random reshuffling is used. This would already be a nice contribution to a line of work on random shuffling methods—but the paper goes beyond that by showing a matching lower bound and designing a (theoretically) better variant algorithm. The reviewers were all in agreement that this paper should be accepted (as a result not much further discussion happened after the original reviews), and I agree with this consensus. The modification seems to improve the paper, although I did not look through it in detail.
1	The authors propose a new method for defending against backdoor attacks which is based on the observation that poisoned samples are more sensitive to transformations than clean samples. They design a metric called \textit{feature consistency towards transformations (FCT)} to distinguish poisoned samples from clean samples in the untrustworthy training set.  The paper received favorable reviews and has made substantial updates during the rebuttal phase to the general satisfaction of the reviewers. I thus recommend accept.
1	This paper proposes a new approach to learning deep generative models with induced structure in the latent representation. All four reviewers gave the same score of 6 to this paper, showing a consensus that the paper is above the bar for acceptance. The authors did a commendable job of detailed replies to reviewer comments, which as R1, R3, and R4 all note has improved the clarity and quality of the paper, addressing their concerns.
0	The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold. The paper proposes a method using a gradient that is unbiased and consistent.  Pros: - Problem setting is new and this paper is one of the first works exploring it. - The procedure comes with some unbiasedness and consistency guarantees.  - Experimental results on a wide variety of datasets and domains.  Cons: - Novelty and technical contribution is limited. - Motivation of the problem setting was found to be unclear. - Some gaps in the experimental section (i.e. needing the use of synthetic data or synthetic modifications of the real data).  Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one-sided regression problem as important or relevant in practice, which was a key reason for rejection. The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation.
1	This paper studies application of masked autoencoders to video data. It is a very empirical paper with lots of ablations and experiments. All three reviewers lean toward the acceptance of the paper. Reviewer C915 has a slight concern regarding the novelty of the paper over concurrent works including [50,53]. The reviewers believe that the ablation study is exhaustive and the paper has a good reproducibility. The authors are encouraged to add new experiments with kinetics pretraining in the final version.
1	This paper proposes a new action selection approach for risk-averse distributional reinforcement learning optimizing CVaR. It first shows that the action selection schemes used in existing approaches do not converge to the desired policies and subsequently shows that the fixed-point of the Bellman operator with the new action selection scheme is the desired optimal CVaR policy as long as it is stationary. It finally provides empirical results showcasing the benefits of the proposed approach.  The reviewers had mixed initial views on this paper. On the positive side, they found the paper to be well written and appreciated the new insights into the convergence of the existing action selection scheme as well as the more principled proposed scheme. On the negative side, there were concerns that (1) the paper does not actually convergence of the algorithm, only a fixed point, (2) that the paper does not provide sufficient discussion of the implications of the presented results, e.g. in the form of a conclusion and (3) that a comparison to CVaR optimization approaches that are not based on distributional RL is missing.  The authors' response addressed serval of these concerns so that all reviewers view this paper positively. Although, this paper still remains borderline and some concerns remain, the AC concurs with the reviewers that this paper has sufficient merits to be accepted, hence a recommendation for acceptance.
1	The paper addresses an issue of existing self-attention module that is mainly designed for data on Euclidean domain; for those on non-Euclidean domains, e.g., those on Riemannian manifold, the paper proposes a Geodesic self-attention counterpart. Experiments on tasks of 3D classification and segmentation show the efficacy. All reviewers acknowledge the problem importance and contributions made in the paper, although a few concerns are raised, including additional ablation studies and comparisons with other methods using geodesic metrics.   In the rebuttal, the authors clearly respond and address the reviewers’ concerns. Acceptance is recommended. Congratulations!
1	The paper proposes a E(n)-equivariant neural PDE solvers that can satisfy boundary conditions provably. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. The paper is nicely written and provides both strong experimental results and theory. Indeed, a range of interesting experiments demonstrate the effectiveness of the proposed method. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions. (The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.) In summary, this paper is a clear accept. Well done!
1	All reviewers recommend acceptance. The authors have addressed several of the reviewers' concerns in their comments, conducted additional experiments, and updated the manuscript accordingly.  A concern was raised regarding the size of the dataset introduced and used by the authors for this work. However, I agree with the authors that it doesn't necessarily make sense to compare this to datasets designed for training video classification and/or generation models; In the compression setting, the quality of individual data points matters much more than their quantity, as the authors argue.  Reviewer 2 was curious about the potential of a pre-trained optical flow module. I believe the authors have convincingly argued that end-to-end learning is likely to be more effective and practical (and indeed, there is plenty of evidence for this in other ML contexts where training data is not scarce). I agree that a direct comparison in the paper would have been interesting, but this would constitute a significant investment of time and effort on the authors' part (as they also point out, training such a module separately could actually be more difficult), and I think it would be unreasonable to make this a condition for acceptance.
1	The authors have addressed all the points raised by the Reviewers to a different extent, including more experiments and discussion points to support the described method.  AnonReviewer1 indicated that a more extensive comparison with other recent and maybe more competitive baseline segmentation methods should have been included. I do agree that it would have been interesting to do so. However, all Reviewers agree that the paper represents a contribution as an application paper and, considering the rebuttal effort, I recommend to accept this work for publication.
1	This paper examines the use of a random equiangular tight frame (ETF) as a replacement mechanism for the final classification layer in a deep neural network, and demonstrates experimental advantages in class-imbalanced training scenarios.  Reviewers gave drastically different assessments of this paper, with ratings ranging from reject to weak accept.  The authors provided extensive responses to all reviewers, and Reviewer amWi participated in an extended discussion with the authors.  Author responses directly addressing concerns raised by other reviewers, such as pointing to ImageNet results in response to Reviewer Nj4c asking for such experiments, appear not to have received subsequent engagement from reviewers.  The Area Chair has taken an detailed look at the paper and the entirety of the discussion, and agrees with Reviewer amWi's assessment.  The work provides an interesting examination of ETF as a novel mechanism to address class imbalanced training; the contributions meet the bar for acceptance to NeurIPS.  Reviewer amWi makes several suggestions regarding presentation of the main contributions as well as additional papers for citation and discussion, which the authors may want to take into consideration when preparing the final version of the paper.
1	This work looks at adapting ViT-like models for unsupervised domain adaptation, by cleverly finding pseudo-labels with 'attention-guided masking'. There's a weak consensus among the reviewers that this work has good empirical results, but somewhat limited novelty. I think the rebuttal discussion has helped improve this work quite a bit, and given the good results, ablations, and the importance of unsupervised domain adaptation, I am recommending acceptance.
1	The paper shows that standard transformers can be trained to generate satisfying traces for Linear Temporal Logic (LTL) formulas. To establish this, the authors train a transformer on a set of formulas, each paired with a single satisfying trace generated using a classical automata-theoretic solver. It is shown that the resulting model can generate satisfying traces on held-out formulas and, in some cases, scale to formulas on which the classical solver fails.  The reviewers generally liked the paper. While the transformer model is standard, the use of deep learning to solve LTL satisfiability is novel. Given the centrality of LTL in Formal Methods, the paper is likely to inspire many follow-up efforts. There were a few concerns about the evaluation; however, I believe that the authors' comments address the most important of them. Given this, I am recommending acceptance. Please add the new experimental results (about out-of-distribution generalization) to the final version of the paper.
1	The paper tackles the problem of causal discovery from mixed data, i.e. when continuous and discrete variables may be present, for both bivariate as well as multivariate settings. It introduces the so-called Linear Mixed (LiM) model and proves full identifiability under certain assumptions, including causal sufficiency, linear continuous functions with additive non-Gaussian noise, and a linear logistic model for the discrete variables. It also describes the associated LiM algorithm that can find this model, consisting of a global search optimisation phase (based on the so-called quadratic penalty method), followed by a local likelihood optimisation phase. The experimental evaluation shows that, when the assumptions hold, the method compares favourably to other approaches that are not designed to handle this setting.  Reviewers were initially fairly critical of the paper, in particular on clarity of the presentation and several errors / omissions in the text. However the authors made an excellent attempt at answering the points raised, clarifying some and promising to address / resolve others in the final version, so that ultimately all reviewers agreed on recommending acceptance, although insisting presentation could/should still be imporved.  I have to admit on reading the paper I am actually slightly more critical than the reviewers. I fully agree presentation should be improved, but I also notice a tendency to emphasise weaknesses of other methods while focussing on strong points of the proposed method, rather than trying to give a fair and balanced assessment of pros and cons. It is technically ok, but rather niche, relying on strong assumptions where the only ‘real-world’ application (Boston) leads to a questionable output model. Essentially the algorithm is likely to obtain the touted ‘full identifiability’ by overfitting on the data, even though for larger sample sizes the performance actually seems to decrease in some cases. That said, it does at least try to expand existing causal discovery methods to the important mixed data realm, and the proposed approach is new and sound in principle, so despite the caveat above I will follow the reviewers and recommend accept.
1	Reviews were split. Reviewers agreed on the importance of the domain but differed in how they valued the contribution over ProteomeTools, which the dataset builds upon. A primary concern was lack of baselines/benchmarking. The rebuttal argues that baselines would not add much value since strong methods have been published in prior work. However, my interpretation of the reviewers ask is not to invent new heuristic baselines but to evaluate the prior methods on the newly prepared dataset. The authors do add evaluation of one such model (Prosit) to the revision. Evaluating more models in the same way could strengthen the contribution, and could be part of the ongoing maintenance of the dataset / code.  Overall, however, the majority of reviewers felt the paper does provide significant value beyond ProteomeTools and other prior work, through the new annotations, tooling, and usage recommendations. The revision adds an appendix (G) to clarify these contributions, and it may be useful to move some of that text into the intro of the paper (as I think many readers will want to know what are the main contributions beyond the base dataset). I agree with the majority that there is significant value added and recommend acceptance.
0	This paper presents a novel perspective of prompt tuning for few-shot visual recognition: a dynamic matching algorithm between the prompt candidate and the visual features. Compared to the existing CoOp and CoCoOp algorithm, the proposed "Optimal Transportation" idea definitely sounds better and indeed achieves better performance. All the reviewers acknowledge the merits of the paper.   Though AC also acknowledges the merits of the paper, there are two unaddressed demerits:   1) As the proposed method is essentially an ensemble method, comparisons with prompt ensemble should be conducted. Unfortunately, the reported "G" in Table 2 and Line 254-265 is not a proper ensemble, because the "G" baseline may degenerate into many duplicate single CoOp models with different random seeds. AC conjectures that this is the reason why G's performance is only slightly different from CoOp. By "proper ensemble", the authors may want to try initializations by "this is a photo"+"a picture of" + "there is an xx of" etc, or, augmenting each image by random crops, each of which corresponds to a "G", and then ensemble.   2) This paper lacks an important "Base to New" setting as proposed by CoCoOp. As the proposed PLOT in this paper has significantly more tunable parameters, AC doubts that it may lead to the overfitting of the training classes but ruins (or forgets) other classes which were used to have good zero-shot performance without training.  Unfortunately, AC regrets to recommend reject and wishes the best of luck in re-submitting the paper to other venues.
0	This paper proposes a method for differentiable pruning that replaces the hard thresholding of standard pruning, with a soft version that permits taking the gradients of the pruning threshold. The proposed benefits are an accuracy that is better or competitive with alternative methods as well. Moreover, the paper suggests the technique to be efficient.  The pros of this paper are that it is working in an interesting setting of differentiable pruning, with the hope of -- in some sense -- simplifying the pruning process or at least unifying the process with standard training.  The technique is plausibly justified in its technical development. The paper also follows with a significant number of experiments.   The cons of this paper are that the conceptual framework -- beyond the initial idea -- is not fully clear. In particular, this paper does not elucidate a clear set of claims and hence, results in the difficulty on the Reviewers part in detangling the claims and identifying the appropriate comparisons.  For example, the paper doesn't take up a simple claim that it is state-of-the-art in accuracy vs parameter measures (and would seem not to given the results of Renda et al. (2020)).  It need not necessarily make this claim, but there are suggestions to such a claim early in the paper. If this is not an intended claim, then the paper can remove any suggestions to such (i.e., the claims around new SoTA for networks not evaluated in prior work).   The paper has a somewhat tentative claim that it is more efficient (in the total number of epochs of training) versus other techniques (Table 3).  However, the presented results are only at a single-point versus other methods.  Renda et al. (2020) directly consider accuracy versus retraining cost trade-offs. Appendix E of that paper provides one-shot pruning results for ResNet-50 showing accuracy on par with that presented here.  The number of retraining epochs is also similar to here. This paper, however, only compares against the most expensive iterative pruning data point in the other paper.  In sum, my recommendation is Reject. This is promising work that needs only (1) to include a few testable claims and (2) to re-organize the results (and perhaps run a limited set of new results) to thoroughly explore those claims. For example, if the most important claim is accuracy vs retraining cost, then it needs to show a more complete trade-off curve of the two results.  Of course, this, in principle, opens the door to comparisons to many other techniques in the literature.
1	This paper tackles an interesting but specialized problem in causal discovery for multiple (or multivariate) time series.  The _exact_ nature of the problem set-up is not at all clear from the paper unless it is read _very_ carefully, and understandably confused at least one reviewer, so if this is accepted the final version should have a significantly revised abstract and introduction.  The setting, as I understand it, is as follows.  We have multiple data sets, each of which consists of $N$ univariate time series, all of length $T$, and assumed to be regularly, and simultaneously, observed.  Each data set is coming from an independent system.  (These systems or data sets are what the MS. confusingly calls "samples".)  Not only are they statistically independent, but the graph of effective connectivity among the $N$ univariate processes is different from one system to another.  However, there is a common functional form, shared across all the systems, for how the future of each of the $N$ nodes is generated from its own past and the past of its neighbors.  This differs from system to system only up to a finite-dimensional set of parameters, which represents graph structure as well as, e.g., connection strength.  Thus for instance we might be looking at neuronal firing rates, and saying that $x_i^t = \mathrm{logit}^{-1}\left(\alpha + \sum_{j \neq i}{w_{ji} x_j^{t-1}}\right)$, where the assumption of a common functional form is showing up in the additivity and the inverse-logit parts.  (This isn't a great model of neuronal response but you get the idea.) As the authors say in their replies to referees, and as the manuscript hints, an obvious application of this would be to multi-electrode-array neuronal recordings, where each data set from a different animal would be recording different neurons, with a different graph, but one might hope for a common neuronal response mechanism across experimental subjects.  (I can't come up with a second convincing application, especially not with fixed $N$ across "samples", which seems important to the encoding/decoding step.)  The innovation in this paper is to separate learning the common functional form of the vector autoregression from learning the graph, thereby allowing for pooling of information about the shared part of the model across data sets.  The reports agree that this is cleverly done, though it is not entirely clear what the limits on the expressive power of this method are, nor under what conditions it will converge on either the correct graph or the correct functional form.  Nonetheless, this is original and innovative work, and while text needs to clarify the intended application, the authors' replies to reviews make me fairly confident this can be done.
1	This paper studies the challenging problem of object-centric generation of visual scenes. While the paper has some novel ideas that make it interesting, its (quantitative and qualitative) comparison with existing methods is currently premature to allow drawing conclusions with sufficient evidence.  Instead of claiming that existing models cannot do well for the more realistic datasets mentioned by reviewer dAqW, it would be more convincing to conduct a comprehensive experimental study by comparing the proposed method with existing methods on a range of datasets, from simple ones to more realistic ones. The synthetic Fishbowl dataset introduced in this paper can be one of them.  Moreover, the clarity of the paper could be improved to make it appeal better to the readers.  All three reviewers engaged actively in discussions (both including and not including the authors). Although one reviewer recommends 6 (weak accept), the reviewer also shares some of the concerns of the other reviewers. As it stands, the paper is not ready for acceptance. If the comments and suggestions are incorporated to revise the paper, it will have potential to be a good paper for future submission.
1	The initial reviews for this paper were 6,6,6, the authors have provided a rebuttal and after the rebuttal the recommendation stayed the same. The reviewers have reached the consensus that the paper is borderline but they have all recommended keeping it above the acceptance threshold. Following the recommendation of the reviewers, the meta reviewer recommends acceptance.
1	Meta Review: The paper introduces an information-theoretic measure to detect collusion (=cooperation between subsets of players) in multi-agent games. The measure is based on comparing distributions of joint actions of pairs of players against the marginal distribution over actions (which is motivated from a mutual information viewpoint and implies using the KL-divergence to compare distributions).  Pro: * A challenging theoretical problem tying into a line of research with significant downstream real-world applications (e.g. collusion in financial markets;) * A theoretically well motivated approach (taking into account the clarifications during the rebuttal) * Reviewers agree that the paper is well written * Compelling experimental results  Cons: * By far the main criticism (raised by two reviewers independently) before the rebuttal was that the measure is flawed, and could potentially mistake coordination between two players as collusion when instead their actions are correlated simply because they are in an adversarial relationship. The authors' response has clarified this as a misunderstanding.  After the clarification by the authors, aVDZ and DVsz have raised their score to a weak accept. To me personally, the main weakness of the paper has been successfully addressed by the authors and I am in favor of acceptance. There are still some open minor issues, and the paper is probably only highly relevant for a sub-community of UAI - that's why I currently do not see the paper as a candidate for an oral.
0	There is a consensus that the contribution is not strong enough to effectively argue for an important novel lead which would justify publication at ICLR.   Authors have also not engaged with the reviewers.  For these rejections, this paper cannot be endorsed for publication at ICLR 2022.
1	This paper provides a test-bed called Myriad for trajectory optimization and system ID in jax, with the hope of engaging RL practitioners to benchmark against the test bed. The proposed test-bed provides examples ranging from different domains (such as medicine, and biology) deviating from traditional domains that are usually the focus on RL benchmarks. Further, the focus is on continuous time settings. Limitations are clearly specified. Authors have done a good job of describing the testbed and comparing existing methods.  One challenge that remains to be addressed is how useful this testbed really is for RL practitioners. This concern has been raised because it seems like the dynamics of the proposed problems fall on the simpler end of the spectrum. Nonetheless, I am currently of the belief that having a JAX based test-bed for such domains is still valuable and I am hoping will contribute more to reproducible RL results in these domains. Currently it appears that the designer makes a lot of choices around the design and set up of the RL framework. It may be a real concern if the general RL community will not adopt the testbed for this precise reason. More importantly, it did strike me as odd that the contribution and abstract claim testbed for imitation learning and RL but do not provide a simple example of how this could be done. This was the main concern of tBG5. I also agree with CQCv's assessment that requiring dynamics equations to be explicitly provided by the user will significantly cost adoption of the test-bed. Reviewer vypE scored the paper very well but failed to justify the score for me to rely significantly on it.   My expectation is that the authors will genuinely deliver on all the asks. Further, continue to improve the library to make it more amenable to testing of RL algorithms with a more friendly API. Overall I want to note that significant effort seems to be required to put together the test-bed but also believe all above concerns are valid and authors are highly recommended to incorporate as many changes as possible. The lack of explicit examples of simple baseline rl testing on the test-bed is concerning, irrespective of the API and potential simplicity of the domains (which I believe is not a huge concern if it warrants RL testing in novel domains). I strongly encourage the authors to incorporate feedback and I believe it will make for a much stronger testbed in practice. Hoping authors deliver on this, to the extent possible by camera-ready deadline and after, I am recommending an accept since the testbed has some utility even in its current form (though I am less optimistic about widespread adoption in its current form).
1	Shapley values are an important approach in extracting meaning from trained deep neural networks, and the paper proposes an innovative approach to address inefficiencies in post-processing to compute Shapley values, by instead incorporating their computation into training.  There was a robust discussion of this paper, and the authors' comments and changes substantially strengthened the paper and the reviewers' view of it, to the point that all reviewers now recommend acceptance.  Some lingering concerns remain that the authors should continue to work to address.  Is the method of computing Shapley values used as the baseline in the paper really state-of-the-art, or artificially weak?  The empirical results were methodologically sound but not as strong as one might expect or hope.  These concerns detract somewhat from enthusiasm, but nevertheless the paper yields an innovation to a widely-used approach to one of the most pressing current research problems.  The reviewers had a number of smaller suggestions that should also be incorporated including more significance testing and reporting of resulting p-values.
1	# Quality:  While the paper presents an interesting approach, Reviewer 2 raised relevant questions about the assumption of the theoretical justification that needs to be thoroughly addressed. Moreover, as noted by Reviewer4, the quality of the paper would also benefit from a more clear connection to existing model-based reinforcement learning literature, besides [Pan et al.]. For example, how much of the proposed approach and results can be applied in other algorithms?  # Clarity:  While the paper is generally well written and only minor suggestions from the reviewers should be implemented.  # Originality: The proposed approach is a small but novel improvement over existing algorithms (to the best of the reviewers and my knowledge).  # Significance of this work:  The paper deal with a relevant and timely topic. However, it is currently very difficult to gauge the significance of this work, and it unclear if the results can be extended beyond toy benchmarks and to other RL algorithms. Several reviewers suggested additional experiments to strengthen the paper.  # Overall: This paper deal with an interesting topic and presents new interesting results. However, the current manuscript is just below the acceptance threshold. Extending the experimental evaluation and improving the clarity of the paper would crucially increase the quality of the paper.
1	As summarized very well in the reviews, this is a well-written paper that makes a solid and elegant contribution to the recently active line of work on smoothed online learning.  The authors have successfully addressed the main concerns brought up in the discussion.  I genuinely agree the paper should be accepted.  As a side note to the authors: I honestly found your reaction to Reviewer gcMM’s comments rather aggressive and incongruous.  Disagreements naturally arise in a discussion and should not be automatically considered as an attempt to “greatly harm the review process and the community at large”.
1	The authors propose a new approach for training image classifiers with complete uncertainty quantification based on generative adversarial networks. The main idea is to use GANs to "shield" each class separately from the out-of-class (OoC) regime. This is done in combination with a one-vs-all classifier in the final DNN layer trained jointly with a class-conditional generator for out-of-class data in an adversarial framework. Finally, these classifiers are then used to model class conditional likelihoods. The empirical validation shows improved OoD detection and FP detection performance when compared to SOTA in this setting.  The reviewers appreciated the clarity of exposition and the positioning with respect to the related works. The unified approach applicable both to FP detection and OoD detection was deemed novel. On the negative side, the method seems to be extremely involved in terms of the required architectural pieces, distinction between low-dim and high-dim settings, primarily low-resolution data used for evaluation, and the number of hyperparameters. During the discussion the authors addressed the main questions raised by the reviewers. Nevertheless, given that all of the reviewers are leaning positive, I'll recommend the acceptance of this work. Please do a full pass in terms of formatting of the whole manuscript, including removing inline tables and figures, removing things like double parenthesis, bolding specific letters (e.g. L247), clarify the flow of information in figure 1 so that one can grasp the high-level overview of the algorithm, and incorporate the remaining points raised during the discussion.
1	This paper presents a powerful, general, scalable, and linearly complex graph Transformer. Positional encodings and structural encodings are redefined with local, global, and relative categories, and an attempt has been made to include  local and global focus attentions in a graph Transformer. All of the reviewers acknowledged the novelty of this work, particularly within the context of the domain, and therefore voted for its acceptance. Please take feedback from reviewers into account when preparing the camera-ready version.
1	The authors design an efficient implementation of nearest neighbor search on a TPU accelerator unit. The implementation is motivated by a refined roofline performance model that takes into account the memory and instruction bottlenecks that are found to be significant and that are not typically optimized for. Empirical results demonstrate that the proposed TPU solver outperforms state-of-the art GPU solvers. The package is available on Tensorflow.  The reviewers agree that the paper is well written, well structured and the proposed method can have significant practical impact.  Some concerns regarding the evaluation came up in the reviews but the additional experiments provided could address most of these concerns. What remains is a question on whether the performance gain over GPU comes from the algorithmic optimization itself or the higher efficiency of the TPU, and whether the algorithmic optimization would be similarly effective on other accelerators. The reviewers agree that performing such an analysis is outside the scope of this work. However, I want to encourage the authors to incorporate additional discussion to help the reader understand what parts of the work are specific to TPUs.  Overall this paper represents a well executed piece of work at the intersection between algorithm design and systems with an open source package that is available to the community. I recommend acceptance.
1	This paper proposes a novel method for training neural rough differential equations, a recent model for processing very long time-series data. The method involves a lower-dimensional embedding of the log-signature, which is obtained via pretrained autoencoder to reduce overhead. The results show significant and consistent improvements over previous methods on long time-series data.  Overall, the reviewers and I all agree that this paper offers a novel and impactful contribution leading to significant improvements over previous state-of-the-art methods for training neural rough differential equations. I recommend acceptance.
1	This paper considers the problem of membership inference. The authors propose to use the tools of random matrix theory in the asymptotic regime to analyze membership inference in the simple case of a linear model on Gaussian data. They start by deriving the explicit advantage of the attack in the asymptotic regime. Further derivations allow the author to analyze several interesting machine learning ingredients, starting from L2 regularization (ridge regression). There, the authors show that regularization has the counterintuitive effect of increasing the performance of membership inference attacks. The referees are leaning toward acceptance and I concur.
1	All reviewers agree this paper presents interesting analysis and results on GAN training dynamics. However they also note several limitations of the proposed setting, namely the assumptions of kernel width and the isolated points model, restrict directly applying results to practical settings. Authors in the response have done a good job of explaining how one can use the observations from the analysis to improve stability of real world training of GANs. Overall I think this work has some promising directions towards improving our understanding of training GANs, hence I suggest acceptance.
1	The paper got four accepts (after the reviewers changed their scores), all with high confidences. The theories are complete and the experiments are solid. The AC found no reason to overturn reviewers' recommendations. However, the AC deemed that all the pieces are just routine, thus only recommended poster.
1	Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. Inspired by childhood psychology, they propose a variant of hide-and-seek game called "Cache" built on top of AI2-THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images.  The paper is well written and motivated, and easy to follow. All reviewers agree that the paper will be a great contribution to the ICLR community. I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research. For these reasons I'm recommending we accept this work as an Oral presentation.
1	We want to acknowledge that there has been a tremendous amount of work done during the discussion period on this paper for clarifying multiple points, adding multiple new comparison methods and new analysis. This is a very different draft than what was submitted and the reviewers acknowledged that. The draft is much closer from acceptance at ICLR than it was at submission time. However, despite all those additions, we do not support a publication at ICLR.   The main issues with the current draft are its positioning and motivations.  Right now the draft is in-between a paper about Knowledge Base Construction (KBC) and a paper analyzing the knowledge contained within a large language model. This in-between came up in multiple places during the discussion and is what causes the biggest confusion around this work. And, since there is no clear choice, the draft has limitation on either side.  * If the main point is around KBC to build general-purpose KBs, then one would expect experiments on downstream tasks powered by a KB, language understanding tasks for instance. Indeed, KBs are just a means to an end and the latest advances in very large language models have shown that KBs were not essential to be state of the art in language understanding tasks (GLUE, QA datasets, etc). So we would like to see whether these enhanced KBs could be beneficial. Or the KBs are studied as a way to encode commonsense like in (Bosselut et al., 2019) or (Davison et al., 2019), but this is not the point of the current draft. * If the main impact of the draft is around what the language models learn, bridging the deep language model and knowledge graph communities through enhanced model transparency, as it has been said in the discussion, then the discussion with (Petroni et al. 19) should be more prominent and the introduction, motivation and experiments of the draft should reflect that.  That's why, even if this work is of solid quality, the current draft can not be accepted.
1	As pointed out by some reviewers, the proposed method basically puts progressive training in the federated context. The theoretical analysis only concerns the centralized or non-federated setting and thus give no insight or guidelines for progressive training in federated learning. The main advantage of saving communication mainly comes from the simple observation that less parameters are computed and communicated during each round before the full end-to-end stage. However, this may cause extra overhead in hyper-parameter tuning including number of stage, learning rate schedules and stage-wise warmup. Despite its potential effectiveness in practice, the current version of the paper falls short of the acceptance bar due to the weakness in novelty and relevant theory for federated learning.
1	The paper considers the estimation of process and observation noise variances in a subclass of linear dynamical systems and provides algorithms with finite sample guarantees. The math is novel and the results are interesting. I am happy to recommend acceptance.
1	Reviewers all find this paper presenting both good theoretical findings and empirical results for an important problem (feature attribution robustness). The approaches authors used in connecting the relationship between Kendall’s rank correlation and cosine similarity, as well as the geometric perspectives, are well received. The presentations are well written, with minor places for quick improvements.   Reviewers have raised various weakness points but we agreed most of them are minor, do not affect the contribution of this paper, and/or can be fixed without much effort.  Overall we recommend acceptance and would like to encourage the authors to further improve this paper presentation following reviewers’ suggestions in the next version.
1	The paper presents a method to integrate language models and hammers (Automated Theorem Provers) for Interactive Theorem Proving. The authors train the language model to recognize an opportunity to invoke a hammer by transforming the training data: they check whether the hammer can be applied at each proof state.   The approach is novel, while being simple. The reviewers are generally happy with the writing of the work. There were some concerns (such as obvious baselines, pointed out byuriH) that seem to be mitigated.  vtgR pointed out the slowness of preprocessing, which I think is an issue that should be explicitly acknowledged in the revision and addressed in future work.   Given the overall positive feedback of the reviewers, I am recommending an "accept" for this work.
0	This paper presents an empirical study of generalization in visual reinforcement learning. This study is carried out in the domain of video games and it addresses the benefits of techniques such as regularization, augmentation and training with auxiliary tasks. The reviewers for this submission were positive about the goal and setups in this paper. They agreed that understanding why present day methods that attempt to improve generalization continue to fall short, is an important problem. However, most reviewers were underwhelmed by the findings presented in the submission. As examples: Reviewer 185P mentions that "the paper does not seem to provide a clear and definite answer to the question" and " I am not convinced the experiments described in this paper support the claims made by the authors." and Reviewer SFef mentions that "Most of the conclusions are already known". Some reviewers also found a lack of clarity and several typos in the initial submission. The authors have provided detailed responses to the reviewers. In particular they have fixed most writing issues. They also detailed why certain algorithms and techniques were benchmarked in this submission and others were left out. I think this is reasonable. One cannot expect a paper to benchmark every algorithm out there, and choosing promising and representative ones is sufficient. My takeaway from detailed discussions about this paper are that: The paper is much improved from a writing point of view and the rebuttal addresses some concerns well. However, I do agree with the reviewers that the findings presented in the paper are for the most part expected. This reduces the value of the paper to readers. When this is the case, it may be beneficial to dig deeper into these findings and present a narrow but deep analysis. Please see Reviewer 185P's suggestions in this regard. Given the above, I am recommending rejection for this conference, but I encourage the authors to take into the reviewers suggestions and resubmit.
0	After reading the paper, reviews and authors’ feedback. The meta-reviewer agrees with the reviewers that the paper touches an interesting topic (reversible computing) but could be improved in the area of presentation and evaluation. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.
1	This paper investigates how well properties invariant to changes such as lightening and background learned in the major class can be transferred to the minor class. In this paper, the authors reveal that invariances do not transfer well to small classes, and suggest that resolving this phenomenon can help increase the performance on imbalanced datasets. From this point of view, the authors propose a generative model-based augmentation technique.  Three reviewers suggested acceptance, and one reviewer judged borderline reject. It seems true that the method is not novel enough, but it is solid and well motivated. In particular, the finding of the paper is interesting and the design of the experiment is well done, so I think that it will have a great influence on research in this field in the future. As the negative reviewer mentioned, the lack of large-scale experiments is a major weakness of this paper. I strongly encourage the final version to supplement the promises made to the reviewer, including adding iNaturalist experiments.
1	The paper is concerned with multi-View Stereo reconstruction from images with several transformers for specific subtasks. SOTA performance is attained. Reviewers acknowledge a technically sound pipeline. The writing is also clear and limitations are addressed. All reviewers recommend the paper for acceptance and so do I. Nevertheless, the authors must include the feedback provided by reviewers e.g. on possibly limited significance of 0.05 mm better results on DTU is significant or not given accuracy limitations of the ground truth. Also connections to pointed out related work must be discussed.
1	This paper introduced a tensor decomposition, and associated theory, which allows for the control of singular values in convolutional layers.  Based upon the reviews, rebuttal, and reviewer discussion, I recommend paper acceptance. All reviewers recommend acceptance. The rebuttal was effective, with one reviewer who initially recommended rejection raising their score.  The authors should be sure to follow through, and update the paper to include changes discussed during the review period. Especially, it seems as if the framing of the paper shifted during the review period from centering on practically computing singular values to practically controlling singular values. From my understanding of the work, I agree this second framing makes more sense.
1	The paper considers the problem of learning a single Monge map between different pairs of probability measures such that the particular transport depends on a given context. The reviewers have found the paper well written and motivated. In addition the approach is novel and interesting. I therefore recommend acceptance.
1	The authors consider the traditional problem of approximating graph convolutional networks using Chebyshev polynomial, which is known as Chebnet. Then, the authors propose a new GNN model called ChebNetII enhancing for reducing the Runge phenomenon; this is an important contribution to GNN. Overall, the reviewers are positive about the paper. Thus, I also vote for acceptance.
1	5 out of 6 reviewers are positive about accepting this work. The meta reviewer agrees with the reviewers that the K-Radar dataset is a clear contribution to the community and thus recommends acceptance. The meta reviewer believes that the authors have answered the negative reviewer's question in a fair enough way. The authors are suggested the authors polish their paper writing and exposition following reviewers' comments.
1	The authors propose a new dataset, namely ImageNet-NOC, for evaluating robustness of image classifiers to corruptions. The dataset may be viewed as an alternative to ImageNet-C which uses a different set of corruptions. To derive this set of corruptions, the authors first develop a notion of similarity between two corruptions, and then propose an iterative algorithm to build a set of corruptions which, intuitively, is sufficient to cover the larger set of corruptions (i.e., enjoys *high coverage*), and assigns a similar importance to each such corruption (i.e., is *balanced*). Then, the authors argue that ImageNet-NOC is superior to ImageNet-C as it achieves a higher degree of balance and coverage.  The reviewers found this to be a borderline paper. The reviewers appreciated the introduced metric and agree that there is no point in evaluating on corruptions which are perfectly correlated. In addition, the systematic approach for generating a set of relevant corruptions is seen as a step in the right direction. The reviewers appreciated the author response and were engaged in the discussion. As it currently stands the reviewers are not convinced that the paper is ready for acceptance. To improve the manuscript the authors could extend Tables 3 and 4 with a wider range of models and investigate qualitative differences between models robust on one dataset, but not on the other. Furthermore, there should be a more detailed discussion of stability and computational properties of algorithm 1. In addition, the authors should provide strong arguments as to why is it not sufficient to add additional corruptions to ImageNet-C and compute a weighted score instead. The latter suggestion could lead to an iterative improvement of the current set of benchmarks and place more emphasis on the methodology. I suggest the authors to incorporate the reviewer's feedback and place more emphasis on the methodology around algorithm 1, rather then on introducing another dataset which is likely to be superseded as soon as we add a couple more corruptions in the mix.
1	This paper studies the generalization error of three-layer relu neural networks, when only the middle layer weights are trained.  The focus is the regression setting, and the goal is to capture the hidden layer interactions. The paper aims to determine how the (hidden) layer interactions influence the double-descent curve. The generalization error bound established depends on the layer width in an interesting manner, which may shed light on understanding deeper networks outside of the kernel regime.  All reviewers rated this work above the bar. As such, I recommend accepting this paper.  There were a few parts that the reviewers found unclear/needs improvement. In particular, some of the clarifications made by the authors in their rebuttal can help make this paper more clear for its future readers.
1	This paper proposes a novel method of training spiking neural networks (SNNs) by matching the intermediate feature representations of SNNs with pre-trained ANNs. The method is on-chip and local, allowing SNNs to be learned directly on neuromorphic hardware.   All reviewers agreed that the problem that the paper target to solve is important, and the proposed method is novel. During the discussion period, the authors successfully addressed the concerns of the reviewers. Therefore, I recommend acceptance.
1	This paper presents a benchmark (in the sense of a large scale ablation and evaluation of existing works/datasets), considering the effects of dataset, "backbone" and training strategies for the problem of human pose and shape estimation. There is a consensus among reviewers that this paper's main limitation is the (over)emphasis on the human mesh recovery (HMR) framework. The forum discussions provide convincing arguments that despite this, the paper still offers with a major contribution to this albeit limited view of the problem. Another common issue raised by reviewers is whether this paper is suitable for the NeurIPS D&B track. I agree with the authors (and Reviewer BCuk) that this paper's premise is indeed well suited for this track, as described in the call for papers and FAQ. Therefore, I dismiss this as a limitation. Finally, there is a strong outlier review, whose exceptionally low score is obliterating this paper's average score. I have read the discussion in detail and while I find that this reviewer is making interesting points, I do not agree that any of them are serious enough to withhold publication of this work. I am content to see the reviewer's points as providing pointers for other future works that may be more influential than the presented one. The possible existence of superior future studies is not a reason to reject this paper. In conclusion, backed by the strong majority of reviewers, I recommend accepting this paper to the NeurIPS 2022 Datasets and Benchmarks program.
